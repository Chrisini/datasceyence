{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75832305-6091-4f3c-980a-53004f01a000",
   "metadata": {},
   "source": [
    "# ð•Šð•–ð•žð•š-ð•Šð•¦ð•¡ð•–ð•£ð•§ð•šð•¤ð•–ð•• ð•ð•–ð•’ð•£ð•Ÿð•šð•Ÿð•˜ ð•¦ð•¤ð•šð•Ÿð•˜ ð•„ð•–ð•’ð•Ÿ ð•‹ð•–ð•’ð•”ð•™ð•–ð•£\n",
    "\n",
    "Implementation of pixel-wise Mean Teacher (MT)\n",
    "    \n",
    "This method is proposed in the paper: \n",
    "    'Mean Teachers are Better Role Models:\n",
    "        Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results'\n",
    "This implementation only supports Gaussian noise as input perturbation, and the two-heads\n",
    "outputs trick is not available.\n",
    "\n",
    "Source:\n",
    "https://github.com/ZHKKKe/PixelSSL/blob/master/pixelssl/ssl_algorithm/ssl_mt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6f142-82ee-4f0a-87f8-e2e9eeeb89b6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f3932a-744a-49ca-a076-f12749cf3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "from helper.dataset.mean_teacher import *\n",
    "from helper.model.mean_teacher import * \n",
    "from helper.sampler.mixed_batch import *\n",
    "from helper.model.block.noise_block import GaussianNoiseBlock\n",
    "\n",
    "#from pixelssl.utils import REGRESSION, CLASSIFICATION\n",
    "#from pixelssl.utils import logger, cmd, tool\n",
    "#from pixelssl.nn import func\n",
    "#from pixelssl.nn.module import patch_replication_callback, GaussianNoiseLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a707779-ab4d-46a4-ab5b-e123f91b827a",
   "metadata": {},
   "source": [
    "# Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d1aa89-5640-4263-8051-96580d7d0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # =============================================================================\n",
    "        # \n",
    "        # =============================================================================\n",
    "        \n",
    "        self.experiment_name = \"tmp\"\n",
    "        self.reduced_data = False\n",
    "        \n",
    "        # smp unet ++ parameters\n",
    "        self.encoder_name = \"efficientnet-b7\"\n",
    "        self.encoder_weights = \"imagenet\"\n",
    "        self.in_channels =  1\n",
    "        self.classes =  2\n",
    "        \n",
    "        self.epochs = 100\n",
    "        \n",
    "        self.gaussian_noise = 0.1 # None\n",
    "        \n",
    "        self.ema_decay = 0.999 # default value\n",
    "        \n",
    "        self.image_size = 500\n",
    "        \n",
    "        self.num_workers = 0\n",
    "        self.iterations = 50\n",
    "        self.n_samples_per_class_per_batch = 10\n",
    "        \n",
    "        self.lbs = 3 #  self.args.labeled_batch_size # .... remove this eventually and replace\n",
    "\n",
    "        # optimisation\n",
    "        self.optimiser = \"sgd\"\n",
    "        self.learning_rate = 0.01\n",
    "        self.weight_decay = 1e-4\n",
    "        self.momentum = 0.9\n",
    "        \n",
    "        self.is_epoch_lrer = True # epoch or batch based learning rate updater\n",
    "        \n",
    "        self.dropout = None\n",
    "        \n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "        \n",
    "        # =============================================================================\n",
    "        # Paths\n",
    "        # =============================================================================\n",
    "        \n",
    "        # input\n",
    "        \n",
    "        self.load_checkpoint_file = None\n",
    "        \n",
    "        # all csv files used for run_mean_teacher.ipybn\n",
    "        self.csv_data_paths = [\n",
    "            {\"path\" : r\"data/data_ichallenge_amd.csv\", \"weight\" : 0.5 }, \n",
    "            {\"path\" : r\"data/data_ichallenge_non_amd.csv\", \"weight\" : 0.5}\n",
    "        ]\n",
    "        \n",
    "        # output\n",
    "        self.logger_path = f\"logger/{self.experiment_name}\"\n",
    "        if not os.path.exists(self.logger_path):\n",
    "            os.makedirs(self.logger_path)\n",
    "            \n",
    "            \n",
    "        self.save_checkpoint_path = f\"logger/{self.experiment_name}/ckpt\"\n",
    "        if not os.path.exists(self.save_checkpoint_path):\n",
    "            os.makedirs(self.save_checkpoint_path)\n",
    "            \n",
    "        \n",
    "    def log(self):\n",
    "        # =============================================================================\n",
    "        # save all class variables to file \"configs.txt\"\n",
    "        # =============================================================================\n",
    "        c = pd.DataFrame.from_dict({'key': self.__dict__.keys(), 'value': self.__dict__.values()})\n",
    "        c.to_csv(os.path.join(self.logger_path, \"configs.txt\"), sep=':', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c64cbd-0bd5-4220-9f34-b7ff564d21ae",
   "metadata": {},
   "source": [
    "# Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605a7f6-d147-4551-b403-928659086b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoutineMT:\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(RoutineMT, self).__init__()\n",
    "        \n",
    "        self.configs = configs\n",
    "        \n",
    "        self.experiment_name = configs.experiment_name\n",
    "        self.ema_decay = configs.ema_decay\n",
    "        \n",
    "        self.load_ckpt = torch.load(configs.load_checkpoint_file) if configs.load_checkpoint_file is not None else None\n",
    "        \n",
    "        self.step_counter = 0\n",
    "        self.total_steps = len(self.dataloader[\"train\"]) * self.args.cons_rampup_epochs # ????\n",
    "\n",
    "    \n",
    "    def run_prepatation(self):\n",
    "        \n",
    "        \n",
    "        # =============================================================================\n",
    "        # MODEL\n",
    "        # =============================================================================\n",
    "        s_model = smp.UnetPlusPlus(\n",
    "                        encoder_name=self.configs.encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights=self.configs.encoder_weights,  # use `imagenet` pre-trained weights for encoder initialization\n",
    "                        in_channels=self.configs.in_channels,          # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                        classes=self.configs.classes,                  # model output channels (number of classes in your dataset)\n",
    "                    )\n",
    "        \n",
    "        t_model = smp.UnetPlusPlus(\n",
    "                        encoder_name=self.configs.encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights=self.configs.encoder_weights,  # use `imagenet` pre-trained weights for encoder initialization\n",
    "                        in_channels=self.configs.in_channels,          # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                        classes=self.configs.classes,                  # model output channels (number of classes in your dataset)\n",
    "                    )\n",
    "        # detach the teacher model\n",
    "        for param in t_model.parameters():\n",
    "            param.detach_()\n",
    "            \n",
    "        self.models = {'s': s_model, \n",
    "                       't': t_model}\n",
    "        \n",
    "         # add gaussian noise\n",
    "        self.gaussian_noiser = GaussianNoiseLayer(self.configs.gaussian_noise).cuda()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Computing Unit\n",
    "        # =============================================================================\n",
    "        self.computing_unit = {\n",
    "            \"s\" : BCE_BinSeg_CU()\n",
    "            \"t\" : BCE_BinSeg_CU()\n",
    "        }\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OPTIMISER\n",
    "        # =============================================================================\n",
    "        self.optimisers = {'s': optimizer_funcs[0](self.models[\"s\"].module.param_groups)\n",
    "                          }\n",
    "\n",
    "        # =============================================================================\n",
    "        # LEARNING RATE SCHEDULER\n",
    "        # =============================================================================\n",
    "        self.lrers = {'s': lrer_funcs[0](self.optimizers['s_optimizer':])\n",
    "                     }\n",
    "        \n",
    "        # =============================================================================\n",
    "        # LOSS FUNTION\n",
    "        # =============================================================================\n",
    "        # TODO: support more types of the consistency criterion\n",
    "        # something with head and each head has a loss function attached??\n",
    "        self.criterions = {'s': criterion_funcs[0](self.args),\n",
    "                           'consist': torch.nn.MSELoss()\n",
    "                          }\n",
    "        \n",
    "        # =============================================================================\n",
    "        # DATASET\n",
    "        # train, val, test\n",
    "        # =============================================================================\n",
    "        self.dataset = {\"train\" : None,\n",
    "                        \"val\" : None,\n",
    "                        \"test\" : None\n",
    "                        }\n",
    "        \n",
    "\n",
    "        \n",
    "        self.dataloader = {\"train\" : None,\n",
    "                           \"val\" : None,\n",
    "                           \"test\" : None\n",
    "                          }\n",
    "\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Resume training\n",
    "        # =============================================================================\n",
    "        if self.load_ckpt:\n",
    "            self.models[\"s\"].load_state_dict(self.load_ckpt['s_model'])\n",
    "            self.models[\"t\"].load_state_dict(self.load_ckpt['t_model'])\n",
    "            self.optimisers[\"s\"].load_state_dict(self.load_ckpt['s_optimizer'])\n",
    "            self.lrers[\"s\"].load_state_dict(self.load_ckpt['s_lrer'])\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, epoch):\n",
    "        \n",
    "        self.models[\"s\"].train()\n",
    "        self.models[\"t\"].train()\n",
    "        \n",
    "        for idx, item in enumerate(self.dataloader[\"train\"]):\n",
    "            # =============================================================================\n",
    "            # Process Batch\n",
    "            # =============================================================================\n",
    "                        \n",
    "            # reset student optimiser\n",
    "            self.optimisers[\"s\"].zero_grad()\n",
    "            \n",
    "            # run student batch\n",
    "            self.computing_unit[\"s\"].run_batch()\n",
    "\n",
    "            # =============================================================================\n",
    "            # Teacher Model\n",
    "            # =============================================================================\n",
    "            \n",
    "            # calculate the ramp-up coefficient of the consistency constraint\n",
    "            # use mean squared error as the consistency cost and ramp up its weight from 0 to its final value during the first 80 epochs. \n",
    "            self.step_counter += 1\n",
    "            total_steps = len(self.dataloader[\"train\"]) * self.args.cons_rampup_epochs # ????\n",
    "            cons_rampup_scale = func.sigmoid_rampup(self.step_counter, total_steps)\n",
    "            \n",
    "            # forward the teacher model\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                self.computing_unit[\"t\"].run_batch()\n",
    "                \n",
    "                t_resulter, t_debugger = self.t_model.forward(t_inp)\n",
    "                if not 'pred' in t_resulter.keys():\n",
    "                    self._pred_err()\n",
    "                t_pred = tool.dict_value(t_resulter, 'pred')\n",
    "                t_activated_pred = tool.dict_value(t_resulter, 'activated_pred')\n",
    "            \n",
    "                # calculate 't_task_loss' for recording\n",
    "                l_t_pred = func.split_tensor_tuple(t_pred, 0, lbs)\n",
    "                l_t_inp = func.split_tensor_tuple(t_inp, 0, lbs)\n",
    "                t_task_loss = self.s_criterion.forward(l_t_pred, l_gt, l_t_inp)\n",
    "                t_task_loss = torch.mean(t_task_loss)\n",
    "                self.meters.update('t_task_loss', t_task_loss.data)\n",
    "            \n",
    "            # =============================================================================\n",
    "            # Consistency Loss\n",
    "            # =============================================================================\n",
    "            # calculate the consistency constraint from the teacher model to the student model\n",
    "            t_pseudo_gt = Variable(t_pred[0].detach().data, requires_grad=False)\n",
    "\n",
    "            if self.args.cons_for_labeled:\n",
    "                cons_loss = self.cons_criterion(s_pred[0], t_pseudo_gt)\n",
    "            elif self.args.unlabeled_batch_size > 0:\n",
    "                cons_loss = self.cons_criterion(s_pred[0][lbs:, ...], t_pseudo_gt[lbs:, ...])\n",
    "            else:\n",
    "                cons_loss = self.zero_tensor\n",
    "            cons_loss = cons_rampup_scale * self.args.cons_scale * torch.mean(cons_loss)\n",
    "\n",
    "            # =============================================================================\n",
    "            # Backprop for student model\n",
    "            # =============================================================================\n",
    "            loss = s_task_loss + cons_loss\n",
    "            loss.backward()\n",
    "            self.optimisers[\"s\"].step()\n",
    "\n",
    "            # =============================================================================\n",
    "            # EMA for teacher model\n",
    "            # =============================================================================\n",
    "            # self._update_ema_variables(self.s_model, self.t_model, self.args.ema_decay, cur_step)\n",
    "            self.ema_decay = min(1 - 1 / (cur_step + 1), self.ema_decay)\n",
    "            for t_param, s_param in zip(self.t_model.parameters(), self.s_model.parameters()):\n",
    "                t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)\n",
    "            \n",
    "            # =============================================================================\n",
    "            # LR Scheduler (Batch)\n",
    "            # =============================================================================\n",
    "            if not self.configs.is_epoch_lrer:\n",
    "                self.s_lrer.step()\n",
    "        \n",
    "        \n",
    "        # =============================================================================\n",
    "        # Process Epoch\n",
    "        # =============================================================================\n",
    "        \n",
    "        self.computing_unit[\"s\"].run_epoch()\n",
    "        self.computing_unit[\"t\"].run_epoch()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # LR Scheduler (Epoch)\n",
    "        # =============================================================================\n",
    "        if self.configs.is_epoch_lrer:\n",
    "            self.s_lrer.step()\n",
    "\n",
    "    def validate(self, data_loader, epoch):\n",
    "        self.s_model.eval()\n",
    "        self.t_model.eval()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # for each batch\n",
    "        # =============================================================================\n",
    "\n",
    "        for idx, item in enumerate(self.dataloader[\"val\"]):\n",
    "            \n",
    "            timer = time.time()\n",
    "            \n",
    "            # =============================================================================\n",
    "            # Student\n",
    "            # =============================================================================\n",
    "\n",
    "            s_resulter, s_debugger = self.s_model.forward(s_inp)\n",
    "            if not 'pred' in s_resulter.keys() or not 'activated_pred' in s_resulter.keys():\n",
    "                self._pred_err()\n",
    "            s_pred = tool.dict_value(s_resulter, 'pred')\n",
    "            s_activated_pred = tool.dict_value(s_resulter, 'activated_pred')\n",
    "\n",
    "            s_task_loss = self.s_criterion.forward(s_pred, gt, s_inp)\n",
    "            s_task_loss = torch.mean(s_task_loss)\n",
    "            self.meters.update('s_task_loss', s_task_loss.data)\n",
    "\n",
    "            # =============================================================================\n",
    "            # Teacher\n",
    "            # =============================================================================\n",
    "            \n",
    "            t_resulter, t_debugger = self.t_model.forward(t_inp)\n",
    "            if not 'pred' in t_resulter.keys() or not 'activated_pred' in t_resulter.keys():\n",
    "                self._pred_err()\n",
    "            t_pred = tool.dict_value(t_resulter, 'pred')\n",
    "            t_activated_pred = tool.dict_value(t_resulter, 'activated_pred')\n",
    "\n",
    "            t_task_loss = self.s_criterion.forward(t_pred, gt, t_inp)\n",
    "            t_task_loss = torch.mean(t_task_loss)\n",
    "            self.meters.update('t_task_loss', t_task_loss.data)\n",
    "            \n",
    "            # =============================================================================\n",
    "            # Pseudo ???\n",
    "            # =============================================================================\n",
    "\n",
    "            t_pseudo_gt = Variable(t_pred[0].detach().data, requires_grad=False)\n",
    "            cons_loss = self.cons_criterion(s_pred[0], t_pseudo_gt)\n",
    "            cons_loss = self.args.cons_scale * torch.mean(cons_loss)\n",
    "            self.meters.update('cons_loss', cons_loss.data)\n",
    "\n",
    "            #self.task_func.metrics(s_activated_pred, gt, s_inp, self.meters, id_str='student')\n",
    "            #self.task_func.metrics(t_activated_pred, gt, t_inp, self.meters, id_str='teacher')\n",
    "            \n",
    "            # =============================================================================\n",
    "            # Logger\n",
    "            # =============================================================================\n",
    "            \n",
    "            self.meters.update('batch_time', time.time() - timer)\n",
    "            if idx % self.args.log_freq == 0:\n",
    "                logger.log_info('step: [{0}][{1}/{2}]\\tbatch-time: {meters[batch_time]:.3f}\\n'\n",
    "                                '  student-{3}\\t=>\\t'\n",
    "                                's-task-loss: {meters[s_task_loss]:.6f}\\t'\n",
    "                                's-cons-loss: {meters[cons_loss]:.6f}\\n'\n",
    "                                '  teacher-{3}\\t=>\\t'\n",
    "                                't-task-loss: {meters[t_task_loss]:.6f}\\n'\n",
    "                                .format(epoch + 1, idx, len(data_loader), self.args.task, meters=self.meters))\n",
    "\n",
    "            if self.args.visualize and idx % self.args.visual_freq == 0:\n",
    "                self._visualize(epoch, idx, False, \n",
    "                                func.split_tensor_tuple(s_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(s_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(gt, 0, 1, reduce_dim=True))\n",
    "    \n",
    "        # =============================================================================\n",
    "        # Metrics\n",
    "        # =============================================================================\n",
    "        # metrics\n",
    "        metrics_info = {'student': '', 'teacher': ''}\n",
    "        for key in sorted(list(self.meters.keys())):\n",
    "            #if self.task_func.METRIC_STR in key:\n",
    "            if True:\n",
    "                for id_str in metrics_info.keys():\n",
    "                    if key.startswith(id_str):\n",
    "                        metrics_info[id_str] += '{0}: {1:.6}\\t'.format(key, self.meters[key])\n",
    "\n",
    "        logger.log_info('Validation metrics:\\n  student-metrics\\t=>\\t{0}\\n  teacher-metrics\\t=>\\t{1}\\n'\n",
    "            .format(metrics_info['student'].replace('_', '-'), metrics_info['teacher'].replace('_', '-')))\n",
    "\n",
    "    def run_cleanup(self, epoch):\n",
    "        # =============================================================================\n",
    "        # Logger\n",
    "        # =============================================================================\n",
    "        \n",
    "        # if save_model is True:\n",
    "        \n",
    "        if epoch > 5 and self.computing_unit[\"s\"].epoch_collector[\"fscore\"] > self.computing_unit[\"s\"].best[\"fscore\"]:\n",
    "            state = {\n",
    "                'name': self.experiment_name,\n",
    "                'epoch': epoch, \n",
    "                's_model': self.models[\"s\"].state_dict(),\n",
    "                't_model': self.models[\"t\"].state_dict(),\n",
    "                's_optim': self.optimisers[\"s\"].state_dict(),\n",
    "                's_lrer': self.lrers[\"s\"].state_dict()\n",
    "            }\n",
    "\n",
    "            checkpoint = os.path.join(self.configs.save_checkpoint_path, 'checkpoint_{0}.ckpt'.format(epoch))\n",
    "            torch.save(state, checkpoint)\n",
    "            \n",
    "            self.computing_unit[\"s\"].best[\"fscore\"] = self.computing_unit[\"s\"].epoch_collector[\"fscore\"]\n",
    "            \n",
    "        self.computing_unit[\"s\"].reset_epoch()\n",
    "        self.computing_unit[\"t\"].reset_epoch()\n",
    "        \n",
    "        \n",
    "    def logger(self, epoch):\n",
    "        # =============================================================================\n",
    "        # Logger\n",
    "        # =============================================================================\n",
    "        \n",
    "        self.meters.update('batch_time', time.time() - timer)\n",
    "        if idx % self.args.log_freq == 0:\n",
    "            logger.log_info('step: [{0}][{1}/{2}]\\tbatch-time: {meters[batch_time]:.3f}\\n'\n",
    "                            '  student-{3}\\t=>\\t'\n",
    "                            's-task-loss: {meters[s_task_loss]:.6f}\\t'\n",
    "                            's-cons-loss: {meters[cons_loss]:.6f}\\n'\n",
    "                            '  teacher-{3}\\t=>\\t'\n",
    "                            't-task-loss: {meters[t_task_loss]:.6f}\\n'\n",
    "                            .format(epoch + 1, idx, len(data_loader), self.args.task, meters=self.meters))\n",
    "\n",
    "        # visualization\n",
    "        if self.args.visualize and idx % self.args.visual_freq == 0:\n",
    "            self._visualize(epoch, idx, True, \n",
    "                            func.split_tensor_tuple(s_inp, 0, 1, reduce_dim=True),\n",
    "                            func.split_tensor_tuple(s_activated_pred, 0, 1, reduce_dim=True),\n",
    "                            func.split_tensor_tuple(t_inp, 0, 1, reduce_dim=True),\n",
    "                            func.split_tensor_tuple(t_activated_pred, 0, 1, reduce_dim=True),\n",
    "                            func.split_tensor_tuple(gt, 0, 1, reduce_dim=True))\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Tool Functions for SSL_MT\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _visualize(self, epoch, idx, is_train, \n",
    "                   s_inp, s_pred, t_inp, t_pred, gt):\n",
    "\n",
    "        visualize_path = self.args.visual_train_path if is_train else self.args.visual_val_path\n",
    "        out_path = os.path.join(visualize_path, '{0}_{1}'.format(epoch, idx))\n",
    "\n",
    "        #self.task_func.visualize(out_path, id_str='student', inp=s_inp, pred=s_pred, gt=gt)\n",
    "        #self.task_func.visualize(out_path, id_str='teacher', inp=t_inp, pred=t_pred, gt=gt)\n",
    "\n",
    "    def _batch_prehandle(self, inp, gt, is_train):\n",
    "        # add extra data augmentation process here if necessary\n",
    "\n",
    "        # 'self.gaussian_noiser' will add the noise to the first input element\n",
    "        s_inp_var, t_inp_var = [], []\n",
    "        for idx, i in enumerate(inp):\n",
    "            if is_train and idx == 0:\n",
    "                s_inp_var.append(self.gaussian_noiser.forward(Variable(i).cuda())) \n",
    "                t_inp_var.append(self.gaussian_noiser.forward(Variable(i).cuda())) \n",
    "            else:\n",
    "                s_inp_var.append(Variable(i).cuda()) \n",
    "                t_inp_var.append(Variable(i).cuda())\n",
    "        s_inp = tuple(s_inp_var)\n",
    "        t_inp = tuple(t_inp_var)\n",
    "        \n",
    "        gt_var = []\n",
    "        for g in gt:\n",
    "            gt_var.append(Variable(g).cuda())\n",
    "        gt = tuple(gt_var)\n",
    "\n",
    "        return s_inp, t_inp, gt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0440659-d578-4ae9-a2d5-88331279c1cc",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de02c93-618e-464e-afc8-8da004a671e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "configs = Configs()\n",
    "configs.log()\n",
    "\n",
    "# Run\n",
    "run = RoutineMT(configs)\n",
    "run.build()\n",
    "\n",
    "for epoch in range(configs.epochs):\n",
    "\n",
    "\n",
    "    run.train(epoch=epoch)\n",
    "    run.val(epoch=epoch)\n",
    "    run.cleanup(epoch=epoch)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    routine.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c939f3-d59b-44d4-9c8d-a32217868ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee2543-5b24-4a14-8264-a42e6ec1b1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d09ad-3fde-4056-98e2-3e7650d69889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44351fae-b7db-4d98-81bc-3885fda3f805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
