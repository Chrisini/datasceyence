{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75832305-6091-4f3c-980a-53004f01a000",
   "metadata": {},
   "source": [
    "# ð•Šð•–ð•žð•š-ð•Šð•¦ð•¡ð•–ð•£ð•§ð•šð•¤ð•–ð•• ð•ð•–ð•’ð•£ð•Ÿð•šð•Ÿð•˜ ð•¦ð•¤ð•šð•Ÿð•˜ ð•„ð•–ð•’ð•Ÿ ð•‹ð•–ð•’ð•”ð•™ð•–ð•£\n",
    "\n",
    "Implementation of pixel-wise Mean Teacher (MT)\n",
    "    \n",
    "This method is proposed in the paper: \n",
    "    'Mean Teachers are Better Role Models:\n",
    "        Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results'\n",
    "This implementation only supports Gaussian noise as input perturbation, and the two-heads\n",
    "outputs trick is not available.\n",
    "\n",
    "Source:\n",
    "https://github.com/ZHKKKe/PixelSSL/blob/master/pixelssl/ssl_algorithm/ssl_mt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f3932a-744a-49ca-a076-f12749cf3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import segmentation_models_pytorch as smp \n",
    "\n",
    "#from pixelssl.utils import REGRESSION, CLASSIFICATION\n",
    "#from pixelssl.utils import logger, cmd, tool\n",
    "#from pixelssl.nn import func\n",
    "#from pixelssl.nn.module import patch_replication_callback, GaussianNoiseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca3442d6-40ca-404b-b8bb-ba8ed3040f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d1aa89-5640-4263-8051-96580d7d0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs():\n",
    "    \n",
    "    def __init__(self, experiment_name):\n",
    "        \n",
    "        self.experiment_name = experiment_name\n",
    "        \n",
    "        self.encoder_name = \"resnext50_32x4d\"\n",
    "        self.encoder_weights = \"imagenet\"\n",
    "        self.in_channels =  1\n",
    "        self.classes =  2\n",
    "        \n",
    "        self.optimiser = \"adam\"\n",
    "        \n",
    "        self.a = \"test\"\n",
    "        \n",
    "    def log(self):\n",
    "        # save all class variables to file \"configs.txt\"\n",
    "        c = pd.DataFrame.from_dict({'key': self.__dict__.keys(), 'value': self.__dict__.values()})\n",
    "        if not os.path.exists(f\"results/{experiment_name}/logger/\"):\n",
    "            os.makedirs(f\"results/{experiment_name}/logger/\")\n",
    "        c.to_csv(f\"results/{experiment_name}/logger/configs.txt\", sep=':', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7605a7f6-d147-4551-b403-928659086b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoutineMT:\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(RoutineMT, self).__init__()\n",
    "        \n",
    "        self.configs = configs\n",
    "\n",
    "        # create models\n",
    "        #self.s_model = func.create_model(model_funcs[0], 's_model', args=self.args)\n",
    "        #self.t_model = func.create_model(model_funcs[0], 't_model', args=self.args)\n",
    "        \n",
    "        self.s_model = smp.Unet(\n",
    "                encoder_name=self.configs.encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                encoder_weights=self.configs.encoder_weights,   # use `imagenet` pre-trained weights for encoder initialization\n",
    "                in_channels=self.configs.in_channels,      # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                classes=self.configs.classes,          # model output channels (number of classes in your dataset)\n",
    "            )\n",
    "\n",
    "        self.t_model = smp.Unet(\n",
    "                encoder_name=self.configs.encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                encoder_weights=self.configs.encoder_weights,   # use `imagenet` pre-trained weights for encoder initialization\n",
    "                in_channels=self.configs.in_channels,      # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                classes=self.configs.classes,          # model output channels (number of classes in your dataset)\n",
    "            )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # call 'patch_replication_callback' to use the `sync_batchnorm` layer\n",
    "        patch_replication_callback(self.s_model)\n",
    "        patch_replication_callback(self.t_model)\n",
    "        # detach the teacher model\n",
    "        for param in self.t_model.parameters():\n",
    "            param.detach_()\n",
    "        self.models = {'s_model': self.s_model, 't_model': self.t_model}\n",
    "\n",
    "        # create optimizers\n",
    "        self.s_optimizer = optimizer_funcs[0](self.s_model.module.param_groups)\n",
    "        self.optimizers = {'s_optimizer': self.s_optimizer}\n",
    "\n",
    "        # create lrers\n",
    "        self.s_lrer = lrer_funcs[0](self.s_optimizer)\n",
    "        self.lrers = {'s_lrer': self.s_lrer}\n",
    "\n",
    "        # create criterions\n",
    "        # TODO: support more types of the consistency criterion\n",
    "        self.cons_criterion = nn.MSELoss()\n",
    "        self.s_criterion = criterion_funcs[0](self.args)\n",
    "        self.criterions = {'s_criterion': self.s_criterion, 'cons_criterion': self.cons_criterion}\n",
    "\n",
    "        # create the gaussian noiser\n",
    "        self.gaussian_noiser = nn.DataParallel(GaussianNoiseLayer(self.args.gaussian_noise_std)).cuda()\n",
    "        \n",
    "    # ---------------------------------------------------------------------\n",
    "    # Interface for task proxy\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    def train(self, data_loader, epoch):\n",
    "        self._train(data_loader, epoch)\n",
    "\n",
    "    def val(self, data_loader, epoch):\n",
    "        self._val(data_loader, epoch)\n",
    "    \n",
    "    def save_ckpt(self, epoch):\n",
    "        self._save_ckpt(epoch)\n",
    "    \n",
    "    def load_ckpt(self):\n",
    "        return self._load_ckpt()\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # All SSL algorithms should implement the following functions\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    def _train(self, data_loader, epoch):\n",
    "        self.meters.reset()\n",
    "        lbs = self.args.labeled_batch_size\n",
    "\n",
    "        self.s_model.train()\n",
    "        self.t_model.train()\n",
    "\n",
    "        for idx, (inp, gt) in enumerate(data_loader):\n",
    "            timer = time.time()\n",
    "            \n",
    "            # 's_inp', 't_inp' and 'gt' are tuples\n",
    "            s_inp, t_inp, gt = self._batch_prehandle(inp, gt, True)\n",
    "            if len(gt) > 1 and idx == 0:\n",
    "                self._inp_warn()\n",
    "\n",
    "            # calculate the ramp-up coefficient of the consistency constraint\n",
    "            cur_step = len(data_loader) * epoch + idx\n",
    "            total_steps = len(data_loader) * self.args.cons_rampup_epochs\n",
    "            cons_rampup_scale = func.sigmoid_rampup(cur_step, total_steps)\n",
    "\n",
    "            self.s_optimizer.zero_grad()\n",
    "\n",
    "            # forward the student model\n",
    "            s_resulter, s_debugger = self.s_model.forward(s_inp)\n",
    "            if not 'pred' in s_resulter.keys() or not 'activated_pred' in s_resulter.keys():\n",
    "                self._pred_err()\n",
    "            s_pred = tool.dict_value(s_resulter, 'pred')\n",
    "            s_activated_pred = tool.dict_value(s_resulter, 'activated_pred')\n",
    "\n",
    "            # calculate the supervised task constraint on the labeled data\n",
    "            l_s_pred = func.split_tensor_tuple(s_pred, 0, lbs)\n",
    "            l_gt = func.split_tensor_tuple(gt, 0, lbs)\n",
    "            l_s_inp = func.split_tensor_tuple(s_inp, 0, lbs)\n",
    "\n",
    "            # 'task_loss' is a tensor of 1-dim & n elements, where n == batch_size\n",
    "            s_task_loss = self.s_criterion.forward(l_s_pred, l_gt, l_s_inp)\n",
    "            s_task_loss = torch.mean(s_task_loss)\n",
    "            self.meters.update('s_task_loss', s_task_loss.data)\n",
    "\n",
    "            # forward the teacher model\n",
    "            with torch.no_grad():\n",
    "                t_resulter, t_debugger = self.t_model.forward(t_inp)\n",
    "                if not 'pred' in t_resulter.keys():\n",
    "                    self._pred_err()\n",
    "                t_pred = tool.dict_value(t_resulter, 'pred')\n",
    "                t_activated_pred = tool.dict_value(t_resulter, 'activated_pred')\n",
    "            \n",
    "                # calculate 't_task_loss' for recording\n",
    "                l_t_pred = func.split_tensor_tuple(t_pred, 0, lbs)\n",
    "                l_t_inp = func.split_tensor_tuple(t_inp, 0, lbs)\n",
    "                t_task_loss = self.s_criterion.forward(l_t_pred, l_gt, l_t_inp)\n",
    "                t_task_loss = torch.mean(t_task_loss)\n",
    "                self.meters.update('t_task_loss', t_task_loss.data)\n",
    "\n",
    "            # calculate the consistency constraint from the teacher model to the student model\n",
    "            t_pseudo_gt = Variable(t_pred[0].detach().data, requires_grad=False)\n",
    "\n",
    "            if self.args.cons_for_labeled:\n",
    "                cons_loss = self.cons_criterion(s_pred[0], t_pseudo_gt)\n",
    "            elif self.args.unlabeled_batch_size > 0:\n",
    "                cons_loss = self.cons_criterion(s_pred[0][lbs:, ...], t_pseudo_gt[lbs:, ...])\n",
    "            else:\n",
    "                cons_loss = self.zero_tensor\n",
    "            cons_loss = cons_rampup_scale * self.args.cons_scale * torch.mean(cons_loss)\n",
    "            self.meters.update('cons_loss', cons_loss.data)\n",
    "\n",
    "            # backward and update the student model\n",
    "            loss = s_task_loss + cons_loss\n",
    "            loss.backward()\n",
    "            self.s_optimizer.step()\n",
    "\n",
    "            # update the teacher model by EMA\n",
    "            self._update_ema_variables(self.s_model, self.t_model, self.args.ema_decay, cur_step)\n",
    "\n",
    "            # logging\n",
    "            self.meters.update('batch_time', time.time() - timer)\n",
    "            if idx % self.args.log_freq == 0:\n",
    "                logger.log_info('step: [{0}][{1}/{2}]\\tbatch-time: {meters[batch_time]:.3f}\\n'\n",
    "                                '  student-{3}\\t=>\\t'\n",
    "                                's-task-loss: {meters[s_task_loss]:.6f}\\t'\n",
    "                                's-cons-loss: {meters[cons_loss]:.6f}\\n'\n",
    "                                '  teacher-{3}\\t=>\\t'\n",
    "                                't-task-loss: {meters[t_task_loss]:.6f}\\n'\n",
    "                                .format(epoch + 1, idx, len(data_loader), self.args.task, meters=self.meters))\n",
    "\n",
    "            # visualization\n",
    "            if self.args.visualize and idx % self.args.visual_freq == 0:\n",
    "                self._visualize(epoch, idx, True, \n",
    "                                func.split_tensor_tuple(s_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(s_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(gt, 0, 1, reduce_dim=True))\n",
    "\n",
    "            # update iteration-based lrers\n",
    "            if not self.args.is_epoch_lrer:\n",
    "                self.s_lrer.step()\n",
    "\n",
    "        # update epoch-based lrers\n",
    "        if self.args.is_epoch_lrer:\n",
    "            self.s_lrer.step()\n",
    "\n",
    "    def _validate(self, data_loader, epoch):\n",
    "        self.meters.reset()\n",
    "\n",
    "        self.s_model.eval()\n",
    "        self.t_model.eval()\n",
    "\n",
    "        for idx, (inp, gt) in enumerate(data_loader):\n",
    "            timer = time.time()\n",
    "\n",
    "            s_inp, t_inp, gt = self._batch_prehandle(inp, gt, False)\n",
    "            if len(gt) > 1 and idx == 0:\n",
    "                self._inp_warn()\n",
    "\n",
    "            s_resulter, s_debugger = self.s_model.forward(s_inp)\n",
    "            if not 'pred' in s_resulter.keys() or not 'activated_pred' in s_resulter.keys():\n",
    "                self._pred_err()\n",
    "            s_pred = tool.dict_value(s_resulter, 'pred')\n",
    "            s_activated_pred = tool.dict_value(s_resulter, 'activated_pred')\n",
    "\n",
    "            s_task_loss = self.s_criterion.forward(s_pred, gt, s_inp)\n",
    "            s_task_loss = torch.mean(s_task_loss)\n",
    "            self.meters.update('s_task_loss', s_task_loss.data)\n",
    "\n",
    "            t_resulter, t_debugger = self.t_model.forward(t_inp)\n",
    "            if not 'pred' in t_resulter.keys() or not 'activated_pred' in t_resulter.keys():\n",
    "                self._pred_err()\n",
    "            t_pred = tool.dict_value(t_resulter, 'pred')\n",
    "            t_activated_pred = tool.dict_value(t_resulter, 'activated_pred')\n",
    "\n",
    "            t_task_loss = self.s_criterion.forward(t_pred, gt, t_inp)\n",
    "            t_task_loss = torch.mean(t_task_loss)\n",
    "            self.meters.update('t_task_loss', t_task_loss.data)\n",
    "\n",
    "            t_pseudo_gt = Variable(t_pred[0].detach().data, requires_grad=False)\n",
    "            cons_loss = self.cons_criterion(s_pred[0], t_pseudo_gt)\n",
    "            cons_loss = self.args.cons_scale * torch.mean(cons_loss)\n",
    "            self.meters.update('cons_loss', cons_loss.data)\n",
    "\n",
    "            #self.task_func.metrics(s_activated_pred, gt, s_inp, self.meters, id_str='student')\n",
    "            #self.task_func.metrics(t_activated_pred, gt, t_inp, self.meters, id_str='teacher')\n",
    "\n",
    "            self.meters.update('batch_time', time.time() - timer)\n",
    "            if idx % self.args.log_freq == 0:\n",
    "                logger.log_info('step: [{0}][{1}/{2}]\\tbatch-time: {meters[batch_time]:.3f}\\n'\n",
    "                                '  student-{3}\\t=>\\t'\n",
    "                                's-task-loss: {meters[s_task_loss]:.6f}\\t'\n",
    "                                's-cons-loss: {meters[cons_loss]:.6f}\\n'\n",
    "                                '  teacher-{3}\\t=>\\t'\n",
    "                                't-task-loss: {meters[t_task_loss]:.6f}\\n'\n",
    "                                .format(epoch + 1, idx, len(data_loader), self.args.task, meters=self.meters))\n",
    "\n",
    "            if self.args.visualize and idx % self.args.visual_freq == 0:\n",
    "                self._visualize(epoch, idx, False, \n",
    "                                func.split_tensor_tuple(s_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(s_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(gt, 0, 1, reduce_dim=True))\n",
    "    \n",
    "        # metrics\n",
    "        metrics_info = {'student': '', 'teacher': ''}\n",
    "        for key in sorted(list(self.meters.keys())):\n",
    "            #if self.task_func.METRIC_STR in key:\n",
    "            if True:\n",
    "                for id_str in metrics_info.keys():\n",
    "                    if key.startswith(id_str):\n",
    "                        metrics_info[id_str] += '{0}: {1:.6}\\t'.format(key, self.meters[key])\n",
    "\n",
    "        logger.log_info('Validation metrics:\\n  student-metrics\\t=>\\t{0}\\n  teacher-metrics\\t=>\\t{1}\\n'\n",
    "            .format(metrics_info['student'].replace('_', '-'), metrics_info['teacher'].replace('_', '-')))\n",
    "\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        state = {\n",
    "            'algorithm': self.NAME,\n",
    "            'epoch': epoch, \n",
    "            's_model': self.s_model.state_dict(),\n",
    "            't_model': self.t_model.state_dict(),\n",
    "            's_optimizer': self.s_optimizer.state_dict(),\n",
    "            's_lrer': self.s_lrer.state_dict()\n",
    "        }\n",
    "\n",
    "        checkpoint = os.path.join(self.args.checkpoint_path, 'checkpoint_{0}.ckpt'.format(epoch))\n",
    "        torch.save(state, checkpoint)\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        checkpoint = torch.load(self.args.resume)\n",
    "\n",
    "        checkpoint_algorithm = tool.dict_value(checkpoint, 'algorithm', default='unknown')\n",
    "        if checkpoint_algorithm != self.NAME:\n",
    "            logger.log_err('Unmatched SSL algorithm format in checkpoint => required: {0} - given: {1}\\n'\n",
    "                           .format(self.NAME, checkpoint_algorithm))\n",
    "\n",
    "        self.s_model.load_state_dict(checkpoint['s_model'])\n",
    "        self.t_model.load_state_dict(checkpoint['t_model'])\n",
    "        self.s_optimizer.load_state_dict(checkpoint['s_optimizer'])\n",
    "        self.s_lrer.load_state_dict(checkpoint['s_lrer'])\n",
    "\n",
    "        return checkpoint['epoch']\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Tool Functions for SSL_MT\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _visualize(self, epoch, idx, is_train, \n",
    "                   s_inp, s_pred, t_inp, t_pred, gt):\n",
    "\n",
    "        visualize_path = self.args.visual_train_path if is_train else self.args.visual_val_path\n",
    "        out_path = os.path.join(visualize_path, '{0}_{1}'.format(epoch, idx))\n",
    "\n",
    "        #self.task_func.visualize(out_path, id_str='student', inp=s_inp, pred=s_pred, gt=gt)\n",
    "        #self.task_func.visualize(out_path, id_str='teacher', inp=t_inp, pred=t_pred, gt=gt)\n",
    "\n",
    "    def _batch_prehandle(self, inp, gt, is_train):\n",
    "        # add extra data augmentation process here if necessary\n",
    "\n",
    "        # 'self.gaussian_noiser' will add the noise to the first input element\n",
    "        s_inp_var, t_inp_var = [], []\n",
    "        for idx, i in enumerate(inp):\n",
    "            if is_train and idx == 0:\n",
    "                s_inp_var.append(self.gaussian_noiser.forward(Variable(i).cuda())) \n",
    "                t_inp_var.append(self.gaussian_noiser.forward(Variable(i).cuda())) \n",
    "            else:\n",
    "                s_inp_var.append(Variable(i).cuda()) \n",
    "                t_inp_var.append(Variable(i).cuda())\n",
    "        s_inp = tuple(s_inp_var)\n",
    "        t_inp = tuple(t_inp_var)\n",
    "        \n",
    "        gt_var = []\n",
    "        for g in gt:\n",
    "            gt_var.append(Variable(g).cuda())\n",
    "        gt = tuple(gt_var)\n",
    "\n",
    "        return s_inp, t_inp, gt\n",
    "\n",
    "    def _update_ema_variables(self, s_model, t_model, ema_decay, cur_step):\n",
    "        # update the teacher model by exponential moving average\n",
    "        ema_decay = min(1 - 1 / (cur_step + 1), ema_decay)\n",
    "        for t_param, s_param in zip(t_model.parameters(), s_model.parameters()):\n",
    "            t_param.data.mul_(ema_decay).add_(1 - ema_decay, s_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0440659-d578-4ae9-a2d5-88331279c1cc",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9de02c93-618e-464e-afc8-8da004a671e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patch_replication_callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23352\\1343033370.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRoutineMT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23352\\1413286831.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, configs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# call 'patch_replication_callback' to use the `sync_batchnorm` layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mpatch_replication_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mpatch_replication_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# detach the teacher model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'patch_replication_callback' is not defined"
     ]
    }
   ],
   "source": [
    "# Configs\n",
    "configs = Configs(experiment_name)\n",
    "configs.log()\n",
    "\n",
    "# Run\n",
    "run = RoutineMT(configs)\n",
    "run.build()\n",
    "\n",
    "for e in range(configs.epochs):\n",
    "\n",
    "    loss_train_epoch, fscore_train_epoch = run.train(epoch=epoch)        \n",
    "    loss_val_epoch, fscore_val_epoch = run.val(epoch=epoch)\n",
    "\n",
    "    routine.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c939f3-d59b-44d4-9c8d-a32217868ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee2543-5b24-4a14-8264-a42e6ec1b1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d09ad-3fde-4056-98e2-3e7650d69889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44351fae-b7db-4d98-81bc-3885fda3f805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
