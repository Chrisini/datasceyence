{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75832305-6091-4f3c-980a-53004f01a000",
   "metadata": {},
   "source": [
    "# ð•Šð•–ð•žð•š-ð•Šð•¦ð•¡ð•–ð•£ð•§ð•šð•¤ð•–ð•• ð•ð•–ð•’ð•£ð•Ÿð•šð•Ÿð•˜ ð•¦ð•¤ð•šð•Ÿð•˜ ð•„ð•–ð•’ð•Ÿ ð•‹ð•–ð•’ð•”ð•™ð•–ð•£\n",
    "\n",
    "Implementation of pixel-wise Mean Teacher (MT)\n",
    "    \n",
    "This method is proposed in the paper: \n",
    "    'Mean Teachers are Better Role Models:\n",
    "        Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results'\n",
    "This implementation only supports Gaussian noise as input perturbation, and the two-heads\n",
    "outputs trick is not available.\n",
    "\n",
    "Source:\n",
    "https://github.com/ZHKKKe/PixelSSL/blob/master/pixelssl/ssl_algorithm/ssl_mt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f3932a-744a-49ca-a076-f12749cf3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from pixelssl.utils import REGRESSION, CLASSIFICATION\n",
    "#from pixelssl.utils import logger, cmd, tool\n",
    "#from pixelssl.nn import func\n",
    "#from pixelssl.nn.module import patch_replication_callback, GaussianNoiseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1aa89-5640-4263-8051-96580d7d0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfd280-105f-4f5d-b3a6-a6d4959bb8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateRoutine:\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.models = {}                        # dict of the models required by the task and algorithm\n",
    "        self.optimizers = {}                    # dict of the optimizers required by the task and algorithm\n",
    "        self.lrers = {}                         # dict of the learn rate required by the task and algorithm\n",
    "        self.criterions = {}                    # dict of the criterions required by the task and algorithm\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Interface for task proxy\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    def build(self, model_funcs, optimizer_funcs, lrer_funcs, criterion_funcs, task_func):\n",
    "        self._build(model_funcs, optimizer_funcs, lrer_funcs, criterion_funcs, task_func)\n",
    "\n",
    "    def train(self, data_loader, epoch):\n",
    "        self._train(data_loader, epoch)\n",
    "\n",
    "    def validate(self, data_loader, epoch):\n",
    "        self._validate(data_loader, epoch)\n",
    "    \n",
    "    def save_checkpoint(self, epoch):\n",
    "        self._save_checkpoint(epoch)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        return self._load_checkpoint()\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # All SSL algorithms should implement the following functions\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    def _build(self, model_funcs, optimizer_funcs, lrer_funcs, criterion_funcs, task_func):\n",
    "        \"\"\" Build the SSL algorithm. \n",
    "        \n",
    "        Each SSL algorithm contains both task-specific components and algorithm-special components.\n",
    "        Each task-specific component has four parts -> (model, optimizer, lrer, criterion).\n",
    "        This function takes the lists of task-specific components as the input and builds them.\n",
    "        The algorithm-special components (e.g., the SSL constraints) will also be built in this function.\n",
    "        Then, this function saves all required components into four dictionaries:\n",
    "            self.models, self.optimizers, self.lrers, self.criterions\n",
    "        Arguments:\n",
    "            model_funcs (list): list of 'pixelssl.task_template.model.TaskModel'\n",
    "            optimizer_funcs (list): list of optimizer function defined in 'pixelssl.nn.optimizer'\n",
    "            lrer_funcs (list): list of learning rate adjust function defined in 'pixelssl.nn.lrer'\n",
    "            criterion_funcs (list): list of 'pixelssl.task_template.criterion.TaskCriterion'\n",
    "            task_func (pixelssl.task_template.func.TaskFunc): instance of 'pixelssl.task_template.func.TaskFunc'\n",
    "                it contains the task-specific functions\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _train(self, data_loader, epoch):\n",
    "        \"\"\" Use the current SSL algorithm to train the task model (for one epoch).\n",
    "        This function should be called after self.build().         \n",
    "        One 'epoch' is defined by browsing the data_loader once.\n",
    "        Arguments:\n",
    "            data_loader (torch.utils.data.DataLoader): task-specific data loader for training \n",
    "            epoch (int): index of current epoch\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _validate(self, data_loader, epoch):\n",
    "        \"\"\" Validate the task model onece.\n",
    "        This function should be called after self.build().\n",
    "        Arguments:\n",
    "            data_loader (torch.utils.data.DataLoader): task-specific data loader for validation\n",
    "            epoch (int): index of current epoch\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _save_checkpoint(self, epoch):\n",
    "        \"\"\" Save the current state of the experiment to the checkpoint file.\n",
    "        Arguments:\n",
    "            epoch (int): index of current epoch\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "            \n",
    "    def _load_checkpoint(self):\n",
    "        \"\"\" Load the experiment status from the given checkpoint file (args.resume).\n",
    "        Returns:\n",
    "            int: index of current epoch\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7605a7f6-d147-4551-b403-928659086b24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ssl_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13764\\3684020201.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mSSLMT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mssl_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SSLBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mNAME\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ssl_mt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mSUPPORTED_TASK_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mREGRESSION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLASSIFICATION\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ssl_base' is not defined"
     ]
    }
   ],
   "source": [
    "class SSLMT(TemplateRoutine):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(SSLMT, self).__init__(args)\n",
    "\n",
    "        # define the student model and the teacher model\n",
    "        self.s_model, self.t_model = None, None\n",
    "        self.s_optimizer = None\n",
    "        self.s_lrer = None\n",
    "        self.s_criterion = None\n",
    "\n",
    "        self.cons_criterion = None\n",
    "        \n",
    "        self.gaussian_noiser = None\n",
    "        self.zero_tensor = torch.zeros(1)\n",
    "\n",
    "    def _build(self, model_funcs, optimizer_funcs, lrer_funcs, criterion_funcs, task_func):\n",
    "        self.task_func = task_func\n",
    "\n",
    "        # create models\n",
    "        self.s_model = func.create_model(model_funcs[0], 's_model', args=self.args)\n",
    "        self.t_model = func.create_model(model_funcs[0], 't_model', args=self.args)\n",
    "        # call 'patch_replication_callback' to use the `sync_batchnorm` layer\n",
    "        patch_replication_callback(self.s_model)\n",
    "        patch_replication_callback(self.t_model)\n",
    "        # detach the teacher model\n",
    "        for param in self.t_model.parameters():\n",
    "            param.detach_()\n",
    "        self.models = {'s_model': self.s_model, 't_model': self.t_model}\n",
    "\n",
    "        # create optimizers\n",
    "        self.s_optimizer = optimizer_funcs[0](self.s_model.module.param_groups)\n",
    "        self.optimizers = {'s_optimizer': self.s_optimizer}\n",
    "\n",
    "        # create lrers\n",
    "        self.s_lrer = lrer_funcs[0](self.s_optimizer)\n",
    "        self.lrers = {'s_lrer': self.s_lrer}\n",
    "\n",
    "        # create criterions\n",
    "        # TODO: support more types of the consistency criterion\n",
    "        self.cons_criterion = nn.MSELoss()\n",
    "        self.s_criterion = criterion_funcs[0](self.args)\n",
    "        self.criterions = {'s_criterion': self.s_criterion, 'cons_criterion': self.cons_criterion}\n",
    "\n",
    "        # create the gaussian noiser\n",
    "        self.gaussian_noiser = nn.DataParallel(GaussianNoiseLayer(self.args.gaussian_noise_std)).cuda()\n",
    "\n",
    "        self._algorithm_warn()\n",
    "\n",
    "    def _train(self, data_loader, epoch):\n",
    "        self.meters.reset()\n",
    "        lbs = self.args.labeled_batch_size\n",
    "\n",
    "        self.s_model.train()\n",
    "        self.t_model.train()\n",
    "\n",
    "        for idx, (inp, gt) in enumerate(data_loader):\n",
    "            timer = time.time()\n",
    "            \n",
    "            # 's_inp', 't_inp' and 'gt' are tuples\n",
    "            s_inp, t_inp, gt = self._batch_prehandle(inp, gt, True)\n",
    "            if len(gt) > 1 and idx == 0:\n",
    "                self._inp_warn()\n",
    "\n",
    "            # calculate the ramp-up coefficient of the consistency constraint\n",
    "            cur_step = len(data_loader) * epoch + idx\n",
    "            total_steps = len(data_loader) * self.args.cons_rampup_epochs\n",
    "            cons_rampup_scale = func.sigmoid_rampup(cur_step, total_steps)\n",
    "\n",
    "            self.s_optimizer.zero_grad()\n",
    "\n",
    "            # forward the student model\n",
    "            s_resulter, s_debugger = self.s_model.forward(s_inp)\n",
    "            if not 'pred' in s_resulter.keys() or not 'activated_pred' in s_resulter.keys():\n",
    "                self._pred_err()\n",
    "            s_pred = tool.dict_value(s_resulter, 'pred')\n",
    "            s_activated_pred = tool.dict_value(s_resulter, 'activated_pred')\n",
    "\n",
    "            # calculate the supervised task constraint on the labeled data\n",
    "            l_s_pred = func.split_tensor_tuple(s_pred, 0, lbs)\n",
    "            l_gt = func.split_tensor_tuple(gt, 0, lbs)\n",
    "            l_s_inp = func.split_tensor_tuple(s_inp, 0, lbs)\n",
    "\n",
    "            # 'task_loss' is a tensor of 1-dim & n elements, where n == batch_size\n",
    "            s_task_loss = self.s_criterion.forward(l_s_pred, l_gt, l_s_inp)\n",
    "            s_task_loss = torch.mean(s_task_loss)\n",
    "            self.meters.update('s_task_loss', s_task_loss.data)\n",
    "\n",
    "            # forward the teacher model\n",
    "            with torch.no_grad():\n",
    "                t_resulter, t_debugger = self.t_model.forward(t_inp)\n",
    "                if not 'pred' in t_resulter.keys():\n",
    "                    self._pred_err()\n",
    "                t_pred = tool.dict_value(t_resulter, 'pred')\n",
    "                t_activated_pred = tool.dict_value(t_resulter, 'activated_pred')\n",
    "            \n",
    "                # calculate 't_task_loss' for recording\n",
    "                l_t_pred = func.split_tensor_tuple(t_pred, 0, lbs)\n",
    "                l_t_inp = func.split_tensor_tuple(t_inp, 0, lbs)\n",
    "                t_task_loss = self.s_criterion.forward(l_t_pred, l_gt, l_t_inp)\n",
    "                t_task_loss = torch.mean(t_task_loss)\n",
    "                self.meters.update('t_task_loss', t_task_loss.data)\n",
    "\n",
    "            # calculate the consistency constraint from the teacher model to the student model\n",
    "            t_pseudo_gt = Variable(t_pred[0].detach().data, requires_grad=False)\n",
    "\n",
    "            if self.args.cons_for_labeled:\n",
    "                cons_loss = self.cons_criterion(s_pred[0], t_pseudo_gt)\n",
    "            elif self.args.unlabeled_batch_size > 0:\n",
    "                cons_loss = self.cons_criterion(s_pred[0][lbs:, ...], t_pseudo_gt[lbs:, ...])\n",
    "            else:\n",
    "                cons_loss = self.zero_tensor\n",
    "            cons_loss = cons_rampup_scale * self.args.cons_scale * torch.mean(cons_loss)\n",
    "            self.meters.update('cons_loss', cons_loss.data)\n",
    "\n",
    "            # backward and update the student model\n",
    "            loss = s_task_loss + cons_loss\n",
    "            loss.backward()\n",
    "            self.s_optimizer.step()\n",
    "\n",
    "            # update the teacher model by EMA\n",
    "            self._update_ema_variables(self.s_model, self.t_model, self.args.ema_decay, cur_step)\n",
    "\n",
    "            # logging\n",
    "            self.meters.update('batch_time', time.time() - timer)\n",
    "            if idx % self.args.log_freq == 0:\n",
    "                logger.log_info('step: [{0}][{1}/{2}]\\tbatch-time: {meters[batch_time]:.3f}\\n'\n",
    "                                '  student-{3}\\t=>\\t'\n",
    "                                's-task-loss: {meters[s_task_loss]:.6f}\\t'\n",
    "                                's-cons-loss: {meters[cons_loss]:.6f}\\n'\n",
    "                                '  teacher-{3}\\t=>\\t'\n",
    "                                't-task-loss: {meters[t_task_loss]:.6f}\\n'\n",
    "                                .format(epoch + 1, idx, len(data_loader), self.args.task, meters=self.meters))\n",
    "\n",
    "            # visualization\n",
    "            if self.args.visualize and idx % self.args.visual_freq == 0:\n",
    "                self._visualize(epoch, idx, True, \n",
    "                                func.split_tensor_tuple(s_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(s_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(gt, 0, 1, reduce_dim=True))\n",
    "\n",
    "            # update iteration-based lrers\n",
    "            if not self.args.is_epoch_lrer:\n",
    "                self.s_lrer.step()\n",
    "\n",
    "        # update epoch-based lrers\n",
    "        if self.args.is_epoch_lrer:\n",
    "            self.s_lrer.step()\n",
    "\n",
    "    def _validate(self, data_loader, epoch):\n",
    "        self.meters.reset()\n",
    "\n",
    "        self.s_model.eval()\n",
    "        self.t_model.eval()\n",
    "\n",
    "        for idx, (inp, gt) in enumerate(data_loader):\n",
    "            timer = time.time()\n",
    "\n",
    "            s_inp, t_inp, gt = self._batch_prehandle(inp, gt, False)\n",
    "            if len(gt) > 1 and idx == 0:\n",
    "                self._inp_warn()\n",
    "\n",
    "            s_resulter, s_debugger = self.s_model.forward(s_inp)\n",
    "            if not 'pred' in s_resulter.keys() or not 'activated_pred' in s_resulter.keys():\n",
    "                self._pred_err()\n",
    "            s_pred = tool.dict_value(s_resulter, 'pred')\n",
    "            s_activated_pred = tool.dict_value(s_resulter, 'activated_pred')\n",
    "\n",
    "            s_task_loss = self.s_criterion.forward(s_pred, gt, s_inp)\n",
    "            s_task_loss = torch.mean(s_task_loss)\n",
    "            self.meters.update('s_task_loss', s_task_loss.data)\n",
    "\n",
    "            t_resulter, t_debugger = self.t_model.forward(t_inp)\n",
    "            if not 'pred' in t_resulter.keys() or not 'activated_pred' in t_resulter.keys():\n",
    "                self._pred_err()\n",
    "            t_pred = tool.dict_value(t_resulter, 'pred')\n",
    "            t_activated_pred = tool.dict_value(t_resulter, 'activated_pred')\n",
    "\n",
    "            t_task_loss = self.s_criterion.forward(t_pred, gt, t_inp)\n",
    "            t_task_loss = torch.mean(t_task_loss)\n",
    "            self.meters.update('t_task_loss', t_task_loss.data)\n",
    "\n",
    "            t_pseudo_gt = Variable(t_pred[0].detach().data, requires_grad=False)\n",
    "            cons_loss = self.cons_criterion(s_pred[0], t_pseudo_gt)\n",
    "            cons_loss = self.args.cons_scale * torch.mean(cons_loss)\n",
    "            self.meters.update('cons_loss', cons_loss.data)\n",
    "\n",
    "            self.task_func.metrics(s_activated_pred, gt, s_inp, self.meters, id_str='student')\n",
    "            self.task_func.metrics(t_activated_pred, gt, t_inp, self.meters, id_str='teacher')\n",
    "\n",
    "            self.meters.update('batch_time', time.time() - timer)\n",
    "            if idx % self.args.log_freq == 0:\n",
    "                logger.log_info('step: [{0}][{1}/{2}]\\tbatch-time: {meters[batch_time]:.3f}\\n'\n",
    "                                '  student-{3}\\t=>\\t'\n",
    "                                's-task-loss: {meters[s_task_loss]:.6f}\\t'\n",
    "                                's-cons-loss: {meters[cons_loss]:.6f}\\n'\n",
    "                                '  teacher-{3}\\t=>\\t'\n",
    "                                't-task-loss: {meters[t_task_loss]:.6f}\\n'\n",
    "                                .format(epoch + 1, idx, len(data_loader), self.args.task, meters=self.meters))\n",
    "\n",
    "            if self.args.visualize and idx % self.args.visual_freq == 0:\n",
    "                self._visualize(epoch, idx, False, \n",
    "                                func.split_tensor_tuple(s_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(s_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(gt, 0, 1, reduce_dim=True))\n",
    "    \n",
    "        # metrics\n",
    "        metrics_info = {'student': '', 'teacher': ''}\n",
    "        for key in sorted(list(self.meters.keys())):\n",
    "            if self.task_func.METRIC_STR in key:\n",
    "                for id_str in metrics_info.keys():\n",
    "                    if key.startswith(id_str):\n",
    "                        metrics_info[id_str] += '{0}: {1:.6}\\t'.format(key, self.meters[key])\n",
    "\n",
    "        logger.log_info('Validation metrics:\\n  student-metrics\\t=>\\t{0}\\n  teacher-metrics\\t=>\\t{1}\\n'\n",
    "            .format(metrics_info['student'].replace('_', '-'), metrics_info['teacher'].replace('_', '-')))\n",
    "\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        state = {\n",
    "            'algorithm': self.NAME,\n",
    "            'epoch': epoch, \n",
    "            's_model': self.s_model.state_dict(),\n",
    "            't_model': self.t_model.state_dict(),\n",
    "            's_optimizer': self.s_optimizer.state_dict(),\n",
    "            's_lrer': self.s_lrer.state_dict()\n",
    "        }\n",
    "\n",
    "        checkpoint = os.path.join(self.args.checkpoint_path, 'checkpoint_{0}.ckpt'.format(epoch))\n",
    "        torch.save(state, checkpoint)\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        checkpoint = torch.load(self.args.resume)\n",
    "\n",
    "        checkpoint_algorithm = tool.dict_value(checkpoint, 'algorithm', default='unknown')\n",
    "        if checkpoint_algorithm != self.NAME:\n",
    "            logger.log_err('Unmatched SSL algorithm format in checkpoint => required: {0} - given: {1}\\n'\n",
    "                           .format(self.NAME, checkpoint_algorithm))\n",
    "\n",
    "        self.s_model.load_state_dict(checkpoint['s_model'])\n",
    "        self.t_model.load_state_dict(checkpoint['t_model'])\n",
    "        self.s_optimizer.load_state_dict(checkpoint['s_optimizer'])\n",
    "        self.s_lrer.load_state_dict(checkpoint['s_lrer'])\n",
    "\n",
    "        return checkpoint['epoch']\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Tool Functions for SSL_MT\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _visualize(self, epoch, idx, is_train, \n",
    "                   s_inp, s_pred, t_inp, t_pred, gt):\n",
    "\n",
    "        visualize_path = self.args.visual_train_path if is_train else self.args.visual_val_path\n",
    "        out_path = os.path.join(visualize_path, '{0}_{1}'.format(epoch, idx))\n",
    "\n",
    "        self.task_func.visualize(out_path, id_str='student', inp=s_inp, pred=s_pred, gt=gt)\n",
    "        self.task_func.visualize(out_path, id_str='teacher', inp=t_inp, pred=t_pred, gt=gt)\n",
    "\n",
    "    def _batch_prehandle(self, inp, gt, is_train):\n",
    "        # add extra data augmentation process here if necessary\n",
    "\n",
    "        # 'self.gaussian_noiser' will add the noise to the first input element\n",
    "        s_inp_var, t_inp_var = [], []\n",
    "        for idx, i in enumerate(inp):\n",
    "            if is_train and idx == 0:\n",
    "                s_inp_var.append(self.gaussian_noiser.forward(Variable(i).cuda())) \n",
    "                t_inp_var.append(self.gaussian_noiser.forward(Variable(i).cuda())) \n",
    "            else:\n",
    "                s_inp_var.append(Variable(i).cuda()) \n",
    "                t_inp_var.append(Variable(i).cuda())\n",
    "        s_inp = tuple(s_inp_var)\n",
    "        t_inp = tuple(t_inp_var)\n",
    "        \n",
    "        gt_var = []\n",
    "        for g in gt:\n",
    "            gt_var.append(Variable(g).cuda())\n",
    "        gt = tuple(gt_var)\n",
    "\n",
    "        return s_inp, t_inp, gt\n",
    "\n",
    "    def _update_ema_variables(self, s_model, t_model, ema_decay, cur_step):\n",
    "        # update the teacher model by exponential moving average\n",
    "        ema_decay = min(1 - 1 / (cur_step + 1), ema_decay)\n",
    "        for t_param, s_param in zip(t_model.parameters(), s_model.parameters()):\n",
    "            t_param.data.mul_(ema_decay).add_(1 - ema_decay, s_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22b260-ef9e-449d-91ae-34a476191326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_mt_routine(configs, model_dict, optimizer_dict, lrer_dict, criterion_dict, task_func):\n",
    "    if not len(model_dict) == len(optimizer_dict) == len(lrer_dict) == len(criterion_dict) == 1:\n",
    "        logger.log_err('The len(element_dict) of SSL_MT should be 1\\n')\n",
    "    elif list(model_dict.keys())[0] != 'model':\n",
    "        logger.log_err('In SSL_MT, the key of element_dict should be \\'model\\',\\n'\n",
    "                       'but \\'{0}\\' is given\\n'.format(model_dict.keys()))\n",
    "\n",
    "    model_funcs = [model_dict['model']]\n",
    "    optimizer_funcs = [optimizer_dict['model']]\n",
    "    lrer_funcs = [lrer_dict['model']]\n",
    "    criterion_funcs = [criterion_dict['model']]\n",
    "\n",
    "    algorithm = SSLMT(args)\n",
    "    algorithm.build(model_funcs, optimizer_funcs, lrer_funcs, criterion_funcs, task_func)\n",
    "    return algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de02c93-618e-464e-afc8-8da004a671e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_mt_routine(configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
