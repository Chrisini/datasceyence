{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d990fa5b-5504-4cf0-81ab-1fb89335497c",
   "metadata": {},
   "source": [
    "# ùïÜùï¶ùï• ùï†ùïó ùïïùïöùï§ùï•ùï£ùïöùïìùï¶ùï•ùïöùï†ùïü\n",
    "\n",
    "Source:\n",
    "* https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd459e-87c5-4600-895a-c6af351716ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b6fdb-5031-4078-89bd-2adff2856b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b6b92-fa91-4e72-9eb6-5dc41eab512d",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4774501f-e743-4362-b0f8-140e584fed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_vae_features = spacy.load('de_core_news_sm')\n",
    "spacy_i_dont_know = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_vae_features(text):\n",
    "    \"\"\"\n",
    "    Tokenizes vae autoencoders from a numpy array\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_i_dont_know(text):\n",
    "    \"\"\"\n",
    "    Class labels???\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "SRC = Field(tokenize = tokenize_vae_features, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_i_dont_know, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e319376-ecf8-4e70-aca4-28a7e178512d",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36148305-619a-4e54-ae76-44b61e68ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e3351b-6152-4f45-9b96-90dee1f9034c",
   "metadata": {},
   "source": [
    "# Model\n",
    "* https://pytorch.org/text/main/tutorials/t5_demo.html#generate-sentiment-classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691237aa-4447-4f23-87ff-776fc826b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(imdb_dataloader))\n",
    "input_text = batch[\"text\"]\n",
    "target = batch[\"label\"]\n",
    "beam_size = 1\n",
    "\n",
    "model_input = transform(input_text)\n",
    "model_output = sequence_generator.generate(model_input, eos_idx=eos_idx, num_beams=beam_size)\n",
    "output_text = transform.decode(model_output.tolist())\n",
    "\n",
    "for i in range(imdb_batch_size):\n",
    "    print(f\"Example {i+1}:\\n\")\n",
    "    print(f\"input_text: {input_text[i]}\\n\")\n",
    "    print(f\"prediction: {output_text[i]}\\n\")\n",
    "    print(f\"target: {target[i]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "940c8925-aa25-43ff-9a83-19b0bbade0ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37800\\2514822396.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Example - Pretrained large xlmr encoder attached to un-initialized classification head\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRobertaClassificationHead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "# Example - Pretrained large xlmr encoder attached to un-initialized classification head\n",
    "\n",
    "import torch, torchtext\n",
    "from torchtext.models import RobertaClassificationHead\n",
    "from torchtext.functional import to_tensor\n",
    "xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\n",
    "classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\n",
    "model = xlmr_large.get_model(head=classifier_head)\n",
    "transform = xlmr_large.transform()\n",
    "input_batch = [\"Hello world\", \"How are you!\"]\n",
    "model_input = to_tensor(transform(input_batch), padding_value=1)\n",
    "output = model(model_input)\n",
    "output.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23038eb9-5836-4536-8489-c3d716c70d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37800\\1106209041.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRobertaEncoderConf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRobertaBundle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRobertaClassificationHead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel_weights_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://download.pytorch.org/models/text/xlmr.base.encoder.pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mencoder_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaEncoderConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250002\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclassifier_head\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaClassificationHead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m768\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaBundle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_conf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_conf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassifier_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_weights_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from torchtext.models import RobertaEncoderConf, RobertaBundle, RobertaClassificationHead\n",
    "model_weights_path = \"https://download.pytorch.org/models/text/xlmr.base.encoder.pt\"\n",
    "encoder_conf = RobertaEncoderConf(vocab_size=250002)\n",
    "classifier_head = RobertaClassificationHead(num_classes=18, input_dim=768)\n",
    "model = RobertaBundle.build_model(encoder_conf=encoder_conf, head=classifier_head, checkpoint=model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e7918-095b-4551-a215-a715a7c58e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OODModel(torch.nn.Module):\n",
    "    # =============================================================================\n",
    "    # use this\n",
    "    # encoder backbone (shufflenet_v2_x1_0) + projection head (MLP)\n",
    "    # output size of MLP: 128 \n",
    "    # this feature vector is then used for the SupConLoss\n",
    "\n",
    "    # Replace model head:\n",
    "    # https://discuss.pytorch.org/t/how-to-replace-a-models-head/109002/2\n",
    "    # shuffle net: (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
    "\n",
    "    # this function is used to train a DecentBlock with the SupConLoss\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, out_classes=20):\n",
    "        super(OODModel, self).__init__()\n",
    "        \n",
    "        self.out_classes = out_classes\n",
    "        \n",
    "        # encoder\n",
    "        try:\n",
    "            shufflenet = shufflenet_v2_x1_0(weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1)\n",
    "        except: \n",
    "            shufflenet = shufflenet_v2_x1_0(pretrained=True)\n",
    "                    \n",
    "        # size of encoder output\n",
    "        encoder_out = shufflenet.fc.in_features\n",
    "            \n",
    "        # placeholder identity operator \n",
    "        shufflenet.fc = torch.nn.Identity()\n",
    "                \n",
    "        # encoder without fully connected classification head (linear layer)\n",
    "        self.encoder = shufflenet\n",
    "        \n",
    "        # maybe we need concatenation or view change here ... 1x1 conv??\n",
    "        \n",
    "        # MLP head\n",
    "        self.mlp_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(encoder_out, encoder_out),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(encoder_out, 128)\n",
    "        )\n",
    "        \n",
    "        self.classification = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, self.out_classes)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.mlp_head(x)\n",
    "        block_output = torch.nn.functional.normalize(x, dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return block_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
