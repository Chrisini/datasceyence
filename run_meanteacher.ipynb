{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75832305-6091-4f3c-980a-53004f01a000",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ð•Šð•–ð•žð•š-ð•Šð•¦ð•¡ð•–ð•£ð•§ð•šð•¤ð•–ð•• ð•ð•–ð•’ð•£ð•Ÿð•šð•Ÿð•˜ ð•¦ð•¤ð•šð•Ÿð•˜ ð•„ð•–ð•’ð•Ÿ ð•‹ð•–ð•’ð•”ð•™ð•–ð•£\n",
    "\n",
    "Implementation of pixel-wise Mean Teacher (MT)\n",
    "    \n",
    "This method is proposed in the paper: \n",
    "    'Mean Teachers are Better Role Models:\n",
    "        Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results'\n",
    "This implementation only supports Gaussian noise as input perturbation, and the two-heads\n",
    "outputs trick is not available.\n",
    "\n",
    "Source:\n",
    "https://github.com/ZHKKKe/PixelSSL/blob/master/pixelssl/ssl_algorithm/ssl_mt.py\n",
    "\n",
    "\n",
    "Todo:\n",
    "* [] get everything on cuda, cpu -> to configs.device\n",
    "* [] metrics for segmentation + unittests\n",
    "* [] track the combined loss - what's the weight, ...\n",
    "\n",
    "\n",
    "Things that we have to set to the device (gpu):\n",
    "* the model\n",
    "* the image that gets fed into the model\n",
    "* the ground truth that gets fed into the criterion\n",
    "* the ground truth that gets fed into the metrics - due to activation functions\n",
    "\n",
    "\n",
    "recall high: most of the roi has been segmented\n",
    "in combination with low precision: general oversegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde2fc8-5e60-4c2e-ad54-aa8aa6206ed5",
   "metadata": {},
   "source": [
    "Using Learning Rate Scheduler:\n",
    "\n",
    "https://discuss.pytorch.org/t/how-to-implement-torch-optim-lr-scheduler-cosineannealinglr/28797/5\n",
    "\n",
    "```\n",
    "steps = 10\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for idx in range(steps):\n",
    "        scheduler.step()\n",
    "        print(scheduler.get_lr())\n",
    "\n",
    "    print('Reset scheduler')\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\n",
    "```            \n",
    "\n",
    "\n",
    "* TODO\n",
    "* BINARY_MODE MULTICLASS_MODE - the loss wants a different encoding ...\n",
    "* ground_truth = torch.nn.functional.one_hot(ground_truth, num_classes)  # N,H*W -> N,H*W, C\n",
    "* ground_truth = ground_truth.permute(0, 2, 1)  # N, C, H*W\n",
    "* RuntimeError: one_hot is only applicable to index tensor.\n",
    "\n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6f142-82ee-4f0a-87f8-e2e9eeeb89b6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f3932a-744a-49ca-a076-f12749cf3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import logging\n",
    "import shutil # to remove dirs\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "from helper.dataset.meanteacher import *\n",
    "# from helper.model.mean_teacher import * \n",
    "from helper.sampler.mixed_batch import *\n",
    "# from helper.model.block.noise_block import GaussianNoiseBlock\n",
    "from helper.compute.bin_seg import BCE_BinSeg_CU\n",
    "from helper.compute.loss.shape import ShapeLoss\n",
    "from helper.compute.loss.dice import DiceLoss\n",
    "from helper.compute.metric.uncertainty import *\n",
    "from helper.compute.metric.segmentation import *\n",
    "\n",
    "\n",
    "#from pixelssl.utils import REGRESSION, CLASSIFICATION\n",
    "#from pixelssl.utils import logger, cmd, tool\n",
    "#from pixelssl.nn import func\n",
    "#from pixelssl.nn.module import patch_replication_callback, GaussianNoiseLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a707779-ab4d-46a4-ab5b-e123f91b827a",
   "metadata": {},
   "source": [
    "# Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d1aa89-5640-4263-8051-96580d7d0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs():\n",
    "    \n",
    "    def __init__(self, reset=False):\n",
    "        \n",
    "        \n",
    "        # =============================================================================\n",
    "        # CHECK THESE\n",
    "        # =============================================================================\n",
    "        reset_2 = True # double control, otherwise all results for this prefix will be deleted\n",
    "        if reset and reset_2:\n",
    "            reset=True\n",
    "        else:\n",
    "            reset=False\n",
    "        self.prefix = \"exp1_mt_dice\"\n",
    "        self.reduced_data = False\n",
    "        # \"cuda\" - use all the available GPUs\n",
    "        # \"cuda:1,3\" - use specific GPUs\n",
    "        self.device = \"cpu\" # \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.base_path = r\"C:/Users/Prinzessin/projects/decentnet\"\n",
    "        self.print_todos = True\n",
    "        # =============================================================================\n",
    "        # CHECK THESE\n",
    "        # =============================================================================\n",
    "        \n",
    "        # smp unet ++ parameters\n",
    "        self.encoder_name = \"efficientnet-b7\"\n",
    "        self.encoder_weights = \"imagenet\"\n",
    "        self.in_channels =  1\n",
    "        self.n_output_neurons = 2\n",
    "        \n",
    "        self.eps = 1e-6\n",
    "        \n",
    "        self.num_workers = 0\n",
    "        self.epochs = 1000\n",
    "        self.rampup_length = 600\n",
    "        \n",
    "        self.gaussian_noise = 0.1 # None\n",
    "        \n",
    "        self.ema_decay = 0.999 # default value\n",
    "        \n",
    "        # Sizes of tensors must match except in dimension 1\n",
    "        # I solved the issue by resizing all the images size divisible to 32.\n",
    "        self.image_size = 128 # 512\n",
    "        \n",
    "        \n",
    "        \n",
    "        # batch size = n_samples_per_class_per_batch * classes\n",
    "        # for mixed batch sampling\n",
    "        self.n_samples_per_class_per_batch = 1\n",
    "        \n",
    "        # self.lbs = 3 #  self.args.labeled_batch_size # .... remove this eventually and replace\n",
    "\n",
    "        # optimisation\n",
    "        self.optimiser = \"sgd\"\n",
    "        self.base_lr = 0.0001\n",
    "        self.min_lr = 0.0001\n",
    "        self.weight_decay = 1e-4\n",
    "        self.momentum = 0.9\n",
    "        \n",
    "        # self.is_epoch_lrer = True # epoch or batch based learning rate updater\n",
    "        \n",
    "        self.dropout = None\n",
    "        \n",
    "        \n",
    "         \n",
    "        # =============================================================================\n",
    "        # Paths\n",
    "        # =============================================================================\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(self.base_path):\n",
    "            os.makedirs(self.base_path)\n",
    "        os.chdir(self.base_path) # this is now the main directory !!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        self.csv_filenames = glob.glob(r\"datasceyence/data_prep/mt_*.csv\")\n",
    "        \n",
    "        print(self.csv_filenames)\n",
    "        \n",
    "        # input\n",
    "        self.load_checkpoint_file = None\n",
    "        \n",
    "        # all csv files used for run_mean_teacher.ipybn\n",
    "        #self.csv_data_paths = [\n",
    "        #    {\"path\" : r\"data/data_ichallenge_amd.csv\"}, \n",
    "        #    {\"path\" : r\"data/data_ichallenge_non_amd.csv\"}\n",
    "        #]\n",
    "        \n",
    "        \n",
    "        # output\n",
    "        self.logger_path = f\"results/{self.prefix}/logs\"\n",
    "        # if path exists: delete, create path\n",
    "        if reset:\n",
    "            if os.path.exists(self.logger_path):\n",
    "                shutil.rmtree(self.logger_path)\n",
    "            os.makedirs(self.logger_path)\n",
    "            \n",
    "        self.result_path = os.path.join(self.logger_path, f\"metrics_{self.prefix}.csv\" )\n",
    "            \n",
    "        self.save_checkpoint_path = f\"results/{self.prefix}/ckpts\"\n",
    "         # if path exists: delete, create path\n",
    "        if reset:\n",
    "            if os.path.exists(self.save_checkpoint_path):\n",
    "                shutil.rmtree(self.save_checkpoint_path)\n",
    "            os.makedirs(self.save_checkpoint_path)\n",
    "\n",
    "\n",
    "    def log(self):\n",
    "        # =============================================================================\n",
    "        # save all class variables to file \"configs.txt\"\n",
    "        # =============================================================================\n",
    "        c = pd.DataFrame.from_dict({'key': self.__dict__.keys(), 'value': self.__dict__.values()})\n",
    "        c.to_csv(os.path.join(self.logger_path, \"configs.txt\"), sep=':', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c64cbd-0bd5-4220-9f34-b7ff564d21ae",
   "metadata": {},
   "source": [
    "# Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7605a7f6-d147-4551-b403-928659086b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoutineMT:\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(RoutineMT, self).__init__()\n",
    "        \n",
    "        self.configs = configs\n",
    "                \n",
    "        self.load_ckpt = torch.load(configs.load_checkpoint_file) if configs.load_checkpoint_file is not None else None\n",
    "        \n",
    "        self.i_item_total = 0\n",
    "        \n",
    "        \n",
    "        # =============================================================================\n",
    "        # Logger\n",
    "        # =============================================================================\n",
    "        \n",
    "        self.writer =  SummaryWriter(log_dir=self.configs.logger_path)\n",
    "        logging.basicConfig(filename=os.path.join(self.configs.logger_path, 'logger.log'), encoding='utf-8', level=logging.DEBUG)\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Models\n",
    "        # =============================================================================\n",
    "        s_model = smp.UnetPlusPlus(\n",
    "                        encoder_name=self.configs.encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights=self.configs.encoder_weights,  # use `imagenet` pre-trained weights for encoder initialization\n",
    "                        in_channels=self.configs.in_channels,          # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                        classes=self.configs.n_output_neurons,         # model output channels (number of classes in your dataset)\n",
    "                    )\n",
    "        \n",
    "        t_model = smp.UnetPlusPlus(\n",
    "                        encoder_name=self.configs.encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights=self.configs.encoder_weights,  # use `imagenet` pre-trained weights for encoder initialization\n",
    "                        in_channels=self.configs.in_channels,          # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                        classes=self.configs.n_output_neurons,         # model output channels (number of classes in your dataset)\n",
    "                    )\n",
    "        \n",
    "        # detach the teacher model\n",
    "        for param in t_model.parameters():\n",
    "            param.detach_()\n",
    "            \n",
    "        # model= nn.DataParallel(model)\n",
    "        # if only one gpu ??\n",
    "        if \"cpu\" in self.configs.device:\n",
    "            self.models = {'s': s_model, \n",
    "                           't': t_model}\n",
    "        else: \n",
    "            self.models = {'s': torch.nn.DataParallel(s_model).to(self.configs.device), \n",
    "                           't': torch.nn.DataParallel(t_model).to(self.configs.device)}\n",
    "        \n",
    "        # add gaussian noise\n",
    "        # currently not in use\n",
    "        # self.gaussian_noiser = GaussianNoiseBlock(self.configs.gaussian_noise).cuda()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Computing Units\n",
    "        # =============================================================================\n",
    "        self.computing_unit = {\n",
    "            \"s_train\" : BCE_BinSeg_CU(n_output_neurons=self.configs.n_output_neurons, mode=\"train\", model=\"s\", device=self.configs.device, writer=self.writer),\n",
    "            \"s_val\"   : BCE_BinSeg_CU(n_output_neurons=self.configs.n_output_neurons, mode=\"val\", model=\"s\", device=self.configs.device, writer=self.writer),\n",
    "            \"t_train\" : BCE_BinSeg_CU(n_output_neurons=self.configs.n_output_neurons, mode=\"train\", model=\"t\", device=self.configs.device, writer=self.writer),\n",
    "            \"s_test\"  : BCE_BinSeg_CU(n_output_neurons=self.configs.n_output_neurons, mode=\"val\", model=\"s\", device=self.configs.device, writer=None),\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # =============================================================================\n",
    "        # Loss functions\n",
    "        # =============================================================================\n",
    "        # TODO: support more types of the consistency criterion\n",
    "        # something with head and each head has a loss function attached??\n",
    "        self.criterions = {'shape': ShapeLoss(device=self.configs.device), # criterion_funcs[0](self.args),\n",
    "                           'pixel': DiceLoss(n_output_neurons=self.configs.n_output_neurons),\n",
    "                           'cons': torch.nn.MSELoss() # softmax missing? there is a softmax mse loss ?? todo\n",
    "                          }\n",
    "        \n",
    "        self.um = UncertaintyMetric(n_noise=4, n_repeat=2, n_output_neurons=self.configs.n_output_neurons)\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Datasets: train, val, test\n",
    "        # =============================================================================\n",
    "                \n",
    "        train_set = MeanTeacherTrainDataset(mode=\"train\", channels=self.configs.in_channels, image_size=self.configs.image_size, csv_filenames=self.configs.csv_filenames, reduced_data=self.configs.reduced_data)\n",
    "        train_mbs = MixedBatchSampler(train_set.get_mbs_labels(), n_samples_per_class_per_batch=self.configs.n_samples_per_class_per_batch)\n",
    "        \n",
    "        print(\"*\"*50)\n",
    "        print(\"all labels\", train_set.get_mbs_labels())\n",
    "        \n",
    "        x = train_set.get_mbs_labels()\n",
    "        leastFrequent = min(x, key=x.count)\n",
    "        amount = x.count(leastFrequent)\n",
    "        print(\"this needs some error catching\")\n",
    "        self.steps = int(amount/self.configs.n_samples_per_class_per_batch)\n",
    "        print(\"steps\", self.steps)\n",
    "                \n",
    "        val_set = MeanTeacherValDataset(mode=\"val\", channels=self.configs.in_channels, image_size=self.configs.image_size, csv_filenames=self.configs.csv_filenames, reduced_data=self.configs.reduced_data)\n",
    "              \n",
    "        self.dataloader = {\"train\" : DataLoader(train_set, batch_sampler=train_mbs),\n",
    "                           \"val\" :   DataLoader(val_set)\n",
    "                          }\n",
    "        \n",
    "        \n",
    "        # =============================================================================\n",
    "        # Optimisers\n",
    "        # =============================================================================\n",
    "        self.optims = {'s': torch.optim.SGD(self.models[\"s\"].parameters(), lr=self.configs.base_lr, momentum=self.configs.momentum) # optimizer_funcs[0](self.models[\"s\"].module.param_groups)\n",
    "                          }\n",
    "\n",
    "        # =============================================================================\n",
    "        # Learning rate schedulers\n",
    "        # =============================================================================\n",
    "        self.lrsers = {'s': torch.optim.lr_scheduler.CosineAnnealingLR(self.optims[\"s\"], T_max=self.steps)\n",
    "                         } # lrer_funcs[0](self.optimizers['s_optimizer'])\n",
    "                  \n",
    "        # =============================================================================\n",
    "        # Resume training\n",
    "        # =============================================================================\n",
    "        if self.load_ckpt:\n",
    "            self.models[\"s\"].load_state_dict(self.load_ckpt['s_model'])\n",
    "            self.models[\"t\"].load_state_dict(self.load_ckpt['t_model'])\n",
    "            self.optims[\"s\"].load_state_dict(self.load_ckpt['s_optimizer'])\n",
    "            self.lrsers[\"s\"].load_state_dict(self.load_ckpt['s_lrer'])\n",
    "\n",
    "\n",
    "    \n",
    "    def run_training(self, i_epoch):\n",
    "        # =============================================================================\n",
    "        # Training\n",
    "        # =============================================================================\n",
    "        \n",
    "        mode=\"train\"\n",
    "        \n",
    "        self.models[\"s\"].train()\n",
    "        self.models[\"t\"].eval() # this should never be train??\n",
    "        \n",
    "        for i_item, item in enumerate(self.dataloader[mode]):\n",
    "            # =============================================================================\n",
    "            # Process Batch\n",
    "            # =============================================================================\n",
    "            \n",
    "            # unlabelled for consistency loss\n",
    "            unlabelled_batch_ids = np.where(item[\"has_mask\"] == False) \n",
    "            # labelled for task loss\n",
    "            labelled_batch_ids = np.where(item[\"has_mask\"] == True) \n",
    "            \n",
    "            # get predictions of student model for all images\n",
    "            s_model_output = self.models[\"s\"](item[\"img\"].to(self.configs.device)) # we want all images (task loss and consistency loss)\n",
    "            \n",
    "            # run batch for student model\n",
    "            self.computing_unit[\"s_train\"].run_batch(configs=self.configs, criterions=self.criterions, model_output=s_model_output[labelled_batch_ids], ground_truth=item[\"msk\"][labelled_batch_ids])\n",
    "            \n",
    "            if False:\n",
    "                print(labelled_batch_ids)\n",
    "                print(unlabelled_batch_ids)\n",
    "                print(\"item\"*40)\n",
    "                print(item)\n",
    "                print(\"item\"*40)\n",
    "                print(\"s model\")\n",
    "                print(s_model_output)\n",
    "                print(item[\"msk\"])\n",
    "                print(s_model_output.shape)\n",
    "                print(item[\"msk\"].shape)\n",
    "                print(\"next\")\n",
    "\n",
    "            # =============================================================================\n",
    "            # Teacher Model\n",
    "            # =============================================================================\n",
    "            \n",
    "            # forward the teacher model\n",
    "            with torch.no_grad():\n",
    "                # get predictions of teacher model for all images\n",
    "                t_model_output = self.models[\"t\"](item[\"img\"].to(self.configs.device)) # we want all images (task loss and consistency loss)               \n",
    "                self.computing_unit[\"t_train\"].run_batch(configs=self.configs, criterions=self.criterions, model_output=t_model_output[labelled_batch_ids], ground_truth=item[\"msk\"][labelled_batch_ids])\n",
    "            \n",
    "            \n",
    "                \n",
    "            if self.configs.print_todos: \n",
    "                print(\"run_meanteacher todo: the uncertainty mask is not good - for consistency loss\")    \n",
    "            # uncertainy_mask = self.um.run(self.models[\"t\"], item[\"img\"].to(self.configs.device))\n",
    "            \n",
    "            # =============================================================================\n",
    "            # Consistency Loss\n",
    "            # =============================================================================\n",
    "            \n",
    "            def sigmoid_rampup(current, rampup_length):\n",
    "                # calculate the ramp-up coefficient of the consistency constraint\n",
    "                # returns vavlue between 0 and 1\n",
    "                # https://github.com/HiLab-git/SSL4MIS/blob/master/code/utils/ramps.py\n",
    "                # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "                if rampup_length == 0:\n",
    "                    return 1.0\n",
    "                else:\n",
    "                    current = np.clip(current, 0.0, rampup_length)\n",
    "                    phase = 1.0 - current / rampup_length\n",
    "                    return float(np.exp(-5.0 * phase * phase))\n",
    "                        \n",
    "            cons_weight = sigmoid_rampup(i_epoch, self.configs.rampup_length)\n",
    "            \n",
    "            if self.writer:\n",
    "                self.writer.add_scalars(\"info/weight\", {\"cons\" : cons_weight}, self.i_item_total)\n",
    "            \n",
    "            # calculate the consistency constraint from the teacher model to the student model\n",
    "            if self.configs.print_todos: \n",
    "                print(\"run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\")\n",
    "            t_pseudo_gt = t_model_output # Variable(t_model_output.detach().data, requires_grad=False)\n",
    "            \n",
    "            if unlabelled_batch_ids:\n",
    "                cons_loss = self.criterions[\"cons\"](s_model_output[unlabelled_batch_ids], t_pseudo_gt[unlabelled_batch_ids])\n",
    "            else:\n",
    "                cons_loss = self.zero_tensor\n",
    "                \n",
    "            if self.writer:\n",
    "                self.writer.add_scalars(\"info/loss\", {\"cons\"  : cons_loss,\n",
    "                                                      \"shape\" : self.computing_unit[\"s_train\"].shape_loss,\n",
    "                                                      \"pixel\" : self.computing_unit[\"s_train\"].pixel_loss}, self.i_item_total)\n",
    "                            \n",
    "            # this is a float\n",
    "            threshold = torch.tensor( ( 0.75 + (0.25*sigmoid_rampup(i_epoch, self.configs.rampup_length)) * np.log(2) ) ).to(self.configs.device)\n",
    "            \n",
    "            #print(uncertainy_mask)\n",
    "            #print(threshold)\n",
    "            \n",
    "            # this is not on the gpu ... now it is ...\n",
    "            #bin_uncertainy_mask = (uncertainy_mask.to(self.configs.device) < threshold) # .float()\n",
    "            \n",
    "            #cons_loss_masked = torch.sum(bin_uncertainy_mask * cons_loss) / (2*torch.sum(bin_uncertainy_mask)+self.configs.eps)\n",
    "\n",
    "            # =============================================================================\n",
    "            # Backprop for student model\n",
    "            # =============================================================================\n",
    "            \n",
    "            # combined loss\n",
    "            \n",
    "            if self.writer:\n",
    "                self.writer.add_scalars(\"info/loss\", {\"cons\" : cons_weight}, self.i_item_total)\n",
    "            \n",
    "            # supervised loss + consistency loss * consistency weight\n",
    "            loss = self.computing_unit[\"s_train\"].task_loss #  + cons_loss_masked * cons_weight\n",
    "            \n",
    "            self.optims[\"s\"].zero_grad()\n",
    "            loss.backward()\n",
    "            self.optims[\"s\"].step()\n",
    "            \n",
    "            self.lrsers[\"s\"].step()\n",
    "            if self.writer:\n",
    "                self.writer.add_scalars(\"info/lr\", {\"s_train\" : self.lrsers[\"s\"].get_last_lr()[0]}, self.i_item_total)\n",
    "            \n",
    "            # =============================================================================\n",
    "            # EMA for teacher model\n",
    "            # https://github.com/HiLab-git/SSL4MIS/blob/master/code/train_uncertainty_aware_mean_teacher_2D.py\n",
    "            # https://github.com/ZHKKKe/PixelSSL/blob/master/pixelssl/ssl_algorithm/ssl_mt.py\n",
    "            # =============================================================================\n",
    "            local_ema_decay = min(1 - 1 / (self.i_item_total + 1), self.configs.ema_decay)\n",
    "            for t_param, s_param in zip(self.models[\"t\"].parameters(), self.models[\"s\"].parameters()):\n",
    "                # model_weights = decay * model_weights + (1 - decay) * new_model_weights\n",
    "                t_param.data = t_param.data * local_ema_decay +  s_param.data * (1 - local_ema_decay)\n",
    "        \n",
    "            self.i_item_total += 1\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Epoch Process (basically logging)\n",
    "        # =============================================================================\n",
    "        self.computing_unit[\"s_train\"].run_epoch(i_epoch=i_epoch)\n",
    "        self.computing_unit[\"t_train\"].run_epoch(i_epoch=i_epoch)\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Epoch LR Scheduler = reset\n",
    "        # =============================================================================\n",
    "        self.lrsers[\"s\"] = torch.optim.lr_scheduler.CosineAnnealingLR(self.optims[\"s\"], T_max=self.steps)\n",
    "        \n",
    "        print(\"lr\")\n",
    "        print(self.lrsers[\"s\"].get_last_lr())\n",
    "        print(self.lrsers[\"s\"].get_last_lr()[0])\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Epoch log and reset (training)\n",
    "        # =============================================================================\n",
    "        \n",
    "        print(\"training task loss\")\n",
    "        print(self.computing_unit[\"s_train\"].epoch_collector[\"loss\"])\n",
    "        \n",
    "        print(\"training fscore\")\n",
    "        print(self.computing_unit[\"s_train\"].epoch_collector[\"fscore\"])\n",
    "        \n",
    "        self.computing_unit[\"s_train\"].log(csv_file_path = self.configs.result_path)\n",
    "        self.computing_unit[\"t_train\"].log(csv_file_path = self.configs.result_path)\n",
    "        self.computing_unit[\"s_train\"].reset_epoch()\n",
    "        self.computing_unit[\"t_train\"].reset_epoch()\n",
    "        \n",
    "        logging.info(\"Traning of epoch %d done\" % i_epoch)\n",
    "\n",
    "    def run_validation(self, i_epoch):\n",
    "        self.models[\"s\"].eval()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # for each batch\n",
    "        # =============================================================================\n",
    "\n",
    "        for i_item, item in enumerate(self.dataloader[\"val\"]):\n",
    "            # =============================================================================\n",
    "            # Student\n",
    "            # =============================================================================\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # get predictions of student model for all images\n",
    "                s_model_output = self.models[\"s\"](item[\"img\"].to(self.configs.device)) # we want all images (task loss and consistency loss)\n",
    "                \n",
    "                # should be based on loss function!!!!! - can we do all of them in the loss function? this is annoying\n",
    "                # s_model_output = torch.nn.functional.softmax(s_model_output, dim=1) # I DON'T KNOOOOW\n",
    "\n",
    "                # run batch for student model\n",
    "                self.computing_unit[\"s_val\"].run_batch(configs=self.configs, criterions=self.criterions, model_output=s_model_output, ground_truth=item[\"msk\"])\n",
    "        \n",
    "        \n",
    "        # =============================================================================\n",
    "        # Epoch Process (basically logging)\n",
    "        # =============================================================================\n",
    "        self.computing_unit[\"s_val\"].run_epoch(i_epoch=i_epoch)\n",
    "\n",
    "        # =============================================================================\n",
    "        # Epoch log and reset (validation)\n",
    "        # =============================================================================\n",
    "        \n",
    "        print(\"validation task loss\")\n",
    "        print(self.computing_unit[\"s_val\"].epoch_collector[\"loss\"])\n",
    "        \n",
    "        print(\"validation fscore\")\n",
    "        print(self.computing_unit[\"s_val\"].epoch_collector[\"fscore\"])\n",
    "        \n",
    "        self.computing_unit[\"s_val\"].log(csv_file_path = self.configs.result_path)\n",
    "        self.computing_unit[\"s_val\"].reset_epoch()\n",
    "        \n",
    "        logging.info(\"Validation of epoch %d done\" % i_epoch)\n",
    "        \n",
    "    def run_final_evaluation(self, i_epoch):\n",
    "                \n",
    "        model = self.models[\"s\"]\n",
    "        \n",
    "        paths = glob.glob(os.path.join(self.configs.save_checkpoint_path, f'checkpoint_{i_epoch}_*.ckpt'))\n",
    "        \n",
    "        model.load_state_dict(torch.load(paths[0])['s_model'])\n",
    "        model.eval()\n",
    "\n",
    "        for i_item, item in enumerate(self.dataloader[\"val\"]):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # get predictions of student model for all images\n",
    "                s_model_output = self.models[\"s\"](item[\"img\"].to(self.configs.device)) # we want all images (task loss and consistency loss)\n",
    "                \n",
    "                # should be based on loss function!!!!! - can we do all of them in the loss function? this is annoying\n",
    "                # s_model_output = torch.nn.functional.softmax(s_model_output, dim=1) # I DON'T KNOOOOW\n",
    "\n",
    "                # run batch for student model\n",
    "                self.computing_unit[\"s_test\"].run_batch(configs=self.configs, criterions=self.criterions, model_output=s_model_output, ground_truth=item[\"msk\"])\n",
    "                \n",
    "                print(s_model_output.shape)\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(item[\"img\"][0].permute(1, 2, 0))\n",
    "                \n",
    "                \n",
    "                _, highest_class = torch.max(s_model_output, 1)  \n",
    "                \n",
    "                print(highest_class.shape)\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(highest_class[0])\n",
    "                \n",
    "            break\n",
    "        \n",
    "        \n",
    "        # =============================================================================\n",
    "        # Epoch Process (basically logging)\n",
    "        # =============================================================================\n",
    "        self.computing_unit[\"s_test\"].run_epoch(i_epoch=i_epoch)\n",
    "\n",
    "        # =============================================================================\n",
    "        # Epoch log and reset (validation)\n",
    "        # =============================================================================\n",
    "        \n",
    "        print(\"Test task loss\")\n",
    "        print(self.computing_unit[\"s_test\"].epoch_collector[\"loss\"])\n",
    "        \n",
    "        print(\"Test fscore\")\n",
    "        print(self.computing_unit[\"s_test\"].epoch_collector[\"fscore\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def log(self, i_epoch):\n",
    "        # =============================================================================\n",
    "        # Save checkpoint\n",
    "        # =============================================================================\n",
    "                \n",
    "        # this cannot woooooooooooooork\n",
    "        # we save the best already in the epoch .... ?\n",
    "            \n",
    "        if i_epoch > 0: #  and self.computing_unit[\"s_val\"].epoch_collector[\"fscore\"] > self.computing_unit[\"s_val\"].top[\"highest_fscore\"]:\n",
    "            checkpoint = {\n",
    "                'name': self.configs.prefix,\n",
    "                'epoch': i_epoch, \n",
    "                's_model': self.models[\"s\"].state_dict(),\n",
    "                't_model': self.models[\"t\"].state_dict(),\n",
    "                's_optim': self.optims[\"s\"].state_dict(),\n",
    "                's_lrer': self.lrsers[\"s\"].state_dict()\n",
    "            }\n",
    "\n",
    "            checkpoint_path = os.path.join(self.configs.save_checkpoint_path, f'checkpoint_{i_epoch}_{self.computing_unit[\"s_val\"].epoch_collector[\"fscore\"]}.ckpt')\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            \n",
    "            # self.computing_unit[\"s_val\"].top[\"fscore\"] = self.computing_unit[\"s_val\"].epoch_collector[\"fscore\"]\n",
    "            \n",
    "            logging.info(\"Saved model at epoch %d\" % i_epoch)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0440659-d578-4ae9-a2d5-88331279c1cc",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9881983-bf04-4df3-a67e-6ecf4d8d9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasceyence/data_prep\\\\mt_data_ichallenge_amd.csv', 'datasceyence/data_prep\\\\mt_data_ichallenge_glaucoma.csv', 'datasceyence/data_prep\\\\mt_data_ichallenge_non_amd.csv', 'datasceyence/data_prep\\\\mt_data_ichallenge_unlabelled.csv']\n",
      "**************************************************\n",
      "all labels [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "this needs some error catching\n",
      "steps 36\n",
      "shape loss tensor(0.4735, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.8162, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(1.9331)\n",
      "pixel loss tensor(0.7504)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.6936, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7190, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.6141)\n",
      "pixel loss tensor(0.8792)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.6052, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7570, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.3738)\n",
      "pixel loss tensor(0.8765)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.5931, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7663, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.4038)\n",
      "pixel loss tensor(0.8724)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.6157, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7266, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.5467)\n",
      "pixel loss tensor(0.8768)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.5996, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7292, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.4232)\n",
      "pixel loss tensor(0.8759)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.5603, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7488, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.3470)\n",
      "pixel loss tensor(0.8750)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.5514, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7417, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.3718)\n",
      "pixel loss tensor(0.8746)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.5161, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7573, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.3446)\n",
      "pixel loss tensor(0.8748)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.5606, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7121, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.5879)\n",
      "pixel loss tensor(0.8819)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.6844, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7256, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.6150)\n",
      "pixel loss tensor(0.8787)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.5531, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7342, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.2945)\n",
      "pixel loss tensor(0.8728)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.6231, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7142, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.4339)\n",
      "pixel loss tensor(0.8775)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n",
      "shape loss tensor(0.5623, grad_fn=<MulBackward0>)\n",
      "pixel loss tensor(0.7553, grad_fn=<DivBackward0>)\n",
      "shape loss tensor(4.2766)\n",
      "pixel loss tensor(0.8731)\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask - for consistency loss\n",
      "run_meanteacher todo: the uncertainty mask is not good - for consistency loss\n",
      "run_meanteacher todo: here inside we have to do some beautifying of the mask\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10164\\3677303549.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi_epoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10164\\4158095471.py\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(self, i_epoch)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# get predictions of student model for all images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0ms_model_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"s\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"img\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# we want all images (task loss and consistency loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# run batch for student model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\segmentation_models_pytorch\\base\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\segmentation_models_pytorch\\encoders\\efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     71\u001b[0m                     \u001b[0mdrop_connect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mblock_number\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mblock_number\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_connect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\efficientnet_pytorch\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_block_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_ratio\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bn0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expand_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_depthwise_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\efficientnet_pytorch\\utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mMemoryEfficientSwish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSwishImplementation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mSwish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\efficientnet_pytorch\\utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, i)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Configs\n",
    "configs = Configs(reset=True)\n",
    "configs.log()\n",
    "\n",
    "# Run\n",
    "run = RoutineMT(configs)\n",
    "\n",
    "for i_epoch in range(configs.epochs):\n",
    "\n",
    "    run.run_training(i_epoch=i_epoch)\n",
    "    run.run_validation(i_epoch=i_epoch)\n",
    "    run.log(i_epoch=i_epoch)\n",
    "\n",
    "    if i_epoch == 5 and self.reduced_data:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86169d2f-0bb9-43f6-afa9-de29396c771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "configs = Configs(reset=False)\n",
    "configs.log()\n",
    "\n",
    "# Run\n",
    "run = RoutineMT(configs)\n",
    "\n",
    "i_epoch = 5\n",
    "\n",
    "run.run_final_evaluation(i_epoch=i_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2bff7e5-7f52-400a-9bff-396ea2fb6a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-234f32380b526f38\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-234f32380b526f38\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6018;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tensorboard --logdir projects\\decentnet\\results\\exp1_mt\\logs port=6005   \n",
    "# http://localhost:6005/?runColorGroup=regex%3A(_s_train|_s_val|_t_)#timeseries\n",
    "\n",
    "# regex: (_s_train|_s_val|_t_)\n",
    "\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "%reload_ext tensorboard\n",
    "\n",
    "\n",
    "%tensorboard --logdir C:/Users/Prinzessin/projects/decentnet/results/exp1_mt_dice/logs --port 6018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e4f5d-b8f3-42d8-9fc5-c7ccd9065b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c65a2-7421-4941-87f9-15c57e332fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
