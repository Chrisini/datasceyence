{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75832305-6091-4f3c-980a-53004f01a000",
   "metadata": {},
   "source": [
    "# ð•Šð•–ð•žð•š-ð•Šð•¦ð•¡ð•–ð•£ð•§ð•šð•¤ð•–ð•• ð•ð•–ð•’ð•£ð•Ÿð•šð•Ÿð•˜ ð•¦ð•¤ð•šð•Ÿð•˜ ð•„ð•–ð•’ð•Ÿ ð•‹ð•–ð•’ð•”ð•™ð•–ð•£\n",
    "\n",
    "Implementation of pixel-wise Mean Teacher (MT)\n",
    "    \n",
    "This method is proposed in the paper: \n",
    "    'Mean Teachers are Better Role Models:\n",
    "        Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results'\n",
    "This implementation only supports Gaussian noise as input perturbation, and the two-heads\n",
    "outputs trick is not available.\n",
    "\n",
    "Source:\n",
    "https://github.com/ZHKKKe/PixelSSL/blob/master/pixelssl/ssl_algorithm/ssl_mt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6f142-82ee-4f0a-87f8-e2e9eeeb89b6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f3932a-744a-49ca-a076-f12749cf3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "from helper.dataset.mean_teacher import *\n",
    "# from helper.model.mean_teacher import * \n",
    "from helper.sampler.mixed_batch import *\n",
    "from helper.model.block.noise_block import GaussianNoiseBlock\n",
    "from helper.compute.compute_bin_seg import BCE_BinSeg_CU\n",
    "from helper.compute.loss.shape import ShapeLoss\n",
    "from helper.compute.loss.dice import DiceLoss\n",
    "\n",
    "#from pixelssl.utils import REGRESSION, CLASSIFICATION\n",
    "#from pixelssl.utils import logger, cmd, tool\n",
    "#from pixelssl.nn import func\n",
    "#from pixelssl.nn.module import patch_replication_callback, GaussianNoiseLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a707779-ab4d-46a4-ab5b-e123f91b827a",
   "metadata": {},
   "source": [
    "# Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d1aa89-5640-4263-8051-96580d7d0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # =============================================================================\n",
    "        # \n",
    "        # =============================================================================\n",
    "        \n",
    "        self.prefix = \"tmp\"\n",
    "        self.reduced_data = False\n",
    "        \n",
    "        # smp unet ++ parameters\n",
    "        self.encoder_name = \"efficientnet-b7\"\n",
    "        self.encoder_weights = \"imagenet\"\n",
    "        self.in_channels =  1\n",
    "        self.classes = 1\n",
    "        \n",
    "        self.epochs = 100\n",
    "        \n",
    "        self.gaussian_noise = 0.1 # None\n",
    "        \n",
    "        self.ema_decay = 0.999 # default value\n",
    "        \n",
    "        # Sizes of tensors must match except in dimension 1\n",
    "        # I solved the issue by resizing all the images size divisible to 32.\n",
    "        self.image_size = 128 # 512\n",
    "        \n",
    "        self.num_workers = 0\n",
    "        self.iterations = 50\n",
    "        \n",
    "        # batch size = n_samples_per_class_per_batch * classes\n",
    "        # for mixed batch sampling\n",
    "        self.n_samples_per_class_per_batch = 1\n",
    "        \n",
    "        self.lbs = 3 #  self.args.labeled_batch_size # .... remove this eventually and replace\n",
    "\n",
    "        # optimisation\n",
    "        self.optimiser = \"sgd\"\n",
    "        self.learning_rate = 0.01\n",
    "        self.min_learning_rate = 0.0001\n",
    "        self.weight_decay = 1e-4\n",
    "        self.momentum = 0.9\n",
    "        \n",
    "        # self.is_epoch_lrer = True # epoch or batch based learning rate updater\n",
    "        \n",
    "        self.dropout = None\n",
    "        \n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        # =============================================================================\n",
    "        # Paths\n",
    "        # =============================================================================\n",
    "        \n",
    "        self.base_path = r\"C:/Users/Prinzessin/projects/decentnet\"\n",
    "        if not os.path.exists(self.base_path):\n",
    "            os.makedirs(self.base_path)\n",
    "        os.chdir(self.base_path) # this is now the main directory !!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        self.csv_filenames = glob.glob(r\"datasceyence/data_prep/mt_*.csv\")\n",
    "        \n",
    "        print(self.csv_filenames)\n",
    "        \n",
    "        # input\n",
    "        self.load_checkpoint_file = None\n",
    "        \n",
    "        # all csv files used for run_mean_teacher.ipybn\n",
    "        #self.csv_data_paths = [\n",
    "        #    {\"path\" : r\"data/data_ichallenge_amd.csv\"}, \n",
    "        #    {\"path\" : r\"data/data_ichallenge_non_amd.csv\"}\n",
    "        #]\n",
    "        \n",
    "        # output\n",
    "        self.logger_path = f\"results/{self.prefix}\"\n",
    "        if not os.path.exists(self.logger_path):\n",
    "            os.makedirs(self.logger_path)\n",
    "            \n",
    "        self.save_checkpoint_path = f\"results/{self.prefix}/ckpts\"\n",
    "        if not os.path.exists(self.save_checkpoint_path):\n",
    "            os.makedirs(self.save_checkpoint_path)\n",
    "            \n",
    "        \n",
    "    def log(self):\n",
    "        # =============================================================================\n",
    "        # save all class variables to file \"configs.txt\"\n",
    "        # =============================================================================\n",
    "        c = pd.DataFrame.from_dict({'key': self.__dict__.keys(), 'value': self.__dict__.values()})\n",
    "        c.to_csv(os.path.join(self.logger_path, \"configs.txt\"), sep=':', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c64cbd-0bd5-4220-9f34-b7ff564d21ae",
   "metadata": {},
   "source": [
    "# Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7605a7f6-d147-4551-b403-928659086b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoutineMT:\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(RoutineMT, self).__init__()\n",
    "        \n",
    "        self.configs = configs\n",
    "        \n",
    "        self.prefix = configs.prefix\n",
    "        self.ema_decay = configs.ema_decay\n",
    "        \n",
    "        self.load_ckpt = torch.load(configs.load_checkpoint_file) if configs.load_checkpoint_file is not None else None\n",
    "        \n",
    "        self.step_counter = 0\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Models\n",
    "        # =============================================================================\n",
    "        s_model = smp.UnetPlusPlus(\n",
    "                        encoder_name=self.configs.encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights=self.configs.encoder_weights,  # use `imagenet` pre-trained weights for encoder initialization\n",
    "                        in_channels=self.configs.in_channels,          # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                        classes=self.configs.classes,                  # model output channels (number of classes in your dataset)\n",
    "                    )\n",
    "        \n",
    "        t_model = smp.UnetPlusPlus(\n",
    "                        encoder_name=self.configs.encoder_name,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "                        encoder_weights=self.configs.encoder_weights,  # use `imagenet` pre-trained weights for encoder initialization\n",
    "                        in_channels=self.configs.in_channels,          # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "                        classes=self.configs.classes,                  # model output channels (number of classes in your dataset)\n",
    "                    )\n",
    "        # detach the teacher model\n",
    "        for param in t_model.parameters():\n",
    "            param.detach_()\n",
    "            \n",
    "        self.models = {'s': s_model, \n",
    "                       't': t_model}\n",
    "        \n",
    "        # add gaussian noise\n",
    "        # currently not in use\n",
    "        self.gaussian_noiser = GaussianNoiseBlock(self.configs.gaussian_noise).cuda()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Computing Units\n",
    "        # =============================================================================\n",
    "        self.computing_unit = {\n",
    "            \"s\" : BCE_BinSeg_CU(),\n",
    "            \"t\" : BCE_BinSeg_CU()\n",
    "        }\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Optimisers\n",
    "        # =============================================================================\n",
    "        self.optims = {'s': torch.optim.SGD(self.models[\"s\"].parameters(), lr=self.configs.learning_rate, momentum=self.configs.momentum) # optimizer_funcs[0](self.models[\"s\"].module.param_groups)\n",
    "                          }\n",
    "\n",
    "        # =============================================================================\n",
    "        # Learning rate schedulers\n",
    "        # =============================================================================\n",
    "        self.lrsers = {'s': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optims[\"s\"], \n",
    "                                                                              T_0 = 32, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.configs.min_learning_rate\n",
    "                                                                               )\n",
    "                      \n",
    "                     } # lrer_funcs[0](self.optimizers['s_optimizer'])\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Loss functions\n",
    "        # =============================================================================\n",
    "        # TODO: support more types of the consistency criterion\n",
    "        # something with head and each head has a loss function attached??\n",
    "        self.criterions = {'s_shape': ShapeLoss(), # criterion_funcs[0](self.args),\n",
    "                           's_pixel': DiceLoss(mode=\"BINARY_MODE\"), \n",
    "                           # TODO\n",
    "                           # BINARY_MODE MULTICLASS_MODE - the loss wants a different encoding ...\n",
    "                           # ground_truth = torch.nn.functional.one_hot(ground_truth, num_classes)  # N,H*W -> N,H*W, C\n",
    "                           # ground_truth = ground_truth.permute(0, 2, 1)  # N, C, H*W\n",
    "                           # RuntimeError: one_hot is only applicable to index tensor.\n",
    "                           'cons': torch.nn.MSELoss()\n",
    "                          }\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Datasets: train, val, test\n",
    "        # =============================================================================\n",
    "                \n",
    "        train_set = MeanTeacherTrainDataset(mode=\"train\", channels=self.configs.in_channels, image_size=self.configs.image_size, csv_filenames=self.configs.csv_filenames)\n",
    "        train_mbs = MixedBatchSampler(train_set.get_mbs_labels(), n_samples_per_class_per_batch=self.configs.n_samples_per_class_per_batch)\n",
    "        \n",
    "        val_set = MeanTeacherValDataset(mode=\"val\", channels=self.configs.in_channels, image_size=self.configs.image_size, csv_filenames=self.configs.csv_filenames)\n",
    "                \n",
    "        self.dataloader = {\"train\" : DataLoader(train_set, batch_sampler=train_mbs),\n",
    "                           \"val\" :   DataLoader(val_set)\n",
    "                          }\n",
    "          \n",
    "        \n",
    "        # =============================================================================\n",
    "        # Resume training\n",
    "        # =============================================================================\n",
    "        if self.load_ckpt:\n",
    "            self.models[\"s\"].load_state_dict(self.load_ckpt['s_model'])\n",
    "            self.models[\"t\"].load_state_dict(self.load_ckpt['t_model'])\n",
    "            self.optims[\"s\"].load_state_dict(self.load_ckpt['s_optimizer'])\n",
    "            self.lrsers[\"s\"].load_state_dict(self.load_ckpt['s_lrer'])\n",
    "\n",
    "\n",
    "    \n",
    "    def run_training(self, epoch):\n",
    "        \n",
    "        mode=\"train\"\n",
    "        \n",
    "        self.models[\"s\"].train()\n",
    "        self.models[\"t\"].train()\n",
    "        \n",
    "        for idx, item in enumerate(self.dataloader[mode]):\n",
    "            # =============================================================================\n",
    "            # Process Batch\n",
    "            # =============================================================================\n",
    "                        \n",
    "            # reset student optimiser\n",
    "            self.optims[\"s\"].zero_grad()\n",
    "            \n",
    "            s_model_output = self.models[\"s\"](item[\"img\"]) # img_t\n",
    "            \n",
    "            s_model_output = torch.nn.functional.softmax(s_model_output, dim=0) # I DON'T KNOOOOW\n",
    "            \n",
    "            print(\"s model\")\n",
    "            print(s_model_output)\n",
    "            print(item[\"msk\"])\n",
    "            print(s_model_output.shape)\n",
    "            print(item[\"msk\"].shape)\n",
    "            print(\"next\")\n",
    "            \n",
    "            # run student batch\n",
    "            s_epoch_collector = self.computing_unit[\"s\"].run_batch(configs=self.configs, criterions=self.criterions, model_output=s_model_output, ground_truth=item[\"msk\"], mode=mode)\n",
    "\n",
    "            print(\"sigmoid\")\n",
    "            \n",
    "            def sigmoid_rampup(current, rampup_length):\n",
    "                \"\"\" Exponential rampup from https://arxiv.org/abs/1610.02242 . \n",
    "                \"\"\"\n",
    "                if rampup_length == 0:\n",
    "                    return 1.0\n",
    "                else:\n",
    "                    current = np.clip(current, 0.0, rampup_length)\n",
    "                    phase = 1.0 - current / rampup_length\n",
    "                    return float(np.exp(-5.0 * phase * phase))\n",
    "            \n",
    "            # calculate the ramp-up coefficient of the consistency constraint\n",
    "            # use mean squared error as the consistency cost and ramp up its weight from 0 to its final value during the first 80 epochs. \n",
    "            self.step_counter += 1\n",
    "            total_steps = 5 # len(self.dataloader[\"train\"]) * 100 # self.args.cons_rampup_epochs # ????\n",
    "            cons_rampup_scale = sigmoid_rampup(self.step_counter, total_steps)\n",
    "            \n",
    "            # =============================================================================\n",
    "            # Teacher Model\n",
    "            # =============================================================================\n",
    "            \n",
    "            # forward the teacher model\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                t_model_output = self.models[\"t\"](item[\"img\"]) # img_t\n",
    "                                \n",
    "                self.computing_unit[\"t\"].run_batch(configs=self.configs, criterions=self.criterions, model_output=t_model_output, ground_truth=item[\"msk\"], mode=mode)\n",
    "                \n",
    "                \"\"\"\n",
    "                t_resulter, t_debugger = self.t_model.forward(t_inp)\n",
    "                if not 'pred' in t_resulter.keys():\n",
    "                    self._pred_err()\n",
    "                t_pred = tool.dict_value(t_resulter, 'pred')\n",
    "                t_activated_pred = tool.dict_value(t_resulter, 'activated_pred')\n",
    "            \n",
    "                # calculate 't_task_loss' for recording\n",
    "                l_t_pred = func.split_tensor_tuple(t_pred, 0, lbs)\n",
    "                l_t_inp = func.split_tensor_tuple(t_inp, 0, lbs)\n",
    "                t_task_loss = self.s_criterion.forward(l_t_pred, l_gt, l_t_inp)\n",
    "                t_task_loss = torch.mean(t_task_loss)\n",
    "                self.meters.update('t_task_loss', t_task_loss.data)\n",
    "                \"\"\"\n",
    "            # =============================================================================\n",
    "            # Consistency Loss\n",
    "            # =============================================================================\n",
    "            # calculate the consistency constraint from the teacher model to the student model\n",
    "            t_pseudo_gt = Variable(t_model_output[0].detach().data, requires_grad=False)\n",
    "\n",
    "            if True: # self.args.cons_for_labeled:\n",
    "                cons_loss = self.criterions[\"cons\"](s_model_output[0], t_pseudo_gt)\n",
    "            elif False: # self.args.unlabeled_batch_size > 0:\n",
    "                cons_loss = self.criterions[\"cons\"](s_model_output[0][lbs:, ...], t_pseudo_gt[lbs:, ...])\n",
    "            else:\n",
    "                cons_loss = self.zero_tensor\n",
    "            cons_loss = cons_rampup_scale * torch.mean(cons_loss) # self.args.cons_scale * \n",
    "\n",
    "            # =============================================================================\n",
    "            # Backprop for student model\n",
    "            # =============================================================================\n",
    "            loss = s_epoch_collector[\"loss\"] + cons_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            print(\"loss\")\n",
    "            print(loss)\n",
    "            \n",
    "            \n",
    "            self.optims[\"s\"].step()\n",
    "            \n",
    "            \n",
    "\n",
    "            # =============================================================================\n",
    "            # EMA for teacher model\n",
    "            # =============================================================================\n",
    "            # self._update_ema_variables(self.s_model, self.t_model, self.args.ema_decay, cur_step)\n",
    "            self.ema_decay = min(1 - 1 / (self.step_counter + 1), self.ema_decay)\n",
    "            for t_param, s_param in zip(self.models[\"t\"].parameters(), self.models[\"s\"].parameters()):\n",
    "                t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)\n",
    "            \n",
    "            # =============================================================================\n",
    "            # LR Scheduler (Batch)\n",
    "            # =============================================================================\n",
    "            # if not self.configs.is_epoch_lrer:\n",
    "            self.lrsers[\"s\"].step()\n",
    "        \n",
    "        \n",
    "        # =============================================================================\n",
    "        # Process Epoch\n",
    "        # =============================================================================\n",
    "        \n",
    "        self.computing_unit[\"s\"].run_epoch()\n",
    "        self.computing_unit[\"t\"].run_epoch()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # LR Scheduler (Epoch)\n",
    "        # =============================================================================\n",
    "        #if self.configs.is_epoch_lrer:\n",
    "        #    self.lrsers[\"s\"].step()\n",
    "\n",
    "    def run_validation(self, data_loader, epoch):\n",
    "        self.s_model.eval()\n",
    "        self.t_model.eval()\n",
    "        \n",
    "        # =============================================================================\n",
    "        # for each batch\n",
    "        # =============================================================================\n",
    "\n",
    "        for idx, item in enumerate(self.dataloader[\"val\"]):\n",
    "            \n",
    "            timer = time.time()\n",
    "            \n",
    "            # =============================================================================\n",
    "            # Student\n",
    "            # =============================================================================\n",
    "\n",
    "            s_resulter, s_debugger = self.s_model.forward(s_inp)\n",
    "            if not 'pred' in s_resulter.keys() or not 'activated_pred' in s_resulter.keys():\n",
    "                self._pred_err()\n",
    "            s_pred = tool.dict_value(s_resulter, 'pred')\n",
    "            s_activated_pred = tool.dict_value(s_resulter, 'activated_pred')\n",
    "\n",
    "            s_task_loss = self.s_criterion.forward(s_pred, gt, s_inp)\n",
    "            s_task_loss = torch.mean(s_task_loss)\n",
    "            self.meters.update('s_task_loss', s_task_loss.data)\n",
    "\n",
    "            # =============================================================================\n",
    "            # Teacher\n",
    "            # =============================================================================\n",
    "            \n",
    "            t_resulter, t_debugger = self.t_model.forward(t_inp)\n",
    "            if not 'pred' in t_resulter.keys() or not 'activated_pred' in t_resulter.keys():\n",
    "                self._pred_err()\n",
    "            t_pred = tool.dict_value(t_resulter, 'pred')\n",
    "            t_activated_pred = tool.dict_value(t_resulter, 'activated_pred')\n",
    "\n",
    "            t_task_loss = self.s_criterion.forward(t_pred, gt, t_inp)\n",
    "            t_task_loss = torch.mean(t_task_loss)\n",
    "            self.meters.update('t_task_loss', t_task_loss.data)\n",
    "            \n",
    "            # =============================================================================\n",
    "            # Pseudo ???\n",
    "            # =============================================================================\n",
    "\n",
    "            t_pseudo_gt = Variable(t_pred[0].detach().data, requires_grad=False)\n",
    "            cons_loss = self.cons_criterion(s_pred[0], t_pseudo_gt)\n",
    "            cons_loss = self.args.cons_scale * torch.mean(cons_loss)\n",
    "            self.meters.update('cons_loss', cons_loss.data)\n",
    "\n",
    "            #self.task_func.metrics(s_activated_pred, gt, s_inp, self.meters, id_str='student')\n",
    "            #self.task_func.metrics(t_activated_pred, gt, t_inp, self.meters, id_str='teacher')\n",
    "            \n",
    "            # =============================================================================\n",
    "            # Logger\n",
    "            # =============================================================================\n",
    "            \n",
    "            self.meters.update('batch_time', time.time() - timer)\n",
    "            if idx % self.args.log_freq == 0:\n",
    "                logger.log_info('step: [{0}][{1}/{2}]\\tbatch-time: {meters[batch_time]:.3f}\\n'\n",
    "                                '  student-{3}\\t=>\\t'\n",
    "                                's-task-loss: {meters[s_task_loss]:.6f}\\t'\n",
    "                                's-cons-loss: {meters[cons_loss]:.6f}\\n'\n",
    "                                '  teacher-{3}\\t=>\\t'\n",
    "                                't-task-loss: {meters[t_task_loss]:.6f}\\n'\n",
    "                                .format(epoch + 1, idx, len(data_loader), self.args.task, meters=self.meters))\n",
    "\n",
    "            if self.args.visualize and idx % self.args.visual_freq == 0:\n",
    "                self._visualize(epoch, idx, False, \n",
    "                                func.split_tensor_tuple(s_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(s_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_inp, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(t_activated_pred, 0, 1, reduce_dim=True),\n",
    "                                func.split_tensor_tuple(gt, 0, 1, reduce_dim=True))\n",
    "    \n",
    "        # =============================================================================\n",
    "        # Metrics\n",
    "        # =============================================================================\n",
    "        # metrics\n",
    "        metrics_info = {'student': '', 'teacher': ''}\n",
    "        for key in sorted(list(self.meters.keys())):\n",
    "            #if self.task_func.METRIC_STR in key:\n",
    "            if True:\n",
    "                for id_str in metrics_info.keys():\n",
    "                    if key.startswith(id_str):\n",
    "                        metrics_info[id_str] += '{0}: {1:.6}\\t'.format(key, self.meters[key])\n",
    "\n",
    "        logger.log_info('Validation metrics:\\n  student-metrics\\t=>\\t{0}\\n  teacher-metrics\\t=>\\t{1}\\n'\n",
    "            .format(metrics_info['student'].replace('_', '-'), metrics_info['teacher'].replace('_', '-')))\n",
    "\n",
    "    def run_cleanup(self, epoch):\n",
    "        # =============================================================================\n",
    "        # Logger\n",
    "        # =============================================================================\n",
    "        \n",
    "        # if save_model is True:\n",
    "        \n",
    "        if epoch > 5 and self.computing_unit[\"s\"].epoch_collector[\"fscore\"] > self.computing_unit[\"s\"].best[\"fscore\"]:\n",
    "            state = {\n",
    "                'name': self.prefix,\n",
    "                'epoch': epoch, \n",
    "                's_model': self.models[\"s\"].state_dict(),\n",
    "                't_model': self.models[\"t\"].state_dict(),\n",
    "                's_optim': self.optims[\"s\"].state_dict(),\n",
    "                's_lrer': self.lrsers[\"s\"].state_dict()\n",
    "            }\n",
    "\n",
    "            checkpoint = os.path.join(self.configs.save_checkpoint_path, 'checkpoint_{0}.ckpt'.format(epoch))\n",
    "            torch.save(state, checkpoint)\n",
    "            \n",
    "            self.computing_unit[\"s\"].best[\"fscore\"] = self.computing_unit[\"s\"].epoch_collector[\"fscore\"]\n",
    "            \n",
    "        self.computing_unit[\"s\"].reset_epoch()\n",
    "        self.computing_unit[\"t\"].reset_epoch()\n",
    "        \n",
    "        \n",
    "    def log(self, epoch):\n",
    "        # =============================================================================\n",
    "        # Logger\n",
    "        # =============================================================================\n",
    "        \n",
    "        self.meters.update('batch_time', time.time() - timer)\n",
    "        if idx % self.args.log_freq == 0:\n",
    "            logger.log_info('step: [{0}][{1}/{2}]\\tbatch-time: {meters[batch_time]:.3f}\\n'\n",
    "                            '  student-{3}\\t=>\\t'\n",
    "                            's-task-loss: {meters[s_task_loss]:.6f}\\t'\n",
    "                            's-cons-loss: {meters[cons_loss]:.6f}\\n'\n",
    "                            '  teacher-{3}\\t=>\\t'\n",
    "                            't-task-loss: {meters[t_task_loss]:.6f}\\n'\n",
    "                            .format(epoch + 1, idx, len(data_loader), self.args.task, meters=self.meters))\n",
    "\n",
    "        # visualization\n",
    "        if self.args.visualize and idx % self.args.visual_freq == 0:\n",
    "            self._visualize(epoch, idx, True, \n",
    "                            func.split_tensor_tuple(s_inp, 0, 1, reduce_dim=True),\n",
    "                            func.split_tensor_tuple(s_activated_pred, 0, 1, reduce_dim=True),\n",
    "                            func.split_tensor_tuple(t_inp, 0, 1, reduce_dim=True),\n",
    "                            func.split_tensor_tuple(t_activated_pred, 0, 1, reduce_dim=True),\n",
    "                            func.split_tensor_tuple(gt, 0, 1, reduce_dim=True))\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Tool Functions for SSL_MT\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _visualize(self, epoch, idx, is_train, \n",
    "                   s_inp, s_pred, t_inp, t_pred, gt):\n",
    "\n",
    "        visualize_path = self.args.visual_train_path if is_train else self.args.visual_val_path\n",
    "        out_path = os.path.join(visualize_path, '{0}_{1}'.format(epoch, idx))\n",
    "\n",
    "        #self.task_func.visualize(out_path, id_str='student', inp=s_inp, pred=s_pred, gt=gt)\n",
    "        #self.task_func.visualize(out_path, id_str='teacher', inp=t_inp, pred=t_pred, gt=gt)\n",
    "\n",
    "    def _batch_prehandle(self, inp, gt, is_train):\n",
    "        # add extra data augmentation process here if necessary\n",
    "\n",
    "        # 'self.gaussian_noiser' will add the noise to the first input element\n",
    "        s_inp_var, t_inp_var = [], []\n",
    "        for idx, i in enumerate(inp):\n",
    "            if is_train and idx == 0:\n",
    "                s_inp_var.append(self.gaussian_noiser.forward(Variable(i).cuda())) \n",
    "                t_inp_var.append(self.gaussian_noiser.forward(Variable(i).cuda())) \n",
    "            else:\n",
    "                s_inp_var.append(Variable(i).cuda()) \n",
    "                t_inp_var.append(Variable(i).cuda())\n",
    "        s_inp = tuple(s_inp_var)\n",
    "        t_inp = tuple(t_inp_var)\n",
    "        \n",
    "        gt_var = []\n",
    "        for g in gt:\n",
    "            gt_var.append(Variable(g).cuda())\n",
    "        gt = tuple(gt_var)\n",
    "\n",
    "        return s_inp, t_inp, gt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0440659-d578-4ae9-a2d5-88331279c1cc",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9de02c93-618e-464e-afc8-8da004a671e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasceyence/data_prep\\\\mt_data_ichallenge_amd.csv', 'datasceyence/data_prep\\\\mt_data_ichallenge_glaucoma.csv', 'datasceyence/data_prep\\\\mt_data_ichallenge_non_amd.csv', 'datasceyence/data_prep\\\\mt_data_ichallenge_unlabelled.csv']\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\images_AMD\\A0044.jpg\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\Disc_Masks_bin\\A0044.bmp\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\images_AMD\\A0057.jpg\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\Disc_Masks_bin\\A0057.bmp\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_Glaucoma_OD_Fovea\\images_Glaucoma\\g0007.jpg\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_Glaucoma_OD_Fovea\\Disc_Masks_bin\\g0007.bmp\n",
      "s model\n",
      "tensor([[[[0.2450, 0.2615, 0.2796,  ..., 0.2385, 0.2138, 0.2352],\n",
      "          [0.2468, 0.3105, 0.2773,  ..., 0.2630, 0.2452, 0.2448],\n",
      "          [0.2420, 0.2571, 0.2047,  ..., 0.2765, 0.3044, 0.2566],\n",
      "          ...,\n",
      "          [0.2363, 0.2081, 0.2156,  ..., 0.2776, 0.2814, 0.2579],\n",
      "          [0.2406, 0.2228, 0.2137,  ..., 0.3294, 0.2736, 0.2420],\n",
      "          [0.2514, 0.2804, 0.2456,  ..., 0.2392, 0.2560, 0.2451]]],\n",
      "\n",
      "\n",
      "        [[[0.2405, 0.2302, 0.2457,  ..., 0.2349, 0.2605, 0.2426],\n",
      "          [0.2445, 0.2242, 0.2601,  ..., 0.2334, 0.2662, 0.2531],\n",
      "          [0.2686, 0.2585, 0.2527,  ..., 0.2770, 0.2449, 0.2531],\n",
      "          ...,\n",
      "          [0.2745, 0.2389, 0.2428,  ..., 0.2289, 0.2222, 0.2141],\n",
      "          [0.2059, 0.2315, 0.1609,  ..., 0.2400, 0.2195, 0.2606],\n",
      "          [0.2381, 0.2814, 0.2204,  ..., 0.2573, 0.2645, 0.2437]]],\n",
      "\n",
      "\n",
      "        [[[0.2333, 0.2512, 0.2548,  ..., 0.2523, 0.2761, 0.2569],\n",
      "          [0.2464, 0.2725, 0.2594,  ..., 0.2380, 0.2660, 0.2611],\n",
      "          [0.2556, 0.2796, 0.2477,  ..., 0.2390, 0.2812, 0.2784],\n",
      "          ...,\n",
      "          [0.2500, 0.2340, 0.2250,  ..., 0.2637, 0.2639, 0.2784],\n",
      "          [0.2193, 0.2161, 0.2235,  ..., 0.2503, 0.2237, 0.2275],\n",
      "          [0.2413, 0.2052, 0.2652,  ..., 0.2850, 0.2594, 0.2322]]],\n",
      "\n",
      "\n",
      "        [[[0.2812, 0.2571, 0.2199,  ..., 0.2743, 0.2496, 0.2653],\n",
      "          [0.2623, 0.1929, 0.2033,  ..., 0.2657, 0.2227, 0.2409],\n",
      "          [0.2338, 0.2048, 0.2949,  ..., 0.2075, 0.1696, 0.2119],\n",
      "          ...,\n",
      "          [0.2392, 0.3190, 0.3166,  ..., 0.2298, 0.2325, 0.2496],\n",
      "          [0.3342, 0.3297, 0.4019,  ..., 0.1802, 0.2832, 0.2699],\n",
      "          [0.2692, 0.2330, 0.2688,  ..., 0.2185, 0.2201, 0.2789]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "torch.Size([4, 1, 128, 128])\n",
      "torch.Size([4, 1, 128, 128])\n",
      "next\n",
      "sigmoid\n",
      "loss\n",
      "tensor(0.3433, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\AppData\\Local\\Temp\\ipykernel_21436\\2947529605.py:212: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\python_arg_parser.cpp:1420.)\n",
      "  t_param.data.mul_(self.ema_decay).add_(1 - self.ema_decay, s_param.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\images_AMD\\A0063.jpg\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\Disc_Masks_bin\\A0063.bmp\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_Glaucoma_OD_Fovea\\images_Glaucoma\\g0040.jpg\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_Glaucoma_OD_Fovea\\Disc_Masks_bin\\g0040.bmp\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\images_AMD\\A0058.jpg\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\Disc_Masks_bin\\A0058.bmp\n",
      "s model\n",
      "tensor([[[[0.2520, 0.2481, 0.2803,  ..., 0.2503, 0.2835, 0.2467],\n",
      "          [0.2733, 0.2980, 0.2621,  ..., 0.1942, 0.2235, 0.2448],\n",
      "          [0.2757, 0.3023, 0.2656,  ..., 0.2196, 0.2918, 0.2940],\n",
      "          ...,\n",
      "          [0.2374, 0.2529, 0.2358,  ..., 0.2951, 0.3170, 0.2565],\n",
      "          [0.2425, 0.2690, 0.2461,  ..., 0.2561, 0.2393, 0.2160],\n",
      "          [0.2429, 0.2321, 0.2542,  ..., 0.2433, 0.2282, 0.2233]]],\n",
      "\n",
      "\n",
      "        [[[0.2565, 0.2852, 0.2087,  ..., 0.2964, 0.2561, 0.2583],\n",
      "          [0.2651, 0.2443, 0.1937,  ..., 0.2585, 0.2363, 0.2763],\n",
      "          [0.2217, 0.2141, 0.2499,  ..., 0.2631, 0.2289, 0.2289],\n",
      "          ...,\n",
      "          [0.2365, 0.2832, 0.3351,  ..., 0.2795, 0.2499, 0.2954],\n",
      "          [0.3116, 0.2790, 0.4080,  ..., 0.2872, 0.3366, 0.3124],\n",
      "          [0.2878, 0.2724, 0.2883,  ..., 0.3147, 0.2730, 0.3067]]],\n",
      "\n",
      "\n",
      "        [[[0.2449, 0.2153, 0.2492,  ..., 0.2454, 0.2349, 0.2387],\n",
      "          [0.2174, 0.2254, 0.3013,  ..., 0.2509, 0.2648, 0.2294],\n",
      "          [0.2423, 0.2468, 0.2470,  ..., 0.2424, 0.2172, 0.2447],\n",
      "          ...,\n",
      "          [0.2275, 0.1889, 0.2063,  ..., 0.2162, 0.2023, 0.2062],\n",
      "          [0.2479, 0.2453, 0.2052,  ..., 0.2710, 0.2462, 0.2485],\n",
      "          [0.2450, 0.3059, 0.2515,  ..., 0.2370, 0.2557, 0.2372]]],\n",
      "\n",
      "\n",
      "        [[[0.2466, 0.2514, 0.2618,  ..., 0.2079, 0.2255, 0.2563],\n",
      "          [0.2441, 0.2322, 0.2428,  ..., 0.2964, 0.2754, 0.2494],\n",
      "          [0.2603, 0.2368, 0.2375,  ..., 0.2749, 0.2621, 0.2324],\n",
      "          ...,\n",
      "          [0.2986, 0.2751, 0.2227,  ..., 0.2091, 0.2309, 0.2419],\n",
      "          [0.1980, 0.2066, 0.1407,  ..., 0.1857, 0.1778, 0.2231],\n",
      "          [0.2243, 0.1897, 0.2060,  ..., 0.2051, 0.2432, 0.2328]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "torch.Size([4, 1, 128, 128])\n",
      "torch.Size([4, 1, 128, 128])\n",
      "next\n",
      "sigmoid\n",
      "loss\n",
      "tensor(0.3644, grad_fn=<AddBackward0>)\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\images_AMD\\A0082.jpg\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\Disc_Masks_bin\\A0082.bmp\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_Glaucoma_OD_Fovea\\images_Glaucoma\\g0015.jpg\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_Glaucoma_OD_Fovea\\Disc_Masks_bin\\g0015.bmp\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\images_AMD\\A0061.jpg\n",
      "C:\\Users\\Prinzessin\\projects\\image_data\\iChallenge_AMD_OD_Fovea_lesions\\Disc_Masks_bin\\A0061.bmp\n",
      "s model\n",
      "tensor([[[[0.2555, 0.2250, 0.2079,  ..., 0.2620, 0.3018, 0.2585],\n",
      "          [0.2625, 0.1771, 0.2353,  ..., 0.3049, 0.2884, 0.2600],\n",
      "          [0.2497, 0.2531, 0.3222,  ..., 0.2570, 0.2417, 0.2339],\n",
      "          ...,\n",
      "          [0.3059, 0.3092, 0.2479,  ..., 0.2094, 0.2101, 0.2205],\n",
      "          [0.2125, 0.2364, 0.1848,  ..., 0.2001, 0.3193, 0.2856],\n",
      "          [0.2307, 0.1987, 0.2228,  ..., 0.1972, 0.2703, 0.3209]]],\n",
      "\n",
      "\n",
      "        [[[0.2096, 0.2661, 0.2795,  ..., 0.2455, 0.2379, 0.2352],\n",
      "          [0.2692, 0.2869, 0.2600,  ..., 0.2248, 0.2404, 0.2306],\n",
      "          [0.2999, 0.2982, 0.2155,  ..., 0.2469, 0.2850, 0.2964],\n",
      "          ...,\n",
      "          [0.2394, 0.2207, 0.2333,  ..., 0.2683, 0.2399, 0.2297],\n",
      "          [0.2211, 0.2406, 0.2457,  ..., 0.2958, 0.2107, 0.2450],\n",
      "          [0.2312, 0.2377, 0.2398,  ..., 0.2505, 0.2337, 0.2134]]],\n",
      "\n",
      "\n",
      "        [[[0.2608, 0.2553, 0.2592,  ..., 0.2350, 0.2612, 0.2526],\n",
      "          [0.2281, 0.3172, 0.2631,  ..., 0.2451, 0.2607, 0.2976],\n",
      "          [0.2496, 0.2834, 0.2345,  ..., 0.2417, 0.2576, 0.2750],\n",
      "          ...,\n",
      "          [0.2159, 0.2345, 0.2390,  ..., 0.2289, 0.2573, 0.2842],\n",
      "          [0.2911, 0.2668, 0.3079,  ..., 0.3017, 0.2341, 0.2421],\n",
      "          [0.2770, 0.3041, 0.2793,  ..., 0.2959, 0.2328, 0.2155]]],\n",
      "\n",
      "\n",
      "        [[[0.2741, 0.2536, 0.2533,  ..., 0.2575, 0.1991, 0.2538],\n",
      "          [0.2402, 0.2188, 0.2416,  ..., 0.2252, 0.2105, 0.2119],\n",
      "          [0.2009, 0.1652, 0.2277,  ..., 0.2545, 0.2157, 0.1946],\n",
      "          ...,\n",
      "          [0.2388, 0.2356, 0.2798,  ..., 0.2935, 0.2926, 0.2656],\n",
      "          [0.2753, 0.2562, 0.2616,  ..., 0.2024, 0.2358, 0.2273],\n",
      "          [0.2611, 0.2594, 0.2582,  ..., 0.2563, 0.2632, 0.2502]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "torch.Size([4, 1, 128, 128])\n",
      "torch.Size([4, 1, 128, 128])\n",
      "next\n",
      "sigmoid\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21436\\3253320090.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m#run.run_validation(epoch=epoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#run.run_cleanup(epoch=epoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21436\\2947529605.py\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[1;31m# =============================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms_epoch_collector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcons_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mBackwardCFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FunctionBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFunctionCtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_HookMixin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;31m# _forward_cls is defined by derived class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;31m# The user should define either backward or vjp but never both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configs\n",
    "configs = Configs()\n",
    "configs.log()\n",
    "\n",
    "# Run\n",
    "run = RoutineMT(configs)\n",
    "\n",
    "for epoch in range(configs.epochs):\n",
    "\n",
    "    run.run_training(epoch=epoch)\n",
    "    #run.run_validation(epoch=epoch)\n",
    "    #run.run_cleanup(epoch=epoch)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    routine.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1781785-eb28-4327-b98f-bc1b0c926181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "input = torch.randn(1, 2, 3)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f74619-f77d-4796-af9c-a2e19ac6fa52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
