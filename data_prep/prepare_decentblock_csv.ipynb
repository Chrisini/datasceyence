{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Chrisini/DecentNet/blob/master/main_concept_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a7844eebc0f54d99853f11f209b48d02",
    "deepnote_cell_type": "markdown",
    "id": "dVHWj5xRJ5Ze",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# â„‚ð• ð•Ÿð•”ð•–ð•¡ð•¥ ð”¼ð•©ð•¥ð•£ð•’ð•”ð•¥ð•šð• ð•Ÿ\n",
    "\n",
    "* extract patches + equivalent segmentation masks patches (these are only used to analyse the clusters for research purposes; will not be avialable in real life)\n",
    "* feature extraction\n",
    "* clustering\n",
    "* limit number of patches per cluster (randomly choose)\n",
    "* concept tiling\n",
    "\n",
    "\n",
    "Todos before running:\n",
    "* [ ] check n_patches_per_cluster\n",
    "* [ ] check whether we use the whole dataset\n",
    "* [ ] check n_clusters\n",
    "* [ ] check whether all function calles are set to true\n",
    "\n",
    "Colours: \n",
    "* 018181 teal\n",
    "* B0EFEE paleturquoise\n",
    "* CD1285 mediumvioletred\n",
    "\n",
    "Remaining questions: \n",
    "* how to compare the cluster quality for different runs\n",
    "* have to fix naming convention for: {self.prefix}/patch_arrays, patch_id\n",
    "\n",
    "Todos:\n",
    "* [ ] evaluation on amd vs non-amd (boxplot), macula vs non-macula (boxplot) - need percentage per cluster 30% amd, 70% non amd, ...\n",
    "\n",
    "\n",
    "idea (unrelated): monte carlo dropout only dropout in fc layers, don't need to put the image through the network multiple times - just once, and through the fc layers multiple times with different dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN3L9pSvCZLX"
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m-LMSsuFCYps"
   },
   "outputs": [],
   "source": [
    "# if biggest_cluster_size is smaller than n_patches_per_cluster -> n_patches_per_cluster = biggest_cluster_size\n",
    "n_patches_per_cluster = 2500 # should be 2500\n",
    "\n",
    "# number of concepts aka number of DecentBlocks\n",
    "n_clusters = 10\n",
    "\n",
    "# resize, according to training of DecentNet\n",
    "dst_img_size=500\n",
    "\n",
    "# patch size\n",
    "dst_patch_size=30\n",
    "\n",
    "# overlapping of patch extraction\n",
    "overlap_ratio_extract=0.2\n",
    "\n",
    "# only use 5 images\n",
    "reduced_data = False\n",
    "\n",
    "# patch extraction type\n",
    "# feature extraction type\n",
    "# clustering type\n",
    "\n",
    "# run functions yes/no\n",
    "run_extract_patches = True\n",
    "\n",
    "run_extract_features = False\n",
    "run_cluster_patches = False\n",
    "\n",
    "run_concepts_from_masks = True\n",
    "\n",
    "run_concept_tiling = True\n",
    "\n",
    "run_boxplot_evaluation = True\n",
    "run_deepdream_evaluation = True\n",
    "\n",
    "# prefix for experiments, use \"tmp\" for testing\n",
    "prefix = \"posneg\"\n",
    "\n",
    "# where all the data is\n",
    "base_path = r\"C:\\Users\\Prinzessin\\projects\\decentnet\" # r\"C:/Users/Christina/Documents/decentnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqaJNangMC1h"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "99c1e4aed58a42a69031e11f9cc1d468",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1670955484658,
    "id": "uT5AePK8J5Zm",
    "outputId": "190f1d83-2055-49fe-860a-405f1fca39fc",
    "source_hash": "f295e0db",
    "tags": []
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: 'C:\\\\Users\\\\Prinzessin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shufflenet_v2_x1_0, ShuffleNet_V2_X1_0_Weights\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(base_path):\n\u001b[1;32m---> 40\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(base_path)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: 'C:\\\\Users\\\\Prinzessin'"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    # install torch dreams\n",
    "    !pip install torch-dreams  --upgrade\n",
    "from torch_dreams import Dreamer\n",
    "from torch_dreams.custom_image_param import CustomImageParam\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import heapq # needed for quilting?\n",
    "import sys\n",
    "import gc # garbage collector\n",
    "import pickle # dump data\n",
    "import shutil # delete dirs \n",
    "\n",
    "# data\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import skimage\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reduction and cluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import shufflenet_v2_x1_0, ShuffleNet_V2_X1_0_Weights\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "os.chdir(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U7VQ0l9Mr2U"
   },
   "source": [
    "# Super Extractor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUPruc8MZPLT"
   },
   "outputs": [],
   "source": [
    "class ConceptExtractor():\n",
    "    # =============================================================================\n",
    "    # generic functions\n",
    "    # =============================================================================\n",
    "\n",
    "    def reset_dirs(self, result_paths: list):\n",
    "        # create new dir or remove all files from dir\n",
    "        for p in result_paths:\n",
    "            if os.path.exists(p):\n",
    "                shutil.rmtree(p)\n",
    "            os.makedirs(p)\n",
    "                \n",
    "    def reset_files(self, file_names: list):\n",
    "        for f in file_names:\n",
    "            if os.path.isfile(f):\n",
    "                os.remove(f) \n",
    "            p = os.path.dirname(f)\n",
    "            if not os.path.exists(p):\n",
    "                os.makedirs(p)\n",
    "\n",
    "    def img_resize(self, img, size):\n",
    "        # center crop\n",
    "        crop_number = int( abs(img.shape[0]-img.shape[1]) /2 )\n",
    "        if img.shape[0] < img.shape[1]:\n",
    "            img = img[0:img.shape[0], crop_number:img.shape[1]-crop_number]\n",
    "        else:\n",
    "            img = img[crop_number:img.shape[0]-crop_number, 0:img.shape[1]]\n",
    "        # resize\n",
    "        img = skimage.transform.resize(img, (size, size))\n",
    "        return img\n",
    "\n",
    "    def load_csv(self, path:str):\n",
    "        # =============================================================================\n",
    "        # csv with index (patch_id) in position zero\n",
    "        # =============================================================================\n",
    "        csv_file = pd.read_csv(path, index_col=0, sep=';')\n",
    "        return csv_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8b2f91e4b97e4eafa2784430343335ba",
    "deepnote_cell_type": "markdown",
    "id": "ErDHmsXNJ5Zq",
    "tags": []
   },
   "source": [
    "# Extract Patches\n",
    "\n",
    "Comparison of segmentation and superpixel algorithms: https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_segmentations.html\n",
    "\n",
    "* overlapping patches\n",
    "\n",
    "Todo: \n",
    "* [ ] save patches as 3D multichannel TIFF (plane, row, column, channel) \n",
    "https://scikit-image.org/skimage-tutorials/lectures/three_dimensional_image_processing.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "sL8z3sryV6cq",
    "outputId": "d605f3e2-a3f2-4bf6-f6e3-6f079c1ae6c5"
   },
   "outputs": [],
   "source": [
    "class PatchExtractor(ConceptExtractor):\n",
    "    # =============================================================================\n",
    "    # Single Image to patches - make batch size = 1 for dataloader\n",
    "    # Masks to \"patch histograms\" aka, count black pixels (single number for each mask)\n",
    "    # exclude patches that have no information in original image\n",
    "\n",
    "    # img, mask, img_patch, mask_patch\n",
    "    # =============================================================================\n",
    "    def __init__(self, dst_img_size:int=None, dst_patch_size:int=30, overlap_ratio:float=0.2, reduced_data:bool=False):\n",
    "\n",
    "        self.prefix = prefix\n",
    "        \n",
    "        self.dst_img_size = dst_img_size\n",
    "        self.dst_patch_size = dst_patch_size \n",
    "        self.overlap = int(self.dst_patch_size * overlap_ratio)\n",
    "\n",
    "        self.patches_of_this_img = [] # for plotting\n",
    "                \n",
    "        # ----------\n",
    "        # inputs\n",
    "        # ----------\n",
    "        # do not add prefix here, due to original data\n",
    "        # images\n",
    "        train_base_path =   r\"data/images/train/*/*\" # /[amd,non-amd]/file.jpg\n",
    "        image_paths = glob.glob(train_base_path)\n",
    "        # path dataset, columns: image_path, basename (of image)\n",
    "        self.path_df = pd.DataFrame({'image_path': image_paths})\n",
    "        self.path_df[\"basename\"] = self.path_df[\"image_path\"].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "        if reduced_data:\n",
    "            self.path_df = self.path_df[:5] # xxxxx\n",
    "        # segmentation masks for cluster evaluation\n",
    "        scar_path =       r\"data/masks/Lesion_Masks/scar/*\"\n",
    "        hemorrhage_path = r\"data/masks/Lesion_Masks/hemorrhage/*\"\n",
    "        exudate_path =    r\"data/masks/Lesion_Masks/exudate/*\"\n",
    "        drusen_path =     r\"data/masks/Lesion_Masks/drusen/*\"\n",
    "        others_path =     r\"data/masks/Lesion_Masks/others/*\"\n",
    "        disc_path =       r\"data/masks/Disc_Masks/*\"\n",
    "        vessel_path =     r\"data/masks/Vessel_Masks_Thresholded/*\"\n",
    "        # mask dict, get all mask paths from dirs\n",
    "        self.mask_paths = {\"lbl_Scars\" : glob.glob(scar_path), # scars mask paths\n",
    "                           \"lbl_Hemorrhages\": glob.glob(hemorrhage_path), # hemorrhage mask paths\n",
    "                           \"lbl_Exudates\": glob.glob(exudate_path), # exudate mask paths, \n",
    "                           \"lbl_Drusen\": glob.glob(drusen_path), # drusen mask paths\n",
    "                           \"lbl_Other_Lesions\": glob.glob(others_path), # other lesions mask paths\n",
    "                           \"lbl_Optic_Disc\": glob.glob(disc_path), # optic disc mask paths\n",
    "                           \"lbl_Vessels\" : glob.glob(vessel_path) # vessel mask paths\n",
    "                           }\n",
    "        \n",
    "        # ----------\n",
    "        # outputs\n",
    "        # ----------\n",
    "        self.patches_path = f\"data/{self.prefix}/patch_arrays\" # reset and fill\n",
    "        self.csv_masks_info = f\"results/{self.prefix}/masks_info.csv\" # only delete specific file\n",
    "        self.reset_dirs([self.patches_path])\n",
    "        self.reset_files([self.csv_masks_info])\n",
    "\n",
    "    def extract_sift_patches():\n",
    "        pass\n",
    "    \n",
    "    def extract_graphcut_patches():\n",
    "        pass\n",
    "\n",
    "    def extract_overlapping_patches(self, img, masks_dict:dict, basename:str):\n",
    "        # =============================================================================\n",
    "        # go through coordinates of image, overlapping\n",
    "        # extract image patches \n",
    "        #  - get image patchs\n",
    "        #  - save each image patch as jpg\n",
    "        # extract mask patches\n",
    "        #  - get mask patches (for each mask)\n",
    "        #  - save black pixel sum in patch file with patch ID (for each mask patch)\n",
    "        # =============================================================================\n",
    "\n",
    "        # reset for each image\n",
    "        self.patches_of_this_img = []\n",
    "        masks_patch_info = {}\n",
    "\n",
    "        patch_id = None\n",
    "        save_patch_path = None\n",
    "\n",
    "        patch_counter = 0\n",
    "        img_w, img_h, _ = img.shape\n",
    "        \n",
    "        # the maths: reset y\n",
    "        y_max = y_min = 0\n",
    "\n",
    "        def process_this_patch(x_minimum:int, x_maximum:int, y_minimum:int, y_maximum:int, counter:int, amd:bool):\n",
    "            # =============================================================================\n",
    "            # check whether at least half of the image patch has information (!= black)\n",
    "            # save image patch as jpg with patch id\n",
    "            # store in dict:\n",
    "            #  - patch id (id)\n",
    "            #  - mask type (header)\n",
    "            #  - percentage of black pixels of current patch and mask type (values)\n",
    "            # =============================================================================\n",
    "            # extract image patch\n",
    "            this_img_patch = img[x_minimum:x_maximum, y_minimum:y_maximum, :] # 3 channels\n",
    "            # check whether image patch contains information\n",
    "            # if amount black pixels is less than half of all pixels\n",
    "            has_info = (this_img_patch == 0).astype(int).sum() < int(len(this_img_patch)/2)\n",
    "            if has_info:\n",
    "                # generate patch id\n",
    "                patch_id = f\"{basename}_{counter}\"\n",
    "                # for plotting\n",
    "                self.patches_of_this_img.append((this_img_patch * 255).astype(np.uint8))\n",
    "                # dict in dict: patch_id, mask type, value\n",
    "                masks_patch_info[patch_id] = {}\n",
    "                for key in self.mask_paths.keys():\n",
    "                    \n",
    "                    # if mask available\n",
    "                    if masks_dict[key] is not None:\n",
    "                        # extract mask patch\n",
    "                        this_mask_patch = masks_dict[key][x_minimum:x_maximum, y_minimum:y_maximum] # 1 channel\n",
    "                        # get percentage of black pixels in image\n",
    "                        percent_black_pixels = int((  (this_mask_patch == 0).astype(int).sum() / len(this_mask_patch.flatten())  ) * 100)\n",
    "                        # round to nearest 5\n",
    "                        percent_black_pixels = 5 * round(percent_black_pixels/5)\n",
    "                        masks_patch_info[patch_id][key] = percent_black_pixels\n",
    "                    else:\n",
    "                        # otherwise percentage is zero\n",
    "                        masks_patch_info[patch_id][key] = 0\n",
    "\n",
    "                if amd == True:\n",
    "                    #masks_patch_info[patch_id][\"lbl_amd\"] = 100\n",
    "                    pass\n",
    "                else:\n",
    "                    pass\n",
    "                    #masks_patch_info[patch_id][\"lbl_amd\"] = 0\n",
    "                \n",
    "                # need to fill macula center in patch here TODO todo TODO todo TODO\n",
    "                #masks_patch_info[patch_id][\"lbl_macula\"] = 0\n",
    "\n",
    "                counter=counter+1\n",
    "\n",
    "            return counter\n",
    "            # =============================================================================\n",
    "\n",
    "        if \"A\" in basename:\n",
    "            amd = True\n",
    "        elif \"N\" in basename:\n",
    "            amd = False\n",
    "        else:\n",
    "            amd = None\n",
    "\n",
    "        # iterate through img coordinates\n",
    "        while y_max < img_h:\n",
    "            # the maths: reset x, get max y value\n",
    "            x_min = x_max = 0\n",
    "            y_max = y_min + self.dst_patch_size\n",
    "            while x_max < img_w:\n",
    "                # the maths: get max x value\n",
    "                x_max = x_min + self.dst_patch_size\n",
    "                if y_max > img_h or x_max > img_w:\n",
    "                    # the maths: border\n",
    "                    xmax = min(img_w, x_max)\n",
    "                    ymax = min(img_h, y_max)\n",
    "                    xmin = max(0, xmax - self.dst_patch_size)\n",
    "                    ymin = max(0, ymax - self.dst_patch_size)\n",
    "                    # call function\n",
    "                    patch_counter = process_this_patch(xmin, xmax, ymin, ymax, patch_counter, amd)\n",
    "                else:\n",
    "                    # call function\n",
    "                    patch_counter = process_this_patch(x_min, x_max, y_min, y_max, patch_counter, amd)\n",
    "                # the maths for width\n",
    "                x_min = x_max - self.overlap\n",
    "            # the maths for height\n",
    "            y_min = y_max - self.overlap\n",
    "\n",
    "        gc.collect()\n",
    "        return masks_patch_info\n",
    "\n",
    "    def plot(self, img):\n",
    "        # =============================================================================\n",
    "        # =============================================================================\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        row = col = 0\n",
    "        n_patches = len(self.patches_of_this_img)\n",
    "\n",
    "        if n_patches != 0:\n",
    "            n_patches_sqrt = math.ceil(math.sqrt(n_patches))\n",
    "\n",
    "            fig, ax = plt.subplots(n_patches_sqrt, n_patches_sqrt, figsize=(10, 10), sharex=True, sharey=True)\n",
    "            for i, patch in enumerate(self.patches_of_this_img):\n",
    "                if i >= n_patches:\n",
    "                    break\n",
    "                ax[row, col].imshow(patch)\n",
    "                row += 1\n",
    "                if row % n_patches_sqrt == 0:\n",
    "                    col += 1\n",
    "                    row = 0\n",
    "            for a in ax.ravel():\n",
    "                a.set_axis_off()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Decent Info: In order to plot patches, you have to run process_patches first\")\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    def load_image_and_masks(self, image_path:str, basename:str):\n",
    "        # =============================================================================\n",
    "        # read image\n",
    "        # read all masks, matching the image's basename\n",
    "        # =============================================================================\n",
    "\n",
    "        img = None\n",
    "        masks_dict = {}\n",
    "        \n",
    "        # read image - 3 channels\n",
    "        img = skimage.io.imread(image_path, as_gray=False)\n",
    "        if self.dst_img_size is not None:\n",
    "            img = self.img_resize(img, self.dst_img_size)\n",
    "\n",
    "        # read masks\n",
    "        for key, value_list in self.mask_paths.items():\n",
    "            \n",
    "            # get only mask path that contains basename\n",
    "            mask_path = next((value for value in value_list if basename in value), None)\n",
    "\n",
    "            if mask_path is not None and isinstance(mask_path, str):\n",
    "                # read mask - 1 channel                \n",
    "                masks_dict[key] = skimage.io.imread(mask_path, as_gray=True)\n",
    "                # resize\n",
    "                if self.dst_img_size is not None:\n",
    "                    masks_dict[key] = self.img_resize(masks_dict[key], self.dst_img_size)\n",
    "            else:\n",
    "                masks_dict[key] = None\n",
    "\n",
    "        gc.collect()\n",
    "        return img, masks_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_df[\"image_path\"])\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "\n",
    "        if idx == len(self): raise StopIteration \n",
    "\n",
    "        basename = self.path_df[\"basename\"][idx]\n",
    "        image_path = self.path_df[\"image_path\"][idx]\n",
    "\n",
    "        img, masks_dict = self.load_image_and_masks(image_path, basename)\n",
    "        masks_patch_info = self.extract_overlapping_patches(img, masks_dict, basename)    \n",
    "\n",
    "        # save this image patch\n",
    "        save_patch_path = os.path.join(self.patches_path, basename+\".tiff\")\n",
    "        #im = Image.fromarray((this_img_patch * 255).astype(np.uint8))\n",
    "        #im.save(save_patch_path)\n",
    "\n",
    "        stacked_img = None \n",
    "        #print(self.patches_of_this_img.shape)\n",
    "\n",
    "        stacked_img = np.stack(self.patches_of_this_img, axis=0)\n",
    "\n",
    "        # self.plot(img)\n",
    "                               \n",
    "        #print(\"plane, row, column, channel\")\n",
    "        #print(stacked_img.shape)\n",
    "        io.imsave(save_patch_path, stacked_img, plugin=\"tifffile\")\n",
    "\n",
    "        # print(save_patch_path)\n",
    "\n",
    "        return masks_patch_info\n",
    "\n",
    "def extract_patches(dst_img_size:int=None, dst_patch_size:int=30, overlap_ratio:float=0.2, reduced_data:bool=False):\n",
    "    # =============================================================================\n",
    "    # FUNCTION CALL TO EXTRACT PATCHES\n",
    "    # - iteration through dataset\n",
    "    # - save patches with ID as png (for clustering, quilting)\n",
    "    #\n",
    "    # here: save patch id and mask patch info as csv (for evaluation of clusters)\n",
    "    # =============================================================================\n",
    "\n",
    "    patch_extractor = PatchExtractor(dst_img_size=dst_img_size, dst_patch_size=dst_patch_size, overlap_ratio=overlap_ratio, reduced_data=reduced_data)\n",
    "    first_iter = True\n",
    "\n",
    "    # key = \"lbl_\" + key\n",
    "    \n",
    "    # concat the dataframes of patches of all images\n",
    "    for masks_patch_info in patch_extractor:\n",
    "        if first_iter is False:\n",
    "            df_new = pd.DataFrame.from_dict(masks_patch_info, orient='index')\n",
    "            frames = [df, df_new]\n",
    "            df = pd.concat(frames)  \n",
    "        else: \n",
    "            df = pd.DataFrame.from_dict(masks_patch_info, orient='index')\n",
    "            first_iter = False\n",
    "\n",
    "    # save patch id and percentage of black pixels for each mask\n",
    "    df.to_csv(patch_extractor.csv_masks_info, index=True, header=True, sep=';')\n",
    "\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Concepts from Clustered Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLvAUOvAi1Jm"
   },
   "source": [
    "## Feature Extractor\n",
    "1. Method: Pretrained model without fc head\n",
    "1. Method: resize and flatten (very simple)\n",
    "1. Method: Pretrained VAE (in progress)\n",
    "1. Method: Traditional feature extraction (in progress)\n",
    "1. Method: Unsupervised contrastive learning (in progress)\n",
    "\n",
    "\n",
    "PCA: If the number of features is larger than the number of samples, then you will be dealing with the â€œcurse of dimensionalityâ€, and your k-means algorithm will not produce good results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AU9OLsSHi8fQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "        \n",
    "\n",
    "class FeatureExtractorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractorModel, self).__init__()\n",
    "        self.fem = shufflenet_v2_x1_0(weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1)\n",
    "        self.fem.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.fem(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OulXB_9HrAS1"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(ConceptExtractor):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.prefix = prefix\n",
    "    \n",
    "        self.feature_vector = []\n",
    "\n",
    "        # feature extraction model\n",
    "        self.model = FeatureExtractorModel().to(device)\n",
    "        self.weights = ShuffleNet_V2_X1_0_Weights.DEFAULT\n",
    "        self.preprocess = self.weights.transforms()\n",
    "        self.model.eval()\n",
    "        \n",
    "        # ----------\n",
    "        # inputs\n",
    "        # ----------\n",
    "        patches_path = f\"data/{self.prefix}/patch_arrays/*\"\n",
    "        patch_array_paths = glob.glob(patches_path) \n",
    "\n",
    "        # path dataset, columns: patch_array_path, basename\n",
    "        self.path_df = pd.DataFrame({'patch_array_path': patch_array_paths})\n",
    "        self.path_df[\"basename\"] = self.path_df[\"patch_array_path\"].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "\n",
    "        # ----------\n",
    "        # outputs\n",
    "        # ----------\n",
    "        self.features_file_path = f\"results/{self.prefix}/features.csv\"\n",
    "        self.pca95_model_path   = f\"results/{self.prefix}/ckpts/pca95model.pkl\"\n",
    "        self.reset_files([self.features_file_path, self.pca95_model_path])\n",
    "        \n",
    "\n",
    "    def load_patches(self, patch_array_path:str):\n",
    "        # =============================================================================\n",
    "        # read patches of this img -  plane (~330), row (30), column (30), channels (3)\n",
    "        # =============================================================================\n",
    "        patches_of_this_img = skimage.io.imread(patch_array_path, as_gray=False)\n",
    "        return patches_of_this_img\n",
    "\n",
    "    def vector_from_flattening(self, patch):\n",
    "        # =============================================================================\n",
    "        # Method: use simple resize and flatten to get feature vector\n",
    "        # =============================================================================\n",
    "        feature_vector = self.img_resize(patch.copy(), 10) \n",
    "        feature_vector = feature_vector.flatten() \n",
    "        return feature_vector\n",
    "\n",
    "    def vector_from_pretrained_model(self, patch):\n",
    "        # =============================================================================\n",
    "        # Method: use pretrained model to get feature vector\n",
    "        # =============================================================================\n",
    "        processed_patch = self.preprocess(torch.tensor(patch).permute(2, 0, 1))\n",
    "        with torch.no_grad():\n",
    "            feature_vector = self.model(processed_patch.unsqueeze(0).to(device))\n",
    "        # When detach is needed, you want to call detach before cpu.     \n",
    "        feature_vector = feature_vector.detach().cpu().flatten().numpy()\n",
    "        return feature_vector\n",
    "\n",
    "    def vector_from_contrastive_learning(self, patch):\n",
    "        pass\n",
    "\n",
    "    def vector_from_autoencoder(self, patch):\n",
    "        pass\n",
    "\n",
    "    def vector_from_filters(self, patch):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # for each image patches array\n",
    "        return len(self.path_df[\"patch_array_path\"])\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "\n",
    "        if idx == len(self): raise StopIteration \n",
    "\n",
    "        patch_array_path = self.path_df[\"patch_array_path\"][idx]\n",
    "        patches = self.load_patches(patch_array_path)\n",
    "        \n",
    "        patch_array_feature_vectors = []\n",
    "        patch_array_ids = []\n",
    "        for i, patch in enumerate(patches):\n",
    "            patch_array_feature_vectors.append( self.vector_from_pretrained_model(patch) )\n",
    "            patch_array_ids.append( self.path_df[\"basename\"][idx] + \"_\" + str(i) )\n",
    "\n",
    "        return patch_array_feature_vectors, patch_array_ids\n",
    "\n",
    "def extract_features():\n",
    "    # =============================================================================\n",
    "    # FUNCTION CALL TO CLUSTER PATCHES\n",
    "    # - feature extraction\n",
    "    # - pca for dimensionality reduction\n",
    "    #\n",
    "    # here: save patch id and feature vector as csv (for clustering)\n",
    "    # =============================================================================\n",
    "    \n",
    "    feature_extractor = FeatureExtractor()\n",
    "\n",
    "    feature_vectors = []\n",
    "    patch_ids = []\n",
    "    for patch_array_feature_vectors, patch_array_ids in feature_extractor:\n",
    "        feature_vectors.extend(patch_array_feature_vectors)\n",
    "        patch_ids.extend(patch_array_ids)\n",
    "\n",
    "    # change type\n",
    "    feature_vectors = np.asarray(feature_vectors)\n",
    "    feature_vectors = feature_vectors.astype(np.float32)\n",
    "\n",
    "    # select the number of components while preserving 99% of the variability in the data\n",
    "    pca95 = PCA(n_components=0.99, random_state=19)\n",
    "    pca95.fit(feature_vectors)\n",
    "    pca95_feature_vectors = pca95.transform(feature_vectors)\n",
    "\n",
    "    with open(feature_extractor.pca95_model_path, \"wb\") as f:\n",
    "        pickle.dump(pca95, f)\n",
    "\n",
    "    df = pd.DataFrame(data=pca95_feature_vectors, \n",
    "                      columns=list(range(len(pca95_feature_vectors[0]))), \n",
    "                      index=patch_ids)\n",
    "\n",
    "    # save patch id and feature vector as csv\n",
    "    df.to_csv(feature_extractor.features_file_path, index=True, header=True, sep=';')\n",
    "\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kNahzE-RFUv"
   },
   "source": [
    "## Clustering\n",
    "\n",
    "* filtering could be done in a smarter way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFqDSmWcI-1m"
   },
   "outputs": [],
   "source": [
    "class ClusterExtractor(ConceptExtractor):\n",
    "    def __init__(self, n_patches_per_cluster:int=5000, n_clusters:int=10):\n",
    "    \n",
    "        # make sure to not use a clustering approach that removes outliers!!\n",
    "        \n",
    "        self.prefix = prefix\n",
    "        \n",
    "        self.n_patches_per_cluster = n_patches_per_cluster\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "        self.unique_labels = []\n",
    "\n",
    "        self.feature_vectors = []\n",
    "        \n",
    "\n",
    "        # ----------\n",
    "        # input\n",
    "        # ----------\n",
    "        self.features_file_path =   f\"results/{self.prefix}/features.csv\"\n",
    "        self.masks_info_path = f\"results/{self.prefix}/masks_info.csv\"\n",
    "        self.patches_path = f\"data/{self.prefix}/patch_arrays\"\n",
    "\n",
    "        self.features_csv = self.load_csv(self.features_file_path)\n",
    "        self.masks_info_csv = self.load_csv(self.masks_info_path)  \n",
    "\n",
    "        # ----------\n",
    "        # output\n",
    "        # ----------\n",
    "        # dirs\n",
    "        self.cluster_path = f\"results/{self.prefix}/clusters\" # visualised patches\n",
    "        self.reset_dirs([self.cluster_path])\n",
    "        # files\n",
    "        self.filtered_masks_info_label_path = f\"results/{self.prefix}/masks_info_label.csv\"\n",
    "        self.kmeans_model_path = f\"results/{self.prefix}/ckpts/kmeans_model.pkl\"\n",
    "        self.reset_files([self.filtered_masks_info_label_path, self.kmeans_model_path])\n",
    "\n",
    "\n",
    "    def run_meanshift_clustering(self, feature_vectors):\n",
    "        # https://scikit-learn.org/stable/modules/clustering.html\n",
    "        # class sklearn.cluster.MeanShift(*, bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)\n",
    "        pass\n",
    "\n",
    "    def run_ward_clustering(self, feature_vectors):\n",
    "        # https://scikit-learn.org/stable/modules/clustering.html\n",
    "        # class sklearn.cluster.AgglomerativeClustering(n_clusters=2, *, affinity='deprecated', metric=None, memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', distance_threshold=None, compute_distances=False)\n",
    "        pass\n",
    "\n",
    "    def run_gmm_clustering(self, feature_vectors):\n",
    "        # https://scikit-learn.org/stable/modules/clustering.html\n",
    "        # class sklearn.mixture.GaussianMixture(n_components=1, *, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)\n",
    "        pass\n",
    "\n",
    "    def run_nearest_neighbour(self, feature_vectors):\n",
    "        # https://scikit-learn.org/stable/modules/neighbors.html\n",
    "        pass\n",
    "\n",
    "    def run_kmeans_clustering(self, feature_vectors):\n",
    "        # https://scikit-learn.org/stable/modules/clustering.html\n",
    "\n",
    "        # todo: could use Elbow Method for determining n_clusters\n",
    "\n",
    "        # k means clustering\n",
    "        kmeans_model = KMeans(n_clusters=self.n_clusters, random_state=19).fit(feature_vectors)\n",
    "        labels = kmeans_model.predict(feature_vectors)\n",
    "\n",
    "        with open(self.kmeans_model_path, \"wb\") as f:\n",
    "            pickle.dump(kmeans_model, f)\n",
    "\n",
    "        df = pd.DataFrame({\"lbl_cluster\": labels}, index=list(feature_vectors.index.values))\n",
    "\n",
    "        self.unique_labels = list(np.unique(labels))\n",
    "        return df\n",
    "\n",
    "    def filter_clusters(self, df):\n",
    "        # shuffled\n",
    "        df = df.sample(frac=1) # .reset_index(drop=True)\n",
    "        # get n rows for each label\n",
    "        df = df.sort_values(\"lbl_cluster\", ascending = False).groupby(\"lbl_cluster\").head(self.n_patches_per_cluster)\n",
    "        return df\n",
    "\n",
    "    def plot_clusters(self, df):\n",
    "        \n",
    "        # for each cluster\n",
    "        for label in self.unique_labels:\n",
    "\n",
    "            patches_for_this_cluster = df.loc[df[\"lbl_cluster\"] == label]\n",
    "            if False:\n",
    "                random.shuffle(patches_for_this_cluster)\n",
    "\n",
    "            print(f\"Cluster size {len(patches_for_this_cluster)}\")\n",
    "\n",
    "            # only allow up to x images to be shown at a time\n",
    "            if len(patches_for_this_cluster) > 8:\n",
    "                patches_for_this_cluster = patches_for_this_cluster[:8]\n",
    "\n",
    "            fig, ax = plt.subplots(ncols=len(patches_for_this_cluster), figsize=(16,2), sharex=True, sharey=True)\n",
    "            for i, (patch_id, patch) in enumerate(patches_for_this_cluster.iterrows()):\n",
    "                print(patch_id)\n",
    "\n",
    "                basename, i_patch = patch_id.split(\"_\")\n",
    "\n",
    "                patch = skimage.io.imread(os.path.join(self.patches_path, basename+\".tiff\"), as_gray=False)[int(i_patch)]\n",
    "                ax[i].imshow(patch)\n",
    "            for a in ax.ravel():\n",
    "                a.set_axis_off()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            fig.savefig(os.path.join(self.cluster_path, f'{label}.png'))\n",
    "\n",
    "    def __len__(self):\n",
    "        # item only needed once\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "\n",
    "        if idx == len(self): raise StopIteration \n",
    "\n",
    "        df = self.run_kmeans_clustering(self.features_csv)\n",
    "        df = self.filter_clusters(df)\n",
    "\n",
    "        self.plot_clusters(df)\n",
    "\n",
    "        # only keeps mask info for filtered df samples\n",
    "        df = pd.concat([df, self.masks_info_csv], axis=1, join=\"inner\")\n",
    "        \n",
    "        df[\"mode\"] = \"train\"\n",
    "\n",
    "        # still a todo\n",
    "        \n",
    "\n",
    "        return df\n",
    "\n",
    "def cluster_patches(n_patches_per_cluster:int=5000, n_clusters:int=10):\n",
    "    # =============================================================================\n",
    "    # FUNCTION CALL TO CLUSTER PATCHES\n",
    "    # - clustering\n",
    "    # - filter patches\n",
    "    #\n",
    "    # here: save patch id, mask info and cluster label as csv (for boxplot and quilting)\n",
    "    # =============================================================================\n",
    "    \n",
    "    cluster_extractor = ClusterExtractor(n_patches_per_cluster=n_patches_per_cluster, n_clusters=n_clusters)\n",
    "    # runs only one time\n",
    "    for df in cluster_extractor:\n",
    "        df.to_csv(cluster_extractor.filtered_masks_info_label_path, index=True, header=True, sep=';')\n",
    "        print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv(f\"results/{prefix}/masks_info_label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Concepts from Segmentation Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Concept Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskConceptExtractor(ConceptExtractor):\n",
    "    # =============================================================================\n",
    "    # generate a dataset with positive and negative cases for each mask type\n",
    "    # focusing on filling this file: masks_info_label.csv\n",
    "    # label: 1p, 1n, 2p, 2n, ...\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, n_patches_per_cluster:int=5000):\n",
    "        \n",
    "        self.prefix = prefix\n",
    "        \n",
    "        # ----------\n",
    "        # inputs\n",
    "        # ----------\n",
    "        \n",
    "        self.masks_info_path = f\"results/{self.prefix}/masks_info.csv\"\n",
    "        self.masks_info_csv = self.load_csv(self.masks_info_path)\n",
    "        \n",
    "        self.column_keys = self.masks_info_csv.columns.values\n",
    "        \n",
    "        print(self.masks_info_csv.columns.values )\n",
    "        \n",
    "        self.n_patches_per_cluster = n_patches_per_cluster\n",
    "        \n",
    "        \n",
    "        # ----------\n",
    "        # output\n",
    "        # ----------\n",
    "        # dirs\n",
    "        self.cluster_path = f\"results/{self.prefix}/clusters\" # visualised patches\n",
    "        self.reset_dirs([self.cluster_path])\n",
    "        # files\n",
    "        self.filtered_masks_info_label_path = f\"results/{self.prefix}/masks_info_label.csv\"\n",
    "        self.reset_files([self.filtered_masks_info_label_path])    \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        # for each image patches array\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "\n",
    "        if idx == len(self): raise StopIteration\n",
    "        \n",
    "        print(self.masks_info_csv.shape)\n",
    "        \n",
    "        dataframes = []\n",
    "        for col_key in self.column_keys:\n",
    "                        \n",
    "            tmp_neg = self.masks_info_csv.loc[self.masks_info_csv[col_key] == 0].sample(frac=1)\n",
    "            tmp_pos = self.masks_info_csv.loc[self.masks_info_csv[col_key] != 0].sample(frac=1)\n",
    "            \n",
    "            max_size = self.n_patches_per_cluster\n",
    "            \n",
    "            \n",
    "            if len(tmp_neg[col_key]) < max_size:\n",
    "                max_size = len(tmp_neg[col_key])\n",
    "                \n",
    "                \n",
    "            if len(tmp_pos[col_key]) < max_size:\n",
    "                max_size = len(tmp_pos[col_key])\n",
    "                \n",
    "                \n",
    "            tmp_neg = tmp_neg[0:max_size][col_key]\n",
    "            tmp_pos = tmp_pos[0:max_size][col_key]\n",
    "                        \n",
    "            tmp = pd.concat( [ tmp_neg, tmp_pos ] )\n",
    "                        \n",
    "            dataframes.append(tmp)\n",
    "            \n",
    "        df = pd.concat(dataframes, axis=1)\n",
    "        \n",
    "        df[\"lbl_cluster\"] = 0\n",
    "        df[\"mode\"] = \"train\"        \n",
    "        \n",
    "        return df\n",
    "\n",
    "def concepts_from_masks(n_patches_per_cluster):\n",
    "    # =============================================================================\n",
    "    # Function call for extracting concepts from segmentation masks\n",
    "    # =============================================================================\n",
    "    \n",
    "    \n",
    "    \n",
    "    concept_extractor = MaskConceptExtractor(n_patches_per_cluster)\n",
    "    # runs only one time\n",
    "    for df in concept_extractor:\n",
    "        df.to_csv(concept_extractor.filtered_masks_info_label_path, index=True, header=True, sep=';')\n",
    "        print(df.shape)\n",
    "        print(df.head())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    feature_extractor = FeatureExtractor()\n",
    "\n",
    "    feature_vectors = []\n",
    "    patch_ids = []\n",
    "    for patch_array_feature_vectors, patch_array_ids in feature_extractor:\n",
    "        feature_vectors.extend(patch_array_feature_vectors)\n",
    "        patch_ids.extend(patch_array_ids)\n",
    "\n",
    "    # change type\n",
    "    feature_vectors = np.asarray(feature_vectors)\n",
    "    feature_vectors = feature_vectors.astype(np.float32)\n",
    "\n",
    "    # select the number of components while preserving 99% of the variability in the data\n",
    "    pca95 = PCA(n_components=0.99, random_state=19)\n",
    "    pca95.fit(feature_vectors)\n",
    "    pca95_feature_vectors = pca95.transform(feature_vectors)\n",
    "\n",
    "    with open(feature_extractor.pca95_model_path, \"wb\") as f:\n",
    "        pickle.dump(pca95, f)\n",
    "\n",
    "    df = pd.DataFrame(data=pca95_feature_vectors, \n",
    "                      columns=list(range(len(pca95_feature_vectors[0]))), \n",
    "                      index=patch_ids)\n",
    "\n",
    "    # save patch id and feature vector as csv\n",
    "    df.to_csv(feature_extractor.features_file_path, index=True, header=True)\n",
    "\n",
    "    print(df.head())\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bdfb173aee0a4262a62d9b4605b546d1",
    "deepnote_cell_type": "markdown",
    "id": "4u_xUvkIJ5Zx",
    "tags": []
   },
   "source": [
    "# Quilted image generation\n",
    "* immitating DeepDream\n",
    "* quilting, using minimum cut and dijkstra's algorithm vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6588380a167c4cf5a29e5cb53aa24db4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1670955501550,
    "id": "IA5g6_gtJ5Zx",
    "source_hash": "d3072237",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QuiltExtractor(ConceptExtractor):\n",
    "    # https://people.eecs.berkeley.edu/~efros/research/quilting/quilting.pdf\n",
    "    # https://github.com/axu2/image-quilting/blob/master/Final_Project_COS429%20(1).ipynb\n",
    "\n",
    "    def __init__(self,  patch_part:float=0.8, overlap_ratio:float=0.1, dst_img_size:int=500):\n",
    "        # we want to get a 500x500 image\n",
    "        # each patch is 30x30 big\n",
    "        \n",
    "        self.prefix = prefix\n",
    "\n",
    "        self.end = self.start = 0\n",
    "\n",
    "        # patch part: only take part of the patch e.g. 80% \n",
    "        self.patch_part = patch_part\n",
    "\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "\n",
    "        # width and height of the result image\n",
    "        self.image_size = dst_img_size\n",
    "        \n",
    "\n",
    "        # ----------\n",
    "        # inputs\n",
    "        # ----------\n",
    "        self.filtered_masks_info_label_path = f\"results/{self.prefix}/masks_info_label.csv\" # label and patch id\n",
    "        self.patch_id_label = self.load_csv(self.filtered_masks_info_label_path)[[\"lbl_cluster\"]]\n",
    "        self.patches_path = f\"data/{self.prefix}/patch_arrays\"\n",
    "        \n",
    "        # ----------\n",
    "        # outputs\n",
    "        # ----------\n",
    "        self.concepts_path = f\"data/{self.prefix}/concepts\" # reset and fill\n",
    "        self.reset_dirs([self.concepts_path])\n",
    "\n",
    "\n",
    "    def run(self, patch_id, label):\n",
    "\n",
    "        basename, i_patch = patch_id.split(\"_\") # A0001_4_vessel\n",
    "        self.patch = skimage.io.imread(os.path.join(self.patches_path, basename+\".tiff\"), as_gray=False)[int(i_patch)]\n",
    "       \n",
    "        self.start = time.time()\n",
    "        \n",
    "        # patch times 4 (with rotation)\n",
    "        combined_patch = self.patch\n",
    "        combined_patch = np.concatenate([combined_patch, np.flip(combined_patch, 0)], axis=0)\n",
    "        combined_patch = np.concatenate([combined_patch, np.flip(combined_patch, 1)], axis=1)\n",
    "        self.texture = combined_patch\n",
    "        self.texture = skimage.util.img_as_float(self.texture)\n",
    "\n",
    "        # 30 + 30\n",
    "        # 60*0.8\n",
    "        self.patch_length = math.ceil(min(self.texture.shape[:2])*self.patch_part)\n",
    "\n",
    "        # overlap ratio: 60*0.8*0.1\n",
    "        self.overlap = math.ceil(self.patch_length * self.overlap_ratio)\n",
    "\n",
    "        # 60*0.8 - 60*0.8*0.1\n",
    "        final_length = math.ceil(self.patch_length - self.overlap)\n",
    "\n",
    "        # amount of patches we can have for the image size\n",
    "        # 500 / (60*0.8 - 60*0.8*0.1)\n",
    "        n_patches = math.ceil(self.image_size / final_length)\n",
    "\n",
    "        # (60*0.8 - 60*0.8*0.1) * 11 + 5 (overlap)\n",
    "        patch_dependent_image_size = math.ceil(final_length * n_patches) + self.overlap\n",
    "\n",
    "        if False:\n",
    "            print(\"overlap\", self.overlap)\n",
    "            print(\"final image size\", patch_dependent_image_size)\n",
    "            print(\"number of patches\", self.n_patches)\n",
    "            print(\"length of 1 patch before reduction\", self.patch_length)\n",
    "            print(\"length of 1 patch after\", final_length)\n",
    "            print(\"a\", n_patches * final_length)\n",
    "            print(\"b\", n_patches * self.patch_length)\n",
    "\n",
    "        # result image with result image size\n",
    "        self.result_image = np.zeros((patch_dependent_image_size, patch_dependent_image_size, self.texture.shape[2]))\n",
    "\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                y = i * final_length\n",
    "                x = j * final_length\n",
    "\n",
    "                if i == 0 and j == 0:\n",
    "                    patch = self.randomPatch()\n",
    "                else:\n",
    "                    patch = self.randomPatch()\n",
    "                    patch = self.minCutPatch(patch, x, y)\n",
    "\n",
    "                \n",
    "                self.result_image[y:y+self.patch_length, x:x+self.patch_length] = patch\n",
    "        \n",
    "        self.end = time.time()\n",
    "        \n",
    "    def plot(self):\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "        \n",
    "        i = 0\n",
    "        axs[i].imshow(self.patch)\n",
    "        axs[i].set_title(\"Patch\")\n",
    "        axs[i].axis(\"off\")\n",
    "        \n",
    "        i+=1\n",
    "        axs[i].imshow(self.texture)\n",
    "        axs[i].set_title(\"Combined\")\n",
    "        axs[i].axis(\"off\")\n",
    "\n",
    "        i+=1\n",
    "        axs[i].imshow(self.result_image)\n",
    "        axs[i].set_title(\"Quilted image\")\n",
    "        axs[i].axis(\"off\")        \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"^ Time needed for one patch: \", self.end - self.start)\n",
    "        \n",
    "    def save(self, patch_id, label):\n",
    "        im = Image.fromarray((self.result_image * 255).astype(np.uint8))\n",
    "        tmp = patch_id.split(\"_\")[0]\n",
    "        p = os.path.join(self.concepts_path, tmp) # \"c\"+str(label)\n",
    "        if not os.path.exists(p):\n",
    "            os.makedirs(p)\n",
    "        im.save(os.path.join(p, f\"dream_c{label}_{patch_id}.jpg\"))\n",
    "        \n",
    "    def randomPatch(self):\n",
    "        h, w, _ = self.texture.shape\n",
    "        i = np.random.randint(h - self.patch_length)\n",
    "        j = np.random.randint(w - self.patch_length)\n",
    "        return self.texture[i:i+self.patch_length, j:j+self.patch_length]\n",
    "\n",
    "    def dijkstra_algorithm(self, errors):\n",
    "        # dijkstra's algorithm vertical\n",
    "        pq = [(error, [i]) for i, error in enumerate(errors[0])]\n",
    "        heapq.heapify(pq)\n",
    "\n",
    "        h, w = errors.shape\n",
    "        seen = set()\n",
    "\n",
    "        while pq:\n",
    "            error, path = heapq.heappop(pq)\n",
    "            curDepth = len(path)\n",
    "            curIndex = path[-1]\n",
    "\n",
    "            if curDepth == h:\n",
    "                return path\n",
    "\n",
    "            for delta in -1, 0, 1:\n",
    "                nextIndex = curIndex + delta\n",
    "\n",
    "                if 0 <= nextIndex < w:\n",
    "                    if (curDepth, nextIndex) not in seen:\n",
    "                        cumError = error + errors[curDepth, nextIndex]\n",
    "                        heapq.heappush(pq, (cumError, path + [nextIndex]))\n",
    "                        seen.add((curDepth, nextIndex))\n",
    "\n",
    "    def minCutPatch(self, patch, x, y):\n",
    "\n",
    "        patch = patch.copy()\n",
    "        dy, dx, _ = patch.shape\n",
    "        minCut = np.zeros_like(patch, dtype=bool)\n",
    "\n",
    "        if x > 0:\n",
    "            left = patch[:, :self.overlap] - self.result_image[y:y+dy, x:x+self.overlap]\n",
    "            leftL2 = np.sum(left**2, axis=2)\n",
    "            for i, j in enumerate(self.dijkstra_algorithm(leftL2)): # errors\n",
    "                minCut[i, :j] = True\n",
    "\n",
    "        if y > 0:\n",
    "            up = patch[:self.overlap, :] - self.result_image[y:y+self.overlap, x:x+dx]\n",
    "            upL2 = np.sum(up**2, axis=2)\n",
    "            for j, i in enumerate(self.dijkstra_algorithm(upL2.T)): # errors\n",
    "                minCut[:i, j] = True\n",
    "\n",
    "        np.copyto(patch, self.result_image[y:y+dy, x:x+dx], where=minCut)\n",
    "        \n",
    "        return patch\n",
    "\n",
    "    def __len__(self):\n",
    "        # amount of segmentation masks\n",
    "        return len(self.patch_id_label)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "\n",
    "        if idx == len(self): raise StopIteration \n",
    "\n",
    "        patch_id = self.patch_id_label.index[idx]\n",
    "        label = self.patch_id_label[\"lbl_cluster\"][idx]\n",
    "\n",
    "        # for each cluster aka for each class label\n",
    "        self.run(patch_id, label)\n",
    "        self.save(patch_id, label)\n",
    "        # self.plot()\n",
    "\n",
    "def concept_tiling(dst_img_size:int=500):\n",
    "    # patch-based texture synthesis\n",
    "\n",
    "    quilt_extractor = QuiltExtractor(patch_part=0.8, overlap_ratio=0.1, dst_img_size=dst_img_size)\n",
    "\n",
    "    # for each patch\n",
    "    for _ in quilt_extractor:            \n",
    "        pass\n",
    "\n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Mx5_Hto2i67"
   },
   "source": [
    "# Evaluation of clusters via boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gQ2kWOvlkhV"
   },
   "outputs": [],
   "source": [
    "class BoxPlotExtractor(ConceptExtractor):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.prefix = prefix\n",
    "        \n",
    "        # ----------\n",
    "        # input\n",
    "        # ----------\n",
    "        self.filtered_masks_info_label_path = f\"results/{self.prefix}/masks_info_label.csv\"\n",
    "        self.masks_info_label_csv = self.load_csv(self.filtered_masks_info_label_path)\n",
    "\n",
    "\n",
    "        # segmentation types  \n",
    "        header_keys = list(self.masks_info_label_csv.keys())\n",
    "        self.seg_types=header_keys\n",
    "        self.seg_types.remove(\"lbl_cluster\")\n",
    "\n",
    "        # class labels\n",
    "        self.unique_classes = np.unique(self.masks_info_label_csv[\"lbl_cluster\"])\n",
    "\n",
    "        # biggest cluster size\n",
    "        biggest_cluster_size = max(self.masks_info_label_csv[\"lbl_cluster\"].value_counts())\n",
    "\n",
    "        print(self.seg_types)\n",
    "\n",
    "        self.combined_info = self.masks_info_label_csv\n",
    "        self.combined_info[\"count\"] = 1\n",
    "        # combined_info = combined_info[(combined_info != 0).all(1)] # no idea\n",
    "\n",
    "        # max size in boxplot value should be 1000 \n",
    "        self.multiplier = 1000 / biggest_cluster_size\n",
    "\n",
    "\n",
    "        # ----------\n",
    "        # outputs\n",
    "        # ----------\n",
    "        self.boxplots_path = f\"results/{self.prefix}/boxplots\" # reset and fill\n",
    "        self.reset_dirs([self.boxplots_path])\n",
    "\n",
    "    def run(self, seg_type:str=\"Vessels\"):\n",
    "        # =============================================================================\n",
    "        # \n",
    "        # =============================================================================\n",
    "\n",
    "        # where lesion, optic disc, ...\n",
    "        sub_info = self.combined_info[[\"lbl_cluster\", f\"{seg_type}\", \"count\"]]\n",
    "\n",
    "        # figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "        # create subframe for each cluster\n",
    "        boxplot_data = []\n",
    "        for label in range(len(self.unique_classes)):\n",
    "            \n",
    "            filtered0 = sub_info.loc[sub_info[\"lbl_cluster\"] == label].dropna()[seg_type]\n",
    "            # remove zero for boxplot\n",
    "            filtered0 = [i for i in filtered0 if i != 0]\n",
    "            boxplot_data.append(filtered0)\n",
    "\n",
    "        # BOXPLOT\n",
    "        print(boxplot_data)\n",
    "        \n",
    "        ax = plt.boxplot(boxplot_data, medianprops=dict(color=\"mediumvioletred\", linewidth=1.5), showfliers=False)\n",
    "        \n",
    "        # count values for each label\n",
    "        # label, percentage, patch_id\n",
    "        sub_info = sub_info.groupby([\"lbl_cluster\", f\"{seg_type}\"])[\"count\"].count().reset_index()\n",
    "\n",
    "        # SCATTER PLOT\n",
    "        # uses absolute value for size which is great for consistency among plots\n",
    "        ax = plt.scatter(  sub_info[\"lbl_cluster\"]+1, sub_info[seg_type], s=sub_info[\"count\"]*self.multiplier, alpha=0.7, c='paleturquoise', edgecolors=\"teal\"  )\n",
    "\n",
    "        # show and save figure\n",
    "        plt.title(seg_type)\n",
    "        plt.xlabel('Concept cluster ID')\n",
    "        plt.ylabel(f'Percentage of \"{seg_type}\" pixels in image') \n",
    "        if \"Vessel\" in seg_type:\n",
    "            plt.yticks(plt.np.arange(-10, 41, 10)) \n",
    "        else:\n",
    "            plt.yticks(plt.np.arange(-10, 111, 10)) \n",
    "        plt.setp( plt.gca().get_yticklabels()[0], visible=False)\n",
    "        plt.setp( plt.gca().get_yticklabels()[-1], visible=False)\n",
    "        plt.show()\n",
    "        fig.savefig(os.path.join(self.boxplots_path, f\"{seg_type}.png\"))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # amount of segmentation types\n",
    "        return len(self.seg_types)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "\n",
    "        if idx == len(self): raise StopIteration\n",
    "        \n",
    "        seg_type = self.seg_types[idx]\n",
    "        if \"lbl\" in seg_type: \n",
    "            print(seg_type)\n",
    "            self.run(seg_type)\n",
    "\n",
    "def evalutation_via_boxplot():\n",
    "\n",
    "    boxplot_extractor = BoxPlotExtractor()\n",
    "\n",
    "    # for each segmentation type\n",
    "    for _ in boxplot_extractor:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOnXNgjLidgx"
   },
   "source": [
    "# Evaluation of clusters via t-sne (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CK9Y7v2-if9s"
   },
   "outputs": [],
   "source": [
    "class tsneExtractor(ConceptExtractor):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.prefix = prefix\n",
    "        \n",
    "        # ----------\n",
    "        # input\n",
    "        # ----------\n",
    "        self.filtered_masks_info_label_path = f\"results/{self.prefix}/masks_info_label.csv\"\n",
    "        self.masks_info_label_csv = self.load_csv(self.filtered_masks_info_label_path)\n",
    "\n",
    "\n",
    "        # ----------\n",
    "        # outputs\n",
    "        # ----------\n",
    "        self.tsne_path = f\"results/{self.prefix}/tsne\" # reset and fill\n",
    "        self.reset_dirs([self.tsne_path])\n",
    "        \n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        # =============================================================================\n",
    "        # todo: use this https://www.kaggle.com/code/aussie84/clustering-with-kmeans-pca-tsne\n",
    "        # =============================================================================\n",
    "\n",
    "        pass\n",
    "\n",
    "        if False:\n",
    "            # where lesion, optic disc, ...\n",
    "            sub_info = self.combined_info[[\"lbl_cluster\", f\"{seg_type}\", \"count\"]]\n",
    "\n",
    "            # figure\n",
    "            fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "            # create subframe for each cluster\n",
    "            boxplot_data = []\n",
    "            for label in range(len(self.unique_classes)):\n",
    "                \n",
    "                filtered0 = sub_info.loc[sub_info[\"lbl_cluster\"] == label][seg_type]\n",
    "                # remove zero for boxplot\n",
    "                filtered0 = [i for i in filtered0 if i != 0]\n",
    "                boxplot_data.append(filtered0)\n",
    "                \n",
    "\n",
    "            # BOXPLOT\n",
    "            ax = plt.boxplot(boxplot_data, medianprops=dict(color=\"mediumvioletred\", linewidth=1.5))\n",
    "            \n",
    "            # count values for each label\n",
    "            # label, percentage, patch_id\n",
    "            sub_info = sub_info.groupby([\"lbl_cluster\", f\"{seg_type}\"])[\"count\"].count().reset_index()\n",
    "\n",
    "            # SCATTER PLOT\n",
    "            # uses absolute value for size which is great for consistency among plots\n",
    "            ax = plt.scatter(  sub_info[\"lbl_cluster\"]+1, sub_info[seg_type], s=sub_info[\"count\"]*self.multiplier, alpha=0.7, c='paleturquoise', edgecolors=\"teal\"  )\n",
    "\n",
    "            # show and save figure\n",
    "            plt.title(seg_type)\n",
    "            plt.xlabel('Concept cluster ID')\n",
    "            plt.ylabel(f'Percentage of \"{seg_type}\" pixels in image') \n",
    "            plt.yticks(plt.np.arange(-10, 111, 10)) \n",
    "            plt.setp( plt.gca().get_yticklabels()[0], visible=False)\n",
    "            plt.setp( plt.gca().get_yticklabels()[-1], visible=False)\n",
    "            plt.show()\n",
    "            fig.savefig(os.path.join(self.boxplots_path, f\"{seg_type}.png\"))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "\n",
    "        if idx == len(self): raise StopIteration \n",
    "\n",
    "        a = 0\n",
    "        self.run()\n",
    "\n",
    "def evalutation_via_tsne():\n",
    "\n",
    "    tsne_extractor = tsneExtractor()\n",
    "\n",
    "    for _ in tsne_extractor:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srBOrKx7SOvm"
   },
   "source": [
    "# Evaluation of quilted images via DeepDream\n",
    "* optimising an image for a specific channel in a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nr3GRCPsSGQz"
   },
   "outputs": [],
   "source": [
    "class DeepDreamExtractor(ConceptExtractor):\n",
    "    # https://github.com/Mayukhdeb/torch-dreams/\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.prefix = prefix\n",
    "\n",
    "        self.model = shufflenet_v2_x1_0(weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        #print(self.model)\n",
    "        self.layers_to_use = [\n",
    "                        self.model.conv1[0], \n",
    "                        self.model.stage2[1].branch2[5],\n",
    "                        self.model.stage2[3].branch2[5],\n",
    "                        self.model.stage4[1].branch2[5],\n",
    "                        self.model.conv5[0]\n",
    "                        ] \n",
    "        self.layers_to_use = [self.layers_to_use[1]]\n",
    "        \n",
    "        self.dreamy_boi = Dreamer(self.model, device = \"cpu\", quiet =  False)\n",
    "        \n",
    "\n",
    "\n",
    "        # ----------\n",
    "        # input\n",
    "        # ----------\n",
    "        image_paths = f\"data/{self.prefix}/concepts/*/*\"\n",
    "        self.image_path = glob.glob(image_paths)[4]\n",
    "        self.param = CustomImageParam(image = self.image_path, device = \"cpu\")\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        def loss_from_layer_func(layer_number = 0, channel_number= 0): \n",
    "            def custom_func(layer_outputs):\n",
    "                loss = layer_outputs[layer_number][channel_number].mean()\n",
    "                return -loss\n",
    "            return custom_func\n",
    "\n",
    "        # Now in order to optimize the **7th channel** of the **first layer** mentioned in `layers_to_use` we define the custom loss as:\n",
    "        func_call = loss_from_layer_func(layer_number= 0, channel_number = 6)\n",
    "\n",
    "        self.image_param = self.dreamy_boi.render(\n",
    "            image_parameter= self.param,\n",
    "            layers = self.layers_to_use,\n",
    "            lr = 2e-4, # PARAMETER\n",
    "            iters = 200,  # PARAMETER\n",
    "            custom_func = func_call\n",
    "        )\n",
    "\n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(nrows= 1, ncols= 2, figsize=(10,5))\n",
    "        try:\n",
    "            ax.flat[0].imshow(cv2.cvtColor(cv2.imread(self.image_path), cv2.COLOR_BGR2RGB))\n",
    "        except:\n",
    "            ax.flat[0].imshow(cv2.imread(self.image_path))\n",
    "            \n",
    "        ax.flat[1].imshow(self.image_param)\n",
    "\n",
    "\n",
    "def evalutation_via_deepdream():\n",
    "    # todo, make iterative with loop for many images\n",
    "    deep_dream_extractor = DeepDreamExtractor()\n",
    "    deep_dream_extractor.run()\n",
    "    deep_dream_extractor.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Mtm7YJERQ2l"
   },
   "source": [
    "# Function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7U3Gdy0OgkU",
    "outputId": "a57ed845-d713-49d3-cb95-ec3bb1350761"
   },
   "outputs": [],
   "source": [
    "if run_extract_patches:\n",
    "    extract_patches(dst_img_size=dst_img_size, \n",
    "                    dst_patch_size=dst_patch_size, \n",
    "                    overlap_ratio=overlap_ratio_extract,\n",
    "                    reduced_data=reduced_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478,
     "referenced_widgets": [
      "c77b926330f24336a97c87ccfd3ea5e5",
      "bf8a9a16068c42c4bd381455a9522f07",
      "ab704b21618a41f0bdc137a296a7f311",
      "50325ec06f8d487a8b065ab18a2dbc03",
      "f96c3d6e709c490cafd4cae167ad409f",
      "0860ee41c98a42ddb3284ce6e5834f0a",
      "9290b4532446467ca64727e1df132934",
      "757dacd5119f4b8ba34e55f0b7c907bf",
      "55ae74da6f66495a84cbc11ca3eceb6c",
      "835e30e3b0784a07b36478ef49378a1b",
      "db758561c03b4bd6a1230e5f3b636151"
     ]
    },
    "id": "9MAfF3OCK_ee",
    "outputId": "e6ec21b3-dd98-47b7-a8d4-66a987023cf8"
   },
   "outputs": [],
   "source": [
    "if run_extract_features:\n",
    "    extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g9Y70WIQA1Mz",
    "outputId": "b55946c4-61f0-4504-a893-43a23bee7d45"
   },
   "outputs": [],
   "source": [
    "if run_cluster_patches:\n",
    "    cluster_patches(n_patches_per_cluster=n_patches_per_cluster, \n",
    "                    n_clusters=n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_concepts_from_masks:\n",
    "    concepts_from_masks(n_patches_per_cluster=n_patches_per_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8s3YKdsKRzN1",
    "outputId": "7bc7afdd-1d01-4edb-94aa-86660dc2bca1"
   },
   "outputs": [],
   "source": [
    "if run_concept_tiling:\n",
    "    concept_tiling(dst_img_size=dst_img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3l5qYF27RxcW",
    "outputId": "252ca7e2-7a22-41b6-846e-c69d879e6759"
   },
   "outputs": [],
   "source": [
    "if run_boxplot_evaluation:\n",
    "    evalutation_via_boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIEn-Opjfh4U"
   },
   "outputs": [],
   "source": [
    "if run_deepdream_evaluation:\n",
    "    evalutation_via_deepdream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ed579f01511f41c1bee80b42fa76f9b5",
  "deepnote_persisted_session": {
   "createdAt": "2022-12-13T18:47:03.063Z"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0860ee41c98a42ddb3284ce6e5834f0a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50325ec06f8d487a8b065ab18a2dbc03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_835e30e3b0784a07b36478ef49378a1b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_db758561c03b4bd6a1230e5f3b636151",
      "value": " 8.79M/8.79M [00:00&lt;00:00, 59.6MB/s]"
     }
    },
    "55ae74da6f66495a84cbc11ca3eceb6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "757dacd5119f4b8ba34e55f0b7c907bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "835e30e3b0784a07b36478ef49378a1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9290b4532446467ca64727e1df132934": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab704b21618a41f0bdc137a296a7f311": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_757dacd5119f4b8ba34e55f0b7c907bf",
      "max": 9218294,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55ae74da6f66495a84cbc11ca3eceb6c",
      "value": 9218294
     }
    },
    "bf8a9a16068c42c4bd381455a9522f07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0860ee41c98a42ddb3284ce6e5834f0a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9290b4532446467ca64727e1df132934",
      "value": "100%"
     }
    },
    "c77b926330f24336a97c87ccfd3ea5e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bf8a9a16068c42c4bd381455a9522f07",
       "IPY_MODEL_ab704b21618a41f0bdc137a296a7f311",
       "IPY_MODEL_50325ec06f8d487a8b065ab18a2dbc03"
      ],
      "layout": "IPY_MODEL_f96c3d6e709c490cafd4cae167ad409f"
     }
    },
    "db758561c03b4bd6a1230e5f3b636151": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f96c3d6e709c490cafd4cae167ad409f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
