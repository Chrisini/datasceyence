{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 𝔻𝕖𝕔𝕖𝕟𝕥ℕ𝕖𝕥: 𝕕𝕚𝕤𝕖𝕟𝕥𝕒𝕟𝕘𝕝𝕖𝕕 𝕟𝕖𝕥\n",
    "\n",
    "Goal: create a sparse and modular ConvNet\n",
    "\n",
    "Todos: \n",
    "* [ ] delete node (filter) if either no input or no output edges\n",
    "* [ ] AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
    "* [ ] cuda error, if one of the decent1x1 has no kernels left - we need at least one input for each 1x1 filter\n",
    "* [ ] can we keep training if filter gets removed (e.g. at reloading model)\n",
    "* [ ] need some working filter removing in general - only at reload rn\n",
    "* [ ] currently commented:             # img, msk = flt.execute() # flattened -> in data/octa500.py\n",
    "* [ ] make sure code runs without the fda (fourier) library\n",
    "\n",
    "\n",
    "Notes:\n",
    "* additionally needed: position, activated channels, connection between channels\n",
    "* within this layer, a whole filter can be deactivated\n",
    "* within a filter, single channels can be deactivated\n",
    "* within this layer, filters can be swapped\n",
    "* the 'value' in the csv file is random if the CI metric is 'random'\n",
    "     \n",
    "* pruning actually doesn't work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf\n",
    "\n",
    "pip install:\n",
    "* pytorch_lightning\n",
    "\n",
    "preprocessing possible:\n",
    "* flatten layers\n",
    "* denoise\n",
    "* crop background\n",
    "\n",
    "\n",
    "warnings:\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73a26-f239-466f-901d-559d192f61de",
   "metadata": {},
   "source": [
    "![uml of code](examples/example_vis/uml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba244e3-3e6a-47e7-aa4b-a22df6054b8a",
   "metadata": {},
   "source": [
    "# conventions\n",
    "\n",
    "id may be image id if available, else batch id\n",
    "\n",
    "* entry image and mask: entry_id5_0_0_0_mo3_gt2.png\n",
    "* mat: mat_id10004_size26_0_0_0_mo2_gt2.mat (size - \n",
    "* hidden layer: hid_id5_3_8_2.png \n",
    "* last layer: pool_2_3_4_cl2.png (global pooling - connected to class n, cl=class)\n",
    "* activated image: cam_id5_mo3_gt2.png\n",
    "* activated image gray: camgray_id5_mo3_gt2.png\n",
    "\n",
    "\n",
    "* circle in: in_2_3_4_ep65.png\n",
    "* circle out: out_2_3_4_ep65.png\n",
    "\n",
    "* filter: filter_2_3_4.csv and filter_2_3_4.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# future imports first\n",
    "# =============================================================================\n",
    "from __future__ import print_function\n",
    "# =============================================================================\n",
    "# sys\n",
    "# =============================================================================\n",
    "import sys \n",
    "sys.path.insert(0, \"helper\")\n",
    "# =============================================================================\n",
    "# alphabetic order misc\n",
    "# =============================================================================\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import scipy.io\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "# =============================================================================\n",
    "# torch\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torchvision\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "# from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "# =============================================================================\n",
    "# datasceyence\n",
    "# =============================================================================\n",
    "from helper.model.decentnet import DecentNet\n",
    "from helper.visualisation import filter_activation\n",
    "from helper.visualisation.colour import *\n",
    "from helper.data.mnist import DataLoaderMNIST\n",
    "from helper.data.retinamnist import DataLoaderRetinaMNIST\n",
    "from helper.data.octmnist import DataLoaderOCTMNIST\n",
    "from helper.data.octa500 import DataLoaderOCTA500\n",
    "from helper.data.organmnist3D import DataLoaderOrganMNIST3D\n",
    "from data.transform.octa500_resize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397b450-c5c6-4da1-b630-7bf80e46891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch 2.0.0 == False\n",
      "tl 2.1.0 == False\n"
     ]
    }
   ],
   "source": [
    "seed = 1997 # was 19 before\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "debug_mode = False # todo - this is to do some print stuff # changed this from debug_mode\"l\" to debug mode\n",
    "\n",
    "print('torch 2.0.0 ==', torch.__version__=='2.0.0')\n",
    "print('tl 2.1.0 ==', pl.__version__=='2.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b0be-245d-49c0-88b4-d94167f7da90",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97670e82-0d27-4b7f-91df-874acf9974a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'input_data_csv': ['data_prep/data_octa500.csv'], 'result_path': 'examples/example_results', 'exp_name': 'counter_seems_to_work', 'load_ckpt_file': 'version_3/checkpoints/mf_stage=3.0_counter=13.0_val_f1_macro=0.45_unpruned=3732.ckpt', 'load_mode': True, 'dataset': 'octa500', 'epochs': None, 'training_stages': [1, 5, 5, 2], 'img_size': 28, 'p_augment': 0.2, 'batch_size': 8, 'log_every_n_steps': 50, 'device': 'cuda', 'num_workers': 0, 'train_size': 5000, 'val_size': 100, 'test_size': 100, 'octa500_id': 199, 'xai_done': False}\n",
      "model kwargs {'in_channels': 1, 'n_classes': None, 'out_dim': [1, 8, 16, 32], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'new_cc_mode': True, 'reset_optimiser_at_update': True, 'optimizer': 'sgd', 'base_lr': 0.1, 'min_lr': 0.001, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 5, 'cc_metric': 'l2', 'ci_metric': 'l2', 'cm_metric': 'not implemented yet', 'update_every_nth_epoch': 3, 'pretrain_epochs': 3, 'prune_keep': 0.95, 'prune_keep_total': 0.4}\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {\n",
    "    'in_channels' : 1, # not in use yet\n",
    "    'n_classes': None, # filled in the dataset\n",
    "    'out_dim' :  [1, 8, 16, 32], # [1, 8, 16, 32], #[1, 16, 24, 32] # entry, decent1, decent2, decent3\n",
    "    'grid_size' : 18*18,\n",
    "    'criterion' : torch.nn.CrossEntropyLoss(), # torch.nn.BCEWithLogitsLoss(),\n",
    "    'new_cc_mode' : True, # this is for using the new connection cost loss term\n",
    "    'reset_optimiser_at_update' : True, # needs to be resetted when pruning # not needed anymore\n",
    "    'optimizer': \"sgd\", # sgd adamw\n",
    "    'base_lr': 0.1, #0.001,\n",
    "    'min_lr' : 0.001, #0.00001,\n",
    "    'momentum' : 0.9,\n",
    "    'lr_update' : 100,\n",
    "    # decentnet\n",
    "    'cc_weight': 5, # high weight as the cc doesn't change a lot\n",
    "    'cc_metric' : 'l2', # connection cost metric (for loss) - distance metric # no idea how the torch works oops\n",
    "    'ci_metric' : 'l2', # todo: should be l2 # channel importance metric (for pruning)\n",
    "    'cm_metric' : 'not implemented yet', # 'count', # crossing minimisation \n",
    "    'update_every_nth_epoch' : 3, # 5 # todo - remove from code\n",
    "    'pretrain_epochs' : 3, # 20 # todo - remove from code\n",
    "    'prune_keep' : 0.95, # 0.97, # in each epoch\n",
    "    'prune_keep_total' : 0.4, # this number is not exact, depends on the prune_keep value\n",
    "}\n",
    "\n",
    "train_kwargs = {\n",
    "    'input_data_csv': [\"data_prep/data_octa500.csv\"],\n",
    "    'result_path': \"examples/example_results\", # \"example_results/lightning_logs\", # not in use??\n",
    "    'exp_name': \"counter_seems_to_work\", # must include dataset name, otherwise mnist is used\n",
    "    'load_ckpt_file' : \"version_3/checkpoints/mf_stage=3.0_counter=13.0_val_f1_macro=0.45_unpruned=3732.ckpt\", # \"version_0/checkpoints/mu_epoch=14-val_f1_macro=0.41-unpruned=5676.ckpt\" # \"version_0/checkpoints/epoch=94-unpruned=1600-val_f1=0.67.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "    'load_mode' : True, # True, False\n",
    "    'dataset' : 'octa500',\n",
    "    'epochs': None,\n",
    "    'training_stages': [1] + [5]*2 + [2], # [epochs] pretrain + [epochs between pruning] * pruning amount + [epochs] after-training\n",
    "    'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ... # MNIST/MedMNIST 28 × 28 Pixel\n",
    "    'p_augment' : 0.2, # probabiliby of torchvision transforms of training data (doesn't apply to all transforms) # 0.1 low, 0.5 half, 1 always\n",
    "    'batch_size': 8, # laptop: 2, pc: 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "    'log_every_n_steps' : 50, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "    'device': \"cuda\",\n",
    "    'num_workers' : 0, # 18, # 18 for seri computer, 0 or 8 for my laptop # make sure smaller than activate dataset sizes\n",
    "    'train_size' : 5000, # total, none = 0, all = -1  (batch size * forward passes per epoch) # set 0 to skip training and just do testing\n",
    "    'val_size' : 100, # total, none = 0, all = -1 (batch size * forward passes per epoch) \n",
    "    'test_size' : 100, # total, none = 0, all = -1 (batch size * forward passes per epoch)\n",
    "    'octa500_id' : 200-1, # not in use - we use preselected data from a csv\n",
    "    'xai_done' : False, # DO NOT CHANGE, WILL BE CHANGED IN CODE\n",
    "}\n",
    "\n",
    "print(\"train kwargs\", train_kwargs)\n",
    "print(\"model kwargs\", model_kwargs)\n",
    "\n",
    "kwargs = {'train_kwargs':train_kwargs, 'model_kwargs':model_kwargs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ee3d0-9c79-4917-a355-093f1daf4855",
   "metadata": {},
   "source": [
    "## check the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d5ff36-c015-4bd6-8b12-c4d0e3b64b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights that stay after epoch\n",
      "0 6000\n",
      "1 5700\n",
      "2 5700\n",
      "3 5700\n",
      "First 5 pairs: [(0, 6000), (1, 5700), (2, 5700), (3, 5700)]\n",
      "Last 5 pairs: [(0, 6000), (1, 5700), (2, 5700), (3, 5700)]\n",
      "Total pairs: 4\n"
     ]
    }
   ],
   "source": [
    "# i have to check where the 6000 comes from, should be calculated\n",
    "\n",
    "breaking = 6000*model_kwargs['prune_keep_total']\n",
    "weights = 6000 # this value is an estimate for a model [1, 8, 16, 32]\n",
    "# 'unpruned' is the logger variable for the value\n",
    "\n",
    "pairs = []\n",
    "print(\"weights that stay after epoch\")\n",
    "for i in range(len(train_kwargs['training_stages'])):\n",
    "    \n",
    "    if (weights < breaking): # weights*model_kwargs['prune_keep']\n",
    "        print(\"stop:\", breaking)\n",
    "        print('you need at least this many epochs:', i)\n",
    "        print('you currently have this many epochs:', len(train_kwargs['training_stages']))\n",
    "        print(\"recommended to add 2*update_every_nth_epoch\")\n",
    "        break\n",
    "    \n",
    "    # not sure whether -1 is correct, have to check\n",
    "    #if i >= model_kwargs['pretrain_epochs'] and ((i-1)%model_kwargs['update_every_nth_epoch'] == 0):\n",
    "    if i >= train_kwargs['training_stages'][0] and i< train_kwargs['training_stages'][-1] and weights > breaking: #  and ((i-1)%model_kwargs['update_every_nth_epoch'] == 0):\n",
    "        weights = int(weights*model_kwargs['prune_keep'])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(i, weights)\n",
    "    pairs.append((i, weights))\n",
    "\n",
    "\n",
    "print(f\"First 5 pairs: {pairs[:5]}\")\n",
    "print(f\"Last 5 pairs: {pairs[-5:]}\")\n",
    "print(f\"Total pairs: {len(pairs)}\")\n",
    "\n",
    "# print(f\"Min i: {min([i for i, a in your_data])}, Max i: {max([i for i, a in your_data])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b3283-2825-4f35-9716-78ccf5bb8b1c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "* the dataset name needs to be part of the experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Empty DataFrame\n",
      "Columns: [img_id, img_path, msk_path, mode, lbl_disease, sex, os-od, age]\n",
      "Index: []\n",
      "0\n",
      "val\n",
      "Empty DataFrame\n",
      "Columns: [img_id, img_path, msk_path, mode, lbl_disease, sex, os-od, age]\n",
      "Index: []\n",
      "0\n",
      "test\n",
      "     img_id                                           img_path  \\\n",
      "0     10001  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "1     10004  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "2     10005  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "3     10007  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "4     10008  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "..      ...                                                ...   \n",
      "175   10288  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "176   10290  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "177   10291  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "178   10297  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "179   10300  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "\n",
      "                                              msk_path  mode  lbl_disease sex  \\\n",
      "0    C://Users/Prinzessin/projects/decentnet/datasc...  test            3   M   \n",
      "1    C://Users/Prinzessin/projects/decentnet/datasc...  test            2   M   \n",
      "2    C://Users/Prinzessin/projects/decentnet/datasc...  test            1   M   \n",
      "3    C://Users/Prinzessin/projects/decentnet/datasc...  test            3   M   \n",
      "4    C://Users/Prinzessin/projects/decentnet/datasc...  test            0   F   \n",
      "..                                                 ...   ...          ...  ..   \n",
      "175  C://Users/Prinzessin/projects/decentnet/datasc...  test            2   M   \n",
      "176  C://Users/Prinzessin/projects/decentnet/datasc...  test            3   F   \n",
      "177  C://Users/Prinzessin/projects/decentnet/datasc...  test            0   F   \n",
      "178  C://Users/Prinzessin/projects/decentnet/datasc...  test            1   M   \n",
      "179  C://Users/Prinzessin/projects/decentnet/datasc...  test            1   M   \n",
      "\n",
      "    os-od  age  \n",
      "0      OD   55  \n",
      "1      OD   53  \n",
      "2      OS   27  \n",
      "3      OD   24  \n",
      "4      OD   51  \n",
      "..    ...  ...  \n",
      "175    OD   64  \n",
      "176    OD   49  \n",
      "177    OD   29  \n",
      "178    OD   48  \n",
      "179    OD   59  \n",
      "\n",
      "[180 rows x 8 columns]\n",
      "180\n",
      "********** DECENT INFO: DataLoader infos **********\n",
      "python_class : OCTA500\n",
      "description : The OCTA500 is based on optical coherence tomography (OCT) images for retinal diseases. The dataset is comprised of 4 diagnosis categories, leading to a multi-class classification task. We split the dataset as needed based on a CSV file into training, validation and testset. Possible split for validation of the decentnet: 0:0:180\n",
      "task : multi-class\n",
      "label : {'0': 'cnv', '1': 'dr', '2': 'amd', '3': 'normal'}\n",
      "n_channels : 1\n",
      "n_samples : {'train': 0, 'val': 0, 'test': 180}\n"
     ]
    }
   ],
   "source": [
    "if 'octmnist' in train_kwargs['dataset']:\n",
    "    # OCTMINST + weights for loss due to heavy imbalanced data\n",
    "    dataloader = DataLoaderOCTMNIST(train_kwargs, model_kwargs)  \n",
    "    print(\"\")\n",
    "    all_labels = []\n",
    "    # Extract all labels from the DataLoader\n",
    "    for inputs, labels in dataloader.train_dataloader:\n",
    "        all_labels.append(labels.flatten())\n",
    "    # Concatenate all labels into a single tensor\n",
    "    \n",
    "    all_labels = torch.cat(all_labels)\n",
    "    sorted_labels, sorted_indices = torch.sort(all_labels)\n",
    "    # Count the occurrences of each class\n",
    "    class_counts = torch.bincount(sorted_labels)\n",
    "    # Calculate weights (inverse of class frequency)\n",
    "    class_weights = 1.0 / class_counts.float()\n",
    "    # Normalize weights (optional, but recommended for stability)\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    print(\"class_counts\", class_counts, \"class_weights:\", class_weights) \n",
    "    if torch.isnan(class_weights).any(): \n",
    "        print(\"DECENT INFO: dataset too small, no weighting used\") \n",
    "    else:\n",
    "        model_kwargs[\"criterion\"] = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # class_mapper = ['cnv', 'dr', 'amd', 'healthy']\n",
    "elif 'retinamnist' in train_kwargs['dataset']:\n",
    "    # RetinaMNIST\n",
    "    dataloader = DataLoaderRetinaMNIST(train_kwargs, model_kwargs)\n",
    "    \n",
    "elif 'octa500' in train_kwargs['dataset']:\n",
    "    # OCTA-500\n",
    "    dataloader = DataLoaderOCTA500(train_kwargs, model_kwargs)\n",
    "elif '3d' in train_kwargs['dataset']:\n",
    "    dataloader = DataLoaderOrganMNIST3D(train_kwargs, model_kwargs)\n",
    "else:\n",
    "    print(\"select a valid dataset\")\n",
    "    \n",
    "class_mapper = dataloader.info[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4c5ae54-a40d-44a2-8fdb-3d8cbba35d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classes: 4\n",
      "train_size 0\n",
      "val_size 0\n",
      "test_size 100\n"
     ]
    }
   ],
   "source": [
    "assert model_kwargs['n_classes'] != None, \"DECENT ERROR: make sure you set the n_classes with the dataset\"  \n",
    "print(\"n_classes:\", model_kwargs['n_classes'])\n",
    "\n",
    "print(\"train_size\", train_kwargs[\"train_size\"])\n",
    "print(\"val_size\", train_kwargs[\"val_size\"])\n",
    "print(\"test_size\", train_kwargs[\"test_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X (Datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # an object with image representations and their positions\n",
    "    # amout of channels need to have same length as m and n lists\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        self.data = data # list of tensors (image representations)\n",
    "        self.ms_x = ms_x # list of integers (m position of each image representation)\n",
    "        self.ns_x = ns_x # list of integers (n position of each image representation)\n",
    "                \n",
    "    def setter(self, data, ms_x, ns_x):\n",
    "        self.data = data\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        \n",
    "    def getter(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a15bead-ee48-487f-8390-0ad2682fe57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in use\n",
    "class SaveLastModelCheckpoint(ModelCheckpoint):\n",
    "    \n",
    "    def _save_last_checkpoint(self, trainer: \"pl.Trainer\", monitor_candidates) -> None:\n",
    "        if not self.save_last:\n",
    "            return\n",
    "        \n",
    "        # self.CHECKPOINT_NAME_LAST = \n",
    "\n",
    "        filepath = self.format_checkpoint_name(monitor_candidates)\n",
    "        \n",
    "        print(\"last filepath\", filepath)\n",
    "        print(\"+ last\", filepath.replace(\"mf\", \"last\"))\n",
    "        \n",
    "        # examples/example_results\\lightning_logs\\trying_counter\\version_6\\checkpoints\\mf_stage=0.0_counter=1.0_val_f1_macro=0.13_unpruned=5980.ckpt\n",
    "        \n",
    "        replace_part = filepath.split(\"_counter=\")[1].replace(\".cktp\", \"\")\n",
    "        print(\"replace_part\", replace_part)\n",
    "        \n",
    "        # remove all from current state\n",
    "        tmp = filepath.replace(replace_part, \"*\").replace(\"mf\", \"last\")\n",
    "        print(\"tmp\", tmp)\n",
    "        for previous in glob.glob(tmp):\n",
    "            print(\"prev\", previous)\n",
    "            self._remove_checkpoint(trainer, previous)\n",
    "        \n",
    "        self._save_checkpoint(trainer, filepath.replace(\"mf\", \"last\"))\n",
    "        \n",
    "        # if there is one with last and same stage - delete\n",
    "\n",
    "        #if self._enable_version_counter:\n",
    "            #version_cnt = self.STARTING_VERSION\n",
    "            #while self.file_exists(filepath, trainer) and filepath != self.last_model_path:\n",
    "                #filepath = self.format_checkpoint_name(monitor_candidates, self.CHECKPOINT_NAME_LAST, ver=version_cnt)\n",
    "                #version_cnt += 1\n",
    "\n",
    "        # set the last model path before saving because it will be part of the state.\n",
    "        #previous, self.last_model_path = self.last_model_path, filepath\n",
    "        #if self.save_last == \"link\" and self._last_checkpoint_saved and self.save_top_k != 0:\n",
    "        #    self._link_checkpoint(trainer, self._last_checkpoint_saved, filepath)\n",
    "        #else:\n",
    "            \n",
    "        #if previous and self._should_remove_checkpoint(trainer, previous, filepath):\n",
    "            #self._remove_checkpoint(trainer, previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81e70c67-cef4-49da-896f-18e92782f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in use \n",
    "class EndModelCheckpoint(ModelCheckpoint):\n",
    "    \n",
    "    def on_train_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint \n",
    "        # if unpruned state != -1\n",
    "        # Save a checkpoint at the end of a defined training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        monitor_candidates = self._monitor_candidates(trainer)\n",
    "        monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "\n",
    "        self._save_last_checkpoint(trainer, monitor_candidates)\n",
    "        pl_module.model.get_everything(counter=trainer.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e499e01-6930-4702-aa7a-f610d4023049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working\n",
    "\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "class OptimiserUpdateCheckpoint(Callback):\n",
    "    \n",
    "    def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if pl_module.reset_optimiser_at_update: # todo - something missing here!!\n",
    "            print(\"update optimiser\")\n",
    "            \n",
    "            # print(pl_module.model.parameters())\n",
    "            \n",
    "            new_optimizers = optim.SGD(pl_module.model.parameters(), lr=0.01, momentum=0.9)\n",
    "            trainer.optimizers = [new_optimizers]\n",
    "            \n",
    "            \n",
    "            # trainer.lr_schedulers = trainer.configure_schedulers([new_schedulers])\n",
    "            # self.model.parameters\n",
    "            \n",
    "            #trainer.optimizers[0] = new_optimizers\n",
    "            \n",
    "            #print(\"* params begin \"*10)\n",
    "            #for param in pl_module.model.parameters():\n",
    "            #    print(param)\n",
    "            #    break\n",
    "            #print(\"-\"*10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in use anymore\n",
    "\n",
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint \n",
    "        # if unpruned state != -1\n",
    "        # Save a checkpoint at the end of a defined training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        # when pruning, then save model!\n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "            print(\"DECENT NOTE: callback on_train_epoch_end\", monitor_candidates[\"epoch\"].item())\n",
    "            if monitor_candidates[\"epoch\"] > 0:\n",
    "                if monitor_candidates[\"unpruned_state\"] != -1:\n",
    "                    print(\"DECENT NOTE: save model\", monitor_candidates[\"epoch\"].item())\n",
    "                    if self._every_n_epochs >= 1 and ((trainer.current_epoch + 1) % self._every_n_epochs) == 0:\n",
    "                        self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                        \n",
    "                    self._save_last_checkpoint(trainer, monitor_candidates)\n",
    "                    \n",
    "                    pl_module.model.get_everything(counter=trainer.current_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "## LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # Lightning Module consists of functions that define the training routine\n",
    "    # train, val, test: before epoch, step, after epoch, ...\n",
    "    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n",
    "    # order for the instance methods:\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\n",
    "    # \n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, kwargs, log_dir, ckpt_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        print(\"ckpt_path:\", ckpt_path)\n",
    "        \n",
    "        if train_kwargs[\"load_mode\"]: # True, False\n",
    "            # ckpt_path = os.path.join(log_dir, train_kwargs[\"load_ckpt_file\"]).replace(\"_xAI\", \"\")\n",
    "            if os.path.isfile(ckpt_path):\n",
    "                print(f\"Found pretrained model at {ckpt_path}, loading...\")\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir, ckpt_path=ckpt_path).to(\"cuda\")\n",
    "            else:\n",
    "                print(f\"DECENT NOTE: Did not find {ckpt_path}, create new model\")\n",
    "                # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "                # self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        else:\n",
    "            print(f\"Create new model\")\n",
    "            # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "            self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "            \n",
    "        # print(self.model)\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optim = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        self.pretrain_epochs = model_kwargs[\"pretrain_epochs\"]\n",
    "        self.image_size = kwargs['train_kwargs'][\"img_size\"]\n",
    "        self.new_cc_mode = kwargs[\"model_kwargs\"][\"new_cc_mode\"]\n",
    "        self.reset_optimiser_at_update = kwargs[\"model_kwargs\"][\"reset_optimiser_at_update\"]\n",
    "        \n",
    "        self.cc_ci = torch.tensor([0]).to(kwargs[\"train_kwargs\"][\"device\"])\n",
    "    \n",
    "        self.counter = 0\n",
    "        \n",
    "        # needed for hparams.yaml file\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        \n",
    "        self.train_metrics = torchmetrics.MetricCollection(\n",
    "            {\n",
    "            \"acc\": torchmetrics.classification.MulticlassAccuracy(num_classes=self.n_classes),\n",
    "            \"f1_macro\": torchmetrics.classification.MulticlassF1Score(num_classes=self.n_classes),\n",
    "            \"f1_micro\" : torchmetrics.classification.MulticlassF1Score(num_classes=self.n_classes, average='micro'),\n",
    "            \"prec\": torchmetrics.classification.MulticlassPrecision(num_classes=self.n_classes),\n",
    "            \"rec\": torchmetrics.classification.MulticlassRecall(num_classes=self.n_classes),\n",
    "            # \"cm\": torchmetrics.classification.MulticlassConfusionMatrix(num_classes=self.n_classes)\n",
    "            }, prefix=\"train_\",)\n",
    "        self.val_metrics = self.train_metrics.clone(prefix=\"val_\")\n",
    "        self.test_metrics = self.train_metrics.clone(prefix=\"test_\")\n",
    "\n",
    "        self.cm = torchmetrics.classification.MulticlassConfusionMatrix(num_classes=self.n_classes)\n",
    "        self.roc_auc = torchmetrics.classification.MulticlassROC(num_classes=self.n_classes)\n",
    "        self.pr_curve = torchmetrics.classification.MulticlassPrecisionRecallCurve(num_classes=self.n_classes)\n",
    "        \n",
    "        print(\"DECENT INFO: init done\")        \n",
    "                \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        # =============================================================================\n",
    "        # we make it possible to use model_output = self(image)\n",
    "        # =============================================================================\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    def set_stage(self, i_prune_stage): #  i_stage, max_epochs):\n",
    "        self.this_prune_stage = i_prune_stage\n",
    "        #self.max_epochs = max_epochs\n",
    "        #self.counter = prev_counter # i_stage * max_epochs # self.prune_stage * self.max_epochs + self.current_epoch\n",
    "\n",
    "    # from here the fit starts    \n",
    "    \n",
    "    def prune(self):\n",
    "        # does not work with on_fit_start\n",
    "        \n",
    "        if self.this_prune_stage < 1:\n",
    "            return\n",
    "        \n",
    "        print(\"DECENT INFO: pruning now at the beginning\")\n",
    "        # pruning and save model\n",
    "\n",
    "        # counter\n",
    "\n",
    "        # update model\n",
    "        # don't update unless pretrain epochs is reached\n",
    "        #if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch >= self.pretrain_epochs:\n",
    "        #    print(\"DECENT NOTE: update model\", self.current_epoch)      \n",
    "\n",
    "        if debug_mode:\n",
    "            print(\"DECENT NOTE: before update\")\n",
    "            print(\"DECENT NOTE: print model ...\")\n",
    "            print(self.model)\n",
    "        \n",
    "        \n",
    "        self.model.update(current_epoch = self.counter)\n",
    "\n",
    "        #if self.reset_optimiser_at_update:\n",
    "            # self.configure_optimizers() # reset optimisers do to big change in loss term + cause of pruned parameters\n",
    "            # self.trainer.accelerator_backend.setup_optimizers(self) xxxxxx\n",
    "            #new_optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "            #self.optimizer.load_state_dict(new_optimizer.state_dict())  \n",
    "\n",
    "            # self.trainer.accelerator.setup_optimizers(self)\n",
    "            #self.trainer.strategy.setup_optimizers(self)\n",
    "            \n",
    "        if debug_mode:\n",
    "            print(\"DECENT NOTE: model updated\")  \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # =============================================================================\n",
    "        # returns:\n",
    "        #    optimiser and lr scheduler\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: configure_optimizers\")\n",
    "        \n",
    "        if self.optim == \"adamw\":\n",
    "            adam = optim.AdamW(self.model.parameters(), lr=self.base_lr)\n",
    "            multistep = optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            \n",
    "             # first a list of optimisers, then a list of learing rate schedulers\n",
    "            return [adam], [multistep]\n",
    "        else:\n",
    "            sgd = optim.SGD(self.model.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            cosine = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser,\n",
    "                                                                    T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                    eta_min = self.min_lr\n",
    "                                                                   )\n",
    "            \n",
    "        for param in self.model.parameters(): # not sure whether this is actually needed\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "        # first a list of optimisers, then a list of learing rate schedulers\n",
    "        return [sgd], [cosine]\n",
    "    \n",
    "        \n",
    "    def on_train_start(self):\n",
    "        # =============================================================================\n",
    "        # logging from first and then iteratively \"previously pruned model\" \n",
    "        # == our current new model\n",
    "        # plot of circular layer - todo - make sure we get this into some other dir\n",
    "        # todo - i have no idea whether the circular plots break the code of node is gone\n",
    "        # =============================================================================\n",
    "\n",
    "        # numel: returns the total number of elements in the input tensor\n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(\"unpruned\", unpruned) # todo: log\n",
    "        self.log('unpruned', float(unpruned), on_step=False, on_epoch=True) # neither ??\n",
    "        \n",
    "        self.log('stage', float(self.this_prune_stage), on_step=False, on_epoch=True)\n",
    "\n",
    "        self.model.plot_incoming_connections(current_epoch=self.this_prune_stage)\n",
    "        self.model.plot_outgoing_connections(current_epoch=self.this_prune_stage)\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        # updates model every nth epoch\n",
    "        # =============================================================================  \n",
    "        \n",
    "        # absolutely never delete this line!!! \n",
    "        self.counter += 1 # = self.prune_stage * self.max_epochs + self.current_epoch\n",
    "        self.log('counter', self.counter, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.log('train_decent2_first_weight_start', self.model.decent2.filter_list[0].weights[0].flatten()[0].detach().cpu().numpy().item(), on_step=False, on_epoch=True)\n",
    "        \n",
    "        print(\"e_start d1:\", self.model.decent1.filter_list[0].weights.flatten()[0])\n",
    "        print(\"e_start d2:\", self.model.decent2.filter_list[0].weights[0].flatten()[0])\n",
    "                \n",
    "        \n",
    "    def on_validation_epoch_start(self): # is this even a thing??\n",
    "        self.log('counter', self.counter, on_step=False, on_epoch=True)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculates loss for a batch\n",
    "        # parameters:\n",
    "        #    batch\n",
    "        #    batch id\n",
    "        # returns:\n",
    "        #    loss\n",
    "        # notes:\n",
    "        #    calling gradcam like self.gradcam(batch) here is dangerous cause changes gradients\n",
    "        # =============================================================================     \n",
    "\n",
    "        \n",
    "        # calculate loss\n",
    "        # loss = torch.tensor(1)\n",
    "        loss = self.run_loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        \n",
    "        # debugging messages!!\n",
    "        if debug_mode:\n",
    "            print(\"t_step d1:\", self.model.decent1.filter_list[0].weights.flatten()[0])\n",
    "            print(\"t_step d2:\", self.model.decent2.filter_list[0].weights[0].flatten()[0])\n",
    "        \n",
    "        if False: # to check the gradients - on 14.11.2024 they seemed fine ..\n",
    "            print(\"next ********************************\")\n",
    "            ignored = []\n",
    "            for i_p, param in enumerate(self.model.parameters()):\n",
    "                if param.grad is not None:\n",
    "                    # print(param.grad)\n",
    "                    print(\"++ para\", i_p, \" \", param.grad.shape, \" \", param.grad.flatten()[0])\n",
    "                else:\n",
    "                    ignored.append(i_p)\n",
    "                    pass\n",
    "                    # (\"++ para NONE\")\n",
    "            print(\"nan element amount:\", len(ignored))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging # 2\n",
    "        # =============================================================================\n",
    "        if False: # batch_idx < 2:\n",
    "            print(\"DECENT NOTE: validation_step\", batch_idx)\n",
    "        \n",
    "        self.run_loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing # 3\n",
    "        # =============================================================================\n",
    "        if debug_mode:\n",
    "            print(\"DECENT NOTE: on_validation_epoch_end\")\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # save model if next iteration model is pruned # 4 \n",
    "        # this needs to be called before callback \n",
    "        # - if internal pytorch lightning convention changes, this will stop working\n",
    "        # =============================================================================\n",
    "        if debug_mode:\n",
    "            print(\"DECENT NOTE: on_train_epoch_end\", self.counter)\n",
    "               \n",
    "        if False:\n",
    "            print(\"current epoch\")\n",
    "            print(((self.counter+1) % self.update_every_nth_epoch) == 0)\n",
    "            print(self.counter+1)\n",
    "            print(self.counter)\n",
    "            print(self.update_every_nth_epoch)\n",
    "        \n",
    "        \n",
    "        #if ((self.current_epoch+1) % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            # if next epoch is an update, set unpruned flag            \n",
    "            \n",
    "            # self.log(f'unpruned_state', 1.0, on_step=False, on_epoch=True)\n",
    "            \n",
    "            # save file\n",
    "            #with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "            #    f.write(\"\\n# parameter requires grad shape #\\n\")\n",
    "            #    for p in self.model.parameters():\n",
    "            #        if p.requires_grad:\n",
    "            #            f.write(str(p.shape))\n",
    "            \n",
    "            \n",
    "        \n",
    "        print(\"e_end d1:\", self.model.decent1.filter_list[0].weights.flatten()[0])\n",
    "        print(\"e_end d2:\", self.model.decent2.filter_list[0].weights[0].flatten()[0])\n",
    "        \n",
    "        self.log('train_decent2_first_weight_end', self.model.decent2.filter_list[0].weights[0].flatten()[0].detach().cpu().numpy().item(), on_step=False, on_epoch=True)\n",
    "        \n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(\"unpruned\", unpruned) # todo: log\n",
    "        # self.log(f'unpruned', float(unpruned), on_step=False, on_epoch=True) # neither ??\n",
    "        \n",
    "    def on_train_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        \n",
    "        # =============================================================================\n",
    "        \n",
    "        # helper/model/decentnet - get_everything() - creates csv\n",
    "        self.model.get_everything(counter='final_test')\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging, plot gradcam\n",
    "        # =============================================================================\n",
    "        if batch_idx < 2:\n",
    "            print(\"DECENT NOTE: test_step\", batch_idx)\n",
    "\n",
    "        # we update mo and gt here\n",
    "        self.run_loss_n_metrics(batch, mode=\"test\")\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.enable_grad():\n",
    "            grad_preds = preds.requires_grad_()\n",
    "            preds2 = self.layer2(grad_preds)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # save image\n",
    "        \n",
    "        if len(batch) == 4:\n",
    "            # if mask is there\n",
    "            img, _, msks, img_id = batch\n",
    "            img_id = img_id.detach().cpu().item()\n",
    "            tmp_b4 = True\n",
    "        else:\n",
    "            # if no mask\n",
    "            img, _ = batch # image and mask come out of this\n",
    "            img_id = batch_idx\n",
    "            msks = None\n",
    "            tmp_b4 = False             \n",
    "        \n",
    "        # print(img.shape)\n",
    "        \n",
    "        # save image\n",
    "        tmp_file_name = f'entry_id{img_id}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        # tmp_img = self.feature_maps.squeeze()[i_map].cpu().detach().numpy()\n",
    "        tmp_img = img.squeeze().cpu().detach().numpy()\n",
    "        tmp_path = os.path.join(self.log_dir, \"img_choice\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_img)\n",
    "        plt.close()\n",
    "        \n",
    "        if tmp_b4:\n",
    "            msks = msks.detach().cpu().numpy().squeeze()\n",
    "            \n",
    "            tmp_msk = msks[0] # 28\n",
    "            \n",
    "            # save mask\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            for o, boundary in enumerate(tmp_msk): # skip last one\n",
    "                # plt.plot(list(range(len(layer))), layer)\n",
    "                plt.plot(boundary[:,1]-0.5, boundary[:,0])\n",
    "            plt.ylim(0, 28 - 1)\n",
    "            plt.gca().invert_yaxis()\n",
    "            #plt.axis('off')\n",
    "            # Save the plot\n",
    "            # plt.savefig('plot_without_axes.png', bbox_inches='tight', pad_inches=0)\n",
    "            tmp_file_name = f'entry_id{img_id}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "            tmp_path = os.path.join(self.log_dir, \"msk_choice\")\n",
    "            os.makedirs(tmp_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(tmp_path, tmp_file_name), bbox_inches='tight', pad_inches=0)\n",
    "            #plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_msk) # todo\n",
    "            plt.close()\n",
    "\n",
    "            # save image + mask (todo)\n",
    "            plt.imshow(tmp_img, cmap=\"gray\")\n",
    "            for o, boundary in enumerate(tmp_msk):\n",
    "                plt.plot(boundary[:,1]-0.5, boundary[:,0])\n",
    "            tmp_file_name = f'entry_id{img_id}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "            tmp_path = os.path.join(self.log_dir, \"img_with_msk\")\n",
    "            os.makedirs(tmp_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(tmp_path, tmp_file_name), bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "            \n",
    "            # save mat file\n",
    "            for msk, msk_size in zip(msks, [28,26,24,22]):\n",
    "                tmp_mat = {'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Fri May 06 15:17:37 2022',\n",
    "                     '__version__': '1.0',\n",
    "                     '__globals__': [],\n",
    "                     'Layer': msk\n",
    "                            }\n",
    "\n",
    "                tmp_file_name = f'mat_id{img_id}_size{msk_size}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.mat'\n",
    "                tmp_path = os.path.join(self.log_dir, \"mat_transformed_choice\")\n",
    "                os.makedirs(tmp_path, exist_ok=True)\n",
    "                scipy.io.savemat(file_name=os.path.join(tmp_path, tmp_file_name), mdict=tmp_mat)\n",
    "            \n",
    "            \n",
    "        \n",
    "            # plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_img)\n",
    "        \n",
    "        # save feature maps of hidden layers and the layer that gets globally pooled\n",
    "        try:\n",
    "            with torch.set_grad_enabled(True): # torch.set_grad_enabled(True):\n",
    "                self.run_xai_gradcam(batch, batch_idx, mode='explain')\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: batch size has to be 1\")\n",
    "            print(e)\n",
    "            \n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            layer = self.model.decent1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            #filter_list.extend(tmp)\n",
    "            \n",
    "            layer = self.model.decent2\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent2' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent3\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent3' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent1x1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1x1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "        # get filter list            \n",
    "        filter_list = []\n",
    "        for l in self.model.decent1.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{1}\")\n",
    "        for l in self.model.decent2.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{2}\")\n",
    "        for l in self.model.decent3.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{3}\")\n",
    "        for l in self.model.decent1x1.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{4}\")\n",
    "        df = pd.DataFrame(filter_list, columns=['filter'])\n",
    "        df.to_csv(os.path.join(self.log_dir, \"all_filters.csv\"), index=False)\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing\n",
    "        # =============================================================================\n",
    "        if False:\n",
    "            print(\"DECENT NOTE: on_test_epoch_end\", self.counter)\n",
    "        \n",
    "        tmp_path = os.path.join(self.log_dir, \"final_plots\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        \n",
    "        # confusion matrix\n",
    "        cm = self.cm.compute()\n",
    "        cm = cm.cpu().numpy()\n",
    "        df_cm = pd.DataFrame(cm, index=list(class_mapper.values()), columns=list(class_mapper.values()))\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        # plt.gca().invert_yaxis() - should not be inverted\n",
    "        plt.savefig(os.path.join(tmp_path, \"confusion_matrix.png\"), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        \n",
    "        # precision-recall curve\n",
    "        pr_precision, pr_recall, pr_thresholds = self.pr_curve.compute()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(self.n_classes):\n",
    "            converted_label = class_mapper.get(str(i))\n",
    "            plt.plot(pr_recall[i].cpu(), pr_precision[i].cpu(), label=f\"{converted_label}\", color=cnv_dr_amd_normal.colors[i]) \n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(\"Precision-Recall Curve\")\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.savefig(os.path.join(tmp_path, \"precision_recall_curve.png\"), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        \n",
    "        roc_fpr, roc_tpr, roc_thresholds = self.roc_auc.compute()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(self.n_classes):\n",
    "            converted_label = class_mapper.get(str(i))\n",
    "            plt.plot(roc_fpr[i].cpu(), roc_tpr[i].cpu(), label=f\"{converted_label}\", color=cnv_dr_amd_normal.colors[i]) # could add AUC here # ... torchmetrics.functional.auroc(preds ... but i don't have access ot the preds here\n",
    "            # plt.plot(roc_fpr[i].cpu(), roc_tpr[i].cpu(), label=f\"Class {i} (AUC = {torchmetrics.functional.auroc(probs[:, i], target == i):.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC-AUC Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(os.path.join(tmp_path, \"roc_auc_curve.png\"), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "    \n",
    "    def run_xai_feature_map(self, batch, batch_idx, layer, layer_str, device='cuda'):\n",
    "        # https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5\n",
    " \n",
    "        # img, label = testset.__getitem__(0) # batch x channel x width x height, class\n",
    "\n",
    "        # img = X(img.to(device).unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "                    \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, msk, img_id = batch\n",
    "            img_id = img_id.detach().cpu().item()\n",
    "        else:\n",
    "            img, ground_truth = batch # image and mask come out of this\n",
    "            img_id = batch_idx\n",
    "            msk = None\n",
    "            \n",
    "        \n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        # print(img.data.shape)\n",
    "\n",
    "        # run feature map\n",
    "        # model, layer, layer_str, log_dir, device=\"cpu\"\n",
    "        fm = filter_activation.DecentFilterActivation(model=self.model, layer=layer, layer_str=layer_str, log_dir=self.log_dir, device=device)\n",
    "        fm.run(tmp_img, img_id)\n",
    "        \n",
    "        filter_list = fm.log()\n",
    "        \n",
    "        return filter_list\n",
    "        \n",
    "        \n",
    "    \n",
    "    def run_xai_gradcam(self, batch, batch_idx, mode='explain'):\n",
    "        # =============================================================================\n",
    "        # grad cam - or just cam?? idk\n",
    "        # todo error: RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n",
    "        # BATCH SIZE HAS TO BE ONE!!!\n",
    "        # grad enable in test mode:\n",
    "        # https://github.com/Project-MONAI/MONAI/discussions/1598\n",
    "        # https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "        # =============================================================================\n",
    "    \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, msk, img_id = batch\n",
    "            img_id = img_id.detach().cpu().item()\n",
    "        else:\n",
    "            img, ground_truth = batch # image and mask come out of this\n",
    "            img_id = batch_idx\n",
    "            msk = None\n",
    "\n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)]) # .requires_grad_()\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        #print(\"nooooooooooo grad, whyyyyy\")\n",
    "        #print(tmp_img1)\n",
    "        #print(img)\n",
    "\n",
    "        #print('b1', tmp_img1)\n",
    "        #print('b2', tmp_img2)\n",
    "\n",
    "        model_output = self(tmp_img1, mode)\n",
    "\n",
    "        #print('c1', tmp_img1)\n",
    "        #print('c2', tmp_img2)\n",
    "\n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "\n",
    "        # get prediction value\n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "\n",
    "        #print('d1', tmp_img1)\n",
    "\n",
    "        #print(\"mo\", model_output)\n",
    "        #print(\"max\", pred_max)\n",
    "        #print(\"backprop\", model_output[:, pred_max])\n",
    "\n",
    "        # backpropagate for gradient tracking\n",
    "        model_output[:, pred_max].backward()\n",
    "\n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        #print('e2', tmp_img2)\n",
    "\n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # draw the heatmap\n",
    "        # plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "        # fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_l2_plot[0])}_n{int(self.n_l2_plot[0])}_{str(current_epoch)}.png\"))\n",
    "        \n",
    "        tmp_path = os.path.join(self.log_dir, \"gradcam\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.imsave(os.path.join(tmp_path, \n",
    "                                f\"cam_id{img_id}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\"\n",
    "                               ), heatmap.detach().cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "        heatmap *= 255.0 / heatmap.max()\n",
    "        pil_heatmap = Image.fromarray(heatmap.detach().cpu().numpy().squeeze()).convert('RGB')\n",
    "       \n",
    "        tmp_path = os.path.join(self.log_dir, \"gradcam\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        pil_heatmap.save(os.path.join(tmp_path, \n",
    "                                      f\"camgray_id{img_id}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" \n",
    "                                     )) \n",
    "            \n",
    "    def run_loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        # =============================================================================\n",
    "        # put image through model, calculate loss and metrics\n",
    "        # use cc term that has been calculated previously\n",
    "        # =============================================================================\n",
    "        \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, mask, img_id = batch\n",
    "        else:\n",
    "            img, ground_truth = batch\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # for test routine \"test_step\"\n",
    "        self.mo = model_output.argmax(dim=1).squeeze().detach().cpu().numpy()\n",
    "        self.gt = ground_truth.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        ground_truth = ground_truth.squeeze()\n",
    "        if len(ground_truth.shape) < 1:\n",
    "            ground_truth = ground_truth.unsqueeze(0)\n",
    "        ce_loss = self.criterion(model_output, ground_truth.long()) # ground_truth_multi_hot)\n",
    "        \n",
    "        \n",
    "        # this thing does not work with the old function - the old connection cost is really bad!!!\n",
    "        # cc = torch.mean(self.model.cc) * self.cc_weight # update_new_connection_cost\n",
    "        if mode == \"train\" and self.new_cc_mode == True:\n",
    "            self.cc_ci = self.model.get_cc_and_ci_loss_term()\n",
    "            \n",
    "            # print(self.cc_ci)\n",
    "            \n",
    "            # print(self.model.decent2.filter_list[1].weights[0][0])\n",
    "            \n",
    "            # get max values to understand what is going on :)\n",
    "            cc_max_decent3 = self.model.decent3.cc_max_of_layer\n",
    "            ci_max_decent3 = self.model.decent3.ci_max_of_layer\n",
    "            #cc_mean = \n",
    "            #ci_mean =                     \n",
    "\n",
    "            \n",
    "            self.log(f'{mode}_cc_max_decent3', cc_max_decent3, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_ci_max_decent3', ci_max_decent3, on_step=False, on_epoch=True)\n",
    "            #self.log(f'{mode}_ci_mean', ci_mean, on_step=False, on_epoch=True)\n",
    "            #self.log(f'{mode}_cc_mean', cc_mean, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_cc', self.cc_ci, on_step=False, on_epoch=True) # this should have a more general name, for the future!!\n",
    "            if debug_mode:\n",
    "                print(\"decent note: self.cc_ci:\", self.cc_ci)\n",
    "                print(\"decent note: cc_max_decent3:\", cc_max_decent3)\n",
    "                print(\"decent note: ci_max_decent3:\", ci_max_decent3)\n",
    "                #print(\"decent note: all_ci = mean:\", all_ci)\n",
    "                #print(\"decent note: all_cc = mean\", all_cc)\n",
    "                print(\"ce_loss\", ce_loss)\n",
    "            \n",
    "            loss = ce_loss + (self.cc_ci * self.cc_weight) # make sure to set the weight in the args, also make sure to use norms that are torch not scipy!!\n",
    "        else:\n",
    "            loss = ce_loss\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "                \n",
    "        if mode == \"train\":\n",
    "            value = self.train_metrics(preds=pred_i, target=ground_truth)\n",
    "            self.log_dict(value, on_step=False, on_epoch=True)\n",
    "                \n",
    "        elif mode == \"val\":\n",
    "            value = self.val_metrics(preds=pred_i, target=ground_truth)\n",
    "            self.log_dict(value, on_step=False, on_epoch=True)\n",
    "                \n",
    "        else:\n",
    "            value = self.test_metrics(preds=pred_i, target=ground_truth)\n",
    "            self.log_dict(value, on_step=False, on_epoch=True)\n",
    "            \n",
    "            \n",
    "            self.cm.update(preds=pred_i, target=ground_truth) # prediction (class)\n",
    "            self.pr_curve.update(preds=model_output, target=ground_truth) # probability\n",
    "            self.roc_auc.update(preds=model_output, target=ground_truth) # probability\n",
    "                        \n",
    "        self.log(f'{mode}_ce_loss', ce_loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        if False: # debug_mode:\n",
    "            print(\"loss\", loss)\n",
    "            print(\"ce_loss\", ce_loss)\n",
    "            print(\"mo\", model_output)\n",
    "            print(\"gt\", ground_truth)\n",
    "            print(\"mo\", self.mo)\n",
    "            print(\"gt\", self.gt)\n",
    "            \n",
    "        if torch.isnan(loss).any():\n",
    "            print(\"DECENT WARNING: Loss contains NaN value(s).\")\n",
    "        \n",
    "        # ce loss + connection cost term\n",
    "        \n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2906-61b6-4f3a-b2fa-7aeeaa12e7e1",
   "metadata": {},
   "source": [
    "## run dev routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_kwargs[\"xai_done\"] = False\n",
    "\n",
    "if train_kwargs[\"train_size\"] > 0:\n",
    "    \n",
    "    # =============================================================================\n",
    "    # train model and run test/xAI routine\n",
    "\n",
    "    # logger - save logs in \"examples/example_results/lightning_logs\"\n",
    "    # light - DecentLightning model\n",
    "    # trainer - pl.Trainer\n",
    "    # trainer.fit\n",
    "    # explainer - pl.Trainer\n",
    "    # explainer.test\n",
    "    # =============================================================================\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "\n",
    "    # THE LOGGER\n",
    "    logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], \"lightning_logs\"), name=train_kwargs[\"exp_name\"])\n",
    "    \n",
    "    ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "\n",
    "    # THE LIGHTNING MODEL\n",
    "    # Initialize the LightningModule\n",
    "    light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir, ckpt_path=ckpt_path)\n",
    "\n",
    "    # THE LIGHTNING TRAINER (for training)\n",
    "    \n",
    "    for i_prune_stage, max_epochs in enumerate(train_kwargs[\"training_stages\"]):\n",
    "        \n",
    "        light.set_stage(i_prune_stage=i_prune_stage)\n",
    "        if i_prune_stage > 0:\n",
    "            light.prune()\n",
    "                \n",
    "        trainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                             accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                             devices=[0],\n",
    "                             # inference_mode=False, # do grad manually\n",
    "                             log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                             logger=logger,\n",
    "                             check_val_every_n_epoch=1,\n",
    "                             max_epochs=max_epochs, # [\"epochs\"], # train_kwargs[\"epochs\"],\n",
    "                             callbacks=[SaveLastModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1_macro\",\n",
    "                                                       filename='mf_{stage}_{counter}_{val_f1_macro:.2f}_{unpruned:.0f}', save_last=True),\n",
    "                                        #ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1_macro\",\n",
    "                                        #               filename='mf_{stage}_{counter}_{val_f1_macro:.2f}_{unpruned:.0f}'), # monitor fscore\n",
    "                                        #EndModelCheckpoint(save_weights_only=True,  mode=\"min\", monitor=\"unpruned\", save_top_k=-1,\n",
    "                                        #                   filename='mu_{epoch}-{val_f1_macro:.2f}-{unpruned:.0f}'), # monitor unpruned\n",
    "                                        #ModelCheckpoint(save_weights_only=True, \n",
    "                                        #                save_last=True, \n",
    "                                        #                filename='mi_{i_stage}_{epoch}-{val_f1_macro:.2f}-{unpruned:.0f}'),\n",
    "                                        #DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                        #                filename='mu_{epoch}-{val_f1_macro:.2f}-{unpruned:.0f}'), # monitor unpruned\n",
    "                                        #OptimiserUpdateCheckpoint(),\n",
    "                                        LearningRateMonitor(\"epoch\")])\n",
    "        \n",
    "        # trainer.save_checkpoint(f\"ml_{light.stage}_{light.counter}-{light.val_f1_macro:.2f}-{light.unpruned:.0f}.ckpt\")\n",
    "\n",
    "        # PRUNE AT BEGINNING OF TRAINING!!!!, skip first stage, i need to set the stage i think ... ? the trainer resets everything ...\n",
    "        \n",
    "        trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "        trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "        # THE TRAIN-RUN\n",
    "        # Train the model using a Trainer\n",
    "        trainer.fit(light, dataloader.train_dataloader, dataloader.val_dataloader)\n",
    "    \n",
    "\n",
    "    # THE LIGHTNING TRAINER (for testing)\n",
    "    # we want the grad to work in test, hence: inference_mode=False\n",
    "    explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"], # is this also wrong?? where is the checkpoint??\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         logger=logger,\n",
    "                         inference_mode=False)\n",
    "\n",
    "    # THE TEST-RUN\n",
    "    # including test\n",
    "    test_result = explainer.test(light, dataloader.xai_dataloader, verbose=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    train_kwargs[\"xai_done\"] = True\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66262489-1c4a-439e-9673-32a40c5c52f1",
   "metadata": {},
   "source": [
    "## run test routine ****************************\n",
    "\n",
    "we need this with the OCTA-500 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f7aa2-96ed-4518-b129-5751bd5e39a8",
   "metadata": {},
   "source": [
    "torch.load(ckpt_path)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e1fef-8dab-44fd-908d-e9f2a901dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 19\n",
      "Missing logger folder: examples/example_results\\lightning_logs\\counter_seems_to_work_xAI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT INFO: be aware, that you have to manually check, whether every output node has an input. otherwise an error may be triggered by cuda\n",
      "DECENT INFO: You are using checkpoint file:  examples/example_results\\lightning_logs\\counter_seems_to_work\\version_3/checkpoints/mf_stage=3.0_counter=13.0_val_f1_macro=0.45_unpruned=3732.ckpt\n",
      "counter_seems_to_work\n",
      "examples/example_results\\lightning_logs\\counter_seems_to_work\\version_3/checkpoints/mf_stage=3.0_counter=13.0_val_f1_macro=0.45_unpruned=3732.ckpt\n",
      "logdir examples/example_results\\lightning_logs\\counter_seems_to_work_xAI\\version_0\n",
      "ckpt_path: examples/example_results\\lightning_logs\\counter_seems_to_work\\version_3/checkpoints/mf_stage=3.0_counter=13.0_val_f1_macro=0.45_unpruned=3732.ckpt\n",
      "Found pretrained model at examples/example_results\\lightning_logs\\counter_seems_to_work\\version_3/checkpoints/mf_stage=3.0_counter=13.0_val_f1_macro=0.45_unpruned=3732.ckpt, loading...\n",
      "DECENT INFO: dimensions are entry, decent1, decent2, decent3, decent1x1 == out [1, 8, 16, 32, 4]\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 7, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 5, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 5, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 5, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 13, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 8, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 12, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 14, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 7, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 5, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 15, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 11, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 11, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 12, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 13, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 7, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 14, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 12, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 11, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 9, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 8, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 10, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 12, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 11, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 8, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 12, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 11, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 10, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 12, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 5, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 15, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 5, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 12, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 8, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 17, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 21, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 20, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 18, 1, 1])\n",
      "[10.]\n",
      "[5.]\n",
      "DECENT INFO: init done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05435bf007b94c0f99b26f087bb7ce8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: test_step 0\n",
      "DECENT NOTE: test_step 1\n"
     ]
    }
   ],
   "source": [
    "if train_kwargs[\"load_mode\"] and not train_kwargs[\"xai_done\"]:\n",
    "\n",
    "    # =============================================================================\n",
    "    # load model and run test/xAI routine\n",
    "\n",
    "    # logger - save logs in \"dumpster\"\n",
    "    # light - DecentLightning model\n",
    "    # explainer - pl.Trainer\n",
    "    # explainer.test\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"DECENT INFO: be aware, that you have to manually check, whether every output node has an input. otherwise an error may be triggered by cuda\")\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "\n",
    "    # train_kwargs[\"load_ckpt_file\"] = \"version_7/checkpoints/epoch=0-val_f1=0.62-unpruned=1560.ckpt\"\n",
    "    \n",
    "    # Check whether pretrained model exists. If yes, load it.\n",
    "    # ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\\debug_oct_no_fc\", 'version_13', 'checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt'])\n",
    "    ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "    print(\"DECENT INFO: You are using checkpoint file: \", ckpt_path)\n",
    "    \n",
    "    # ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "\n",
    "    print(train_kwargs[\"exp_name\"])\n",
    "    \n",
    "    print(ckpt_path)\n",
    "\n",
    "    \n",
    "    if os.path.isfile(ckpt_path):\n",
    "\n",
    "        # tmp = +\"_xAI\"\n",
    "        \n",
    "        # THE LOGGER\n",
    "        logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"]+\"_xAI\") # the xAI routine for an experiment\n",
    "        # logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "        \n",
    "        print(\"logdir\", logger.log_dir)\n",
    "        \n",
    "        # THELIGHTNING MODEL\n",
    "        # load from checkpoint doesn't work, since our architecture is 'messed up' through pruning\n",
    "        # light = DecentLightning.load_from_checkpoint(state_dict, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n",
    "        # use this line instead:\n",
    "        light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir, ckpt_path=ckpt_path)\n",
    "\n",
    "        # THE LIGHTNING TRAINER (for testing)\n",
    "        # we want the grad to work in test, hence: inference_mode=False\n",
    "        explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                             accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                             #devices=[0], # why is this not on??\n",
    "                             logger=logger,\n",
    "                             inference_mode=False)\n",
    "\n",
    "        # THE TEST-RUN\n",
    "        # only test\n",
    "        test_result = explainer.test(light, dataloader.xai_dataloader, verbose=False)\n",
    "\n",
    "    else:\n",
    "        print('DECENT ERROR: not a file - may have been resetted in dev routine, check the load_ckpt_file, set dev routine to False and run everything')\n",
    "\n",
    "    \n",
    "print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696de4a-76ea-4b95-9d11-e1cf8a733a42",
   "metadata": {},
   "source": [
    "# random nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "light.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in light.model.decent3.filter_list:\n",
    "    print(a.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2797f-8a72-46db-86ae-41868e90828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(DecentNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617dcfd1-2827-4b69-8b31-fe8cd1f677ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f2c63-9855-4e48-aec5-5d4b1a971392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(value) \n",
    "        #print(img)\n",
    "        #print(ground_truth)\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "                #print(\"loss\", loss)\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8261ef-19a3-4ec3-b621-11234709efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "            try:\n",
    "                ta = self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "                tf = self.train_f1(preds=pred_i, target=ground_truth) \n",
    "                tp = self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            except Exception as e:\n",
    "                print(\"DECENT ERROR: we are experiencing this CUDA ERROR most likely, because our decent1x1 has too little filters.\")\n",
    "                print(\"We need the same number as classes. It can happen, that all in-connections to a filter in decent1x1 got pruned and hence it is gone.\")\n",
    "                print(\"preds\", pred_i)\n",
    "                print(\"target\", ground_truth)\n",
    "                print(e)\n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"train info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", ta)\n",
    "                print(\"f\", tf)\n",
    "                print(\"p\", tp)\n",
    "                print(\"l\", loss)\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            va = self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            vf = self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            vp = self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"val info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", va)\n",
    "                print(\"f\", vf)\n",
    "                print(\"p\", vp)\n",
    "                print(\"l\", loss)\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            print(pred_i)\n",
    "            print(ground_truth)\n",
    "            ta = self.test_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            tf = self.test_f1(preds=pred_i, target=ground_truth) \n",
    "            tp = self.test_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.test_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.test_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.test_prec, on_step=False, on_epoch=True)\n",
    "            \"\"\"\n",
    "        \n",
    "        \n",
    "            try:\n",
    "            pass # print('pred i', pred_i.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics pred\")\n",
    "            print(e)\n",
    "        try:\n",
    "            pass # print('gt', ground_truth.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics gt\")\n",
    "            print(e)\n",
    "        \n",
    "\n",
    "        #print('self mo', self.mo)\n",
    "        #print('self gt', self.gt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"gt\", ground_truth)\n",
    "        print(\"gt shape\", ground_truth.shape)\n",
    "        print(\"gt type\", ground_truth.type())\n",
    "        print(torch.zeros(ground_truth.size(0), self.n_classes))\n",
    "        \n",
    "        if len(ground_truth.shape) < 2:\n",
    "            ground_truth_tmp_tmp = ground_truth.unsqueeze(1)\n",
    "        else:\n",
    "            ground_truth = ground_truth.transpose(1, 0)\n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth_tmp.size(0), self.n_classes).scatter_(1, ground_truth_tmp.to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
