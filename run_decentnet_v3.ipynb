{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 𝔻𝕖𝕔𝕖𝕟𝕥ℕ𝕖𝕥: 𝕕𝕚𝕤𝕖𝕟𝕥𝕒𝕟𝕘𝕝𝕖𝕕 𝕟𝕖𝕥\n",
    "\n",
    "Goal: create a sparse and modular ConvNet\n",
    "\n",
    "Todos: \n",
    "* [ ] delete node (filter) if either no input or no output edges\n",
    "* [ ] AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
    "* [ ] cuda error, if one of the decent1x1 has no kernels left - we need at least one input for each 1x1 filter\n",
    "\n",
    "\n",
    "Notes:\n",
    "* additionally needed: position, activated channels, connection between channels\n",
    "* within this layer, a whole filter can be deactivated\n",
    "* within a filter, single channels can be deactivated\n",
    "* within this layer, filters can be swapped\n",
    "     \n",
    "* pruning actually doesn\"t work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf\n",
    "\n",
    "pip install:\n",
    "* pytorch_lightning\n",
    "\n",
    "\n",
    "warnings:\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73a26-f239-466f-901d-559d192f61de",
   "metadata": {},
   "source": [
    "![uml of code](examples/example_vis/uml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba244e3-3e6a-47e7-aa4b-a22df6054b8a",
   "metadata": {},
   "source": [
    "# conventions\n",
    "\n",
    "* entry image: entry_id5_0_0_0_mo3_gt2.png\n",
    "* hidden layer: hid_id5_3_8_2.png\n",
    "* last layer (global pooling - connected to class n): pool_2_3_4_gp2.png\n",
    "* activated image: cam_id5_mo3_gt2.png\n",
    "* activated image gray: camg_id5_mo3_gt2.png\n",
    "\n",
    "\n",
    "* circle in: in_2_3_4_ep65.png\n",
    "* circle out: out_2_3_4_ep65.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# alphabetic order misc\n",
    "# =============================================================================\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys \n",
    "sys.path.insert(0, \"helper\")\n",
    "from typing import Optional, List, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "# =============================================================================\n",
    "# torch\n",
    "# =============================================================================\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# =============================================================================\n",
    "# datasceyence\n",
    "# =============================================================================\n",
    "from helper.visualisation.feature_map import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397b450-c5c6-4da1-b630-7bf80e46891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch 2.0.0 == False\n",
      "tl 2.1.0 == False\n"
     ]
    }
   ],
   "source": [
    "seed = 1997 # was 19 before\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "debug_model = False\n",
    "\n",
    "print('torch 2.0.0 ==', torch.__version__=='2.0.0')\n",
    "print('tl 2.1.0 ==', pl.__version__=='2.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b0be-245d-49c0-88b4-d94167f7da90",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97670e82-0d27-4b7f-91df-874acf9974a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'result_path': 'examples/example_results', 'exp_name': 'debug_oct_no_fc', 'load_ckpt_file': 'version_13/checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt', 'epochs': 3, 'img_size': 28, 'batch_size': 2, 'log_every_n_steps': 4, 'device': 'cuda', 'num_workers': 0, 'train_size': 8, 'val_size': 8, 'test_size': 8}\n",
      "model kwargs {'n_classes': 10, 'out_dim': [1, 4, 4, 8], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 10, 'cc_metric': 'l2', 'ci_metric': 'random', 'cm_metric': 'not implemented yet', 'update_every_nth_epoch': 1, 'pretrain_epochs': 1, 'prune_keep': 0.7, 'prune_keep_total': 0.4}\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {\n",
    "    'n_classes': 10,\n",
    "    'out_dim' :  [1, 4, 4, 8], # [1, 8, 16, 32], #[1, 16, 24, 32]\n",
    "    'grid_size' : 18*18,\n",
    "    'criterion': torch.nn.CrossEntropyLoss(),# torch.nn.BCEWithLogitsLoss(),\n",
    "    'optimizer': \"sgd\", # sgd adamw\n",
    "    'base_lr': 0.001,\n",
    "    'min_lr' : 0.00001,\n",
    "    'momentum' : 0.9,\n",
    "    'lr_update' : 100,\n",
    "    'cc_weight': 10,\n",
    "    'cc_metric' : 'l2', # connection cost metric (for loss) - distance metric\n",
    "    'ci_metric' : 'random', # channel importance metric (for pruning)\n",
    "    'cm_metric' : 'not implemented yet', # 'count', # crossing minimisation \n",
    "    'update_every_nth_epoch' : 1, # 5\n",
    "    'pretrain_epochs' : 1, # 20\n",
    "    'prune_keep' : 0.7, # 0.97, # in each epoch\n",
    "    'prune_keep_total' : 0.4, # this number is not exact, depends on the prune_keep value\n",
    "}\n",
    "\n",
    "train_kwargs = {\n",
    "    'result_path': \"examples/example_results\", # \"example_results/lightning_logs\", # not in use??\n",
    "    'exp_name': \"debug_oct_no_fc\", # must include oct or retina\n",
    "    'load_ckpt_file' : 'version_13/checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt', # \"version_0/checkpoints/epoch=94-unpruned=1600-val_f1=0.67.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "    'epochs': 3, # including the pretrain epochs - no adding up\n",
    "    'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ... # MNIST/MedMNIST 28 × 28 Pixel\n",
    "    'batch_size': 2, # 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "    'log_every_n_steps' : 4, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "    'device': \"cuda\",\n",
    "    'num_workers' : 0, # 18, # 18 for computer, 0 for laptop\n",
    "    'train_size' : (2 * 4), # total or percentage\n",
    "    'val_size' : (2 * 4), # total or percentage\n",
    "    'test_size' : 8, # total or percentage - 0 for all\n",
    "}\n",
    "\n",
    "print(\"train kwargs\", train_kwargs)\n",
    "print(\"model kwargs\", model_kwargs)\n",
    "\n",
    "kwargs = {'train_kwargs':train_kwargs, 'model_kwargs':model_kwargs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ee3d0-9c79-4917-a355-093f1daf4855",
   "metadata": {},
   "source": [
    "## check the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3d5ff36-c015-4bd6-8b12-c4d0e3b64b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4200\n"
     ]
    }
   ],
   "source": [
    "breaking = 6000*model_kwargs['prune_keep_total']\n",
    "weights = 6000 # this value is an estimate for a model [1, 8, 16, 32]\n",
    "# 'unpruned' is the logger variable for the value\n",
    "\n",
    "for i in range(train_kwargs['epochs']):\n",
    "    \n",
    "    if (weights < breaking): # weights*model_kwargs['prune_keep']\n",
    "        print(\"stop:\", breaking)\n",
    "        print('you need at least this many epochs:', i)\n",
    "        print('you currently have this many epochs:', train_kwargs['epochs'])\n",
    "        print(\"recommended to add 2*update_every_nth_epoch\")\n",
    "        break\n",
    "    \n",
    "    # not sure whether -1 is correct, have to check\n",
    "    if i > model_kwargs['pretrain_epochs'] and ((i-1)%model_kwargs['update_every_nth_epoch'] == 0):\n",
    "        weights = int(weights*model_kwargs['prune_keep'])\n",
    "    \n",
    "        print(i, weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {},
   "source": [
    "# DecentNet trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(size=train_kwargs[\"img_size\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])   \n",
    "\n",
    "rgb_transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(size=train_kwargs[\"img_size\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])   \n",
    "    \n",
    "if 'oct' in train_kwargs['exp_name']:\n",
    "    from medmnist import OCTMNIST\n",
    "    dataset = OCTMNIST(split=\"train\", transform=transform, download=True)\n",
    "    testset = OCTMNIST(split=\"test\", transform=transform, download=True) \n",
    "    model_kwargs['n_classes'] = 4\n",
    "    \n",
    "    indices = np.arange(len(dataset))  \n",
    "    train_indices, val_indices = train_test_split(indices, train_size=train_kwargs[\"train_size\"], test_size=train_kwargs[\"val_size\"], stratify=dataset.labels)\n",
    "    \n",
    "elif 'retina' in train_kwargs['exp_name']:\n",
    "    from medmnist import RetinaMNIST\n",
    "    dataset = RetinaMNIST(split=\"train\", transform=rgb_transform, download=True)\n",
    "    testset = RetinaMNIST(split=\"test\", transform=rgb_transform, download=True) \n",
    "    model_kwargs['n_classes'] = 5\n",
    "    \n",
    "    indices = np.arange(len(dataset))  \n",
    "    train_indices, val_indices = train_test_split(indices, train_size=train_kwargs[\"train_size\"], test_size=train_kwargs[\"val_size\"], stratify=dataset.labels)\n",
    "    \n",
    "else:\n",
    "    dataset = torchvision.datasets.MNIST(root=\"examples/example_data\", train=True, transform=transform, download=True)\n",
    "    testset = torchvision.datasets.MNIST(root=\"examples/example_data\", train=False, transform=transform, download=True)\n",
    "    model_kwargs['n_classes'] = 10\n",
    "    \n",
    "    indices = np.arange(len(dataset))  \n",
    "    train_indices, val_indices = train_test_split(indices, train_size=train_kwargs[\"train_size\"], test_size=train_kwargs[\"val_size\"], stratify=dataset.targets)\n",
    "\n",
    "\n",
    "train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_subset, shuffle=True, batch_size=train_kwargs[\"batch_size\"], num_workers=train_kwargs[\"num_workers\"])\n",
    "\n",
    "val_subset = torch.utils.data.Subset(dataset, val_indices)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_subset, shuffle=False, batch_size=train_kwargs[\"batch_size\"], num_workers=train_kwargs[\"num_workers\"]) # , persistent_workers=True)\n",
    "\n",
    "# batch size has to be 1\n",
    "if train_kwargs[\"test_size\"] > 0:\n",
    "    testset = torch.utils.data.Subset(testset, range(train_kwargs[\"test_size\"]))\n",
    "xai_dataloader = torch.utils.data.DataLoader(testset, shuffle=False, batch_size=1, num_workers=train_kwargs[\"num_workers\"]) # , persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a51afc1-0512-43ac-9e43-582b0a21e1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python_class': 'OCTMNIST', 'description': 'The OCTMNIST is based on a prior dataset of 109,309 valid optical coherence tomography (OCT) images for retinal diseases. The dataset is comprised of 4 diagnosis categories, leading to a multi-class classification task. We split the source training set with a ratio of 9:1 into training and validation set, and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−1,536)×(277−512). We center-crop the images and resize them into 1×28×28.', 'url': 'https://zenodo.org/record/6496656/files/octmnist.npz?download=1', 'MD5': 'c68d92d5b585d8d81f7112f81e2d0842', 'task': 'multi-class', 'label': {'0': 'choroidal neovascularization', '1': 'diabetic macular edema', '2': 'drusen', '3': 'normal'}, 'n_channels': 1, 'n_samples': {'train': 97477, 'val': 10832, 'test': 1000}, 'license': 'CC BY 4.0'}\n"
     ]
    }
   ],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "info = INFO['octmnist']\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a63827d6-1e9f-4537-a86f-00515d8578cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1a0bf5e-8191-4182-a682-d1c34495a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bac8cc05-fbe0-4125-910d-55c171f68101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.tensor(dataset.labels).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # an object with image representations and their positions\n",
    "    # amout of channels need to have same length as m and n lists\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        self.data = data # list of tensors (image representations)\n",
    "        self.ms_x = ms_x # list of integers (m position of each image representation)\n",
    "        self.ns_x = ns_x # list of integers (n position of each image representation)\n",
    "                \n",
    "    def setter(self, data, ms_x, ns_x):\n",
    "        self.data = data\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        \n",
    "    def getter(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfa123-54e1-4053-94c3-52b45ec0859c",
   "metadata": {},
   "source": [
    "## DecentFilter\n",
    "* conv2d problem: https://stackoverflow.com/questions/61269421/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # convolution happens in here\n",
    "    # one filter has multiple channels (aka weights)\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, ms_in, ns_in, m_this, n_this,\n",
    "                 load_weight=None,\n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 padding_mode=\"zeros\",\n",
    "                 dilation=3, \n",
    "                 # transposed=None, \n",
    "                 device=None, \n",
    "                 dtype=None):\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        \n",
    "        # padding\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        \n",
    "         \n",
    "        # convolution\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding_mode = padding_mode\n",
    "        self.padding = padding\n",
    "        self.dilation = _pair(dilation)\n",
    "        #self.transposed = transposed\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"weight shape init\")\n",
    "        #print(self.weights.shape)\n",
    "            \n",
    "        # bias    \n",
    "        if False: \n",
    "            # bias:\n",
    "            # where should the bias be???\n",
    "            self.bias = Parameter(torch.empty(1, **factory_kwargs))\n",
    "        else:\n",
    "            #self.bias = False\n",
    "            # we only use bias via instance normalisation\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # reset weights and bias in filter\n",
    "        if load_weight == None:\n",
    "            \n",
    "            \n",
    "            # position, currently not trainable \n",
    "            # self.non_trainable_param = nn.Parameter(torch.Tensor([1.0]), requires_grad=False)\n",
    "            self.ms_in = nn.Parameter(torch.Tensor(ms_in), requires_grad=False) # ms_in # list\n",
    "            self.ns_in = nn.Parameter(torch.Tensor(ns_in), requires_grad=False) # ns_in # list\n",
    "            self.m_this = nn.Parameter(torch.Tensor([m_this]), requires_grad=False) # m_this # single integer\n",
    "            self.n_this = nn.Parameter(torch.Tensor([n_this]), requires_grad=False) # n_this # single integer\n",
    "            \n",
    "            # weights\n",
    "            assert len(ms_in) == len(ns_in), \"ms_in and ns_in are not of same length\"\n",
    "            self.n_weights = len(ms_in)\n",
    "            \n",
    "            # weight\n",
    "            # filters x channels x kernel x kernel\n",
    "            # self.weights = torch.autograd.Variable(torch.randn(1,n_weights,*self.kernel_size)).to(\"cuda\")\n",
    "            # self.weights = torch.nn.Parameter(torch.randn(1,n_weights,*self.kernel_size))\n",
    "            self.weights = torch.nn.Parameter(torch.empty((1, self.n_weights, *self.kernel_size), **factory_kwargs))\n",
    "            self.reset_parameters()\n",
    "        else: # load from checkpoint\n",
    "            \n",
    "            # print(ms_in.type, ns_in.type, load_weight.type, m_this.type, n_this.type)\n",
    "            \n",
    "            self.ms_in = nn.Parameter(ms_in, requires_grad=False).to(device)\n",
    "            self.ns_in = nn.Parameter(ns_in, requires_grad=False).to(device)\n",
    "            self.m_this = nn.Parameter(m_this, requires_grad=False).to(device)\n",
    "            self.n_this = nn.Parameter(n_this, requires_grad=False).to(device)\n",
    "            self.weights = nn.Parameter(load_weight).to(device)\n",
    "            \n",
    "            \n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*self.kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        torch.nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))        \n",
    "        \n",
    "    def forward(self, x:X) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # first, we have to remove channels in X\n",
    "        # this is because some channels in the filter are pruned (aka gone)\n",
    "        # then we can apply convolution\n",
    "        # parameters:\n",
    "        #    x = batch x channels x width x height\n",
    "        # returns:\n",
    "        #    x_data: batch x filters x width x height\n",
    "        # saves:\n",
    "        #    self.weights = 1 filter x channels x kernel x kernel\n",
    "        # =============================================================================\n",
    "        \n",
    "        \n",
    "        # POSITION MATCHER\n",
    "        # Find the indices (IDs) of channel pairs that exist in both the X and then filter\n",
    "        common_pairs = [[i_in, i_x] for i_in, (m_in, n_in) in enumerate(zip(self.ms_in, self.ns_in)) for i_x, (m_x, n_x) in enumerate(zip(x.ms_x, x.ns_x)) if (m_in==m_x and n_in==n_x)]\n",
    "        \n",
    "        if False:\n",
    "            print(common_pairs)\n",
    "            print(len(self.ms_in))\n",
    "            print(len(self.ns_in))\n",
    "            print(len(x.ms_x))\n",
    "            print(len(x.ns_x))\n",
    "\n",
    "            for pair in common_pairs:\n",
    "                print(f\"Common pair at indices {pair}: {self.ms_in[pair[0]], tmp_ms[pair[1]]}, {self.ns_in[pair[0]], tmp_ns[pair[1]]}\")\n",
    "        \n",
    "        common_pairs_a = np.array(common_pairs)\n",
    "        try:\n",
    "            f_ids = common_pairs_a[:,0]\n",
    "            x_ids = common_pairs_a[:,1]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            \"\"\"\n",
    "            print(\"error: no common pairs\")\n",
    "            print(\"pairs\", common_pairs_a)\n",
    "            print(\"pairs shape\", common_pairs_a.shape)\n",
    "            print(\"len ms in\", len(self.ms_in))\n",
    "            print(\"len ns in\", len(self.ns_in))\n",
    "            print(\"len ms x\", len(x.ms_x))\n",
    "            print(\"len ns x\", len(x.ns_x))\n",
    "            print(e)\n",
    "            \"\"\"\n",
    "            \n",
    "            # in this case the whole filter should be removed\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        # filter data and weights based on common pairs of data and weights\n",
    "        tmp_d = x.data[:, x_ids, :, :]\n",
    "        tmp_w = self.weights[:, f_ids, :, :]\n",
    "        \n",
    "        # the final convolution\n",
    "        if self.padding_mode != 'zeros':\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(F.pad(tmp_d, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            tmp_w, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, groups=1)\n",
    "        else:\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(tmp_d, tmp_w, self.bias, self.stride, self.padding, self.dilation, groups=1)\n",
    "        \n",
    "        #print(\"tmp_w\", tmp_w.shape)\n",
    "        \n",
    "        # print(x_data.shape, \"- batch x filters x width x height\")\n",
    "        return x_data\n",
    "    \n",
    "    def setter(self, value, m_this, n_this):\n",
    "        # preliminary, not in use\n",
    "        self.weights = value # weights in this filter\n",
    "        self.m_this = m_this # single integer\n",
    "        self.n_this = n_this # single integer\n",
    "    \n",
    "    def getter(self):\n",
    "        # preliminary, not in use\n",
    "        return self.weights, self.m_this, self.n_this\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'DecentFilter(weights: ' + str(self.weights.shape) + ' at position: m_this=' + str(self.m_this) + ', n_this=' + str(self.n_this) + ')' + \\\n",
    "    '\\n with inputs: ms_in= ' + ', '.join(str(int(m.item())) for m in self.ms_in) + ', ns_in= ' + ', '.join(str(int(n.item())) for n in self.ns_in) + ')'\n",
    "    __repr__ = __str__\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdab4-e0b0-4523-882f-dad3ebf06090",
   "metadata": {},
   "source": [
    "## DecentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fddb74c7-5940-424d-aab8-2c75efd914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLayer(torch.nn.Module):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # we save filters of the layer in the self.filter_list\n",
    "    # each filter has a position (m_this, n_this)\n",
    "    # each filter has input positions (ms_in, ns_in)\n",
    "    #    - these vary between filters, as some are pruned\n",
    "    # at the moment we have to loop through the filter list\n",
    "    # convolution is applied to each filter separately which makes this very slow\n",
    "    #\n",
    "    # =============================================================================\n",
    "    __constants__ = ['stride', 'padding', 'dilation', # 'groups',\n",
    "                     'padding_mode', # 'n_channels', #  'output_padding', # 'n_filters',\n",
    "                     'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "                \n",
    "    def __init__(self, ms_in:list, ns_in:list, n_filters:int,\n",
    "                 kernel_size: _size_2_t,  \n",
    "                 stride: _size_2_t = 1,  \n",
    "                 padding: Union[str, _size_2_t] = 0,  \n",
    "                 dilation: _size_2_t = 1,\n",
    "                 model_kwargs=None,\n",
    "                 layer_name=None,\n",
    "                 #prune_keep:float = 0.9,\n",
    "                 #prune_keep_total:float = 0.5,\n",
    "                 #transposed: bool = False, \n",
    "                 #grid_size:int=81,\n",
    "                 #ci_metric=\"l2\",\n",
    "                 #output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 #groups: int = 1,\n",
    "                 ckpt_path='',\n",
    "                 bias: bool = True,  # not in use\n",
    "                 padding_mode: str = \"zeros\",  # not in use\n",
    "                 device=None,  # not in use\n",
    "                 dtype=None) -> None:\n",
    "        # =============================================================================\n",
    "        # initialisation\n",
    "        # parameters:\n",
    "        #    a lot.\n",
    "        # =============================================================================\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_name = layer_name\n",
    "        \n",
    "        # prune numbers\n",
    "        self.prune_keep = model_kwargs[\"prune_keep\"] # in each update [0.0:1.0]\n",
    "        self.prune_keep_total = model_kwargs[\"prune_keep_total\"] # total [0.0:1.0]\n",
    "        \n",
    "        # importance metric for pruning\n",
    "        self.ci_metric = model_kwargs[\"ci_metric\"]\n",
    "        # distance metric for loss\n",
    "        self.cc_metric = model_kwargs[\"cc_metric\"]\n",
    "        \n",
    "        # from prev layer\n",
    "        self.ms_in = ms_in\n",
    "        self.ns_in = ns_in\n",
    "        \n",
    "        self.original_size = len(self.ms_in) * n_filters\n",
    "        \n",
    "        \n",
    "        self.grid_size = model_kwargs[\"grid_size\"]\n",
    "        self.grid_sqrt = math.sqrt(self.grid_size)\n",
    "        assert self.grid_sqrt == int(self.grid_sqrt), f\"square root ({self.grid_sqrt}) from grid size {self.grid_size} not possible; possible exampes: 81 (9*9), 144 (12*12)\"\n",
    "        self.grid_sqrt = int(self.grid_sqrt)\n",
    "        \n",
    "        if ckpt_path == '':\n",
    "\n",
    "            # use techniques from coo matrix\n",
    "            self.geometry_array = np.full(self.grid_size, np.nan)\n",
    "            # plus 1 here cause of to_sparse array\n",
    "            self.geometry_array[0:n_filters] = range(1,n_filters+1)\n",
    "            np.random.shuffle(self.geometry_array)\n",
    "            self.geometry_array = self.geometry_array.reshape((self.grid_sqrt,self.grid_sqrt), order='C')\n",
    "            self.geometry_array = torch.tensor(self.geometry_array)\n",
    "            self.geometry_array = self.geometry_array.to_sparse(sparse_dim=2).to(\"cuda\")\n",
    "\n",
    "            #print(self.geometry_array)\n",
    "            #print(self.geometry_array.values())\n",
    "\n",
    "            self.filter_list = torch.nn.ModuleList([])\n",
    "            for i_filter in range(n_filters):\n",
    "                # minus 1 here cause of to_sparse array\n",
    "                index = (self.geometry_array.values()-1 == i_filter).nonzero(as_tuple=True)[0]\n",
    "                self.m_this = self.geometry_array.indices()[0][index]\n",
    "                self.n_this = self.geometry_array.indices()[1][index]\n",
    "                f = DecentFilter(self.ms_in, self.ns_in, self.m_this, self.n_this, \n",
    "                                 kernel_size=kernel_size, \n",
    "                                 stride=stride, padding=padding, dilation=dilation)\n",
    "                self.filter_list.append(f)\n",
    "                # self.register_parameter(f\"filter {i_filter}\", f.weights)\n",
    "\n",
    "                #torch.nn.Parameter(torch.empty((1, n_channels, *kernel_size), **factory_kwargs))\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            state_dict = torch.load(ckpt_path)\n",
    "            \n",
    "            # print(state_dict)\n",
    "            \n",
    "            self.filter_list = torch.nn.ModuleList([])\n",
    "            \n",
    "            # 'model.decent1.filter_list.0.ms_in'\n",
    "            \n",
    "            # self.layer_name -> decent1\n",
    "            # self.filter_list element 0,1,2,3 -> filter_list.0 \n",
    "            # self.ms_in -> ms\n",
    "            \n",
    "            # init the first one\n",
    "            if False:\n",
    "                self.m_this = state_dict['state_dict'][f'model.{self.layer_name}.filter_list.0.m_this']\n",
    "                self.n_this = state_dict['state_dict'][f'model.{self.layer_name}.filter_list.0.n_this']\n",
    "                self.ms_in  = torch.tensor([0])\n",
    "                self.ns_in  = torch.tensor([0])\n",
    "                load_weight = state_dict['state_dict'][f'model.{self.layer_name}.filter_list.0.weights']\n",
    "\n",
    "                start = 1 if self.layer_name == 'decent1' else 0\n",
    "                \n",
    "            for i_filter in range(0, n_filters):\n",
    "                \n",
    "                self.m_this = state_dict['state_dict'][f'model.{self.layer_name}.filter_list.{i_filter}.m_this']\n",
    "                self.n_this = state_dict['state_dict'][f'model.{self.layer_name}.filter_list.{i_filter}.n_this']\n",
    "                self.ms_in  = state_dict['state_dict'][f'model.{self.layer_name}.filter_list.{i_filter}.ms_in']\n",
    "                self.ns_in  = state_dict['state_dict'][f'model.{self.layer_name}.filter_list.{i_filter}.ns_in']\n",
    "                load_weight = state_dict['state_dict'][f'model.{self.layer_name}.filter_list.{i_filter}.weights']\n",
    "                \n",
    "                '''\n",
    "                ('model.decent1.filter_list.0.ms_in',\n",
    "                  tensor([0.], device='cuda:0')),\n",
    "                 ('model.decent1.filter_list.0.ns_in',\n",
    "                  tensor([0.], device='cuda:0')),\n",
    "                 ('model.decent1.filter_list.0.m_this',\n",
    "                  tensor([10.], device='cuda:0')),\n",
    "                 ('model.decent1.filter_list.0.n_this',\n",
    "                  tensor([10.], device='cuda:0')),\n",
    "                 ('model.decent1.filter_list.0.weights',\n",
    "                  tensor([[[[ 0.1544, -0.2856,  0.2014],\n",
    "                            [-0.1769, -0.2436, -0.1253],\n",
    "                            [-0.2834, -0.4189,  0.3109]]]], device='cuda:0')),\n",
    "                '''\n",
    "                \n",
    "                \n",
    "                f = DecentFilter(self.ms_in, self.ns_in, self.m_this, self.n_this, \n",
    "                                 load_weight=load_weight,\n",
    "                                 kernel_size=kernel_size, \n",
    "                                 stride=stride, padding=padding, dilation=dilation)\n",
    "                self.filter_list.append(f)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def run_layer_connection_cost(self) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # compute connection cost for this layer - based on distance\n",
    "        # returns:\n",
    "        #    connection cost for the loss function\n",
    "        # notes:\n",
    "        #    currently using l2 norm, doesn't work that well\n",
    "        # sources:\n",
    "        #    adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        #    https://stackoverflow.com/questions/74086766/how-to-find-total-cost-of-each-path-in-graph-using-dictionary-in-python\n",
    "        # nonsense?\n",
    "        #    i don't even know what the following comments are about ... \n",
    "        #    based on previous layer (cause I only have input ms_in, n_in information)\n",
    "        #    mean( sum( of connection cost between this filter and all incoming filters\n",
    "        #    need it for loss - aka all layers, all filters together\n",
    "        #    need it for swapping - this layer, all filters\n",
    "        #    only the active ones (we need to use the indices for that)\n",
    "        #    for swapping i need ??\n",
    "        # =============================================================================\n",
    "         \n",
    "        from scipy.spatial.distance import cdist\n",
    "        \n",
    "        # connection cost list\n",
    "        cc = []\n",
    "        \n",
    "        \n",
    "        for f in self.filter_list:\n",
    "            # for each filter we use the current position and all incoming positions\n",
    "\n",
    "            #mn = torch.cat([torch.tensor(f.m_this), torch.tensor(f.n_this)])\n",
    "            #print(mn.shape)\n",
    "            #msns = torch.cat([torch.tensor(f.ms_in), torch.tensor(f.ns_in)]) # .transpose(1,0)\n",
    "            #print(msns.shape)\n",
    "            #cc.append(torch.cdist(mn.unsqueeze(dim=0), msns.transpose(1,0), 'euclidean') / 8) # number comes from 9*9 = 81 [0-8]\n",
    "        \n",
    "            mn = torch.cat([f.m_this.unsqueeze(0), f.n_this.unsqueeze(0)]).transpose(1,0)\n",
    "            msns = torch.cat([f.ms_in.unsqueeze(0), f.ns_in.unsqueeze(0)]).transpose(1,0)\n",
    "            #print(mn)\n",
    "            #print(msns)\n",
    "\n",
    "            # mean ( l2 norm as distance metric / normalisation term for l2 norm)\n",
    "            # mean of distances\n",
    "            # normalise with max=grid square root, min=0\n",
    "            # mean from all non-nan values\n",
    "            # \n",
    "            \n",
    "            if self.cc_metric == 'l1':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'cityblock') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'l2':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'euclidean') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'l2_torch':\n",
    "                cc.append(torch.nanmean( torch.cdist( a=mn.float(), b=msns.float(), p=2) /self.grid_sqrt ))\n",
    "            elif self.cc_metric == 'linf':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'chebyshev') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'cos':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'cosine') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'jac':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'jaccard') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'cor':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'correlation') /self.grid_sqrt ) ))\n",
    "                \n",
    "\n",
    "        # mean connection cost of a layer\n",
    "        # mean from all non-nan values\n",
    "        return torch.nanmean(torch.tensor(cc))\n",
    "    \n",
    "    def run_channel_importance(self, i_f:int) -> list:\n",
    "        # =============================================================================\n",
    "        # compute channel importance metric for pruning\n",
    "        # calculate the norm of each weight in filter with id i_f\n",
    "        # we need to call this in a loop to go through each filter\n",
    "        # returns:\n",
    "        #     ci: channel importance list of a filter\n",
    "        # notes:\n",
    "        #     based on l2 norm = magnitude = euclidean distance\n",
    "        # nonsense?\n",
    "        #    maybe the kernel trigger todo\n",
    "        #    print(self.filter_list[i_f].weights.shape)\n",
    "        #    print(self.filter_list[i_f].weights[:,i_w].shape)\n",
    "        # =============================================================================\n",
    "        \n",
    "        ci = []\n",
    "        \n",
    "        # print(\"DECENT NOTE: weight shape\", self.filter_list[i_f].weights.shape)\n",
    "        \n",
    "        for i_w in range(self.filter_list[i_f].weights.shape[1]): # todo, sure this is 1 and not 0???\n",
    "            # importance of a kernel in a layer\n",
    "            \n",
    "            if self.ci_metric == 'l1':\n",
    "                # weight dependent - filter norm\n",
    "                print(\"nooooooooooooooooo\")\n",
    "                pass\n",
    "                \n",
    "                # ci.append(self.filter_list[i_f].weights[:,i_w].norm(2).detach().cpu().numpy())\n",
    "                \n",
    "            elif self.ci_metric == 'l2':\n",
    "                # weight dependent - filter norm\n",
    "                ci.append(self.filter_list[i_f].weights[:,i_w].norm(2).detach().cpu().numpy()) # .detach().cpu().numpy()\n",
    "                pass\n",
    "            \n",
    "            elif self.ci_metric == '':\n",
    "                # weight dependent - filter correlation\n",
    "                print(\"nooooooooooooooooo\")\n",
    "                pass\n",
    "            \n",
    "            elif self.ci_metric == '':\n",
    "                # activation-based\n",
    "                print(\"nooooooooooooooooo\")\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                # mutual information\n",
    "                print(\"nooooooooooooooooo\")\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                # Hessian matrix / Taylor\n",
    "                print(\"nooooooooooooooooo\")\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                print(\"nooooooooooooooooo\")\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == 'random':\n",
    "                ci.append( np.array(random.random()) )\n",
    "\n",
    "        \n",
    "        return ci \n",
    "    \n",
    "    def run_swap_filter(self):\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # we swap filters within the layer\n",
    "        # based on connection cost\n",
    "        # filter can move a maximum of two positions per swap\n",
    "        # change positions\n",
    "        # change\n",
    "        # =============================================================================\n",
    "        print(\"swap here\")\n",
    "        self.m_this = self.m_this # single integer\n",
    "        self.n_this = self.n_this # single integer\n",
    "        pass\n",
    "    \n",
    "    def run_grow_filter(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet* 100\n",
    "        # introduce new filters in a layer\n",
    "        # based on \n",
    "        # algorithmic growth process \n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def run_grow_channel(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # introduce new channel in a layer\n",
    "        # based on connection cost??\n",
    "        # algorithmic growth process \n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def run_prune_filter(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # delete filter in a layer - this will be done somewhere else\n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def run_prune_channel(self, i_f:int, keep_ids:list) -> None:\n",
    "        # =============================================================================\n",
    "        # delete channels in a filter based on keep_ids\n",
    "        # based on importance score\n",
    "        # only keep \"the best\" weights\n",
    "        # pruning based on a metric\n",
    "        # nonsense?\n",
    "        #    delete layer with id\n",
    "        #    delete channels in each layer with id\n",
    "        #    channel deactivation\n",
    "        #    require_grad = False/True for each channel\n",
    "        #    deactivate_ids = [1, 2, 6]\n",
    "        #    self.active[deactivate_ids] = False\n",
    "        #    print(\"weight\")\n",
    "        #    print(self.weight.shape)\n",
    "        #    print(self.weight[:,self.active,:,:].shape)\n",
    "        #    this is totally wrong - iterative will break after first iteration\n",
    "        #    print()\n",
    "        #    Good to hear it’s working, although I would think you’ll get an error at some point in your code, as the cuda() call creates a non-leaf tensor.\n",
    "        #    self.weight = torch.nn.Parameter(  self.weight[:,self.active,:,:] ) # .detach().cpu().numpy()\n",
    "        #    self.weight = self.weight.cuda()\n",
    "        #    print(self.weight.shape)\n",
    "        #    print(self.active)\n",
    "        #    print(\"prune here\")\n",
    "        #    for f in self.filter_list:\n",
    "        #        f.update()\n",
    "        # =============================================================================\n",
    "        \n",
    "        if False:\n",
    "            for i in keep_ids:\n",
    "                print(i)\n",
    "                print(self.filter_list[i_f].ms_in[i])\n",
    "                print(torch.nn.Parameter(self.filter_list[i_f].ms_in[keep_ids]) )\n",
    "        \n",
    "        if random.randint(1, 100) == 5:\n",
    "            print()\n",
    "            print(\"info at random intervals\")\n",
    "            print(keep_ids)\n",
    "            print(self.filter_list[i_f].weights[:, keep_ids, :, :].shape)\n",
    "            print(self.filter_list[i_f].weights.shape)        \n",
    "        \n",
    "        # todo: check, this may create more parameters ...\n",
    "        \n",
    "        # prune weights, ms and ns based on the 'keep ids'\n",
    "        self.filter_list[i_f].weights = torch.nn.Parameter(self.filter_list[i_f].weights[:, keep_ids, :, :])\n",
    "        self.filter_list[i_f].ms_in = torch.nn.Parameter(self.filter_list[i_f].ms_in[keep_ids], requires_grad=False) # this becomes a grad here, hence turn off again with False\n",
    "        #[self.filter_list[i_f].ms_in[i] for i in keep_ids] # self.ms_in[remove_ids]\n",
    "        self.filter_list[i_f].ns_in = torch.nn.Parameter(self.filter_list[i_f].ns_in[keep_ids], requires_grad=False)\n",
    "        # [self.filter_list[i_f].ns_in[i] for i in keep_ids] # self.ns_in[remove_ids]\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward(self, x: X) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # calculate representation x for each filter in this layer\n",
    "        # =============================================================================\n",
    "        \n",
    "        output_list = []\n",
    "        m_list = []\n",
    "        n_list = []\n",
    "        for f in self.filter_list:\n",
    "            # output = filter(input)\n",
    "            out = f(x)\n",
    "            # if filter has no channels left\n",
    "            if out is not None:\n",
    "                output_list.append(out)\n",
    "                m_list.append(f.m_this)\n",
    "                n_list.append(f.n_this)\n",
    "        x.ms_x = m_list\n",
    "        x.ns_x = n_list\n",
    "        x.data = torch.cat(output_list, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def get_filter_positions(self):\n",
    "        # =============================================================================\n",
    "        # in use for next layer input (initialisation of the model)\n",
    "        # =============================================================================\n",
    "        \n",
    "        ms_this = []\n",
    "        ns_this = []\n",
    "        for f in self.filter_list:\n",
    "            ms_this.append(f.m_this)\n",
    "            ns_this.append(f.n_this)\n",
    "        \n",
    "        return ms_this, ns_this\n",
    "    \n",
    "    def get_everything(self):\n",
    "        \n",
    "        sources = [] # source m,n,l-1\n",
    "        targets = [] # target m,n,l\n",
    "        target_groups = []\n",
    "        values = [] # connection value = ci value of channel in target connected to ms[i], ns[i] \n",
    "        \n",
    "        # for each filter\n",
    "        for i_f, f in enumerate(self.filter_list):\n",
    "            # for each channel\n",
    "            \n",
    "            # print(\"ms_in shape\", f.ms_in.shape)\n",
    "            for i_s in range(len(f.ms_in)):\n",
    "                \n",
    "                s = str(int(f.ms_in[i_s].item()))+'_'+str(int(f.ns_in[i_s].item()))\n",
    "                sources.append(s)\n",
    "\n",
    "                t = str(int(f.m_this.item()))+'_'+str(int(f.n_this.item()))\n",
    "                targets.append(t)\n",
    "\n",
    "                target_groups.append(self.layer_name)\n",
    "            \n",
    "            # get all channel importances\n",
    "            ci = np.array(self.run_channel_importance(i_f)).flatten()\n",
    "            #print(\"CI\"*50)\n",
    "            #print(ci)\n",
    "            values.extend(ci)\n",
    "            \n",
    "            \"\"\"\n",
    "            try:\n",
    "                values.append( [val.item() for tmp in ci for val in tmp] )\n",
    "            except:\n",
    "                try:\n",
    "                    a = [val.item() for val in ci]\n",
    "                    values.append(  [val for tmp in a for val in tmp])\n",
    "                except:\n",
    "                    print(\"empty channel importance??\")\n",
    "                    values = []\n",
    "                    \n",
    "            \"\"\"\n",
    "            \n",
    "        #print(\"lengths note:\", len(sources), len(targets), len(target_groups), len(values))\n",
    "        \n",
    "        return {'source':sources, 'target':targets, 'target_group':target_groups, 'value':values}\n",
    "            \n",
    "    \n",
    "    def update(self):\n",
    "        # =============================================================================\n",
    "        # currently: calculate importance metric for the prune_channel method\n",
    "        # remove channels based on self.prune_keep\n",
    "        # layerwise pruning - percentage of layer\n",
    "        # =============================================================================\n",
    "        \n",
    "        all_ci = []\n",
    "        all_len = 0\n",
    "        for i_f in range(len(self.filter_list)):\n",
    "            all_len += len(self.filter_list[i_f].ms_in)\n",
    "            # list of lists\n",
    "            all_ci.append(self.run_channel_importance(i_f))\n",
    "            #tmp_ids = sorted(range(len(all_ci)), key=lambda sub: all_ci[sub])\n",
    "          \n",
    "        #print(all_len) # this is the size of the previous pruning\n",
    "        #print(self.original_size)\n",
    "        #print(self.prune_keep_total)\n",
    "        #print(int(self.original_size * self.prune_keep_total))\n",
    "        \n",
    "        #self.log(f'{self.original_size}_active_channels', all_len, on_step=True, on_epoch=True)\n",
    "        \n",
    "        if all_len < int(self.original_size * self.prune_keep_total):\n",
    "            # if n percent have been pruned, stop this layer\n",
    "            print(\"pruning done for this layer\")\n",
    "        else:\n",
    "            # pruning\n",
    "            n = int(all_len*self.prune_keep)\n",
    "            all_ci_flatten = [item for row in all_ci for item in row] # don't have equal lengths, so no numpy possible\n",
    "            index = sorted(range(all_len), key=lambda sub: all_ci_flatten[sub])[-n] # error, out of range\n",
    "            threshold_value = all_ci_flatten[index]\n",
    "\n",
    "            for i_f in range(len(self.filter_list)):\n",
    "\n",
    "                # channel importance list for this filter\n",
    "                ci = all_ci[i_f] # self.run_channel_importance(i_f)\n",
    "\n",
    "                #print(ci)\n",
    "                #print(threshold_value)\n",
    "                # torch.where()\n",
    "                            \n",
    "                indices = np.where(ci >= threshold_value)[0] # just need the x axis\n",
    "\n",
    "                # indices should be list/np/detached\n",
    "                self.run_prune_channel(i_f, indices)\n",
    "                \n",
    "                #print(\"prune done\")\n",
    "                # ci = ci[indices] # probably not useful\n",
    "        \n",
    "            \n",
    "            # print(\"channel importance ci\", ci)\n",
    "            # keep_ids = random.sample(range(0, 8), 5)\n",
    "            #keep_ids = sorted(range(len(ci)), key=lambda sub: ci[sub])[amout_remove:]\n",
    "            #print(keep_ids)\n",
    "            \n",
    "            # delete filters with no input channels - no we need to still find, if there is a filter that is\n",
    "            # not used by any later filter - for that we probably want to do the same as with sugiyama ...\n",
    "            # if shape[1] == 0, then we can remove the whole filter from the list\n",
    "            if False:\n",
    "                try:\n",
    "                    remove_list = []\n",
    "                    for i_f, f in enumerate(self.filter_list):\n",
    "                        if f.weights.shape[1] == 0: \n",
    "                            print(\"DECENT NOTE: we just removed a filter that was empty\")\n",
    "                            remove_list.append(i_f)\n",
    "                            self.filter_list.pop(i_f)\n",
    "                except Exception as e:\n",
    "                    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c186cf1-c7db-45da-b779-f7ab88f3896f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentNet(nn.Module):\n",
    "    def __init__(self, model_kwargs, log_dir=\"\", ckpt_path='') -> None:\n",
    "        super(DecentNet, self).__init__()\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        out_dim = model_kwargs[\"out_dim\"]\n",
    "        out_dim.append(self.n_classes) # out_dim = [1, 32, 48, 64, 10]     \n",
    "        \n",
    "        grid_size = model_kwargs[\"grid_size\"]\n",
    "        assert not any(i > grid_size for i in out_dim), f\"filters need to be less than {grid_size}\"\n",
    "        self.grid_sqrt = int(math.sqrt(grid_size))\n",
    "        \n",
    "        self.ci_metric = model_kwargs[\"ci_metric\"]\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        # backbone\n",
    "        \n",
    "        # initialise input positions of first layer\n",
    "        ms_in_1 = [torch.tensor(0)]\n",
    "        ns_in_1 = [torch.tensor(0)]\n",
    "        if ckpt_path == '':\n",
    "            assert out_dim[0] == len(ms_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ms_in_1={len(ms_in_1)})\"\n",
    "            assert out_dim[0] == len(ns_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ns_in_1={len(ns_in_1)})\"\n",
    "        self.decent1 = DecentLayer(ms_in=ms_in_1, ns_in=ns_in_1, n_filters=out_dim[1], \n",
    "                                   kernel_size=3, stride=1, padding=0, dilation=1, \n",
    "                                   model_kwargs=model_kwargs, \n",
    "                                   layer_name='decent1',\n",
    "                                   ckpt_path=ckpt_path)\n",
    "        \n",
    "        # get position of previous layer as input for this layer\n",
    "        ms_in_2,ns_in_2 = self.decent1.get_filter_positions()\n",
    "        if ckpt_path == '':\n",
    "            assert out_dim[1] == len(ms_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ms_in_2={len(ms_in_2)})\"\n",
    "            assert out_dim[1] == len(ns_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ns_in_2={len(ns_in_2)})\"\n",
    "        self.decent2 = DecentLayer(ms_in=ms_in_2, ns_in=ns_in_2, n_filters=out_dim[2], \n",
    "                                   kernel_size=3, stride=1, padding=0, dilation=1,\n",
    "                                   model_kwargs=model_kwargs, \n",
    "                                   layer_name='decent2',\n",
    "                                   ckpt_path=ckpt_path)\n",
    "        \n",
    "        ms_in_3,ns_in_3 = self.decent2.get_filter_positions()\n",
    "        if ckpt_path == '':\n",
    "            assert out_dim[2] == len(ms_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ms_in_3={len(ms_in_3)})\"\n",
    "            assert out_dim[2] == len(ns_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ns_in_3={len(ns_in_3)})\"\n",
    "        self.decent3 = DecentLayer(ms_in=ms_in_3, ns_in=ns_in_3, n_filters=out_dim[3], \n",
    "                                   kernel_size=3, stride=1, padding=0, dilation=1, \n",
    "                                   model_kwargs=model_kwargs, \n",
    "                                   layer_name='decent3',\n",
    "                                   ckpt_path=ckpt_path)\n",
    "        \n",
    "        ms_in_1x1,ns_in_1x1 = self.decent3.get_filter_positions()\n",
    "        if ckpt_path == '':\n",
    "            assert out_dim[3] == len(ms_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ms_in_1x1={len(ms_in_1x1)})\"\n",
    "            assert out_dim[3] == len(ns_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ns_in_1x1={len(ns_in_1x1)})\"\n",
    "        self.decent1x1 = DecentLayer(ms_in=ms_in_1x1, ns_in=ns_in_1x1, n_filters=out_dim[-1], \n",
    "                                     kernel_size=1, stride=1, padding=0, dilation=1, \n",
    "                                     model_kwargs=model_kwargs, \n",
    "                                     layer_name='decent1x1',\n",
    "                                     ckpt_path=ckpt_path)\n",
    "        \n",
    "        #self.tmp = torchvision.models.squeezenet1_0(torchvision.models.SqueezeNet1_0_Weights.IMAGENET1K_V1)\n",
    "        #self.tmp.classifier[1] = torch.nn.Conv2d(512, 10, kernel_size=(3,3))\n",
    "        \n",
    "        # head\n",
    "        self.fc = torch.nn.Linear(out_dim[-1], out_dim[-1])\n",
    "    \n",
    "        # activation\n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        # bias\n",
    "        self.bias1 = torch.nn.InstanceNorm2d(out_dim[1])\n",
    "        self.bias2 = torch.nn.InstanceNorm2d(out_dim[2])\n",
    "        self.bias3 = torch.nn.InstanceNorm2d(out_dim[3])\n",
    "        self.bias1x1 = torch.nn.InstanceNorm2d(out_dim[-1])\n",
    "        \n",
    "        # activation\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # init connection cost\n",
    "        self.cc = []\n",
    "        self.update_connection_cost()\n",
    "        \n",
    "        # get a position in filter list\n",
    "        self.m_l2_plot = self.decent2.filter_list[0].m_this.detach().cpu().numpy()\n",
    "        self.n_l2_plot = self.decent2.filter_list[0].n_this.detach().cpu().numpy()  \n",
    "        \n",
    "        print(self.m_l2_plot)\n",
    "        print(self.n_l2_plot)\n",
    "        \n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "                f.write(\"\\n# plot #\\n\")\n",
    "                for p in self.model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        f.write('m:' + str(self.m_l2_plot) + ', n: ' + str(self.n_l2_plot) + '\\n')\n",
    "        \"\"\"\n",
    "        # self.plot_layer_of_1_channel(current_epoch=0) - not working here, dir not created yet\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        # hook on the data (for gradcam or something similar)\n",
    "        # https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
    "        if mode == 'explain':\n",
    "            output = x.data.register_hook(self.activations_hook)\n",
    "            #'cannot register a hook on a tensor that doesn't require gradient'\n",
    "        \n",
    "        \n",
    "        # global max pooling for MIL\n",
    "        # https://discuss.pytorch.org/t/global-max-pooling/1345\n",
    "        # Global Average Pooling is a pooling operation designed to replace fully connected layers in classical CNNs. \n",
    "        # The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer.\n",
    "        # Similar to global average pooling, to implement global max pooling in PyTorch, \n",
    "        # one needs to use the regular max pooling class with a kernel size equal to the size of the feature map at that point\n",
    "        x.data = F.max_pool2d(x.data, kernel_size=x.data.size()[2:])\n",
    "        \n",
    "        # or flatten\n",
    "        x.data = x.data.reshape(x.data.size(0), -1)\n",
    "        \n",
    "        # we still want the fc ???\n",
    "        # x.data = self.fc(x.data) \n",
    "        \n",
    "        # x.data = self.sigmoid(x.data)\n",
    "        \n",
    "        # x.data = self.tmp(x.data)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    \n",
    "    def activations_hook(self, grad):\n",
    "        # hook for the gradients of the activations\n",
    "        self.gradients = grad\n",
    "    def get_activations_gradient(self):\n",
    "        # method for the gradient extraction\n",
    "        return self.gradients\n",
    "    def get_activations(self, x):\n",
    "        # method for the activation exctraction\n",
    "        \n",
    "        #print('0', x)\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        #print('1', x)\n",
    "\n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        #print('2', x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        #print('3', x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        #print('1x1', x)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    def plot_incoming_connections(self, current_epoch=0):\n",
    "        # analyse incoming conenctions\n",
    "        # which kernels were pruned in this filter\n",
    "        # decent3 = orange\n",
    "        # decent2 = cyan\n",
    "        # decent1 = pink\n",
    "        \n",
    "        # get each filter position that has a channel that matches\n",
    "        ms = []; ns = []\n",
    "        \n",
    "        #print(self.decent2.filter_list)\n",
    "        #print(\"**********************\")\n",
    "        #print(self.decent3.filter_list)\n",
    "\n",
    "        \n",
    "        # use first filter in the list of this layer\n",
    "        this_filter = self.decent2.filter_list[0] # orange\n",
    "        \n",
    "        m_tmp = this_filter.m_this.detach().cpu().numpy()\n",
    "        n_tmp = this_filter.n_this.detach().cpu().numpy()\n",
    "        ms_tmp = this_filter.ms_in.detach().cpu().numpy()\n",
    "        ns_tmp = this_filter.ns_in.detach().cpu().numpy()\n",
    "                \n",
    "        # visualising the previous and current layer neurons\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        ax.scatter(m_tmp, n_tmp, s=100000, color='tab:cyan', alpha=0.1) # previous layer\n",
    "        ax.scatter(m_tmp, n_tmp, s=50000, color='tab:cyan',alpha=0.2) # previous layer\n",
    "        ax.scatter(m_tmp, n_tmp, s=25000, color='tab:cyan',alpha=0.3) # previous layer\n",
    "        ax.scatter(m_tmp, n_tmp, s=500, color='tab:cyan') # previous layer\n",
    "        ax.scatter(ms_tmp, ns_tmp, color='tab:pink') # next layer\n",
    "        plt.xlim(0, self.grid_sqrt) # m coordinate of grid_size field\n",
    "        plt.ylim(0, self.grid_sqrt) # n coordinate of grid_size field\n",
    "        ax.grid() # enable grid line\n",
    "        fig.savefig(os.path.join(self.log_dir, f\"in_{self.ci_metric}_m{int(self.m_l2_plot[0])}_n{int(self.n_l2_plot[0])}_{str(current_epoch)}.png\"))\n",
    "    \n",
    "    def plot_outgoing_connections(self, current_epoch=0): # plot_layer_of_1_channel\n",
    "        # analyse outgoing conenctions\n",
    "        # which filters in the next layer are influenced by this filter\n",
    "        \n",
    "        # get each filter position that has a channel that matches\n",
    "        ms = []; ns = []\n",
    "        \n",
    "        #print(self.decent2.filter_list)\n",
    "        #print(\"**********************\")\n",
    "        #print(self.decent3.filter_list)\n",
    "\n",
    "        \n",
    "        # go through all filters in this layer\n",
    "        for f in self.decent3.filter_list:\n",
    "            \n",
    "            # if filter position in prev layer matches any channel in this layer\n",
    "            if any(pair == (self.m_l2_plot, self.n_l2_plot) for pair in zip(f.ms_in.detach().cpu().numpy(), f.ns_in.detach().cpu().numpy())):\n",
    "                \n",
    "                #print('match', f.m_this, f.n_this)\n",
    "                \n",
    "                # save position of each filter in this layer\n",
    "                ms.append(f.m_this.detach().cpu().numpy())\n",
    "                ns.append(f.n_this.detach().cpu().numpy())\n",
    "              \n",
    "            if False:\n",
    "                    print(\"nooooooooooooooo\")\n",
    "                    print(f.ms_in)\n",
    "                    print(self.m_l2_plot)\n",
    "                    print(f.ns_in)\n",
    "                    print(self.n_l2_plot)\n",
    "\n",
    "                    print((self.m_l2_plot, self.n_l2_plot))\n",
    "                \n",
    "        # visualising the previous and current layer neurons\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        ax.scatter(self.m_l2_plot, self.n_l2_plot, s=100000, color='tab:cyan', alpha=0.1) # previous layer\n",
    "        ax.scatter(self.m_l2_plot, self.n_l2_plot, s=50000, color='tab:cyan',alpha=0.2) # previous layer\n",
    "        ax.scatter(self.m_l2_plot, self.n_l2_plot, s=25000, color='tab:cyan',alpha=0.3) # previous layer\n",
    "        ax.scatter(self.m_l2_plot, self.n_l2_plot, s=500, color='tab:cyan') # previous layer\n",
    "        ax.scatter(ms, ns, color='tab:orange') # next layer\n",
    "        plt.xlim(0, self.grid_sqrt) # m coordinate of grid_size field\n",
    "        plt.ylim(0, self.grid_sqrt) # n coordinate of grid_size field\n",
    "        ax.grid() # enable grid line\n",
    "        fig.savefig(os.path.join(self.log_dir, f\"out_{self.ci_metric}_m{int(self.m_l2_plot[0])}_n{int(self.n_l2_plot[0])}_{str(current_epoch)}.png\"))\n",
    "    \n",
    "    def update_connection_cost(self):\n",
    "        self.cc = []\n",
    "        # self.cc.append(self.decent1.run_layer_connection_cost()) # maybe not even needed ...\n",
    "        self.cc.append(self.decent2.run_layer_connection_cost())\n",
    "        self.cc.append(self.decent3.run_layer_connection_cost())\n",
    "        self.cc.append(self.decent1x1.run_layer_connection_cost())\n",
    "        self.cc = torch.mean(torch.tensor(self.cc))\n",
    "\n",
    "    def update(self, current_epoch):\n",
    "        # =============================================================================\n",
    "        # update_every_nth_epoch\n",
    "        # adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        # =============================================================================\n",
    "        \n",
    "        # update decent layers\n",
    "        \n",
    "        #self.decent1.update()\n",
    "        self.decent2.update()\n",
    "        self.decent3.update()\n",
    "        self.decent1x1.update()\n",
    "        \n",
    "        # visualisation\n",
    "        self.plot_incoming_connections(current_epoch)\n",
    "        self.plot_outgoing_connections(current_epoch)\n",
    "    \n",
    "        # connection cost has to be calculated after pruning\n",
    "        # self.cc which is updated is used for loss function\n",
    "        self.update_connection_cost()\n",
    "        \n",
    "    def get_everything(self, current_epoch):\n",
    "        \n",
    "        d1 = self.decent1.get_everything()\n",
    "        d2 = self.decent2.get_everything()\n",
    "        d3 = self.decent3.get_everything()\n",
    "        d1x1 = self.decent1x1.get_everything()\n",
    "        \n",
    "        #print(d1)\n",
    "        #print(d2)\n",
    "        #print(d3)\n",
    "        \n",
    "        df1 = pd.DataFrame(d1)\n",
    "        #print(df1.head())\n",
    "        \n",
    "        df2 = pd.DataFrame(d2)\n",
    "        #print(df2.head())\n",
    "        \n",
    "        df3 = pd.DataFrame(d3)\n",
    "        #print(df3.head())\n",
    "        \n",
    "        df1x1 = pd.DataFrame(d1x1)\n",
    "        \n",
    "        frames = [df1, df2, df3, df1x1]\n",
    "        result = pd.concat(frames)\n",
    "        \n",
    "        result.to_csv(os.path.join(self.log_dir, f'out_{str(current_epoch)}.csv'), index=False)  \n",
    "        \n",
    "        # return d1, d2, d3, d1x1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6978be4e-2b6d-447c-9130-1fd53a440580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "\n",
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint\n",
    "        # Save a checkpoint at the end of the training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "            print(\"DECENT NOTE: callback on_train_epoch_end\", monitor_candidates[\"epoch\"])\n",
    "            if monitor_candidates[\"epoch\"] > 0:\n",
    "                if monitor_candidates[\"unpruned_state\"] != -1:\n",
    "                    print(\"DECENT NOTE: save model\", monitor_candidates[\"epoch\"])\n",
    "                    if self._every_n_epochs >= 1 and ((trainer.current_epoch + 1) % self._every_n_epochs) == 0:\n",
    "                        self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                    self._save_last_checkpoint(trainer, monitor_candidates)\n",
    "                    \n",
    "                    pl_module.model.get_everything(current_epoch=trainer.current_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6475c8a5-b216-4fb5-8d82-bfa8e7472eab",
   "metadata": {},
   "source": [
    "# TMP from helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a01b7e24-be9f-4c63-a21b-68b5f259a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# alphabetic order misc\n",
    "# =============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# =============================================================================\n",
    "# torch\n",
    "# =============================================================================\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "class FeatureMap_new():\n",
    "    # =============================================================================\n",
    "    # unit test: \n",
    "    #    examples/utest_vis_feature_map.ipynb\n",
    "    # sources:\n",
    "    #    https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030\n",
    "    #    https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c\n",
    "    #    https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/4\n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, model, layer, layer_str, log_dir='', device=\"cpu\"):\n",
    "        # =============================================================================\n",
    "        # Set model and layer\n",
    "        # =============================================================================\n",
    "\n",
    "        self.device = device\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        # model\n",
    "        self.model = model.to(device).eval()\n",
    "        \n",
    "        # the (conv) layer to be visualised\n",
    "        self.layer = layer\n",
    "        self.layer_str = layer_str\n",
    "        \n",
    "        #print(\"\")\n",
    "        #print(\"Layer:\", self.layer)\n",
    "        #print(\"\")\n",
    "\n",
    "    def run(self, img_tensor, batch_idx):\n",
    "        # =============================================================================\n",
    "        # Feature map visualisation using hooks       \n",
    "        # A high activation means a certain feature was found. \n",
    "        # A feature map is called the activations of a layer after the convolutional operation.\n",
    "        # =============================================================================\n",
    "        \n",
    "        #print(\"i\", img_tensor.data.shape)\n",
    "        self.ii = img_tensor.data\n",
    "        #self.m = image_tensor.ms_x # but i need 'this m' ... from the filter\n",
    "        #self.n = image_tensor.ns_x\n",
    "        self.batch_idx = batch_idx\n",
    "    \n",
    "            \n",
    "        # hook = Hook(module=self.layer)\n",
    "        \n",
    "        active = {}\n",
    "        def get_active(name):\n",
    "            def hook(model, input, output): # hi\n",
    "                active[name] = output.data.detach()\n",
    "            return hook\n",
    "\n",
    "\n",
    "        model = self.model.eval()\n",
    "        self.layer.register_forward_hook(get_active(self.layer_str))\n",
    "        \n",
    "        try:\n",
    "            output = self.model(img_tensor, mode='explain')\n",
    "        except:\n",
    "            output = self.model(img_tensor)\n",
    "        \n",
    "        \n",
    "        self.feature_maps = active[self.layer_str]\n",
    "\n",
    "        # print('self.feature_maps', self.feature_maps.data.shape)\n",
    "        \n",
    "        '''\n",
    "        output = self.model(img_tensor, mode='explain')\n",
    "        self.feature_maps = hook.output.data # .squeeze()\n",
    "        \n",
    "        print('o', output.shape)\n",
    "        print('i', self.ii.shape)\n",
    "        print('h', hook.output)\n",
    "        print('f', self.feature_maps.shape)\n",
    "        '''\n",
    "\n",
    "    def log(self):\n",
    "        # =============================================================================\n",
    "        # plot and save 15 random feature maps + original image\n",
    "        # called for each layer\n",
    "        '''\n",
    "        * entry image: entry_id5_0_0_0_mo3_gt2.png\n",
    "        * hidden layer: hid_id5_3_8_2.png\n",
    "        * last layer (global pooling - connected to class n): pool_2_3_4_gp2.png\n",
    "        * activated image: cam_id5_mo3_gt2.png\n",
    "        * activated image gray: camg_id5_mo3_gt2.png\n",
    "\n",
    "\n",
    "        * circle in: in_2_3_4_ep65.png\n",
    "        * circle out: out_2_3_4_ep65.png\n",
    "        '''\n",
    "        # =============================================================================\n",
    "        \n",
    "        \n",
    "        # self.feature_maps\n",
    "        \n",
    "        print(self.feature_maps.shape)\n",
    "        \n",
    "        for i_fm in range(self.feature_maps.shape[1]):\n",
    "        \n",
    "            \n",
    "            m = 0\n",
    "            n = 0\n",
    "            # cl = 0 \n",
    "\n",
    "            if self.layer_str == 'decent1':\n",
    "                m = self.model.decent1.filter_list[i_fm].m_this.data.squeeze().detach().cpu().numpy().item()\n",
    "                n = self.model.decent1.filter_list[i_fm].n_this.data.squeeze().detach().cpu().numpy().item()\n",
    "                tmp_file_name = f'hid_id{self.batch_idx}_{int(m)}_{int(n)}_{1}.png'\n",
    "            elif self.layer_str == 'decent2':\n",
    "                m = self.model.decent2.filter_list[i_fm].m_this.data.squeeze().detach().cpu().numpy().item()\n",
    "                n = self.model.decent2.filter_list[i_fm].n_this.data.squeeze().detach().cpu().numpy().item()\n",
    "                tmp_file_name = f'hid_id{self.batch_idx}_{int(m)}_{int(n)}_{2}.png'\n",
    "            elif self.layer_str == 'decent3':\n",
    "                m = self.model.decent3.filter_list[i_fm].m_this.data.squeeze().detach().cpu().numpy().item()\n",
    "                n = self.model.decent3.filter_list[i_fm].n_this.data.squeeze().detach().cpu().numpy().item()\n",
    "                tmp_file_name = f'hid_id{self.batch_idx}_{int(m)}_{int(n)}_{3}.png'\n",
    "            elif self.layer_str == 'decent1x1':\n",
    "                m = self.model.decent1x1.filter_list[i_fm].m_this.data.squeeze().detach().cpu().numpy().item()\n",
    "                n = self.model.decent1x1.filter_list[i_fm].n_this.data.squeeze().detach().cpu().numpy().item()\n",
    "                # the class that is connected to the last layer's filters via global pooling\n",
    "                # the class has the same order as the list ... i hope ...\n",
    "                tmp_file_name = f'pool_id{self.batch_idx}_{int(m)}_{int(n)}_{4}_cl{i_fm}.png'\n",
    "            else:\n",
    "                print(\"DECENT NOTE: WE GOT A PROBLEM HERE\")\n",
    "\n",
    "            tmp_img = self.feature_maps.squeeze()[i_fm].cpu().detach().numpy()\n",
    "            \n",
    "            # [Errno 22] Invalid argument: \"examples/example_results\\\\lightning_logs\\\\dumpster\\\\version_13\\\\\n",
    "            # hid_id0_Parameter containing:\\ntensor([1.], device='cuda:0')_Parameter containing:\\ntensor([6.], device='cuda:0')_1.png\"\n",
    "            \n",
    "            # plt_cam_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\n",
    "            plt.imsave(os.path.join(self.log_dir, tmp_file_name), tmp_img)\n",
    "        \n",
    "        \n",
    "        # plt.figure(figsize=(100,100))\n",
    "        \n",
    "        '''\n",
    "        # print(\"amount of feature maps:\", amount)\n",
    "        if amount < 9:\n",
    "            sample_amount = amount\n",
    "            y_axis = 3\n",
    "            x_axis = 3\n",
    "            # if x_axis == 1: x_axis = 2\n",
    "        elif amount < 16:\n",
    "            sample_amount = amount\n",
    "            y_axis = 4\n",
    "            x_axis = 4\n",
    "        else:\n",
    "            sample_amount = 16\n",
    "            x_axis = 4\n",
    "            y_axis = 4\n",
    "        \n",
    "        fig, axarr = plt.subplots(x_axis, y_axis)\n",
    "            \n",
    "        # currently not random\n",
    "        random_samples = range(0, amount) # random.sample(range(0, amount), sample_amount)\n",
    "        # print(\"random_samples\", random_samples)\n",
    "        counter = 0  \n",
    "        idx, idx2 = [0, 0]\n",
    "        for idx in range(0, x_axis):\n",
    "\n",
    "            for idx2 in range(0, y_axis):\n",
    "                \n",
    "                axarr[idx, idx2].axis('off')\n",
    "                try:\n",
    "                    #print(self.feature_maps.squeeze().shape)\n",
    "                    #print(\"try 1\")\n",
    "                    axarr[idx, idx2].imshow(self.feature_maps.squeeze()[random_samples[counter]].cpu().detach().numpy())\n",
    "                    counter += 1\n",
    "                    \n",
    "                except:\n",
    "                    try:\n",
    "                        #print(\"try 2\")\n",
    "                        axarr[idx, idx2].imshow(self.feature_maps.cpu().detach().numpy())\n",
    "                        counter += 1\n",
    "                    except:\n",
    "                        try:\n",
    "                            #print(\"try 3\")\n",
    "                            axarr[idx, idx2].imshow( (self.feature_maps.squeeze()[random_samples[counter]]).cpu().detach().numpy().transpose(1, 2, 0))\n",
    "                        except Exception as e:\n",
    "                            #print(\"not possible to show feature maps image\")\n",
    "                            #print(self.feature_maps.shape)\n",
    "                            #print(e)\n",
    "                            axarr[idx, idx2].axis('off')\n",
    "\n",
    "            \n",
    "\n",
    "        # overwrite first image with original image\n",
    "        try:\n",
    "            axarr[x_axis-1, y_axis-1].imshow(self.ii.cpu().detach().numpy().transpose(1, 2, 0))\n",
    "        except:\n",
    "            try:\n",
    "                axarr[x_axis-1, y_axis-1].imshow(self.ii.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "            except:\n",
    "                try: \n",
    "                    axarr[x_axis-1, y_axis-1].imshow(self.ii.squeeze(1).cpu().detach().numpy().transpose(1, 2, 0))\n",
    "                except Exception as e:\n",
    "                    print(\"not possible to show original image\")\n",
    "                    print(e)'''\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3659280f-27a9-4667-9be6-cc341595ee5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(torch.nn.Parameter(torch.tensor([5.0])).data.squeeze().detach().cpu().numpy().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "## DecentLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # Lightning Module consists of functions that define the training routine\n",
    "    # train, val, test: before epoch, step, after epoch, ...\n",
    "    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n",
    "    # order for the instance methods:\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\n",
    "    # \n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, kwargs, log_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        if train_kwargs[\"load_ckpt_file\"] != '':\n",
    "            self.ckpt_path = os.path.join(log_dir, train_kwargs[\"load_ckpt_file\"])\n",
    "            if os.path.isfile(ckpt_path):\n",
    "                print(f\"Found pretrained model at {ckpt_path}, loading...\")\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir, ckpt_path=ckpt_path).to(\"cuda\")\n",
    "            else:\n",
    "                # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        else:\n",
    "            # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "            self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "            \n",
    "        # print(self.model)\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        self.pretrain_epochs = model_kwargs[\"pretrain_epochs\"]\n",
    "        \n",
    "        # needed for hparams.yaml file\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if False:\n",
    "            self.metric = { \"train_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"train_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"val_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"val_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "                   }\n",
    "        else:\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "\n",
    "            \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        # =============================================================================\n",
    "        # we make it possible to use model_output = self(image)\n",
    "        # =============================================================================\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # =============================================================================\n",
    "        # returns:\n",
    "        #    optimiser and lr scheduler\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimiser = optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        else:\n",
    "            optimiser = optim.SGD(self.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        # initial plot of circular layer\n",
    "        # updates model every nth epoch\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: on_train_epoch_start\", self.current_epoch)\n",
    "        \n",
    "        # plot random layer (the circular plot)\n",
    "        if self.current_epoch == 0:\n",
    "            self.model.plot_incoming_connections(current_epoch=0)\n",
    "            self.model.plot_outgoing_connections(current_epoch=0)\n",
    "\n",
    "        # update model\n",
    "         # don't update unless pretrain epochs is reached\n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch >= self.pretrain_epochs:\n",
    "            print(\"DECENT NOTE: update model\", self.current_epoch)        \n",
    "            if debug_model:\n",
    "                print(\"DECENT NOTE: before update\")\n",
    "                print(self.model)\n",
    "            self.model.update(current_epoch = self.current_epoch)\n",
    "            if True: \n",
    "                print(\"DECENT NOTE: after update\")\n",
    "                print(self.model)\n",
    "                \n",
    "            print(\"DECENT NOTE: model updated\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculates loss for a batch # 1\n",
    "        # parameters:\n",
    "        #    batch\n",
    "        #    batch id\n",
    "        # returns:\n",
    "        #    loss\n",
    "        # notes:\n",
    "        #    calling gradcam like self.gradcam(batch) is dangerous cause changes gradients\n",
    "        # =============================================================================     \n",
    "        if False: # batch_idx < 2: # print first two steps\n",
    "            print(\"DECENT NOTE: training_step\", batch_idx)\n",
    "\n",
    "        # calculate loss\n",
    "        # loss = torch.tensor(1)\n",
    "        loss = self.run_loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging # 2\n",
    "        # =============================================================================\n",
    "        if False: # batch_idx < 2:\n",
    "            print(\"DECENT NOTE: validation_step\", batch_idx)\n",
    "        \n",
    "        self.run_loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing # 3\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_validation_epoch_end\")\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # save model if next iteration model is pruned # 4 \n",
    "        # this needs to be called before callback \n",
    "        # - if internal pytorch lightning convention changes, this will stop working\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_train_epoch_end\", self.current_epoch)\n",
    "               \n",
    "        if False:\n",
    "            print(\"current epoch\")\n",
    "            print(((self.current_epoch+1) % self.update_every_nth_epoch) == 0)\n",
    "            print(self.current_epoch+1)\n",
    "            print(self.current_epoch)\n",
    "            print(self.update_every_nth_epoch)\n",
    "        \n",
    "        # numel: returns the total number of elements in the input tensor\n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.log(f'unpruned', unpruned, on_step=False, on_epoch=True) \n",
    "        \n",
    "        if ((self.current_epoch+1) % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            # if next epoch is an update, set unpruned flag            \n",
    "            self.log(f'unpruned_state', 1, on_step=False, on_epoch=True)\n",
    "            \n",
    "            # save file\n",
    "            with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "                f.write(\"\\n# parameter requires grad shape #\\n\")\n",
    "                for p in self.model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        f.write(str(p.shape))\n",
    "            \n",
    "        else:\n",
    "            # else set unpruned flag to -1, then model won't be saved\n",
    "            self.log(f'unpruned_state', -1, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.model.get_everything(current_epoch='final_test')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging, plot gradcam\n",
    "        # =============================================================================\n",
    "        if batch_idx < 2:\n",
    "            print(\"DECENT NOTE: test_step\", batch_idx)\n",
    "\n",
    "        self.run_loss_n_metrics(batch, mode=\"test\")\n",
    "\n",
    "                # .requires_grad_()\n",
    "        \n",
    "                \n",
    "        \"\"\"\n",
    "        with torch.enable_grad():\n",
    "            grad_preds = preds.requires_grad_()\n",
    "            preds2 = self.layer2(grad_preds)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        # save image\n",
    "        \n",
    "        img, _ = batch\n",
    "        \n",
    "        print(img.shape)\n",
    "        \n",
    "        tmp_file_name = f'entry_id{batch_idx}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        # tmp_img = self.feature_maps.squeeze()[i_fm].cpu().detach().numpy()\n",
    "        \n",
    "        tmp_img = img.squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        plt.imsave(os.path.join(self.log_dir, tmp_file_name), tmp_img)\n",
    "        \n",
    "        \n",
    "        # save feature maps of hidden layers and the layer that gets globally pooled\n",
    "        try:\n",
    "            with torch.set_grad_enabled(True): # torch.set_grad_enabled(True):\n",
    "                self.run_xai_gradcam(batch, batch_idx, mode='explain')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"batch size has to be 1\")\n",
    "            \n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            layer = self.model.decent1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent2\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent2' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent3\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent3' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent1x1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1x1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            \n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_test_epoch_end\", self.current_epoch)\n",
    "        pass\n",
    "    \n",
    "    def run_xai_feature_map(self, batch, batch_idx, layer, layer_str, device='cuda'):\n",
    "        # https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5\n",
    " \n",
    "        # img, label = testset.__getitem__(0) # batch x channel x width x height, class\n",
    "\n",
    "        # img = X(img.to(device).unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        \n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        # print(img.data.shape)\n",
    "\n",
    "        # run feature map\n",
    "        # model, layer, layer_str, log_dir, device=\"cpu\"\n",
    "        fm = FeatureMap_new(model=self.model, layer=layer, layer_str=layer_str, log_dir=self.log_dir, device=device)\n",
    "        fm.run(tmp_img, batch_idx)\n",
    "        fm.log()\n",
    "    \n",
    "    def run_xai_gradcam(self, batch, batch_idx, mode='explain'):\n",
    "        # =============================================================================\n",
    "        # grad cam - or just cam?? idk\n",
    "        # todo error: RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n",
    "        # BATCH SIZE HAS TO BE ONE!!!\n",
    "        # grad enable in test mode:\n",
    "        # https://github.com/Project-MONAI/MONAI/discussions/1598\n",
    "        # https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "        # =============================================================================\n",
    "    \n",
    "        img, ground_truth = batch\n",
    "\n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)]) # .requires_grad_()\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        #print(\"nooooooooooo grad, whyyyyy\")\n",
    "        #print(tmp_img1)\n",
    "        #print(img)\n",
    "\n",
    "        #print('b1', tmp_img1)\n",
    "        #print('b2', tmp_img2)\n",
    "\n",
    "        model_output = self(tmp_img1, mode)\n",
    "\n",
    "        #print('c1', tmp_img1)\n",
    "        #print('c2', tmp_img2)\n",
    "\n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "\n",
    "        # get prediction value\n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "\n",
    "        #print('d1', tmp_img1)\n",
    "\n",
    "        #print(\"mo\", model_output)\n",
    "        #print(\"max\", pred_max)\n",
    "        #print(\"backprop\", model_output[:, pred_max])\n",
    "\n",
    "        # backpropagate for gradient tracking\n",
    "        model_output[:, pred_max].backward()\n",
    "\n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        #print('e2', tmp_img2)\n",
    "\n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # draw the heatmap\n",
    "        # plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "        # fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_l2_plot[0])}_n{int(self.n_l2_plot[0])}_{str(current_epoch)}.png\"))\n",
    "\n",
    "        plt.imsave(os.path.join( self.log_dir, f\"plt_cam_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" ), heatmap.detach().cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "        heatmap *= 255.0 / heatmap.max()\n",
    "        pil_heatmap = Image.fromarray(heatmap.detach().cpu().numpy().squeeze()).convert('RGB')\n",
    "        pil_heatmap.save(os.path.join( self.log_dir, f\"pil_cam_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" ) ) \n",
    "            \n",
    "    def run_loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        # =============================================================================\n",
    "        # put image through model, calculate loss and metrics\n",
    "        # use cc term that has been calculated previously\n",
    "        # =============================================================================\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # for test routine\n",
    "        self.mo = model_output.argmax(dim=1).squeeze().detach().cpu().numpy()\n",
    "        self.gt = ground_truth.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        print('self mo', self.mo)\n",
    "        print('self gt', self.gt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"gt\", ground_truth)\n",
    "        print(\"gt shape\", ground_truth.shape)\n",
    "        print(\"gt type\", ground_truth.type())\n",
    "        print(torch.zeros(ground_truth.size(0), self.n_classes))\n",
    "        \n",
    "        if len(ground_truth.shape) < 2:\n",
    "            ground_truth_tmp_tmp = ground_truth.unsqueeze(1)\n",
    "        else:\n",
    "            ground_truth = ground_truth.transpose(1, 0)\n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth_tmp.size(0), self.n_classes).scatter_(1, ground_truth_tmp.to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \"\"\"\n",
    "\n",
    "        ground_truth = ground_truth.squeeze()\n",
    "        if len(ground_truth.shape) < 1:\n",
    "            ground_truth = ground_truth.unsqueeze(0)\n",
    "        loss = self.criterion(model_output, ground_truth.long()) # ground_truth_multi_hot)\n",
    "        cc = torch.mean(self.model.cc) * self.cc_weight\n",
    "        \n",
    "        #print(\"loss\", loss)\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "        \n",
    "        try:\n",
    "            pass # print('pred i', pred_i.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        try:\n",
    "            pass # print('gt', ground_truth.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        \n",
    "        if mode == \"train\":\n",
    "            ta = self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            tf = self.train_f1(preds=pred_i, target=ground_truth) \n",
    "            tp = self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"train info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", ta)\n",
    "                print(\"f\", tf)\n",
    "                print(\"p\", tp)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        elif mode == \"val\":\n",
    "            va = self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            vf = self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            vp = self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"val info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", va)\n",
    "                print(\"f\", vf)\n",
    "                print(\"p\", vp)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        else:\n",
    "            print(pred_i)\n",
    "            print(ground_truth)\n",
    "            ta = self.test_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            tf = self.test_f1(preds=pred_i, target=ground_truth) \n",
    "            tp = self.test_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.test_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.test_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.test_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "afaef32c-9dc5-40c3-813b-0b9a9c69b7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([5]).squeeze().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2906-61b6-4f3a-b2fa-7aeeaa12e7e1",
   "metadata": {},
   "source": [
    "## run dev routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    \n",
    "    train_kwargs['load_ckpt_file'] = \"\"\n",
    "    \n",
    "    # =============================================================================\n",
    "    # this is the main function, run this cell!!!\n",
    "\n",
    "    # dataset\n",
    "    # logger\n",
    "    # trainer\n",
    "    # trainer.fit\n",
    "    # trainer.test\n",
    "    # =============================================================================\n",
    "\n",
    "    # \"examples/example_results/lightning_logs\"\n",
    "    logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"])\n",
    "    trainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         # inference_mode=False, # do grad manually\n",
    "                         log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                         logger=logger,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         max_epochs=train_kwargs[\"epochs\"],\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1\",\n",
    "                                                   filename='{epoch}-{val_f1:.2f}-{unpruned:.0f}'),\n",
    "                                    DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                                    filename='{epoch}-{unpruned:.0f}-{val_f1:.2f}'),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "\n",
    "    # Initialize the LightningModule and LightningDataModule\n",
    "    light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "    # Train the model using a Trainer\n",
    "    trainer.fit(light, train_dataloader, val_dataloader)\n",
    "\n",
    "    # we don't save the positions here ...\n",
    "    # light = DecentLightning.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, kwargs=kwargs) # Load best checkpoint after training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6138914-f0ad-440e-88cd-1d7d239b2566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66262489-1c4a-439e-9673-32a40c5c52f1",
   "metadata": {},
   "source": [
    "## run test routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "891e1fef-8dab-44fd-908d-e9f2a901dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples/example_results\\lightning_logs\\debug_oct_no_fc\\version_13/checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt\n",
      "Found pretrained model at examples/example_results\\lightning_logs\\debug_oct_no_fc\\version_13/checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt, loading...\n",
      "[10.]\n",
      "[5.]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "# Check whether pretrained model exists. If yes, load it and skip training\n",
    "ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "\n",
    "# ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\\debug_oct_no_fc\", 'version_13', 'checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt'])\n",
    "print(ckpt_path)\n",
    "if os.path.isfile(ckpt_path):\n",
    "    # save the logs somewhere else\n",
    "    logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "    \n",
    "    \n",
    "    light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "else:\n",
    "    print('not a dir')\n",
    "    \n",
    "    \n",
    "    #print(state_dict)\n",
    "    \n",
    "    \n",
    "    #light = DecentLightning.load_from_checkpoint(state_dict, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "73f9f7ff-a6f1-4947-bcf9-07b9141fd3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdec45c40af4464b0e1ff7097a0a000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: test_step 0\n",
      "self mo 0\n",
      "self gt 3\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 4, 26, 26])\n",
      "torch.Size([1, 4, 24, 24])\n",
      "torch.Size([1, 7, 22, 22])\n",
      "torch.Size([1, 4, 22, 22])\n",
      "DECENT NOTE: test_step 1\n",
      "self mo 3\n",
      "self gt 2\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([2], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 4, 26, 26])\n",
      "torch.Size([1, 4, 24, 24])\n",
      "torch.Size([1, 7, 22, 22])\n",
      "torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 3\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 4, 26, 26])\n",
      "torch.Size([1, 4, 24, 24])\n",
      "torch.Size([1, 7, 22, 22])\n",
      "torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 3\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 4, 26, 26])\n",
      "torch.Size([1, 4, 24, 24])\n",
      "torch.Size([1, 7, 22, 22])\n",
      "torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 0\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 4, 26, 26])\n",
      "torch.Size([1, 4, 24, 24])\n",
      "torch.Size([1, 7, 22, 22])\n",
      "torch.Size([1, 4, 22, 22])\n",
      "self mo 3\n",
      "self gt 2\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([2], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 4, 26, 26])\n",
      "torch.Size([1, 4, 24, 24])\n",
      "torch.Size([1, 7, 22, 22])\n",
      "torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 1\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 4, 26, 26])\n",
      "torch.Size([1, 4, 24, 24])\n",
      "torch.Size([1, 7, 22, 22])\n",
      "torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 0\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 4, 26, 26])\n",
      "torch.Size([1, 4, 24, 24])\n",
      "torch.Size([1, 7, 22, 22])\n",
      "torch.Size([1, 4, 22, 22])\n",
      "DECENT NOTE: on_test_epoch_end 0\n"
     ]
    }
   ],
   "source": [
    "# Test best model on test set\n",
    "\n",
    "# we want the grad to work in test, hence: inference_mode=False\n",
    "# logger_x = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                     accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                     #devices=[0],\n",
    "                     logger=logger,\n",
    "                     inference_mode=False)\n",
    "\n",
    "test_result = explainer.test(light, xai_dataloader, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "39a83b94-b458-4390-8bfd-cd98ffc9d109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecentNet(\n",
       "  (decent1): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([10.]), n_this=Parameter containing:\n",
       "      tensor([10.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([1.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([2.]), n_this=Parameter containing:\n",
       "      tensor([1.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([11.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "    )\n",
       "  )\n",
       "  (decent2): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([10.]), n_this=Parameter containing:\n",
       "      tensor([5.]))\n",
       "       with inputs: ms_in= 1, 13, ns_in= 6, 11)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([0.]))\n",
       "       with inputs: ms_in= 2, ns_in= 1)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 10, 1, ns_in= 10, 6)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([9.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 2, 13, ns_in= 1, 11)\n",
       "    )\n",
       "  )\n",
       "  (decent3): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([3.]))\n",
       "       with inputs: ms_in= 13, 13, 9, ns_in= 0, 6, 7)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([17.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 10, 13, 9, ns_in= 5, 0, 7)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([8.]))\n",
       "       with inputs: ms_in= 13, 9, ns_in= 0, 7)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([0.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 10, ns_in= 5)\n",
       "      (4): DecentFilter(weights: torch.Size([1, 0, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([12.]), n_this=Parameter containing:\n",
       "      tensor([12.]))\n",
       "       with inputs: ms_in= , ns_in= )\n",
       "      (5): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 13, 13, 9, ns_in= 0, 6, 7)\n",
       "      (6): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([12.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 10, 13, ns_in= 5, 0)\n",
       "      (7): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([14.]), n_this=Parameter containing:\n",
       "      tensor([13.]))\n",
       "       with inputs: ms_in= 9, ns_in= 7)\n",
       "    )\n",
       "  )\n",
       "  (decent1x1): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 5, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 17, 5, 0, 12, 5, ns_in= 14, 8, 7, 12, 14)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 3, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([9.]))\n",
       "       with inputs: ms_in= 7, 17, 12, ns_in= 3, 14, 12)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 3, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([8.]), n_this=Parameter containing:\n",
       "      tensor([16.]))\n",
       "       with inputs: ms_in= 7, 12, 14, ns_in= 3, 12, 13)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([2.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 7, 5, 0, 14, ns_in= 3, 8, 7, 13)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (mish1): Mish()\n",
       "  (mish2): Mish()\n",
       "  (mish3): Mish()\n",
       "  (mish1x1): Mish()\n",
       "  (bias1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias2): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias3): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias1x1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f306c6d-6155-4af0-b913-42b25857b745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.1454,  0.0630, -0.0694],\n",
      "          [ 0.0387,  0.0244,  0.0492],\n",
      "          [-0.1644,  0.1041,  0.0733]],\n",
      "\n",
      "         [[-0.1633, -0.1444, -0.1245],\n",
      "          [-0.0479, -0.1443, -0.1416],\n",
      "          [-0.0718,  0.0821,  0.1490]],\n",
      "\n",
      "         [[ 0.1454,  0.0939, -0.0066],\n",
      "          [ 0.0131, -0.1436,  0.0012],\n",
      "          [ 0.0219,  0.0305, -0.1540]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1517,  0.1525,  0.0600],\n",
      "          [ 0.1450, -0.0782,  0.1121],\n",
      "          [-0.1087,  0.0695, -0.1349]],\n",
      "\n",
      "         [[-0.0730, -0.0375, -0.0486],\n",
      "          [-0.0833,  0.0144, -0.0475],\n",
      "          [ 0.0187,  0.1132,  0.0586]],\n",
      "\n",
      "         [[ 0.1233, -0.1262, -0.1722],\n",
      "          [ 0.1132, -0.0828, -0.0672],\n",
      "          [-0.0723, -0.1196, -0.0322]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0967,  0.1259, -0.0251],\n",
      "          [ 0.0019, -0.1603, -0.0013],\n",
      "          [ 0.0898, -0.0864,  0.0869]],\n",
      "\n",
      "         [[ 0.1026, -0.1284,  0.0164],\n",
      "          [-0.0226,  0.1546, -0.1396],\n",
      "          [-0.1697,  0.1495, -0.1540]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1447, -0.0024,  0.0995],\n",
      "          [ 0.1154,  0.0252,  0.0705],\n",
      "          [ 0.0135, -0.0474,  0.0671]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([], size=(1, 0, 3, 3), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.1276, -0.1124,  0.1603],\n",
      "          [ 0.1055,  0.0320, -0.0999],\n",
      "          [ 0.0277,  0.1088,  0.0606]],\n",
      "\n",
      "         [[ 0.1165,  0.1430, -0.1472],\n",
      "          [ 0.1578,  0.0391, -0.1267],\n",
      "          [-0.0065, -0.1542, -0.0411]],\n",
      "\n",
      "         [[ 0.1286,  0.1455,  0.1268],\n",
      "          [-0.1144, -0.0499, -0.0918],\n",
      "          [-0.1624,  0.1519, -0.0034]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0033,  0.0902,  0.0925],\n",
      "          [-0.0982, -0.0427, -0.0777],\n",
      "          [ 0.0786, -0.0256,  0.0870]],\n",
      "\n",
      "         [[ 0.0782,  0.1143, -0.1039],\n",
      "          [-0.0845,  0.1323,  0.0843],\n",
      "          [ 0.0389, -0.0477, -0.1412]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1446, -0.1667,  0.0340],\n",
      "          [-0.0478, -0.0662,  0.0525],\n",
      "          [ 0.1255,  0.0972, -0.1665]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for a in light.model.decent3.filter_list:\n",
    "    print(a.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c44cd3-15c4-4344-a6be-4c45ffea2070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0696de4a-76ea-4b95-9d11-e1cf8a733a42",
   "metadata": {},
   "source": [
    "# random nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbb24c8e-571a-405c-81c0-68b6563707a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([10.0000, 15.4000], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(torch.tensor([10.0, 15.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14953a49-823e-46c6-b48b-affde35f45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    tmp_listi = [nn.Parameter(torch.tensor([10.0, 15.0, 3.5])), \n",
    "                nn.Parameter(torch.tensor([4.0, 15.0])),\n",
    "                nn.Parameter(torch.tensor([5.0, 15.0])),\n",
    "                nn.Parameter(torch.tensor([6.0, 12.0, 4.5]))\n",
    "                              ]\n",
    "    # nn.ModuleList(\n",
    "\n",
    "    remove = []\n",
    "    for i, aaaa in enumerate(tmp_listi):\n",
    "        #print(aaaa)\n",
    "        if aaaa.shape[0] == 3:\n",
    "            print(aaaa)\n",
    "            remove.append(i)\n",
    "\n",
    "            tmp_listi.pop(i)\n",
    "\n",
    "    print('listi')\n",
    "    print(tmp_listi)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        state_dict = torch.load(ckpt_path)\n",
    "        print(state_dict['state_dict'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    x = []\n",
    "    x.extend([324, 23]) # Nothing is printed because the return value is None\n",
    "    x.extend([324, 23])\n",
    "    x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78e491af-5e12-488a-a72a-2a11ce158dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([10.0000, 15.0000,  3.5000], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 6.0000, 12.0000,  4.5000], requires_grad=True)\n",
      "[0, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp_listi = [nn.Parameter(torch.tensor([10.0, 15.0, 3.5])), \n",
    "            nn.Parameter(torch.tensor([4.0, 15.0])),\n",
    "            nn.Parameter(torch.tensor([6.0, 12.0, 4.5]))\n",
    "                          ]\n",
    "# nn.ModuleList(\n",
    "\n",
    "remove = []\n",
    "for i, aaaa in enumerate(tmp_listi):\n",
    "    #print(aaaa)\n",
    "    if aaaa.shape[0] == 3:\n",
    "        print(aaaa)\n",
    "        remove.append(i)\n",
    "    \n",
    "\n",
    "\n",
    "print(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7899e827-6fa5-467a-9089-a6df73d92eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecentError: layer not working, run not defined\n",
      "name 'run_explain' is not defined\n",
      "{'test accuracy on valset': 0.25}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    result = {\"test accuracy on valset\": test_result[0][\"test_acc\"]}\n",
    "except:\n",
    "    result = 0\n",
    "\n",
    "try:     \n",
    "    layer = light.model.decent2 # .filter_list[7]weights\n",
    "    run_explain(light, layer, device='cuda')\n",
    "except Exception as e:\n",
    "    print(\"DecentError: layer not working, run not defined\" )\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67ccb396-da0e-4942-998b-676d1fb8234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7212581\n",
      "0.7065036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7212581, 0.7065036]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[np.array(0.7212581)], [np.array(0.7065036)]]\n",
    "\n",
    "for e in a:\n",
    "    for i in e:\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "flattened = [val.item() for tmp in a for val in tmp] \n",
    " \n",
    "\n",
    "a\n",
    "a\n",
    "\n",
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6ee6f3b-11df-4ef3-9fd9-a110fa996159",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [[0.2847180664539337, 0.29257622361183167, 0.2561589181423187, 0.2416810840368271, 0.2306821644306183], [0.24937507510185242, 0.294414222240448, 0.2743309736251831, 0.2431085854768753, 0.23851336538791656], [0.30998995900154114, 0.2402188777923584, 0.22844929993152618, 0.2568821907043457], [0.28166717290878296, 0.22486495971679688, 0.2940433919429779, 0.26794517040252686]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d48391f-1f3e-4d78-aa61-79dc7eeac5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    a=np.array([np.array(0.69074845)])\n",
    "\n",
    "    print(a.flatten())\n",
    "\n",
    "    flattened = [val for tmp in a for val in tmp] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f165e1d-6d72-47d1-bf97-bfcd334b0a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>target_group</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0</td>\n",
       "      <td>10_10</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.715384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_0</td>\n",
       "      <td>1_6</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.699586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_0</td>\n",
       "      <td>2_1</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.509230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_0</td>\n",
       "      <td>13_11</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_0</td>\n",
       "      <td>14_1</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.633644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0_0</td>\n",
       "      <td>12_2</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.887088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14_1</td>\n",
       "      <td>10_5</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.235114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10_10</td>\n",
       "      <td>13_0</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.248237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1_6</td>\n",
       "      <td>13_0</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.236018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12_2</td>\n",
       "      <td>13_0</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.290866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2_1</td>\n",
       "      <td>13_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.230140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14_1</td>\n",
       "      <td>13_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.308720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10_10</td>\n",
       "      <td>9_7</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.294763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13_11</td>\n",
       "      <td>9_7</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.260555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10_10</td>\n",
       "      <td>0_16</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.232217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1_6</td>\n",
       "      <td>0_16</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.309217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14_1</td>\n",
       "      <td>0_16</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.231851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12_2</td>\n",
       "      <td>0_16</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.241174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10_10</td>\n",
       "      <td>12_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.303949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2_1</td>\n",
       "      <td>12_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.236642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13_11</td>\n",
       "      <td>12_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.279753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14_1</td>\n",
       "      <td>12_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.250331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12_2</td>\n",
       "      <td>12_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.249110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10_5</td>\n",
       "      <td>7_3</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.284718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13_0</td>\n",
       "      <td>7_3</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.292576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9_7</td>\n",
       "      <td>7_3</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.256159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10_5</td>\n",
       "      <td>17_14</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.249375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13_0</td>\n",
       "      <td>17_14</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.294414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9_7</td>\n",
       "      <td>17_14</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.274331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0_16</td>\n",
       "      <td>17_14</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.243109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10_5</td>\n",
       "      <td>5_8</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.309990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>12_6</td>\n",
       "      <td>5_8</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.256882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10_5</td>\n",
       "      <td>0_7</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.281667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0_16</td>\n",
       "      <td>0_7</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.294043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12_6</td>\n",
       "      <td>0_7</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.267945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7_3</td>\n",
       "      <td>5_14</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.428868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>17_14</td>\n",
       "      <td>5_14</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.507207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5_8</td>\n",
       "      <td>5_14</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.150944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0_7</td>\n",
       "      <td>5_14</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.384323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7_3</td>\n",
       "      <td>7_9</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.337815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>17_14</td>\n",
       "      <td>7_9</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.387488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5_8</td>\n",
       "      <td>7_9</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0_7</td>\n",
       "      <td>7_9</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.020934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7_3</td>\n",
       "      <td>8_16</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.410962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>17_14</td>\n",
       "      <td>8_16</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.311382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5_8</td>\n",
       "      <td>8_16</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.442951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0_7</td>\n",
       "      <td>8_16</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.212011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>7_3</td>\n",
       "      <td>2_7</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.464132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>17_14</td>\n",
       "      <td>2_7</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.198097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5_8</td>\n",
       "      <td>2_7</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.328148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0_7</td>\n",
       "      <td>2_7</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.277719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source target target_group     value\n",
       "0     0_0  10_10      decent1  0.715384\n",
       "1     0_0    1_6      decent1  0.699586\n",
       "2     0_0    2_1      decent1  0.509230\n",
       "3     0_0  13_11      decent1  0.684500\n",
       "4     0_0   14_1      decent1  0.633644\n",
       "5     0_0   12_2      decent1  0.887088\n",
       "6    14_1   10_5      decent2  0.235114\n",
       "7   10_10   13_0      decent2  0.248237\n",
       "8     1_6   13_0      decent2  0.236018\n",
       "9    12_2   13_0      decent2  0.290866\n",
       "10    2_1   13_6      decent2  0.230140\n",
       "11   14_1   13_6      decent2  0.308720\n",
       "12  10_10    9_7      decent2  0.294763\n",
       "13  13_11    9_7      decent2  0.260555\n",
       "14  10_10   0_16      decent2  0.232217\n",
       "15    1_6   0_16      decent2  0.309217\n",
       "16   14_1   0_16      decent2  0.231851\n",
       "17   12_2   0_16      decent2  0.241174\n",
       "18  10_10   12_6      decent2  0.303949\n",
       "19    2_1   12_6      decent2  0.236642\n",
       "20  13_11   12_6      decent2  0.279753\n",
       "21   14_1   12_6      decent2  0.250331\n",
       "22   12_2   12_6      decent2  0.249110\n",
       "23   10_5    7_3      decent3  0.284718\n",
       "24   13_0    7_3      decent3  0.292576\n",
       "25    9_7    7_3      decent3  0.256159\n",
       "26   10_5  17_14      decent3  0.249375\n",
       "27   13_0  17_14      decent3  0.294414\n",
       "28    9_7  17_14      decent3  0.274331\n",
       "29   0_16  17_14      decent3  0.243109\n",
       "30   10_5    5_8      decent3  0.309990\n",
       "31   12_6    5_8      decent3  0.256882\n",
       "32   10_5    0_7      decent3  0.281667\n",
       "33   0_16    0_7      decent3  0.294043\n",
       "34   12_6    0_7      decent3  0.267945\n",
       "35    7_3   5_14    decent1x1  0.428868\n",
       "36  17_14   5_14    decent1x1  0.507207\n",
       "37    5_8   5_14    decent1x1  0.150944\n",
       "38    0_7   5_14    decent1x1  0.384323\n",
       "39    7_3    7_9    decent1x1  0.337815\n",
       "40  17_14    7_9    decent1x1  0.387488\n",
       "41    5_8    7_9    decent1x1  0.144600\n",
       "42    0_7    7_9    decent1x1  0.020934\n",
       "43    7_3   8_16    decent1x1  0.410962\n",
       "44  17_14   8_16    decent1x1  0.311382\n",
       "45    5_8   8_16    decent1x1  0.442951\n",
       "46    0_7   8_16    decent1x1  0.212011\n",
       "47    7_3    2_7    decent1x1  0.464132\n",
       "48  17_14    2_7    decent1x1  0.198097\n",
       "49    5_8    2_7    decent1x1  0.328148\n",
       "50    0_7    2_7    decent1x1  0.277719"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3d0e6-ce41-4a30-b68f-7c70bbf92113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2005a8f1-b68b-42f8-a653-cd905542361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing yet - currently part of the main running dev thingi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5d6dd-be92-4db7-bdd8-40d886f10adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43468c27-f130-4118-8d11-275282921c0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25492\\2355735933.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d301c-cbad-4154-a053-137a3263c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# useless, always the same filters\n",
    "\n",
    "for i_filter in range(100):\n",
    "    try:\n",
    "        layer = model.model.decent2.filter_list[i_filter] # i_filter] # .filter_list[7]weights\n",
    "        run_explain(model, layer, device='cuda')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2568e15-95ba-4c8c-9a51-b8cb5050ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "layer = model.model.decent2 # .filter_list[7]weights\n",
    "run_explain(model, layer, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f4513-419a-433b-97ff-6f54c5a6d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b09e4-644a-4cca-9da3-a47986de1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca471c2c-819f-44aa-a8f9-e318aacf3f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 2,\n",
       " 'global_step': 12,\n",
       " 'pytorch-lightning_version': '2.0.6',\n",
       " 'state_dict': OrderedDict([('model.decent1.filter_list.0.ms_in',\n",
       "               tensor([0.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.0.ns_in',\n",
       "               tensor([0.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.0.m_this',\n",
       "               tensor([10.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.0.n_this',\n",
       "               tensor([10.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.0.weights',\n",
       "               tensor([[[[ 0.2632, -0.2342,  0.2128],\n",
       "                         [ 0.0784, -0.2114, -0.2797],\n",
       "                         [-0.1454, -0.4081,  0.1473]]]], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.1.ms_in',\n",
       "               tensor([0.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.1.ns_in',\n",
       "               tensor([0.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.1.m_this',\n",
       "               tensor([1.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.1.n_this',\n",
       "               tensor([6.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.1.weights',\n",
       "               tensor([[[[ 0.2998, -0.1746, -0.1431],\n",
       "                         [-0.3181, -0.2788, -0.0103],\n",
       "                         [-0.2634,  0.0585, -0.3345]]]], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.2.ms_in',\n",
       "               tensor([0.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.2.ns_in',\n",
       "               tensor([0.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.2.m_this',\n",
       "               tensor([2.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.2.n_this',\n",
       "               tensor([1.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.2.weights',\n",
       "               tensor([[[[ 0.0255, -0.1996, -0.2756],\n",
       "                         [-0.1767, -0.0078, -0.1549],\n",
       "                         [ 0.1127, -0.1591,  0.2145]]]], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.3.ms_in',\n",
       "               tensor([0.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.3.ns_in',\n",
       "               tensor([0.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.3.m_this',\n",
       "               tensor([13.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.3.n_this',\n",
       "               tensor([11.], device='cuda:0')),\n",
       "              ('model.decent1.filter_list.3.weights',\n",
       "               tensor([[[[ 0.0262, -0.2647, -0.0699],\n",
       "                         [ 0.2854,  0.2975,  0.2920],\n",
       "                         [ 0.3040, -0.1008, -0.1858]]]], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.0.ms_in',\n",
       "               tensor([ 1., 13.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.0.ns_in',\n",
       "               tensor([ 6., 11.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.0.m_this',\n",
       "               tensor([10.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.0.n_this',\n",
       "               tensor([5.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.0.weights',\n",
       "               tensor([[[[-0.0183,  0.0529, -0.1104],\n",
       "                         [ 0.0051,  0.0743,  0.0377],\n",
       "                         [-0.0021,  0.0803, -0.1219]],\n",
       "               \n",
       "                        [[ 0.0298, -0.0546, -0.0898],\n",
       "                         [ 0.0870,  0.0794,  0.0711],\n",
       "                         [ 0.0479, -0.0932, -0.0134]]]], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.1.ms_in',\n",
       "               tensor([2.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.1.ns_in',\n",
       "               tensor([1.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.1.m_this',\n",
       "               tensor([13.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.1.n_this',\n",
       "               tensor([0.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.1.weights',\n",
       "               tensor([[[[-0.1731,  0.0167, -0.0765],\n",
       "                         [ 0.0046, -0.1156,  0.1149],\n",
       "                         [-0.0919,  0.0993,  0.0588]]]], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.2.ms_in',\n",
       "               tensor([10.,  1.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.2.ns_in',\n",
       "               tensor([10.,  6.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.2.m_this',\n",
       "               tensor([13.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.2.n_this',\n",
       "               tensor([6.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.2.weights',\n",
       "               tensor([[[[ 0.0634,  0.1368, -0.0849],\n",
       "                         [-0.1543, -0.0339, -0.0668],\n",
       "                         [-0.1065, -0.0222,  0.1330]],\n",
       "               \n",
       "                        [[-0.0616, -0.1557,  0.0459],\n",
       "                         [ 0.1559,  0.0999,  0.0918],\n",
       "                         [-0.0716, -0.0067,  0.1097]]]], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.3.ms_in',\n",
       "               tensor([ 2., 13.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.3.ns_in',\n",
       "               tensor([ 1., 11.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.3.m_this',\n",
       "               tensor([9.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.3.n_this',\n",
       "               tensor([7.], device='cuda:0')),\n",
       "              ('model.decent2.filter_list.3.weights',\n",
       "               tensor([[[[ 0.1163, -0.0995,  0.0233],\n",
       "                         [-0.0954, -0.0177,  0.1060],\n",
       "                         [ 0.1262,  0.1094,  0.0983]],\n",
       "               \n",
       "                        [[ 0.0218,  0.1408, -0.1382],\n",
       "                         [-0.1095,  0.0691,  0.0351],\n",
       "                         [-0.0454, -0.0144,  0.1241]]]], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.0.ms_in',\n",
       "               tensor([13., 13.,  9.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.0.ns_in',\n",
       "               tensor([0., 6., 7.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.0.m_this',\n",
       "               tensor([7.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.0.n_this',\n",
       "               tensor([3.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.0.weights',\n",
       "               tensor([[[[-0.1454,  0.0630, -0.0694],\n",
       "                         [ 0.0387,  0.0244,  0.0492],\n",
       "                         [-0.1644,  0.1041,  0.0733]],\n",
       "               \n",
       "                        [[-0.1633, -0.1444, -0.1245],\n",
       "                         [-0.0479, -0.1443, -0.1416],\n",
       "                         [-0.0718,  0.0821,  0.1490]],\n",
       "               \n",
       "                        [[ 0.1454,  0.0939, -0.0066],\n",
       "                         [ 0.0131, -0.1436,  0.0012],\n",
       "                         [ 0.0219,  0.0305, -0.1540]]]], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.1.ms_in',\n",
       "               tensor([10., 13.,  9.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.1.ns_in',\n",
       "               tensor([5., 0., 7.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.1.m_this',\n",
       "               tensor([17.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.1.n_this',\n",
       "               tensor([14.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.1.weights',\n",
       "               tensor([[[[ 0.1517,  0.1525,  0.0600],\n",
       "                         [ 0.1450, -0.0782,  0.1121],\n",
       "                         [-0.1087,  0.0695, -0.1349]],\n",
       "               \n",
       "                        [[-0.0730, -0.0375, -0.0486],\n",
       "                         [-0.0833,  0.0144, -0.0475],\n",
       "                         [ 0.0187,  0.1132,  0.0586]],\n",
       "               \n",
       "                        [[ 0.1233, -0.1262, -0.1722],\n",
       "                         [ 0.1132, -0.0828, -0.0672],\n",
       "                         [-0.0723, -0.1196, -0.0322]]]], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.2.ms_in',\n",
       "               tensor([13.,  9.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.2.ns_in',\n",
       "               tensor([0., 7.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.2.m_this',\n",
       "               tensor([5.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.2.n_this',\n",
       "               tensor([8.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.2.weights',\n",
       "               tensor([[[[ 0.0967,  0.1259, -0.0251],\n",
       "                         [ 0.0019, -0.1603, -0.0013],\n",
       "                         [ 0.0898, -0.0864,  0.0869]],\n",
       "               \n",
       "                        [[ 0.1026, -0.1284,  0.0164],\n",
       "                         [-0.0226,  0.1546, -0.1396],\n",
       "                         [-0.1697,  0.1495, -0.1540]]]], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.3.ms_in',\n",
       "               tensor([10.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.3.ns_in',\n",
       "               tensor([5.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.3.m_this',\n",
       "               tensor([0.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.3.n_this',\n",
       "               tensor([7.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.3.weights',\n",
       "               tensor([[[[ 0.1447, -0.0024,  0.0995],\n",
       "                         [ 0.1154,  0.0252,  0.0705],\n",
       "                         [ 0.0135, -0.0474,  0.0671]]]], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.4.ms_in',\n",
       "               tensor([], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.4.ns_in',\n",
       "               tensor([], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.4.m_this',\n",
       "               tensor([12.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.4.n_this',\n",
       "               tensor([12.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.4.weights',\n",
       "               tensor([], device='cuda:0', size=(1, 0, 3, 3))),\n",
       "              ('model.decent3.filter_list.5.ms_in',\n",
       "               tensor([13., 13.,  9.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.5.ns_in',\n",
       "               tensor([0., 6., 7.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.5.m_this',\n",
       "               tensor([5.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.5.n_this',\n",
       "               tensor([14.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.5.weights',\n",
       "               tensor([[[[-0.1276, -0.1124,  0.1603],\n",
       "                         [ 0.1055,  0.0320, -0.0999],\n",
       "                         [ 0.0277,  0.1088,  0.0606]],\n",
       "               \n",
       "                        [[ 0.1165,  0.1430, -0.1472],\n",
       "                         [ 0.1578,  0.0391, -0.1267],\n",
       "                         [-0.0065, -0.1542, -0.0411]],\n",
       "               \n",
       "                        [[ 0.1286,  0.1455,  0.1268],\n",
       "                         [-0.1144, -0.0499, -0.0918],\n",
       "                         [-0.1624,  0.1519, -0.0034]]]], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.6.ms_in',\n",
       "               tensor([10., 13.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.6.ns_in',\n",
       "               tensor([5., 0.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.6.m_this',\n",
       "               tensor([12.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.6.n_this',\n",
       "               tensor([14.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.6.weights',\n",
       "               tensor([[[[-0.0033,  0.0902,  0.0925],\n",
       "                         [-0.0982, -0.0427, -0.0777],\n",
       "                         [ 0.0786, -0.0256,  0.0870]],\n",
       "               \n",
       "                        [[ 0.0782,  0.1143, -0.1039],\n",
       "                         [-0.0845,  0.1323,  0.0843],\n",
       "                         [ 0.0389, -0.0477, -0.1412]]]], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.7.ms_in',\n",
       "               tensor([9.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.7.ns_in',\n",
       "               tensor([7.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.7.m_this',\n",
       "               tensor([14.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.7.n_this',\n",
       "               tensor([13.], device='cuda:0')),\n",
       "              ('model.decent3.filter_list.7.weights',\n",
       "               tensor([[[[ 0.1446, -0.1667,  0.0340],\n",
       "                         [-0.0478, -0.0662,  0.0525],\n",
       "                         [ 0.1255,  0.0972, -0.1665]]]], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.0.ms_in',\n",
       "               tensor([17.,  5.,  0., 12.,  5.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.0.ns_in',\n",
       "               tensor([14.,  8.,  7., 12., 14.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.0.m_this',\n",
       "               tensor([5.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.0.n_this',\n",
       "               tensor([14.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.0.weights',\n",
       "               tensor([[[[ 0.3443]],\n",
       "               \n",
       "                        [[-0.2665]],\n",
       "               \n",
       "                        [[-0.0681]],\n",
       "               \n",
       "                        [[-0.1480]],\n",
       "               \n",
       "                        [[ 0.0514]]]], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.1.ms_in',\n",
       "               tensor([ 7., 17., 12.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.1.ns_in',\n",
       "               tensor([ 3., 14., 12.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.1.m_this',\n",
       "               tensor([7.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.1.n_this',\n",
       "               tensor([9.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.1.weights',\n",
       "               tensor([[[[ 0.1614]],\n",
       "               \n",
       "                        [[ 0.1524]],\n",
       "               \n",
       "                        [[-0.2170]]]], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.2.ms_in',\n",
       "               tensor([ 7., 12., 14.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.2.ns_in',\n",
       "               tensor([ 3., 12., 13.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.2.m_this',\n",
       "               tensor([8.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.2.n_this',\n",
       "               tensor([16.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.2.weights',\n",
       "               tensor([[[[ 0.2467]],\n",
       "               \n",
       "                        [[ 0.3265]],\n",
       "               \n",
       "                        [[-0.2089]]]], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.3.ms_in',\n",
       "               tensor([ 7.,  5.,  0., 14.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.3.ns_in',\n",
       "               tensor([ 3.,  8.,  7., 13.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.3.m_this',\n",
       "               tensor([2.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.3.n_this',\n",
       "               tensor([7.], device='cuda:0')),\n",
       "              ('model.decent1x1.filter_list.3.weights',\n",
       "               tensor([[[[-0.1046]],\n",
       "               \n",
       "                        [[-0.3141]],\n",
       "               \n",
       "                        [[ 0.3186]],\n",
       "               \n",
       "                        [[ 0.1188]]]], device='cuda:0')),\n",
       "              ('model.fc.weight',\n",
       "               tensor([[-0.2129, -0.0166,  0.0253,  0.1852],\n",
       "                       [ 0.1062,  0.0039, -0.0154, -0.1638],\n",
       "                       [-0.0805,  0.0026, -0.0029,  0.3826],\n",
       "                       [ 0.4250, -0.3960, -0.1373, -0.1534]], device='cuda:0')),\n",
       "              ('model.fc.bias',\n",
       "               tensor([-0.1004,  0.1555, -0.0834,  0.4060], device='cuda:0'))]),\n",
       " 'loops': {'fit_loop': {'state_dict': {},\n",
       "   'epoch_loop.state_dict': {'_batches_that_stepped': 12},\n",
       "   'epoch_loop.batch_progress': {'total': {'ready': 12,\n",
       "     'completed': 12,\n",
       "     'started': 12,\n",
       "     'processed': 12},\n",
       "    'current': {'ready': 4, 'completed': 4, 'started': 4, 'processed': 4},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_loop.scheduler_progress': {'total': {'ready': 3, 'completed': 3},\n",
       "    'current': {'ready': 1, 'completed': 1}},\n",
       "   'epoch_loop.automatic_optimization.state_dict': {},\n",
       "   'epoch_loop.automatic_optimization.optim_progress': {'optimizer': {'step': {'total': {'ready': 12,\n",
       "       'completed': 12},\n",
       "      'current': {'ready': 4, 'completed': 4}},\n",
       "     'zero_grad': {'total': {'ready': 12, 'completed': 12, 'started': 12},\n",
       "      'current': {'ready': 4, 'completed': 4, 'started': 4}}}},\n",
       "   'epoch_loop.manual_optimization.state_dict': {},\n",
       "   'epoch_loop.manual_optimization.optim_step_progress': {'total': {'ready': 0,\n",
       "     'completed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0}},\n",
       "   'epoch_loop.val_loop.state_dict': {},\n",
       "   'epoch_loop.val_loop.batch_progress': {'total': {'ready': 4,\n",
       "     'completed': 4,\n",
       "     'started': 4,\n",
       "     'processed': 4},\n",
       "    'current': {'ready': 4, 'completed': 4, 'started': 4, 'processed': 4},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_progress': {'total': {'ready': 3,\n",
       "     'completed': 2,\n",
       "     'started': 3,\n",
       "     'processed': 3},\n",
       "    'current': {'ready': 3, 'completed': 2, 'started': 3, 'processed': 3}}},\n",
       "  'validate_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'test_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'predict_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}}}},\n",
       " 'hparams_name': 'kwargs',\n",
       " 'hyper_parameters': {'kwargs': {'train_kwargs': {'result_path': 'examples/example_results',\n",
       "    'exp_name': 'debug_oct_no_fc',\n",
       "    'load_ckpt_file': '',\n",
       "    'epochs': 3,\n",
       "    'img_size': 28,\n",
       "    'batch_size': 2,\n",
       "    'log_every_n_steps': 4,\n",
       "    'device': 'cuda',\n",
       "    'num_workers': 0,\n",
       "    'train_size': 8,\n",
       "    'val_size': 8,\n",
       "    'test_size': 8},\n",
       "   'model_kwargs': {'n_classes': 4,\n",
       "    'out_dim': [1, 4, 4, 8, 4],\n",
       "    'grid_size': 324,\n",
       "    'criterion': CrossEntropyLoss(),\n",
       "    'optimizer': 'sgd',\n",
       "    'base_lr': 0.001,\n",
       "    'min_lr': 1e-05,\n",
       "    'momentum': 0.9,\n",
       "    'lr_update': 100,\n",
       "    'cc_weight': 10,\n",
       "    'cc_metric': 'l2',\n",
       "    'ci_metric': 'random',\n",
       "    'cm_metric': 'not implemented yet',\n",
       "    'update_every_nth_epoch': 1,\n",
       "    'pretrain_epochs': 1,\n",
       "    'prune_keep': 0.7,\n",
       "    'prune_keep_total': 0.4}},\n",
       "  'log_dir': 'examples/example_results\\\\lightning_logs\\\\debug_oct_no_fc\\\\version_13'}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(\"examples/example_results/lightning_logs/debug_oct_no_fc/version_13/checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt\") # .keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['loops'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['state_dict'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
