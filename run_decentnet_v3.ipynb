{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ð”»ð•–ð•”ð•–ð•Ÿð•¥â„•ð•–ð•¥: ð••ð•šð•¤ð•–ð•Ÿð•¥ð•’ð•Ÿð•˜ð•ð•–ð•• ð•Ÿð•–ð•¥\n",
    "\n",
    "Goal: create a sparse and modular ConvNet\n",
    "\n",
    "Todos: \n",
    "* [ ] delete node (filter) if either no input or no output edges\n",
    "* [ ] AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
    "* [ ] cuda error, if one of the decent1x1 has no kernels left - we need at least one input for each 1x1 filter\n",
    "* [ ] can we keep training if filter gets removed (e.g. at reloading model)\n",
    "* [ ] need some working filter removing in general - only at reload rn\n",
    "* [ ] currently commented:             # img, msk = flt.execute() # flattened -> in data/octa500.py\n",
    "* [ ] make sure code runs without the fda (fourier) library\n",
    "\n",
    "\n",
    "Notes:\n",
    "* additionally needed: position, activated channels, connection between channels\n",
    "* within this layer, a whole filter can be deactivated\n",
    "* within a filter, single channels can be deactivated\n",
    "* within this layer, filters can be swapped\n",
    "* the 'value' in the csv file is random if the CI metric is 'random'\n",
    "     \n",
    "* pruning actually doesn't work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf\n",
    "\n",
    "pip install:\n",
    "* pytorch_lightning\n",
    "\n",
    "preprocessing possible:\n",
    "* flatten layers\n",
    "* denoise\n",
    "* crop background\n",
    "\n",
    "\n",
    "warnings:\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73a26-f239-466f-901d-559d192f61de",
   "metadata": {},
   "source": [
    "![uml of code](examples/example_vis/uml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba244e3-3e6a-47e7-aa4b-a22df6054b8a",
   "metadata": {},
   "source": [
    "# conventions\n",
    "\n",
    "id may be image id if available, else batch id\n",
    "\n",
    "* entry image and mask: entry_id5_0_0_0_mo3_gt2.png\n",
    "* mat: mat_id10004_size26_0_0_0_mo2_gt2.mat (size - \n",
    "* hidden layer: hid_id5_3_8_2.png \n",
    "* last layer: pool_2_3_4_cl2.png (global pooling - connected to class n, cl=class)\n",
    "* activated image: cam_id5_mo3_gt2.png\n",
    "* activated image gray: camgray_id5_mo3_gt2.png\n",
    "\n",
    "\n",
    "* circle in: in_2_3_4_ep65.png\n",
    "* circle out: out_2_3_4_ep65.png\n",
    "\n",
    "* filter: filter_2_3_4.csv and filter_2_3_4.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# future imports first\n",
    "# =============================================================================\n",
    "from __future__ import print_function\n",
    "# =============================================================================\n",
    "# sys\n",
    "# =============================================================================\n",
    "import sys \n",
    "sys.path.insert(0, \"helper\")\n",
    "# =============================================================================\n",
    "# alphabetic order misc\n",
    "# =============================================================================\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import scipy.io\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "# =============================================================================\n",
    "# torch\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torchvision\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "# from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "# =============================================================================\n",
    "# datasceyence\n",
    "# =============================================================================\n",
    "from helper.model.decentnet import DecentNet\n",
    "from helper.visualisation import filter_activation\n",
    "from helper.visualisation.colour import *\n",
    "from helper.data.mnist import DataLoaderMNIST\n",
    "from helper.data.retinamnist import DataLoaderRetinaMNIST\n",
    "from helper.data.octmnist import DataLoaderOCTMNIST\n",
    "from helper.data.octa500 import DataLoaderOCTA500\n",
    "from helper.data.organmnist3D import DataLoaderOrganMNIST3D\n",
    "from data.transform.octa500_resize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397b450-c5c6-4da1-b630-7bf80e46891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch 2.0.0 == False\n",
      "tl 2.1.0 == False\n"
     ]
    }
   ],
   "source": [
    "seed = 1997 # was 19 before\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "debug_model = False # todo - this is to do some print stuff\n",
    "\n",
    "print('torch 2.0.0 ==', torch.__version__=='2.0.0')\n",
    "print('tl 2.1.0 ==', pl.__version__=='2.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b0be-245d-49c0-88b4-d94167f7da90",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97670e82-0d27-4b7f-91df-874acf9974a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'input_data_csv': ['data_prep/data_octa500.csv'], 'result_path': 'examples/example_results', 'exp_name': 'trying_optimiser', 'load_ckpt_file': 'version_4/checkpoints/mu_epoch=5-val_f1_macro=0.46-unpruned=5676.ckpt', 'load_mode': False, 'dataset': 'octmnist', 'epochs': 4, 'img_size': 28, 'p_augment': 0.2, 'batch_size': 2, 'log_every_n_steps': 50, 'device': 'cuda', 'num_workers': 0, 'train_size': 16, 'val_size': 10, 'test_size': 10, 'octa500_id': 199, 'xai_done': False}\n",
      "model kwargs {'in_channels': 1, 'n_classes': None, 'out_dim': [1, 8, 16, 32], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'new_cc_mode': True, 'reset_optimiser_at_update': True, 'optimizer': 'sgd', 'base_lr': 0.1, 'min_lr': 0.001, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 5, 'cc_metric': 'l2', 'ci_metric': 'l2', 'cm_metric': 'not implemented yet', 'update_every_nth_epoch': 3, 'pretrain_epochs': 3, 'prune_keep': 0.95, 'prune_keep_total': 0.4}\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {\n",
    "    'in_channels' : 1, # not in use yet\n",
    "    'n_classes': None, # filled in the dataset\n",
    "    'out_dim' :  [1, 8, 16, 32], # [1, 8, 16, 32], #[1, 16, 24, 32] # entry, decent1, decent2, decent3\n",
    "    'grid_size' : 18*18,\n",
    "    'criterion' : torch.nn.CrossEntropyLoss(), # torch.nn.BCEWithLogitsLoss(),\n",
    "    'new_cc_mode' : True, # this is for using the new connection cost loss term\n",
    "    'reset_optimiser_at_update' : True, # needs to be resetted when pruning\n",
    "    'optimizer': \"sgd\", # sgd adamw\n",
    "    'base_lr': 0.1, #0.001,\n",
    "    'min_lr' : 0.001, #0.00001,\n",
    "    'momentum' : 0.9,\n",
    "    'lr_update' : 100,\n",
    "    # decentnet\n",
    "    'cc_weight': 5, # high weight as the cc doesn't change a lot\n",
    "    'cc_metric' : 'l2', # connection cost metric (for loss) - distance metric # no idea how the torch works oops\n",
    "    'ci_metric' : 'l2', # todo: should be l2 # channel importance metric (for pruning)\n",
    "    'cm_metric' : 'not implemented yet', # 'count', # crossing minimisation \n",
    "    'update_every_nth_epoch' : 3, # 5\n",
    "    'pretrain_epochs' : 3, # 20\n",
    "    'prune_keep' : 0.95, # 0.97, # in each epoch\n",
    "    'prune_keep_total' : 0.4, # this number is not exact, depends on the prune_keep value\n",
    "}\n",
    "\n",
    "train_kwargs = {\n",
    "    'input_data_csv': [\"data_prep/data_octa500.csv\"],\n",
    "    'result_path': \"examples/example_results\", # \"example_results/lightning_logs\", # not in use??\n",
    "    'exp_name': \"trying_optimiser\", # must include dataset name, otherwise mnist is used\n",
    "    'load_ckpt_file' : \"version_4/checkpoints/mu_epoch=5-val_f1_macro=0.46-unpruned=5676.ckpt\", # \"version_0/checkpoints/mu_epoch=14-val_f1_macro=0.41-unpruned=5676.ckpt\" # \"version_0/checkpoints/epoch=94-unpruned=1600-val_f1=0.67.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "    'load_mode' : False, # True, False\n",
    "    'dataset' : 'octmnist',\n",
    "    'epochs': 4, # including the pretrain epochs - no adding up\n",
    "    'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ... # MNIST/MedMNIST 28 Ã— 28 Pixel\n",
    "    'p_augment' : 0.2, # probabiliby of torchvision transforms of training data (doesn't apply to all transforms) # 0.1 low, 0.5 half, 1 always\n",
    "    'batch_size': 2, # laptop: 2, pc: 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "    'log_every_n_steps' : 50, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "    'device': \"cuda\",\n",
    "    'num_workers' : 0, # 18, # 18 for seri computer, 0 or 8 for my laptop # make sure smaller than activate dataset sizes\n",
    "    'train_size' : 16, # total, none = 0, all = -1  (batch size * forward passes per epoch) # set 0 to skip training and just do testing\n",
    "    'val_size' : 10, # total, none = 0, all = -1 (batch size * forward passes per epoch) \n",
    "    'test_size' : 10, # total, none = 0, all = -1 (batch size * forward passes per epoch)\n",
    "    'octa500_id' : 200-1, # not in use - we use preselected data from a csv\n",
    "    'xai_done' : False, # DO NOT CHANGE, WILL BE CHANGED IN CODE\n",
    "}\n",
    "\n",
    "print(\"train kwargs\", train_kwargs)\n",
    "print(\"model kwargs\", model_kwargs)\n",
    "\n",
    "kwargs = {'train_kwargs':train_kwargs, 'model_kwargs':model_kwargs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44bc85-7644-4382-bfeb-8b78179ee8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f0ee3d0-9c79-4917-a355-093f1daf4855",
   "metadata": {},
   "source": [
    "## check the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d5ff36-c015-4bd6-8b12-c4d0e3b64b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights that stay\n",
      "First 5 pairs: [(0, 6000), (1, 6000), (2, 6000), (3, 6000)]\n",
      "Last 5 pairs: [(0, 6000), (1, 6000), (2, 6000), (3, 6000)]\n",
      "Total pairs: 4\n"
     ]
    }
   ],
   "source": [
    "breaking = 6000*model_kwargs['prune_keep_total']\n",
    "weights = 6000 # this value is an estimate for a model [1, 8, 16, 32]\n",
    "# 'unpruned' is the logger variable for the value\n",
    "\n",
    "pairs = []\n",
    "print(\"weights that stay\")\n",
    "for i in range(train_kwargs['epochs']):\n",
    "    \n",
    "    if (weights < breaking): # weights*model_kwargs['prune_keep']\n",
    "        print(\"stop:\", breaking)\n",
    "        print('you need at least this many epochs:', i)\n",
    "        print('you currently have this many epochs:', train_kwargs['epochs'])\n",
    "        print(\"recommended to add 2*update_every_nth_epoch\")\n",
    "        break\n",
    "    \n",
    "    # not sure whether -1 is correct, have to check\n",
    "    if i >= model_kwargs['pretrain_epochs'] and ((i-1)%model_kwargs['update_every_nth_epoch'] == 0):\n",
    "        weights = int(weights*model_kwargs['prune_keep'])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #print(i, weights)\n",
    "    pairs.append((i, weights))\n",
    "\n",
    "\n",
    "print(f\"First 5 pairs: {pairs[:5]}\")\n",
    "print(f\"Last 5 pairs: {pairs[-5:]}\")\n",
    "print(f\"Total pairs: {len(pairs)}\")\n",
    "\n",
    "# print(f\"Min i: {min([i for i, a in your_data])}, Max i: {max([i for i, a in your_data])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b3283-2825-4f35-9716-78ccf5bb8b1c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "* the dataset name needs to be part of the experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "********** DECENT INFO: DataLoader infos **********\n",
      "python_class : OCTMNIST\n",
      "description : The OCTMNIST is based on a prior dataset of 109,309 valid optical coherence tomography (OCT) images for retinal diseases. The dataset is comprised of 4 diagnosis categories, leading to a multi-class classification task. We split the source training set with a ratio of 9:1 into training and validation set, and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384âˆ’1,536)Ã—(277âˆ’512). We center-crop the images and resize them into 1Ã—28Ã—28.\n",
      "url : https://zenodo.org/records/10519652/files/octmnist.npz?download=1\n",
      "MD5 : c68d92d5b585d8d81f7112f81e2d0842\n",
      "task : multi-class\n",
      "label : {'0': 'cnv', '1': 'dme', '2': 'drusen', '3': 'normal'}\n",
      "n_channels : 1\n",
      "n_samples : {'train': 97477, 'val': 10832, 'test': 1000}\n",
      "license : CC BY 4.0\n",
      "\n",
      "class_counts tensor([7, 4, 0, 5]) class_weights: tensor([0., 0., nan, 0.])\n",
      "DECENT INFO: dataset too small, no weighting used\n",
      "n_classes: 4\n"
     ]
    }
   ],
   "source": [
    "if 'octmnist' in train_kwargs['dataset']:\n",
    "    # OCTMINST + weights for loss due to heavy imbalanced data\n",
    "    dataloader = DataLoaderOCTMNIST(train_kwargs, model_kwargs)  \n",
    "    print(\"\")\n",
    "    all_labels = []\n",
    "    # Extract all labels from the DataLoader\n",
    "    for inputs, labels in dataloader.train_dataloader:\n",
    "        all_labels.append(labels.flatten())\n",
    "    # Concatenate all labels into a single tensor\n",
    "    \n",
    "    all_labels = torch.cat(all_labels)\n",
    "    sorted_labels, sorted_indices = torch.sort(all_labels)\n",
    "    # Count the occurrences of each class\n",
    "    class_counts = torch.bincount(sorted_labels)\n",
    "    # Calculate weights (inverse of class frequency)\n",
    "    class_weights = 1.0 / class_counts.float()\n",
    "    # Normalize weights (optional, but recommended for stability)\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    print(\"class_counts\", class_counts, \"class_weights:\", class_weights) \n",
    "    if torch.isnan(class_weights).any(): \n",
    "        print(\"DECENT INFO: dataset too small, no weighting used\") \n",
    "    else:\n",
    "        model_kwargs[\"criterion\"] = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # class_mapper = ['cnv', 'dr', 'amd', 'healthy']\n",
    "elif 'retinamnist' in train_kwargs['dataset']:\n",
    "    # RetinaMNIST\n",
    "    dataloader = DataLoaderRetinaMNIST(train_kwargs, model_kwargs)\n",
    "    \n",
    "elif 'octa500' in train_kwargs['dataset']:\n",
    "    # OCTA-500\n",
    "    dataloader = DataLoaderOCTA500(train_kwargs, model_kwargs)\n",
    "elif '3d' in train_kwargs['dataset']:\n",
    "    dataloader = DataLoaderOrganMNIST3D(train_kwargs, model_kwargs)\n",
    "else:\n",
    "    print(\"select a valid dataset\")\n",
    "    \n",
    "class_mapper = dataloader.info[\"label\"]\n",
    "    \n",
    "assert model_kwargs['n_classes'] != None, \"DECENT ERROR: make sure you set the n_classes with the dataset\"  \n",
    "print(\"n_classes:\", model_kwargs['n_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2291086-b7cb-4af4-b7ca-6ef037ca67a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9e76d-0b29-463d-a3a1-c60f081a74b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X (Datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # an object with image representations and their positions\n",
    "    # amout of channels need to have same length as m and n lists\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        self.data = data # list of tensors (image representations)\n",
    "        self.ms_x = ms_x # list of integers (m position of each image representation)\n",
    "        self.ns_x = ns_x # list of integers (n position of each image representation)\n",
    "                \n",
    "    def setter(self, data, ms_x, ns_x):\n",
    "        self.data = data\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        \n",
    "    def getter(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e499e01-6930-4702-aa7a-f610d4023049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working\n",
    "\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "class OptimiserUpdateCheckpoint(Callback):\n",
    "    \n",
    "    def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if pl_module.reset_optimiser_at_update: # todo - something missing here!!\n",
    "            print(\"update optimiser\")\n",
    "            \n",
    "            # print(pl_module.model.parameters())\n",
    "            \n",
    "            new_optimizers = optim.SGD(pl_module.model.parameters(), lr=0.01, momentum=0.9)\n",
    "            trainer.optimizers = [new_optimizers]\n",
    "            \n",
    "            \n",
    "            # trainer.lr_schedulers = trainer.configure_schedulers([new_schedulers])\n",
    "            # self.model.parameters\n",
    "            \n",
    "            #trainer.optimizers[0] = new_optimizers\n",
    "            \n",
    "            #print(\"* params begin \"*10)\n",
    "            #for param in pl_module.model.parameters():\n",
    "            #    print(param)\n",
    "            #    break\n",
    "            #print(\"-\"*10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint \n",
    "        # if unpruned state != -1\n",
    "        # Save a checkpoint at the end of a defined training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "            print(\"DECENT NOTE: callback on_train_epoch_end\", monitor_candidates[\"epoch\"].item())\n",
    "            if monitor_candidates[\"epoch\"] > 0:\n",
    "                if monitor_candidates[\"unpruned_state\"] != -1:\n",
    "                    print(\"DECENT NOTE: save model\", monitor_candidates[\"epoch\"].item())\n",
    "                    if self._every_n_epochs >= 1 and ((trainer.current_epoch + 1) % self._every_n_epochs) == 0:\n",
    "                        self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                    self._save_last_checkpoint(trainer, monitor_candidates)\n",
    "                    \n",
    "                    pl_module.model.get_everything(current_epoch=trainer.current_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "## LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # Lightning Module consists of functions that define the training routine\n",
    "    # train, val, test: before epoch, step, after epoch, ...\n",
    "    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n",
    "    # order for the instance methods:\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\n",
    "    # \n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, kwargs, log_dir, ckpt_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        print(\"ckpt_path:\", ckpt_path)\n",
    "        \n",
    "        if train_kwargs[\"load_mode\"]: # True, False\n",
    "            # ckpt_path = os.path.join(log_dir, train_kwargs[\"load_ckpt_file\"]).replace(\"_xAI\", \"\")\n",
    "            if os.path.isfile(ckpt_path):\n",
    "                print(f\"Found pretrained model at {ckpt_path}, loading...\")\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir, ckpt_path=ckpt_path).to(\"cuda\")\n",
    "            else:\n",
    "                print(f\"DECENT NOTE: Did not find {ckpt_path}, create new model\")\n",
    "                # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "                # self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        else:\n",
    "            print(f\"Create new model\")\n",
    "            # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "            self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "            \n",
    "        # print(self.model)\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optim = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        self.pretrain_epochs = model_kwargs[\"pretrain_epochs\"]\n",
    "        self.image_size = kwargs['train_kwargs'][\"img_size\"]\n",
    "        self.new_cc_mode = kwargs[\"model_kwargs\"][\"new_cc_mode\"]\n",
    "        self.reset_optimiser_at_update = kwargs[\"model_kwargs\"][\"reset_optimiser_at_update\"]\n",
    "        \n",
    "        self.cc_ci = torch.tensor([0]).to(kwargs[\"train_kwargs\"][\"device\"])\n",
    "    \n",
    "        \n",
    "        \n",
    "        # needed for hparams.yaml file\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if False:\n",
    "            self.metric = { \"train_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"train_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"val_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"val_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "                       }\n",
    "        elif False:\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            \n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            \n",
    "            self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "        else:\n",
    "            self.train_metrics = torchmetrics.MetricCollection(\n",
    "                {\n",
    "                \"acc\": torchmetrics.classification.MulticlassAccuracy(num_classes=self.n_classes),\n",
    "                \"f1_macro\": torchmetrics.classification.MulticlassF1Score(num_classes=self.n_classes),\n",
    "                \"f1_micro\" : torchmetrics.classification.MulticlassF1Score(num_classes=self.n_classes, average='micro'),\n",
    "                \"prec\": torchmetrics.classification.MulticlassPrecision(num_classes=self.n_classes),\n",
    "                \"rec\": torchmetrics.classification.MulticlassRecall(num_classes=self.n_classes),\n",
    "                # \"cm\": torchmetrics.classification.MulticlassConfusionMatrix(num_classes=self.n_classes)\n",
    "                }, prefix=\"train_\",)\n",
    "            self.val_metrics = self.train_metrics.clone(prefix=\"val_\")\n",
    "            self.test_metrics = self.train_metrics.clone(prefix=\"test_\")\n",
    "            \n",
    "            self.cm = torchmetrics.classification.MulticlassConfusionMatrix(num_classes=self.n_classes)\n",
    "            self.roc_auc = torchmetrics.classification.MulticlassROC(num_classes=self.n_classes)\n",
    "            self.pr_curve = torchmetrics.classification.MulticlassPrecisionRecallCurve(num_classes=self.n_classes)\n",
    "        \n",
    "        print(\"init\")\n",
    "        #print(self.train_metrics)\n",
    "        #print(self.val_metrics)\n",
    "            \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        # =============================================================================\n",
    "        # we make it possible to use model_output = self(image)\n",
    "        # =============================================================================\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # =============================================================================\n",
    "        # returns:\n",
    "        #    optimiser and lr scheduler\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: configure_optimizers\")\n",
    "        \n",
    "        if self.optim == \"adamw\":\n",
    "            self.optimizer = optim.AdamW(self.model.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            return [self.optimizer] # , [lr_scheduler]\n",
    "        else:\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            \"\"\"\n",
    "            lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "        return [self.optimizer] # , [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        # initial plot of circular layer\n",
    "        # updates model every nth epoch\n",
    "        # =============================================================================  \n",
    "        \n",
    "        self.log('train_decent2_first_weight_start', self.model.decent2.filter_list[0].weights[0].flatten()[0].detach().cpu().numpy().item(), on_step=False, on_epoch=True)\n",
    "        \n",
    "        \n",
    "        print(\"e_start d1:\", self.model.decent1.filter_list[0].weights.flatten()[0])\n",
    "        print(\"e_start d2:\", self.model.decent2.filter_list[0].weights[0].flatten()[0])\n",
    "        \n",
    "        if False:\n",
    "            print(\"DECENT NOTE: on_train_epoch_start\", self.current_epoch)\n",
    "        \n",
    "        # plot random layer (the circular plot)\n",
    "        if self.current_epoch == 0:\n",
    "            self.model.plot_incoming_connections(current_epoch=0)\n",
    "            self.model.plot_outgoing_connections(current_epoch=0)\n",
    "\n",
    "        # update model\n",
    "        # don't update unless pretrain epochs is reached\n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch >= self.pretrain_epochs:\n",
    "            print(\"DECENT NOTE: update model\", self.current_epoch)        \n",
    "            if debug_model:\n",
    "                print(\"DECENT NOTE: before update\")\n",
    "                print(\"DECENT NOTE: print model ...\")\n",
    "                print(self.model)\n",
    "            self.model.update(current_epoch = self.current_epoch)\n",
    "            \n",
    "            if self.reset_optimiser_at_update:\n",
    "                # self.configure_optimizers() # reset optimisers do to big change in loss term + cause of pruned parameters\n",
    "                # self.trainer.accelerator_backend.setup_optimizers(self) xxxxxx\n",
    "                #new_optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "                #self.optimizer.load_state_dict(new_optimizer.state_dict())  \n",
    "                \n",
    "                # self.trainer.accelerator.setup_optimizers(self)\n",
    "                #self.trainer.strategy.setup_optimizers(self)\n",
    "                \n",
    "                \n",
    "                pass\n",
    "            if False:\n",
    "                print(\"DECENT NOTE: model updated\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculates loss for a batch # 1\n",
    "        # parameters:\n",
    "        #    batch\n",
    "        #    batch id\n",
    "        # returns:\n",
    "        #    loss\n",
    "        # notes:\n",
    "        #    calling gradcam like self.gradcam(batch) is dangerous cause changes gradients\n",
    "        # =============================================================================     \n",
    "        if False: # batch_idx < 2: # print first two steps\n",
    "            print(\"DECENT NOTE: training_step\", batch_idx)\n",
    "\n",
    "    \n",
    "        # calculate loss\n",
    "        # loss = torch.tensor(1)\n",
    "        loss = self.run_loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        if False: # to check the gradients - on 14.11.2024 they seemed fine ..\n",
    "            print(\"next ********************************\")\n",
    "            ignored = []\n",
    "            for i_p, param in enumerate(self.model.parameters()):\n",
    "                if param.grad is not None:\n",
    "                    # print(param.grad)\n",
    "                    print(\"++ para\", i_p, \" \", param.grad.shape, \" \", param.grad.flatten()[0])\n",
    "                else:\n",
    "                    ignored.append(i_p)\n",
    "                    pass\n",
    "                    # (\"++ para NONE\")\n",
    "            print(\"nan element amount:\", len(ignored))\n",
    "            \n",
    "        #self.model.decent1\n",
    "        #self.model.decent2 xxx\n",
    "        print(\"t_step d1:\", self.model.decent1.filter_list[0].weights.flatten()[0])\n",
    "        print(\"t_step d2:\", self.model.decent2.filter_list[0].weights[0].flatten()[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging # 2\n",
    "        # =============================================================================\n",
    "        if False: # batch_idx < 2:\n",
    "            print(\"DECENT NOTE: validation_step\", batch_idx)\n",
    "        \n",
    "        self.run_loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing # 3\n",
    "        # =============================================================================\n",
    "        if False:\n",
    "            print(\"DECENT NOTE: on_validation_epoch_end\")\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # save model if next iteration model is pruned # 4 \n",
    "        # this needs to be called before callback \n",
    "        # - if internal pytorch lightning convention changes, this will stop working\n",
    "        # =============================================================================\n",
    "        \n",
    "        if False:\n",
    "            print(\"DECENT NOTE: on_train_epoch_end\", self.current_epoch)\n",
    "               \n",
    "        if False:\n",
    "            print(\"current epoch\")\n",
    "            print(((self.current_epoch+1) % self.update_every_nth_epoch) == 0)\n",
    "            print(self.current_epoch+1)\n",
    "            print(self.current_epoch)\n",
    "            print(self.update_every_nth_epoch)\n",
    "        \n",
    "        # numel: returns the total number of elements in the input tensor\n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.log(f'unpruned', float(unpruned), on_step=False, on_epoch=True) \n",
    "        \n",
    "        if ((self.current_epoch+1) % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            # if next epoch is an update, set unpruned flag            \n",
    "            self.log(f'unpruned_state', 1.0, on_step=False, on_epoch=True)\n",
    "            \n",
    "            # save file\n",
    "            with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "                f.write(\"\\n# parameter requires grad shape #\\n\")\n",
    "                for p in self.model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        f.write(str(p.shape))\n",
    "            \n",
    "        else:\n",
    "            # else set unpruned flag to -1, then model won't be saved\n",
    "            self.log(f'unpruned_state', -1.0, on_step=False, on_epoch=True)\n",
    "            \n",
    "        \n",
    "        print(\"e_end d1:\", self.model.decent1.filter_list[0].weights.flatten()[0])\n",
    "        print(\"e_end d2:\", self.model.decent2.filter_list[0].weights[0].flatten()[0])\n",
    "        \n",
    "        self.log('train_decent2_first_weight_end', self.model.decent2.filter_list[0].weights[0].flatten()[0].detach().cpu().numpy().item(), on_step=False, on_epoch=True)\n",
    "\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.model.get_everything(current_epoch='final_test')\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging, plot gradcam\n",
    "        # =============================================================================\n",
    "        if batch_idx < 2:\n",
    "            print(\"DECENT NOTE: test_step\", batch_idx)\n",
    "\n",
    "        # we update mo and gt here\n",
    "        self.run_loss_n_metrics(batch, mode=\"test\")\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.enable_grad():\n",
    "            grad_preds = preds.requires_grad_()\n",
    "            preds2 = self.layer2(grad_preds)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # save image\n",
    "        \n",
    "        if len(batch) == 4:\n",
    "            img, _, msks, img_id = batch\n",
    "            img_id = img_id.detach().cpu().item()\n",
    "            tmp_b4 = True\n",
    "        else:\n",
    "            img, _ = batch # image and mask come out of this\n",
    "            img_id = batch_idx\n",
    "            msks = None\n",
    "            tmp_b4 = False             \n",
    "        \n",
    "        # print(img.shape)\n",
    "        \n",
    "        # save image\n",
    "        tmp_file_name = f'entry_id{img_id}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        # tmp_img = self.feature_maps.squeeze()[i_map].cpu().detach().numpy()\n",
    "        tmp_img = img.squeeze().cpu().detach().numpy()\n",
    "        tmp_path = os.path.join(self.log_dir, \"img_choice\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_img)\n",
    "        plt.close()\n",
    "        \n",
    "        if tmp_b4:\n",
    "            msks = msks.detach().cpu().numpy().squeeze()\n",
    "            \n",
    "            tmp_msk = msks[0] # 28\n",
    "            \n",
    "            # save mask\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            for o, boundary in enumerate(tmp_msk): # skip last one\n",
    "                # plt.plot(list(range(len(layer))), layer)\n",
    "                plt.plot(boundary[:,1]-0.5, boundary[:,0])\n",
    "            plt.ylim(0, 28 - 1)\n",
    "            plt.gca().invert_yaxis()\n",
    "            #plt.axis('off')\n",
    "            # Save the plot\n",
    "            # plt.savefig('plot_without_axes.png', bbox_inches='tight', pad_inches=0)\n",
    "            tmp_file_name = f'entry_id{img_id}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "            tmp_path = os.path.join(self.log_dir, \"msk_choice\")\n",
    "            os.makedirs(tmp_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(tmp_path, tmp_file_name), bbox_inches='tight', pad_inches=0)\n",
    "            #plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_msk) # todo\n",
    "            plt.close()\n",
    "\n",
    "            # save image + mask (todo)\n",
    "            plt.imshow(tmp_img, cmap=\"gray\")\n",
    "            for o, boundary in enumerate(tmp_msk):\n",
    "                plt.plot(boundary[:,1]-0.5, boundary[:,0])\n",
    "            tmp_file_name = f'entry_id{img_id}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "            tmp_path = os.path.join(self.log_dir, \"img_with_msk\")\n",
    "            os.makedirs(tmp_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(tmp_path, tmp_file_name), bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "            \n",
    "            # save mat file\n",
    "            for msk, msk_size in zip(msks, [28,26,24,22]):\n",
    "                tmp_mat = {'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Fri May 06 15:17:37 2022',\n",
    "                     '__version__': '1.0',\n",
    "                     '__globals__': [],\n",
    "                     'Layer': msk\n",
    "                            }\n",
    "\n",
    "                tmp_file_name = f'mat_id{img_id}_size{msk_size}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.mat'\n",
    "                tmp_path = os.path.join(self.log_dir, \"mat_transformed_choice\")\n",
    "                os.makedirs(tmp_path, exist_ok=True)\n",
    "                scipy.io.savemat(file_name=os.path.join(tmp_path, tmp_file_name), mdict=tmp_mat)\n",
    "            \n",
    "            \n",
    "        \n",
    "            # plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_img)\n",
    "        \n",
    "        # save feature maps of hidden layers and the layer that gets globally pooled\n",
    "        try:\n",
    "            with torch.set_grad_enabled(True): # torch.set_grad_enabled(True):\n",
    "                self.run_xai_gradcam(batch, batch_idx, mode='explain')\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: batch size has to be 1\")\n",
    "            print(e)\n",
    "            \n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            layer = self.model.decent1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            #filter_list.extend(tmp)\n",
    "            \n",
    "            layer = self.model.decent2\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent2' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent3\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent3' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent1x1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1x1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "        # get filter list            \n",
    "        filter_list = []\n",
    "        for l in self.model.decent1.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{1}\")\n",
    "        for l in self.model.decent2.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{2}\")\n",
    "        for l in self.model.decent3.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{3}\")\n",
    "        for l in self.model.decent1x1.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{4}\")\n",
    "        df = pd.DataFrame(filter_list, columns=['filter'])\n",
    "        df.to_csv(os.path.join(self.log_dir, \"all_filters.csv\"), index=False)\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing\n",
    "        # =============================================================================\n",
    "        if False:\n",
    "            print(\"DECENT NOTE: on_test_epoch_end\", self.current_epoch)\n",
    "        \n",
    "        tmp_path = os.path.join(self.log_dir, \"final_plots\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        \n",
    "        # confusion matrix\n",
    "        cm = self.cm.compute()\n",
    "        cm = cm.cpu().numpy()\n",
    "        df_cm = pd.DataFrame(cm, index=list(class_mapper.values()), columns=list(class_mapper.values()))\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        # plt.gca().invert_yaxis() - should not be inverted\n",
    "        plt.savefig(os.path.join(tmp_path, \"confusion_matrix.png\"), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        \n",
    "        # precision-recall curve\n",
    "        pr_precision, pr_recall, pr_thresholds = self.pr_curve.compute()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(self.n_classes):\n",
    "            converted_label = class_mapper.get(str(i))\n",
    "            plt.plot(pr_recall[i].cpu(), pr_precision[i].cpu(), label=f\"{converted_label}\", color=cnv_dr_amd_normal.colors[i]) \n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(\"Precision-Recall Curve\")\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.savefig(os.path.join(tmp_path, \"precision_recall_curve.png\"), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        \n",
    "        roc_fpr, roc_tpr, roc_thresholds = self.roc_auc.compute()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(self.n_classes):\n",
    "            converted_label = class_mapper.get(str(i))\n",
    "            plt.plot(roc_fpr[i].cpu(), roc_tpr[i].cpu(), label=f\"{converted_label}\", color=cnv_dr_amd_normal.colors[i]) # could add AUC here # ... torchmetrics.functional.auroc(preds ... but i don't have access ot the preds here\n",
    "            # plt.plot(roc_fpr[i].cpu(), roc_tpr[i].cpu(), label=f\"Class {i} (AUC = {torchmetrics.functional.auroc(probs[:, i], target == i):.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC-AUC Curve\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(os.path.join(tmp_path, \"roc_auc_curve.png\"), bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "    \n",
    "    def run_xai_feature_map(self, batch, batch_idx, layer, layer_str, device='cuda'):\n",
    "        # https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5\n",
    " \n",
    "        # img, label = testset.__getitem__(0) # batch x channel x width x height, class\n",
    "\n",
    "        # img = X(img.to(device).unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "                    \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, msk, img_id = batch\n",
    "            img_id = img_id.detach().cpu().item()\n",
    "        else:\n",
    "            img, ground_truth = batch # image and mask come out of this\n",
    "            img_id = batch_idx\n",
    "            msk = None\n",
    "            \n",
    "        \n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        # print(img.data.shape)\n",
    "\n",
    "        # run feature map\n",
    "        # model, layer, layer_str, log_dir, device=\"cpu\"\n",
    "        fm = filter_activation.DecentFilterActivation(model=self.model, layer=layer, layer_str=layer_str, log_dir=self.log_dir, device=device)\n",
    "        fm.run(tmp_img, img_id)\n",
    "        \n",
    "        filter_list = fm.log()\n",
    "        \n",
    "        return filter_list\n",
    "        \n",
    "        \n",
    "    \n",
    "    def run_xai_gradcam(self, batch, batch_idx, mode='explain'):\n",
    "        # =============================================================================\n",
    "        # grad cam - or just cam?? idk\n",
    "        # todo error: RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n",
    "        # BATCH SIZE HAS TO BE ONE!!!\n",
    "        # grad enable in test mode:\n",
    "        # https://github.com/Project-MONAI/MONAI/discussions/1598\n",
    "        # https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "        # =============================================================================\n",
    "    \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, msk, img_id = batch\n",
    "            img_id = img_id.detach().cpu().item()\n",
    "        else:\n",
    "            img, ground_truth = batch # image and mask come out of this\n",
    "            img_id = batch_idx\n",
    "            msk = None\n",
    "\n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)]) # .requires_grad_()\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        #print(\"nooooooooooo grad, whyyyyy\")\n",
    "        #print(tmp_img1)\n",
    "        #print(img)\n",
    "\n",
    "        #print('b1', tmp_img1)\n",
    "        #print('b2', tmp_img2)\n",
    "\n",
    "        model_output = self(tmp_img1, mode)\n",
    "\n",
    "        #print('c1', tmp_img1)\n",
    "        #print('c2', tmp_img2)\n",
    "\n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "\n",
    "        # get prediction value\n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "\n",
    "        #print('d1', tmp_img1)\n",
    "\n",
    "        #print(\"mo\", model_output)\n",
    "        #print(\"max\", pred_max)\n",
    "        #print(\"backprop\", model_output[:, pred_max])\n",
    "\n",
    "        # backpropagate for gradient tracking\n",
    "        model_output[:, pred_max].backward()\n",
    "\n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        #print('e2', tmp_img2)\n",
    "\n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # draw the heatmap\n",
    "        # plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "        # fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_l2_plot[0])}_n{int(self.n_l2_plot[0])}_{str(current_epoch)}.png\"))\n",
    "        \n",
    "        tmp_path = os.path.join(self.log_dir, \"gradcam\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.imsave(os.path.join(tmp_path, \n",
    "                                f\"cam_id{img_id}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\"\n",
    "                               ), heatmap.detach().cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "        heatmap *= 255.0 / heatmap.max()\n",
    "        pil_heatmap = Image.fromarray(heatmap.detach().cpu().numpy().squeeze()).convert('RGB')\n",
    "       \n",
    "        tmp_path = os.path.join(self.log_dir, \"gradcam\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        pil_heatmap.save(os.path.join(tmp_path, \n",
    "                                      f\"camgray_id{img_id}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" \n",
    "                                     )) \n",
    "            \n",
    "    def run_loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        # =============================================================================\n",
    "        # put image through model, calculate loss and metrics\n",
    "        # use cc term that has been calculated previously\n",
    "        # =============================================================================\n",
    "        \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, mask, img_id = batch\n",
    "        else:\n",
    "            img, ground_truth = batch\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # for test routine \"test_step\"\n",
    "        self.mo = model_output.argmax(dim=1).squeeze().detach().cpu().numpy()\n",
    "        self.gt = ground_truth.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        ground_truth = ground_truth.squeeze()\n",
    "        if len(ground_truth.shape) < 1:\n",
    "            ground_truth = ground_truth.unsqueeze(0)\n",
    "        ce_loss = self.criterion(model_output, ground_truth.long()) # ground_truth_multi_hot)\n",
    "        \n",
    "        \n",
    "        # this thing does not work with the old function - the old connection cost is really bad!!!\n",
    "        # cc = torch.mean(self.model.cc) * self.cc_weight # update_new_connection_cost\n",
    "        if mode == \"train\" and self.new_cc_mode == True:\n",
    "            self.cc_ci = self.model.get_cc_and_ci_loss_term()\n",
    "            \n",
    "            # print(self.cc_ci)\n",
    "            \n",
    "            # print(self.model.decent2.filter_list[1].weights[0][0])\n",
    "            \n",
    "            # get max values to understand what is going on :)\n",
    "            cc_max_decent3 = self.model.decent3.cc_max_of_layer\n",
    "            ci_max_decent3 = self.model.decent3.ci_max_of_layer\n",
    "            #cc_mean = \n",
    "            #ci_mean =                     \n",
    "\n",
    "            \n",
    "            self.log(f'{mode}_cc_max_decent3', cc_max_decent3, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_ci_max_decent3', ci_max_decent3, on_step=False, on_epoch=True)\n",
    "            #self.log(f'{mode}_ci_mean', ci_mean, on_step=False, on_epoch=True)\n",
    "            #self.log(f'{mode}_cc_mean', cc_mean, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_cc', self.cc_ci, on_step=False, on_epoch=True) # this should have a more general name, for the future!!\n",
    "            if debug_model:\n",
    "                print(\"decent note: self.cc_ci:\", self.cc_ci)\n",
    "                print(\"decent note: cc_max_decent3:\", cc_max_decent3)\n",
    "                print(\"decent note: ci_max_decent3:\", ci_max_decent3)\n",
    "                #print(\"decent note: all_ci = mean:\", all_ci)\n",
    "                #print(\"decent note: all_cc = mean\", all_cc)\n",
    "                print(\"ce_loss\", ce_loss)\n",
    "            \n",
    "            loss = ce_loss + (self.cc_ci * self.cc_weight) # make sure to set the weight in the args, also make sure to use norms that are torch not scipy!!\n",
    "        else:\n",
    "            loss = ce_loss\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "                \n",
    "        if mode == \"train\":\n",
    "            value = self.train_metrics(preds=pred_i, target=ground_truth)\n",
    "            self.log_dict(value, on_step=False, on_epoch=True)\n",
    "                \n",
    "        elif mode == \"val\":\n",
    "            value = self.val_metrics(preds=pred_i, target=ground_truth)\n",
    "            self.log_dict(value, on_step=False, on_epoch=True)\n",
    "                \n",
    "        else:\n",
    "            value = self.test_metrics(preds=pred_i, target=ground_truth)\n",
    "            self.log_dict(value, on_step=False, on_epoch=True)\n",
    "            \n",
    "            \n",
    "            self.cm.update(preds=pred_i, target=ground_truth) # prediction (class)\n",
    "            self.pr_curve.update(preds=model_output, target=ground_truth) # probability\n",
    "            self.roc_auc.update(preds=model_output, target=ground_truth) # probability\n",
    "                        \n",
    "        self.log(f'{mode}_ce_loss', ce_loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        \n",
    "        if False: # debug_model:\n",
    "            print(\"loss\", loss)\n",
    "            print(\"ce_loss\", ce_loss)\n",
    "            print(\"mo\", model_output)\n",
    "            print(\"gt\", ground_truth)\n",
    "            print(\"mo\", self.mo)\n",
    "            print(\"gt\", self.gt)\n",
    "            \n",
    "        if torch.isnan(loss).any():\n",
    "            print(\"DECENT WARNING: Loss contains NaN value(s).\")\n",
    "        \n",
    "        # ce loss + connection cost term\n",
    "        \n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2906-61b6-4f3a-b2fa-7aeeaa12e7e1",
   "metadata": {},
   "source": [
    "## run dev routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_path: examples/example_results\\lightning_logs\\trying_optimiser\\version_4/checkpoints/mu_epoch=5-val_f1_macro=0.46-unpruned=5676.ckpt\n",
      "Create new model\n",
      "DECENT INFO: dimensions are entry, decent1, decent2, decent3, decent1x1 == out [1, 8, 16, 32, 4]\n",
      "[10.]\n",
      "[5.]\n",
      "init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                           | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model         | DecentNet                      | 7.7 K \n",
      "1 | criterion     | CrossEntropyLoss               | 0     \n",
      "2 | train_metrics | MetricCollection               | 0     \n",
      "3 | val_metrics   | MetricCollection               | 0     \n",
      "4 | test_metrics  | MetricCollection               | 0     \n",
      "5 | cm            | MulticlassConfusionMatrix      | 0     \n",
      "6 | roc_auc       | MulticlassROC                  | 0     \n",
      "7 | pr_curve      | MulticlassPrecisionRecallCurve | 0     \n",
      "-----------------------------------------------------------------\n",
      "6.0 K     Trainable params\n",
      "1.7 K     Non-trainable params\n",
      "7.7 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: configure_optimizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1153a6bd12c42938fd277790c65ee48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_start d1: tensor(0.3124, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(0.0553, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(0.3124, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.0553, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(0.4723, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.0327, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(0.6028, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.0151, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(0.7250, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.0018, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(0.8407, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.0088, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(0.9548, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.0357, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.0570, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.0596, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.1514, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.0813, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.2395, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(-0.1009, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 0\n",
      "e_start d1: tensor(1.2395, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(-0.1009, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.2395, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.1009, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.3216, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.1187, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.3961, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.1351, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.4629, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.1499, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.5235, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.1632, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.5779, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.1753, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.6271, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.1860, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.6711, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.1955, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.7106, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(-0.2040, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 1\n",
      "e_start d1: tensor(1.7106, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(-0.2040, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.7106, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.2040, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.7467, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.2117, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.7790, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.2186, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.8082, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.2245, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.8345, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.2298, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.8584, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.2346, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.8798, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.2390, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.8988, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(-0.2428, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9156, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(-0.2464, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 2\n",
      "DECENT NOTE: save model 2\n",
      "e_start d1: tensor(1.9156, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(-0.2464, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: update model 3\n",
      "decent2-decent2-decent2-decent2-decent2-\n",
      "DECENT NOTE: prune channels ...\n",
      "decent3-decent3-decent3-decent3-decent3-\n",
      "DECENT NOTE: prune channels ...\n",
      "decent1x1-decent1x1-decent1x1-decent1x1-decent1x1-\n",
      "DECENT NOTE: prune channels ...\n",
      "t_step d1: tensor(1.9156, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9306, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9442, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9564, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9673, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9772, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9861, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9941, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:615: UserWarning: Checkpoint directory examples/example_results\\lightning_logs\\trying_optimiser\\version_32\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                           | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model         | DecentNet                      | 7.3 K \n",
      "1 | criterion     | CrossEntropyLoss               | 0     \n",
      "2 | train_metrics | MetricCollection               | 0     \n",
      "3 | val_metrics   | MetricCollection               | 0     \n",
      "4 | test_metrics  | MetricCollection               | 0     \n",
      "5 | cm            | MulticlassConfusionMatrix      | 0     \n",
      "6 | roc_auc       | MulticlassROC                  | 0     \n",
      "7 | pr_curve      | MulticlassPrecisionRecallCurve | 0     \n",
      "-----------------------------------------------------------------\n",
      "5.7 K     Trainable params\n",
      "1.6 K     Non-trainable params\n",
      "7.3 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: configure_optimizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764c9824c0d34979a43520c8738c3a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_start d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 0\n",
      "e_start d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 1\n",
      "e_start d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 2\n",
      "DECENT NOTE: save model 2\n",
      "e_start d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: update model 3\n",
      "decent2-decent2-decent2-decent2-decent2-\n",
      "DECENT NOTE: prune channels ...\n",
      "decent3-decent3-decent3-decent3-decent3-\n",
      "DECENT NOTE: prune channels ...\n",
      "decent1x1-decent1x1-decent1x1-decent1x1-decent1x1-\n",
      "DECENT NOTE: prune channels ...\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0014, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0010, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0007, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0004, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(2.0001, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                           | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model         | DecentNet                      | 6.9 K \n",
      "1 | criterion     | CrossEntropyLoss               | 0     \n",
      "2 | train_metrics | MetricCollection               | 0     \n",
      "3 | val_metrics   | MetricCollection               | 0     \n",
      "4 | test_metrics  | MetricCollection               | 0     \n",
      "5 | cm            | MulticlassConfusionMatrix      | 0     \n",
      "6 | roc_auc       | MulticlassROC                  | 0     \n",
      "7 | pr_curve      | MulticlassPrecisionRecallCurve | 0     \n",
      "-----------------------------------------------------------------\n",
      "5.4 K     Trainable params\n",
      "1.5 K     Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: configure_optimizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c88ae7c11194983a4c5ab72cfc98d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_start d1: tensor(2.0001, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0001, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0001, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9996, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9994, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9998, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9992, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9998, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9992, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9998, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9992, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9998, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9993, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9998, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(0.9994, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 0\n",
      "e_start d1: tensor(1.9998, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(0.9994, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9998, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9994, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9998, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9995, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9995, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9996, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9996, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9997, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0001, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0002, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(2.0002, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(1.0005, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 1\n",
      "e_start d1: tensor(2.0002, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(1.0005, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0002, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0005, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0002, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0008, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0003, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0010, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0003, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0011, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0004, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0013, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0004, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0015, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0005, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0017, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0005, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0018, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(2.0004, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(1.0017, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 2\n",
      "DECENT NOTE: save model 2\n",
      "e_start d1: tensor(2.0004, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(1.0017, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: update model 3\n",
      "decent2-decent2-decent2-decent2-decent2-\n",
      "DECENT NOTE: prune channels ...\n",
      "decent3-decent3-decent3-decent3-decent3-\n",
      "DECENT NOTE: prune channels ...\n",
      "decent1x1-decent1x1-decent1x1-decent1x1-decent1x1-\n",
      "DECENT NOTE: prune channels ...\n",
      "t_step d1: tensor(2.0004, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0004, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0003, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0002, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0002, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(2.0001, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9998, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9995, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9993, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                           | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model         | DecentNet                      | 6.5 K \n",
      "1 | criterion     | CrossEntropyLoss               | 0     \n",
      "2 | train_metrics | MetricCollection               | 0     \n",
      "3 | val_metrics   | MetricCollection               | 0     \n",
      "4 | test_metrics  | MetricCollection               | 0     \n",
      "5 | cm            | MulticlassConfusionMatrix      | 0     \n",
      "6 | roc_auc       | MulticlassROC                  | 0     \n",
      "7 | pr_curve      | MulticlassPrecisionRecallCurve | 0     \n",
      "-----------------------------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.5 K     Total params\n",
      "0.026     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: configure_optimizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5e454a209241299e2160337447b6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_start d1: tensor(1.9993, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9993, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9993, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9990, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9993, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9988, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9986, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9986, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9982, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9985, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9979, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9984, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9980, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9983, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9979, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9982, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(0.9977, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 0\n",
      "e_start d1: tensor(1.9982, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(0.9977, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9982, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9977, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9982, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9976, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9979, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9972, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9977, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9971, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9975, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9970, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9973, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9968, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9971, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9967, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9969, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9964, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9966, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(0.9962, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 1\n",
      "e_start d1: tensor(1.9966, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(0.9962, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9966, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9962, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9960, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9963, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9953, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9960, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9948, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9957, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9943, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9956, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9939, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9956, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9935, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9956, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9932, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9958, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9929, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(0.9958, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 2\n",
      "DECENT NOTE: save model 2\n",
      "e_start d1: tensor(1.9929, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(0.9958, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: update model 3\n",
      "decent2-decent2-decent2-decent2-decent2-\n",
      "DECENT NOTE: prune channels ...\n",
      "decent3-decent3-decent3-decent3-decent3-\n",
      "DECENT NOTE: prune channels ...\n",
      "decent1x1-decent1x1-decent1x1-decent1x1-decent1x1-\n",
      "DECENT NOTE: prune channels ...\n",
      "t_step d1: tensor(1.9929, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9924, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9920, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9917, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9914, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9912, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9908, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9904, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9900, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                           | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model         | DecentNet                      | 6.2 K \n",
      "1 | criterion     | CrossEntropyLoss               | 0     \n",
      "2 | train_metrics | MetricCollection               | 0     \n",
      "3 | val_metrics   | MetricCollection               | 0     \n",
      "4 | test_metrics  | MetricCollection               | 0     \n",
      "5 | cm            | MulticlassConfusionMatrix      | 0     \n",
      "6 | roc_auc       | MulticlassROC                  | 0     \n",
      "7 | pr_curve      | MulticlassPrecisionRecallCurve | 0     \n",
      "-----------------------------------------------------------------\n",
      "4.8 K     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: configure_optimizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0644187055cc46b298e9ab57de661283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_start d1: tensor(1.9900, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9900, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9899, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0001, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9898, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0003, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9897, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0004, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9894, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1.0000, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9893, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9997, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9891, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9995, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9888, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9993, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9886, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(0.9991, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 0\n",
      "e_start d1: tensor(1.9886, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(0.9991, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9886, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9991, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9883, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9989, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9879, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9988, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9876, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9987, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9874, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9986, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9872, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9986, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9869, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9986, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9867, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9986, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9864, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(0.9988, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 1\n",
      "e_start d1: tensor(1.9864, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(0.9988, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9864, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9988, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9862, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9989, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9861, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9990, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9858, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9992, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9855, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9994, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9853, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9994, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9851, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9995, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9850, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(0.9995, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9848, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 2\n",
      "DECENT NOTE: save model 2\n",
      "e_start d1: tensor(1.9848, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_start d2: tensor(0.9999, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: update model 3\n",
      "decent2-decent2-decent2-decent2-decent2-\n",
      "DECENT NOTE: prune channels ...\n",
      "decent3-decent3-decent3-decent3-decent3-\n",
      "DECENT NOTE: prune channels ...\n",
      "\n",
      "DECENT INFO at random intervals\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "torch.Size([1, 10, 3, 3])\n",
      "torch.Size([1, 10, 3, 3])\n",
      "\n",
      "DECENT INFO at random intervals\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "torch.Size([1, 11, 3, 3])\n",
      "torch.Size([1, 11, 3, 3])\n",
      "decent1x1-decent1x1-decent1x1-decent1x1-decent1x1-\n",
      "DECENT NOTE: prune channels ...\n",
      "t_step d1: tensor(1.9848, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9838, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9826, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9820, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9812, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9804, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9798, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d1: tensor(1.9792, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "t_step d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_end d1: tensor(1.9786, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "e_end d2: tensor(1., device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "DECENT NOTE: callback on_train_epoch_end 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7dcc1150ff4884b1eb43c97640d165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: test_step 0\n",
      "DECENT NOTE: test_step 1\n"
     ]
    }
   ],
   "source": [
    "train_kwargs[\"xai_done\"] = False\n",
    "\n",
    "if train_kwargs[\"train_size\"] > 0:\n",
    "    \n",
    "    # =============================================================================\n",
    "    # train model and run test/xAI routine\n",
    "\n",
    "    # logger - save logs in \"examples/example_results/lightning_logs\"\n",
    "    # light - DecentLightning model\n",
    "    # trainer - pl.Trainer\n",
    "    # trainer.fit\n",
    "    # explainer - pl.Trainer\n",
    "    # explainer.test\n",
    "    # =============================================================================\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "\n",
    "    # THE LOGGER\n",
    "    logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"])\n",
    "    \n",
    "    ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "\n",
    "    # THE LIGHTNING MODEL\n",
    "    # Initialize the LightningModule\n",
    "    light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir, ckpt_path=ckpt_path)\n",
    "\n",
    "    # THE LIGHTNING TRAINER (for training)\n",
    "    \n",
    "    for t in range(5):\n",
    "    \n",
    "        trainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                             accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                             devices=[0],\n",
    "                             # inference_mode=False, # do grad manually\n",
    "                             log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                             logger=logger,\n",
    "                             check_val_every_n_epoch=1,\n",
    "                             max_epochs=train_kwargs[\"epochs\"],\n",
    "                             callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1_macro\",\n",
    "                                                       filename='mf_{epoch}-{val_f1_macro:.2f}-{unpruned:.0f}'), # monitor fscore\n",
    "                                        DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                                        filename='mu_{epoch}-{val_f1_macro:.2f}-{unpruned:.0f}'), # monitor unpruned\n",
    "                                        #OptimiserUpdateCheckpoint(),\n",
    "                                        LearningRateMonitor(\"epoch\")])\n",
    "\n",
    "        trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "        trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "        # THE TRAIN-RUN\n",
    "        # Train the model using a Trainer\n",
    "        trainer.fit(light, dataloader.train_dataloader, dataloader.val_dataloader)\n",
    "    \n",
    "\n",
    "    # THE LIGHTNING TRAINER (for testing)\n",
    "    # we want the grad to work in test, hence: inference_mode=False\n",
    "    explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"], # is this also wrong?? where is the checkpoint??\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         logger=logger,\n",
    "                         inference_mode=False)\n",
    "\n",
    "    # THE TEST-RUN\n",
    "    # including test\n",
    "    test_result = explainer.test(light, dataloader.xai_dataloader, verbose=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    train_kwargs[\"xai_done\"] = True\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf49c2-e5de-4d64-9f01-62687b71b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db1705-c20f-4154-92de-a903700c4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(DecentNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66262489-1c4a-439e-9673-32a40c5c52f1",
   "metadata": {},
   "source": [
    "## run test routine ****************************\n",
    "\n",
    "we need this with the OCTA-500 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f7aa2-96ed-4518-b129-5751bd5e39a8",
   "metadata": {},
   "source": [
    "torch.load(ckpt_path)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e1fef-8dab-44fd-908d-e9f2a901dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_kwargs[\"load_mode\"] and not train_kwargs[\"xai_done\"]:\n",
    "\n",
    "    # =============================================================================\n",
    "    # load model and run test/xAI routine\n",
    "\n",
    "    # logger - save logs in \"dumpster\"\n",
    "    # light - DecentLightning model\n",
    "    # explainer - pl.Trainer\n",
    "    # explainer.test\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"DECENT INFO: be aware, that you have to manually check, whether every output node has an input. otherwise an error may be triggered by cuda\")\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "\n",
    "    # train_kwargs[\"load_ckpt_file\"] = \"version_7/checkpoints/epoch=0-val_f1=0.62-unpruned=1560.ckpt\"\n",
    "    \n",
    "    # Check whether pretrained model exists. If yes, load it.\n",
    "    # ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\\debug_oct_no_fc\", 'version_13', 'checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt'])\n",
    "    ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "    print(\"DECENT INFO: You are using checkpoint file: \", ckpt_path)\n",
    "    \n",
    "    # ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "\n",
    "    print(train_kwargs[\"exp_name\"])\n",
    "    \n",
    "    print(ckpt_path)\n",
    "\n",
    "    \n",
    "    if os.path.isfile(ckpt_path):\n",
    "\n",
    "        # tmp = +\"_xAI\"\n",
    "        \n",
    "        # THE LOGGER\n",
    "        logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"]+\"_xAI\") # the xAI routine for an experiment\n",
    "        # logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "        \n",
    "        print(\"logdir\", logger.log_dir)\n",
    "        \n",
    "        # THELIGHTNING MODEL\n",
    "        # load from checkpoint doesn't work, since our architecture is 'messed up' through pruning\n",
    "        # light = DecentLightning.load_from_checkpoint(state_dict, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n",
    "        # use this line instead:\n",
    "        light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir, ckpt_path=ckpt_path)\n",
    "\n",
    "        # THE LIGHTNING TRAINER (for testing)\n",
    "        # we want the grad to work in test, hence: inference_mode=False\n",
    "        explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                             accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                             #devices=[0], # why is this not on??\n",
    "                             logger=logger,\n",
    "                             inference_mode=False)\n",
    "\n",
    "        # THE TEST-RUN\n",
    "        # only test\n",
    "        test_result = explainer.test(light, dataloader.xai_dataloader, verbose=False)\n",
    "\n",
    "    else:\n",
    "        print('DECENT ERROR: not a file - may have been resetted in dev routine, check the load_ckpt_file, set dev routine to False and run everything')\n",
    "\n",
    "    \n",
    "print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696de4a-76ea-4b95-9d11-e1cf8a733a42",
   "metadata": {},
   "source": [
    "# random nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "light.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in light.model.decent3.filter_list:\n",
    "    print(a.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2797f-8a72-46db-86ae-41868e90828e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617dcfd1-2827-4b69-8b31-fe8cd1f677ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(range(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f2c63-9855-4e48-aec5-5d4b1a971392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(value) \n",
    "        #print(img)\n",
    "        #print(ground_truth)\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "                #print(\"loss\", loss)\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8261ef-19a3-4ec3-b621-11234709efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "            try:\n",
    "                ta = self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "                tf = self.train_f1(preds=pred_i, target=ground_truth) \n",
    "                tp = self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            except Exception as e:\n",
    "                print(\"DECENT ERROR: we are experiencing this CUDA ERROR most likely, because our decent1x1 has too little filters.\")\n",
    "                print(\"We need the same number as classes. It can happen, that all in-connections to a filter in decent1x1 got pruned and hence it is gone.\")\n",
    "                print(\"preds\", pred_i)\n",
    "                print(\"target\", ground_truth)\n",
    "                print(e)\n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"train info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", ta)\n",
    "                print(\"f\", tf)\n",
    "                print(\"p\", tp)\n",
    "                print(\"l\", loss)\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            va = self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            vf = self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            vp = self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"val info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", va)\n",
    "                print(\"f\", vf)\n",
    "                print(\"p\", vp)\n",
    "                print(\"l\", loss)\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            print(pred_i)\n",
    "            print(ground_truth)\n",
    "            ta = self.test_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            tf = self.test_f1(preds=pred_i, target=ground_truth) \n",
    "            tp = self.test_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.test_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.test_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.test_prec, on_step=False, on_epoch=True)\n",
    "            \"\"\"\n",
    "        \n",
    "        \n",
    "            try:\n",
    "            pass # print('pred i', pred_i.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics pred\")\n",
    "            print(e)\n",
    "        try:\n",
    "            pass # print('gt', ground_truth.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics gt\")\n",
    "            print(e)\n",
    "        \n",
    "\n",
    "        #print('self mo', self.mo)\n",
    "        #print('self gt', self.gt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"gt\", ground_truth)\n",
    "        print(\"gt shape\", ground_truth.shape)\n",
    "        print(\"gt type\", ground_truth.type())\n",
    "        print(torch.zeros(ground_truth.size(0), self.n_classes))\n",
    "        \n",
    "        if len(ground_truth.shape) < 2:\n",
    "            ground_truth_tmp_tmp = ground_truth.unsqueeze(1)\n",
    "        else:\n",
    "            ground_truth = ground_truth.transpose(1, 0)\n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth_tmp.size(0), self.n_classes).scatter_(1, ground_truth_tmp.to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
