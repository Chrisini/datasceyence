{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ùîªùïñùïîùïñùïüùï•‚Ñïùïñùï•: ùïïùïöùï§ùïñùïüùï•ùïíùïüùïòùïùùïñùïï ùïüùïñùï•\n",
    "\n",
    "Goal: create a sparse and modular ConvNet\n",
    "\n",
    "Todos: \n",
    "* [ ] delete node (filter) if either no input or no output edges\n",
    "* [ ] AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
    "* [ ] cuda error, if one of the decent1x1 has no kernels left - we need at least one input for each 1x1 filter\n",
    "* [ ] can we keep training if filter gets removed (e.g. at reloading model)\n",
    "* [ ] need some working filter removing in general - only at reload rn\n",
    "\n",
    "\n",
    "Notes:\n",
    "* additionally needed: position, activated channels, connection between channels\n",
    "* within this layer, a whole filter can be deactivated\n",
    "* within a filter, single channels can be deactivated\n",
    "* within this layer, filters can be swapped\n",
    "* the 'value' in the csv file is random if the CI metric is 'random'\n",
    "     \n",
    "* pruning actually doesn't work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf\n",
    "\n",
    "pip install:\n",
    "* pytorch_lightning\n",
    "\n",
    "preprocessing possible:\n",
    "* flatten layers\n",
    "* denoise\n",
    "* crop background\n",
    "\n",
    "\n",
    "warnings:\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73a26-f239-466f-901d-559d192f61de",
   "metadata": {},
   "source": [
    "![uml of code](examples/example_vis/uml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba244e3-3e6a-47e7-aa4b-a22df6054b8a",
   "metadata": {},
   "source": [
    "# conventions\n",
    "\n",
    "* entry image: entry_id5_0_0_0_mo3_gt2.png\n",
    "* hidden layer: hid_id5_3_8_2.png\n",
    "* last layer (global pooling - connected to class n): pool_2_3_4_gp2.png\n",
    "* activated image: cam_id5_mo3_gt2.png\n",
    "* activated image gray: camgray_id5_mo3_gt2.png\n",
    "\n",
    "\n",
    "* circle in: in_2_3_4_ep65.png\n",
    "* circle out: out_2_3_4_ep65.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# sys\n",
    "# =============================================================================\n",
    "import sys \n",
    "sys.path.insert(0, \"helper\")\n",
    "# =============================================================================\n",
    "# alphabetic order misc\n",
    "# =============================================================================\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "# =============================================================================\n",
    "# torch\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torchvision\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "# from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "# =============================================================================\n",
    "# datasceyence\n",
    "# =============================================================================\n",
    "from helper.model.decentnet import DecentNet\n",
    "from helper.visualisation import feature_map\n",
    "from helper.data.mnist import DataMNIST\n",
    "from helper.data.retinamnist import DataRetinaMNIST\n",
    "from helper.data.octmnist import DataOCTMNIST\n",
    "from helper.data.octa500 import DataOCTA500\n",
    "from helper.data.organmnist3D import DataOrganMNIST3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397b450-c5c6-4da1-b630-7bf80e46891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch 2.0.0 == False\n",
      "tl 2.1.0 == False\n"
     ]
    }
   ],
   "source": [
    "seed = 1997 # was 19 before\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "debug_model = False\n",
    "\n",
    "print('torch 2.0.0 ==', torch.__version__=='2.0.0')\n",
    "print('tl 2.1.0 ==', pl.__version__=='2.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b0be-245d-49c0-88b4-d94167f7da90",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97670e82-0d27-4b7f-91df-874acf9974a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'result_path': 'examples/example_results', 'exp_name': 'debug_octmnist_no_fc', 'load_ckpt_file': '', 'epochs': 1, 'img_size': 28, 'batch_size': 2, 'log_every_n_steps': 4, 'device': 'cuda', 'num_workers': 0, 'train_size': 8, 'val_size': 8, 'test_size': 8}\n",
      "model kwargs {'in_channels': 1, 'n_classes': None, 'out_dim': [1, 4, 4, 4], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 10, 'cc_metric': 'l2', 'ci_metric': 'l2', 'cm_metric': 'not implemented yet', 'update_every_nth_epoch': 1, 'pretrain_epochs': 1, 'prune_keep': 0.7, 'prune_keep_total': 0.4}\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {\n",
    "    'in_channels' : 1, # not in use yet\n",
    "    'n_classes': None, # filled in the dataset\n",
    "    'out_dim' :  [1, 4, 4, 4], # [1, 8, 16, 32], #[1, 16, 24, 32] # entry, decent1, decent2, decent3\n",
    "    'grid_size' : 18*18,\n",
    "    'criterion': torch.nn.CrossEntropyLoss(),# torch.nn.BCEWithLogitsLoss(),\n",
    "    'optimizer': \"sgd\", # sgd adamw\n",
    "    'base_lr': 0.001,\n",
    "    'min_lr' : 0.00001,\n",
    "    'momentum' : 0.9,\n",
    "    'lr_update' : 100,\n",
    "    # decentnet\n",
    "    'cc_weight': 10,\n",
    "    'cc_metric' : 'l2', # connection cost metric (for loss) - distance metric\n",
    "    'ci_metric' : 'l2', # channel importance metric (for pruning)\n",
    "    'cm_metric' : 'not implemented yet', # 'count', # crossing minimisation \n",
    "    'update_every_nth_epoch' : 1, # 5\n",
    "    'pretrain_epochs' : 1, # 20\n",
    "    'prune_keep' : 0.7, # 0.97, # in each epoch\n",
    "    'prune_keep_total' : 0.4, # this number is not exact, depends on the prune_keep value\n",
    "}\n",
    "\n",
    "train_kwargs = {\n",
    "    'result_path': \"examples/example_results\", # \"example_results/lightning_logs\", # not in use??\n",
    "    'exp_name': \"debug_octmnist_no_fc\", # must include dataset name, otherwise mnist is used\n",
    "    'load_ckpt_file' : '', # \"version_0/checkpoints/epoch=94-unpruned=1600-val_f1=0.67.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "    'epochs': 1, # including the pretrain epochs - no adding up\n",
    "    'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ... # MNIST/MedMNIST 28 √ó 28 Pixel\n",
    "    'batch_size': 2, # 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "    'log_every_n_steps' : 4, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "    'device': \"cuda\",\n",
    "    'num_workers' : 0, # 18, # 18 for computer, 0 for laptop\n",
    "    'train_size' : (2 * 4), # total or percentage (batch size * forward passes per epoch)\n",
    "    'val_size' : (2 * 4), # total or percentage (batch size * forward passes per epoch)\n",
    "    'test_size' : 8, # total or percentage - 0 for all\n",
    "}\n",
    "\n",
    "print(\"train kwargs\", train_kwargs)\n",
    "print(\"model kwargs\", model_kwargs)\n",
    "\n",
    "kwargs = {'train_kwargs':train_kwargs, 'model_kwargs':model_kwargs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ee3d0-9c79-4917-a355-093f1daf4855",
   "metadata": {},
   "source": [
    "## check the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d5ff36-c015-4bd6-8b12-c4d0e3b64b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "breaking = 6000*model_kwargs['prune_keep_total']\n",
    "weights = 6000 # this value is an estimate for a model [1, 8, 16, 32]\n",
    "# 'unpruned' is the logger variable for the value\n",
    "\n",
    "for i in range(train_kwargs['epochs']):\n",
    "    \n",
    "    if (weights < breaking): # weights*model_kwargs['prune_keep']\n",
    "        print(\"stop:\", breaking)\n",
    "        print('you need at least this many epochs:', i)\n",
    "        print('you currently have this many epochs:', train_kwargs['epochs'])\n",
    "        print(\"recommended to add 2*update_every_nth_epoch\")\n",
    "        break\n",
    "    \n",
    "    # not sure whether -1 is correct, have to check\n",
    "    if i > model_kwargs['pretrain_epochs'] and ((i-1)%model_kwargs['update_every_nth_epoch'] == 0):\n",
    "        weights = int(weights*model_kwargs['prune_keep'])\n",
    "    \n",
    "        print(i, weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b3283-2825-4f35-9716-78ccf5bb8b1c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "* the dataset name needs to be part of the experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "python_class : OCTMNIST\n",
      "description : The OCTMNIST is based on a prior dataset of 109,309 valid optical coherence tomography (OCT) images for retinal diseases. The dataset is comprised of 4 diagnosis categories, leading to a multi-class classification task. We split the source training set with a ratio of 9:1 into training and validation set, and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384‚àí1,536)√ó(277‚àí512). We center-crop the images and resize them into 1√ó28√ó28.\n",
      "url : https://zenodo.org/record/6496656/files/octmnist.npz?download=1\n",
      "MD5 : c68d92d5b585d8d81f7112f81e2d0842\n",
      "task : multi-class\n",
      "label : {'0': 'choroidal neovascularization', '1': 'diabetic macular edema', '2': 'drusen', '3': 'normal'}\n",
      "n_channels : 1\n",
      "n_samples : {'train': 97477, 'val': 10832, 'test': 1000}\n",
      "license : CC BY 4.0\n",
      "n_classes: 4\n"
     ]
    }
   ],
   "source": [
    "if 'octmnist' in train_kwargs['exp_name']:\n",
    "    # OCTMINST\n",
    "    data = DataOCTMNIST(train_kwargs, model_kwargs)   \n",
    "elif 'retinamnist' in train_kwargs['exp_name']:\n",
    "    # RetinaMNIST\n",
    "    data = DataRetinaMNIST(train_kwargs, model_kwargs)\n",
    "elif 'octa500' in train_kwargs['exp_name']:\n",
    "    # OCTA-500\n",
    "    data = DataOCT500(train_kwargs, model_kwargs)\n",
    "elif '3d' in train_kwargs['exp_name']:\n",
    "    data = DataOrganMNIST3D(train_kwargs, model_kwargs)\n",
    "else:\n",
    "    # MNIST\n",
    "    data = DataMNIST(train_kwargs, model_kwargs)\n",
    "\n",
    "assert model_kwargs['n_classes'] != None, \"DECENT ERROR: make sure you set the n_classes with the dataset\"  \n",
    "print(\"n_classes:\", model_kwargs['n_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X (Datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # an object with image representations and their positions\n",
    "    # amout of channels need to have same length as m and n lists\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        self.data = data # list of tensors (image representations)\n",
    "        self.ms_x = ms_x # list of integers (m position of each image representation)\n",
    "        self.ns_x = ns_x # list of integers (n position of each image representation)\n",
    "                \n",
    "    def setter(self, data, ms_x, ns_x):\n",
    "        self.data = data\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        \n",
    "    def getter(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint\n",
    "        # Save a checkpoint at the end of the training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "            print(\"DECENT NOTE: callback on_train_epoch_end\", monitor_candidates[\"epoch\"].item())\n",
    "            if monitor_candidates[\"epoch\"] > 0:\n",
    "                if monitor_candidates[\"unpruned_state\"] != -1:\n",
    "                    print(\"DECENT NOTE: save model\", monitor_candidates[\"epoch\"].item())\n",
    "                    if self._every_n_epochs >= 1 and ((trainer.current_epoch + 1) % self._every_n_epochs) == 0:\n",
    "                        self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                    self._save_last_checkpoint(trainer, monitor_candidates)\n",
    "                    \n",
    "                    pl_module.model.get_everything(current_epoch=trainer.current_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "## LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # Lightning Module consists of functions that define the training routine\n",
    "    # train, val, test: before epoch, step, after epoch, ...\n",
    "    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n",
    "    # order for the instance methods:\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\n",
    "    # \n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, kwargs, log_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        if train_kwargs[\"load_ckpt_file\"] != '':\n",
    "            self.ckpt_path = os.path.join(log_dir, train_kwargs[\"load_ckpt_file\"])\n",
    "            if os.path.isfile(ckpt_path):\n",
    "                print(f\"Found pretrained model at {ckpt_path}, loading...\")\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir, ckpt_path=ckpt_path).to(\"cuda\")\n",
    "            else:\n",
    "                # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        else:\n",
    "            # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "            self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "            \n",
    "        # print(self.model)\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        self.pretrain_epochs = model_kwargs[\"pretrain_epochs\"]\n",
    "        \n",
    "        # needed for hparams.yaml file\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if False:\n",
    "            self.metric = { \"train_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"train_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"val_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"val_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "                       }\n",
    "        else:\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            \n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            \n",
    "            self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "\n",
    "            \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        # =============================================================================\n",
    "        # we make it possible to use model_output = self(image)\n",
    "        # =============================================================================\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # =============================================================================\n",
    "        # returns:\n",
    "        #    optimiser and lr scheduler\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimiser = optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        else:\n",
    "            optimiser = optim.SGD(self.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        # initial plot of circular layer\n",
    "        # updates model every nth epoch\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: on_train_epoch_start\", self.current_epoch)\n",
    "        \n",
    "        # plot random layer (the circular plot)\n",
    "        if self.current_epoch == 0:\n",
    "            self.model.plot_incoming_connections(current_epoch=0)\n",
    "            self.model.plot_outgoing_connections(current_epoch=0)\n",
    "\n",
    "        # update model\n",
    "         # don't update unless pretrain epochs is reached\n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch >= self.pretrain_epochs:\n",
    "            print(\"DECENT NOTE: update model\", self.current_epoch)        \n",
    "            if debug_model:\n",
    "                print(\"DECENT NOTE: before update\")\n",
    "                print(\"DECENT NOTE: print model ...\")\n",
    "                print(self.model)\n",
    "            self.model.update(current_epoch = self.current_epoch)\n",
    "            if True: \n",
    "                print(\"DECENT NOTE: after update\")\n",
    "                print(\"DECENT NOTE: print model ...\")\n",
    "                print(self.model)\n",
    "                \n",
    "            print(\"DECENT NOTE: model updated\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculates loss for a batch # 1\n",
    "        # parameters:\n",
    "        #    batch\n",
    "        #    batch id\n",
    "        # returns:\n",
    "        #    loss\n",
    "        # notes:\n",
    "        #    calling gradcam like self.gradcam(batch) is dangerous cause changes gradients\n",
    "        # =============================================================================     \n",
    "        if False: # batch_idx < 2: # print first two steps\n",
    "            print(\"DECENT NOTE: training_step\", batch_idx)\n",
    "\n",
    "        # calculate loss\n",
    "        # loss = torch.tensor(1)\n",
    "        loss = self.run_loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging # 2\n",
    "        # =============================================================================\n",
    "        if False: # batch_idx < 2:\n",
    "            print(\"DECENT NOTE: validation_step\", batch_idx)\n",
    "        \n",
    "        self.run_loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing # 3\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_validation_epoch_end\")\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # save model if next iteration model is pruned # 4 \n",
    "        # this needs to be called before callback \n",
    "        # - if internal pytorch lightning convention changes, this will stop working\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_train_epoch_end\", self.current_epoch)\n",
    "               \n",
    "        if False:\n",
    "            print(\"current epoch\")\n",
    "            print(((self.current_epoch+1) % self.update_every_nth_epoch) == 0)\n",
    "            print(self.current_epoch+1)\n",
    "            print(self.current_epoch)\n",
    "            print(self.update_every_nth_epoch)\n",
    "        \n",
    "        # numel: returns the total number of elements in the input tensor\n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.log(f'unpruned', unpruned, on_step=False, on_epoch=True) \n",
    "        \n",
    "        if ((self.current_epoch+1) % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            # if next epoch is an update, set unpruned flag            \n",
    "            self.log(f'unpruned_state', 1, on_step=False, on_epoch=True)\n",
    "            \n",
    "            # save file\n",
    "            with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "                f.write(\"\\n# parameter requires grad shape #\\n\")\n",
    "                for p in self.model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        f.write(str(p.shape))\n",
    "            \n",
    "        else:\n",
    "            # else set unpruned flag to -1, then model won't be saved\n",
    "            self.log(f'unpruned_state', -1, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.model.get_everything(current_epoch='final_test')\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging, plot gradcam\n",
    "        # =============================================================================\n",
    "        if batch_idx < 2:\n",
    "            print(\"DECENT NOTE: test_step\", batch_idx)\n",
    "\n",
    "        self.run_loss_n_metrics(batch, mode=\"test\")\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.enable_grad():\n",
    "            grad_preds = preds.requires_grad_()\n",
    "            preds2 = self.layer2(grad_preds)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # save image\n",
    "        \n",
    "        img, _ = batch # image and mask come out of this\n",
    "        \n",
    "        print(img.shape)\n",
    "        \n",
    "        # save image\n",
    "        tmp_file_name = f'entry_id{batch_idx}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        # tmp_img = self.feature_maps.squeeze()[i_map].cpu().detach().numpy()\n",
    "        tmp_img = img.squeeze().cpu().detach().numpy()\n",
    "        tmp_path = os.path.join(self.log_dir, \"img_choice\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_img)\n",
    "        \n",
    "        \n",
    "        # save mask (todo)\n",
    "        tmp_file_name = f'entry_id{batch_idx}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        dummy_mask = []\n",
    "        dummy_mask.append(np.random.randint(8, 11, 28).tolist())\n",
    "        dummy_mask.append(np.random.randint(12, 14, 28).tolist())\n",
    "        dummy_mask.append(np.random.randint(15, 18, 28).tolist())\n",
    "        dummy_mask.append(np.random.randint(19, 21, 28).tolist())\n",
    "        dummy_mask.append(np.random.randint(21, 23, 28).tolist())\n",
    "        dummy_mask = np.array(dummy_mask)\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        for o, layer in enumerate(dummy_mask):\n",
    "            plt.plot(list(range(len(layer))), layer)\n",
    "        plt.ylim(0, 28 - 1)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.axis('off')\n",
    "        # Save the plot\n",
    "        # plt.savefig('plot_without_axes.png', bbox_inches='tight', pad_inches=0)\n",
    "        # tmp_msk = dummy_msak\n",
    "        tmp_path = os.path.join(self.log_dir, \"msk_choice\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.savefig(os.path.join(tmp_path, tmp_file_name), bbox_inches='tight', pad_inches=0)\n",
    "        #plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_msk) # todo\n",
    "        \n",
    "        # save image + mask (todo)\n",
    "        tmp_file_name = f'entry_id{batch_idx}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        plt.imshow(tmp_img, cmap=\"gray\")\n",
    "        for o, layer in enumerate(dummy_mask):\n",
    "            plt.plot(list(range(len(layer))), layer)\n",
    "        tmp_path = os.path.join(self.log_dir, \"img_with_msk\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.savefig(os.path.join(tmp_path, tmp_file_name), bbox_inches='tight', pad_inches=0)\n",
    "        \n",
    "        # plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_img)\n",
    "        \n",
    "        # save feature maps of hidden layers and the layer that gets globally pooled\n",
    "        try:\n",
    "            with torch.set_grad_enabled(True): # torch.set_grad_enabled(True):\n",
    "                self.run_xai_gradcam(batch, batch_idx, mode='explain')\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: batch size has to be 1\")\n",
    "            print(e)\n",
    "            \n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            layer = self.model.decent1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            #filter_list.extend(tmp)\n",
    "            \n",
    "            layer = self.model.decent2\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent2' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent3\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent3' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent1x1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1x1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "        # get filter list            \n",
    "        filter_list = []\n",
    "        for l in self.model.decent1.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{1}\")\n",
    "        for l in self.model.decent2.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{2}\")\n",
    "        for l in self.model.decent3.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{3}\")\n",
    "        for l in self.model.decent1x1.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{4}\")\n",
    "        df = pd.DataFrame(filter_list, columns=['filter'])\n",
    "        df.to_csv(os.path.join(self.log_dir, \"all_filters.csv\"), index=False)\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_test_epoch_end\", self.current_epoch)\n",
    "        pass\n",
    "    \n",
    "    def run_xai_feature_map(self, batch, batch_idx, layer, layer_str, device='cuda'):\n",
    "        # https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5\n",
    " \n",
    "        # img, label = testset.__getitem__(0) # batch x channel x width x height, class\n",
    "\n",
    "        # img = X(img.to(device).unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        \n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        # print(img.data.shape)\n",
    "\n",
    "        # run feature map\n",
    "        # model, layer, layer_str, log_dir, device=\"cpu\"\n",
    "        fm = feature_map.DecentFeatureMap(model=self.model, layer=layer, layer_str=layer_str, log_dir=self.log_dir, device=device)\n",
    "        fm.run(tmp_img, batch_idx)\n",
    "        \n",
    "        filter_list = fm.log()\n",
    "        \n",
    "        return filter_list\n",
    "        \n",
    "        \n",
    "    \n",
    "    def run_xai_gradcam(self, batch, batch_idx, mode='explain'):\n",
    "        # =============================================================================\n",
    "        # grad cam - or just cam?? idk\n",
    "        # todo error: RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n",
    "        # BATCH SIZE HAS TO BE ONE!!!\n",
    "        # grad enable in test mode:\n",
    "        # https://github.com/Project-MONAI/MONAI/discussions/1598\n",
    "        # https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "        # =============================================================================\n",
    "    \n",
    "        img, ground_truth = batch\n",
    "\n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)]) # .requires_grad_()\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        #print(\"nooooooooooo grad, whyyyyy\")\n",
    "        #print(tmp_img1)\n",
    "        #print(img)\n",
    "\n",
    "        #print('b1', tmp_img1)\n",
    "        #print('b2', tmp_img2)\n",
    "\n",
    "        model_output = self(tmp_img1, mode)\n",
    "\n",
    "        #print('c1', tmp_img1)\n",
    "        #print('c2', tmp_img2)\n",
    "\n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "\n",
    "        # get prediction value\n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "\n",
    "        #print('d1', tmp_img1)\n",
    "\n",
    "        #print(\"mo\", model_output)\n",
    "        #print(\"max\", pred_max)\n",
    "        #print(\"backprop\", model_output[:, pred_max])\n",
    "\n",
    "        # backpropagate for gradient tracking\n",
    "        model_output[:, pred_max].backward()\n",
    "\n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        #print('e2', tmp_img2)\n",
    "\n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # draw the heatmap\n",
    "        # plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "        # fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_l2_plot[0])}_n{int(self.n_l2_plot[0])}_{str(current_epoch)}.png\"))\n",
    "        \n",
    "        tmp_path = os.path.join(self.log_dir, \"gradcam\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.imsave(os.path.join(tmp_path, \n",
    "                                f\"cam_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\"\n",
    "                               ), heatmap.detach().cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "        heatmap *= 255.0 / heatmap.max()\n",
    "        pil_heatmap = Image.fromarray(heatmap.detach().cpu().numpy().squeeze()).convert('RGB')\n",
    "       \n",
    "        tmp_path = os.path.join(self.log_dir, \"gradcam\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        pil_heatmap.save(os.path.join(tmp_path, \n",
    "                                      f\"camgray_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" \n",
    "                                     )) \n",
    "            \n",
    "    def run_loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        # =============================================================================\n",
    "        # put image through model, calculate loss and metrics\n",
    "        # use cc term that has been calculated previously\n",
    "        # =============================================================================\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # for test routine\n",
    "        self.mo = model_output.argmax(dim=1).squeeze().detach().cpu().numpy()\n",
    "        self.gt = ground_truth.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        print('self mo', self.mo)\n",
    "        print('self gt', self.gt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"gt\", ground_truth)\n",
    "        print(\"gt shape\", ground_truth.shape)\n",
    "        print(\"gt type\", ground_truth.type())\n",
    "        print(torch.zeros(ground_truth.size(0), self.n_classes))\n",
    "        \n",
    "        if len(ground_truth.shape) < 2:\n",
    "            ground_truth_tmp_tmp = ground_truth.unsqueeze(1)\n",
    "        else:\n",
    "            ground_truth = ground_truth.transpose(1, 0)\n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth_tmp.size(0), self.n_classes).scatter_(1, ground_truth_tmp.to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \"\"\"\n",
    "\n",
    "        ground_truth = ground_truth.squeeze()\n",
    "        if len(ground_truth.shape) < 1:\n",
    "            ground_truth = ground_truth.unsqueeze(0)\n",
    "        loss = self.criterion(model_output, ground_truth.long()) # ground_truth_multi_hot)\n",
    "        cc = torch.mean(self.model.cc) * self.cc_weight\n",
    "        \n",
    "        #print(\"loss\", loss)\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "        \n",
    "        try:\n",
    "            pass # print('pred i', pred_i.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics pred\")\n",
    "            print(e)\n",
    "        try:\n",
    "            pass # print('gt', ground_truth.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics gt\")\n",
    "            print(e)\n",
    "        \n",
    "        \n",
    "        if mode == \"train\":\n",
    "            try:\n",
    "                ta = self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "                tf = self.train_f1(preds=pred_i, target=ground_truth) \n",
    "                tp = self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            except Exception as e:\n",
    "                print(\"DECENT ERROR: we are experiencing this CUDA ERROR most likely, because our decent1x1 has too little filters.\")\n",
    "                print(\"We need the same number as classes. It can happen, that all in-connections to a filter in decent1x1 got pruned and hence it is gone.\")\n",
    "                print(\"preds\", pred_i)\n",
    "                print(\"target\", ground_truth)\n",
    "                print(e)\n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"train info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", ta)\n",
    "                print(\"f\", tf)\n",
    "                print(\"p\", tp)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        elif mode == \"val\":\n",
    "            va = self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            vf = self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            vp = self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"val info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", va)\n",
    "                print(\"f\", vf)\n",
    "                print(\"p\", vp)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        else:\n",
    "            print(pred_i)\n",
    "            print(ground_truth)\n",
    "            ta = self.test_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            tf = self.test_f1(preds=pred_i, target=ground_truth) \n",
    "            tp = self.test_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.test_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.test_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.test_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2906-61b6-4f3a-b2fa-7aeeaa12e7e1",
   "metadata": {},
   "source": [
    "## run dev routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT INFO: dimensions are entry, decent1, decent2, decent3, decent1x1 == out [1, 4, 4, 4, 4, 4]\n",
      "[10.]\n",
      "[5.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name       | Type                | Params\n",
      "----------------------------------------------------\n",
      "0  | model      | DecentNet           | 496   \n",
      "1  | criterion  | CrossEntropyLoss    | 0     \n",
      "2  | train_acc  | MulticlassAccuracy  | 0     \n",
      "3  | train_f1   | MulticlassF1Score   | 0     \n",
      "4  | train_prec | MulticlassPrecision | 0     \n",
      "5  | val_acc    | MulticlassAccuracy  | 0     \n",
      "6  | val_f1     | MulticlassF1Score   | 0     \n",
      "7  | val_prec   | MulticlassPrecision | 0     \n",
      "8  | test_acc   | MulticlassAccuracy  | 0     \n",
      "9  | test_f1    | MulticlassF1Score   | 0     \n",
      "10 | test_prec  | MulticlassPrecision | 0     \n",
      "----------------------------------------------------\n",
      "360       Trainable params\n",
      "136       Non-trainable params\n",
      "496       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: configure_optimizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self mo [2 1]\n",
      "self gt [3 0]\n",
      "self mo [3 3]\n",
      "self gt [3 3]\n",
      "DECENT NOTE: on_validation_epoch_end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b78d07d8f84cea9da6598a009e87c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: on_train_epoch_start 0\n",
      "self mo [3 2]\n",
      "self gt [1 0]\n",
      "self mo [0 1]\n",
      "self gt [3 3]\n",
      "self mo [1 3]\n",
      "self gt [0 3]\n",
      "self mo [2 1]\n",
      "self gt [3 0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self mo [1 3]\n",
      "self gt [3 0]\n",
      "self mo [3 1]\n",
      "self gt [3 3]\n",
      "self mo [1 3]\n",
      "self gt [0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self mo [3 3]\n",
      "self gt [3 0]\n",
      "DECENT NOTE: on_validation_epoch_end\n",
      "DECENT NOTE: on_train_epoch_end 0\n",
      "DECENT NOTE: callback on_train_epoch_end 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d096f5f9850c4c9c9d79a287a7237538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: test_step 0\n",
      "self mo 3\n",
      "self gt 3\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 4, 24, 24])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "DECENT NOTE: test_step 1\n",
      "self mo 1\n",
      "self gt 2\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([2], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 4, 24, 24])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 3\n",
      "self gt 3\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 4, 24, 24])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 3\n",
      "self gt 3\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 4, 24, 24])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 3\n",
      "self gt 0\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 4, 24, 24])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 3\n",
      "self gt 2\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([2], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 4, 24, 24])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 3\n",
      "self gt 1\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([1], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 4, 24, 24])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 3\n",
      "self gt 0\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 4, 24, 24])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "DECENT NOTE: on_test_epoch_end 0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    \n",
    "    # =============================================================================\n",
    "    # train model and run test/xAI routine\n",
    "\n",
    "    # logger - save logs in \"examples/example_results/lightning_logs\"\n",
    "    # light - DecentLightning model\n",
    "    # trainer - pl.Trainer\n",
    "    # trainer.fit\n",
    "    # explainer - pl.Trainer\n",
    "    # explainer.test\n",
    "    # =============================================================================\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "    train_kwargs[\"load_ckpt_file\"] = \"\" # needed\n",
    "\n",
    "    # THE LOGGER\n",
    "    logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"])\n",
    "\n",
    "    # THE LIGHTNING MODEL\n",
    "    # Initialize the LightningModule\n",
    "    light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "    # THE LIGHTNING TRAINER (for training)\n",
    "    trainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         # inference_mode=False, # do grad manually\n",
    "                         log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                         logger=logger,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         max_epochs=train_kwargs[\"epochs\"],\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1\",\n",
    "                                                   filename='{epoch}-{val_f1:.2f}-{unpruned:.0f}'),\n",
    "                                    DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                                    filename='{epoch}-{unpruned:.0f}-{val_f1:.2f}'),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # THE TRAIN-RUN\n",
    "    # Train the model using a Trainer\n",
    "    trainer.fit(light, data.train_dataloader, data.val_dataloader)\n",
    "    \n",
    "\n",
    "    # THE LIGHTNING TRAINER (for testing)\n",
    "    # we want the grad to work in test, hence: inference_mode=False\n",
    "    explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         logger=logger,\n",
    "                         inference_mode=False)\n",
    "\n",
    "    # THE TEST-RUN\n",
    "    # including test\n",
    "    test_result = explainer.test(light, data.xai_dataloader, verbose=False)\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66262489-1c4a-439e-9673-32a40c5c52f1",
   "metadata": {},
   "source": [
    "## run test routine ****************************\n",
    "\n",
    "we need this with the OCTA-500 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f7aa2-96ed-4518-b129-5751bd5e39a8",
   "metadata": {},
   "source": [
    "torch.load(ckpt_path)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "891e1fef-8dab-44fd-908d-e9f2a901dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples/example_results\\lightning_logs\\debug_octmnist_no_fc\\version_3/checkpoints/epoch=9-unpruned=554-val_f1=0.38.ckpt\n",
      "Found pretrained model at examples/example_results\\lightning_logs\\debug_octmnist_no_fc\\version_3/checkpoints/epoch=9-unpruned=554-val_f1=0.38.ckpt, loading...\n",
      "DECENT INFO: dimensions are entry, decent1, decent2, decent3, decent1x1 == out [1, 4, 8, 16, 4, 4]\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 5, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 5, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 1, 1])\n",
      "[10.]\n",
      "[5.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e476280b97fe49b29bf2afbd9134aed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: test_step 0\n",
      "self mo 0\n",
      "self gt 3\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 16, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "DECENT NOTE: test_step 1\n",
      "self mo 0\n",
      "self gt 2\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([2], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 16, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 3\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 16, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 3\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 16, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 0\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 16, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 2\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([2], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 16, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 2\n",
      "self gt 1\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([1], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 16, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 0\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 16, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "DECENT NOTE: on_test_epoch_end 0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "\n",
    "    # =============================================================================\n",
    "    # load model and run test/xAI routine\n",
    "\n",
    "    # logger - save logs in \"dumpster\"\n",
    "    # light - DecentLightning model\n",
    "    # explainer - pl.Trainer\n",
    "    # explainer.test\n",
    "    # =============================================================================\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "    \n",
    "    image_choice = \"examples/example_data/octa500/10001_199.bmp\"\n",
    "    mask_choice = \"examples/example_data/octa500/10001.mat\"\n",
    "    train_kwargs[\"load_ckpt_file\"] = \"version_3/checkpoints/epoch=9-unpruned=554-val_f1=0.38.ckpt\"\n",
    "    \n",
    "    # Check whether pretrained model exists. If yes, load it.\n",
    "    # ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\\debug_oct_no_fc\", 'version_13', 'checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt'])\n",
    "    ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "    print(ckpt_path)\n",
    "    \n",
    "    if os.path.isfile(ckpt_path):\n",
    "\n",
    "        # THE LOGGER\n",
    "        logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "\n",
    "        # THE LIGHTNING MODEL\n",
    "        # load from checkpoint doesn't work, since our architecture is 'messed up' through pruning\n",
    "        # light = DecentLightning.load_from_checkpoint(state_dict, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n",
    "        # use this line instead:\n",
    "        light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "        # THE LIGHTNING TRAINER (for testing)\n",
    "        # we want the grad to work in test, hence: inference_mode=False\n",
    "        explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                             accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                             #devices=[0],\n",
    "                             logger=logger,\n",
    "                             inference_mode=False)\n",
    "\n",
    "        # THE TEST-RUN\n",
    "        # only test\n",
    "        test_result = explainer.test(light, data.xai_dataloader, verbose=False)\n",
    "\n",
    "    else:\n",
    "        print('DECENT ERROR: not a dir - may have been resetted in dev routine, check the load_ckpt_file, set dev routine to False and run everything')\n",
    "        print('current load_ckpt_file is:', train_kwargs[\"load_ckpt_file\"])\n",
    "\n",
    "    \n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696de4a-76ea-4b95-9d11-e1cf8a733a42",
   "metadata": {},
   "source": [
    "# random nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecentNet(\n",
       "  (decent1): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([10.]), n_this=Parameter containing:\n",
       "      tensor([10.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([1.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([2.]), n_this=Parameter containing:\n",
       "      tensor([1.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([11.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "    )\n",
       "  )\n",
       "  (decent2): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([10.]), n_this=Parameter containing:\n",
       "      tensor([5.]))\n",
       "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([0.]))\n",
       "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([9.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
       "    )\n",
       "  )\n",
       "  (decent3): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([3.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, ns_in= 5, 0, 6, 7)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([17.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, ns_in= 5, 0, 6, 7)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([8.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, ns_in= 5, 0, 6, 7)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([0.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, ns_in= 5, 0, 6, 7)\n",
       "    )\n",
       "  )\n",
       "  (decent1x1): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 0, ns_in= 3, 14, 8, 7)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([9.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 0, ns_in= 3, 14, 8, 7)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([8.]), n_this=Parameter containing:\n",
       "      tensor([16.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 0, ns_in= 3, 14, 8, 7)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([2.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 0, ns_in= 3, 14, 8, 7)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (mish1): Mish()\n",
       "  (mish2): Mish()\n",
       "  (mish3): Mish()\n",
       "  (mish1x1): Mish()\n",
       "  (bias1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias2): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias3): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias1x1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.1190,  0.0429,  0.0431],\n",
      "          [-0.0664,  0.0787,  0.1053],\n",
      "          [-0.0977, -0.0982,  0.1126]],\n",
      "\n",
      "         [[-0.0659, -0.0519, -0.1073],\n",
      "          [ 0.1113, -0.0095, -0.0888],\n",
      "          [-0.0366,  0.0833, -0.0998]],\n",
      "\n",
      "         [[ 0.0741,  0.0949, -0.1075],\n",
      "          [ 0.1194,  0.0360, -0.0832],\n",
      "          [-0.0021, -0.1089, -0.0309]],\n",
      "\n",
      "         [[ 0.0929,  0.1053,  0.0896],\n",
      "          [-0.0775, -0.0288, -0.0589],\n",
      "          [-0.1170,  0.1031, -0.0018]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0741,  0.0982, -0.0508],\n",
      "          [-0.0468,  0.1043,  0.0617],\n",
      "          [ 0.0076, -0.0469, -0.1167]],\n",
      "\n",
      "         [[-0.0884,  0.0885,  0.0143],\n",
      "          [ 0.0522,  0.1053,  0.0667],\n",
      "          [-0.1077,  0.1058, -0.0967]],\n",
      "\n",
      "         [[ 0.1092, -0.1108,  0.0321],\n",
      "          [-0.0335, -0.0449,  0.0372],\n",
      "          [ 0.0845,  0.0582, -0.1296]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0752, -0.0972,  0.1185],\n",
      "          [ 0.1088, -0.0653, -0.0746],\n",
      "          [-0.0446,  0.0982, -0.1131]],\n",
      "\n",
      "         [[ 0.1021,  0.0929,  0.0806],\n",
      "          [ 0.0661, -0.0602, -0.1041],\n",
      "          [-0.0721,  0.0341,  0.0027]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0856, -0.0795,  0.1019],\n",
      "          [ 0.0331, -0.0970, -0.0346],\n",
      "          [ 0.0147, -0.1081,  0.1078]],\n",
      "\n",
      "         [[-0.0632,  0.0885,  0.0519],\n",
      "          [ 0.0446,  0.0603,  0.0883],\n",
      "          [ 0.1045, -0.1136, -0.1127]],\n",
      "\n",
      "         [[-0.1104,  0.0407,  0.0701],\n",
      "          [ 0.0308,  0.0674,  0.0701],\n",
      "          [ 0.0471,  0.1037, -0.1070]],\n",
      "\n",
      "         [[ 0.1136, -0.1063, -0.0283],\n",
      "          [-0.0924, -0.0823, -0.0943],\n",
      "          [-0.0356, -0.0164,  0.0964]],\n",
      "\n",
      "         [[-0.0744,  0.1075,  0.0348],\n",
      "          [ 0.0942, -0.0281,  0.0962],\n",
      "          [-0.0675, -0.1026,  0.0321]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0724,  0.0228,  0.0007],\n",
      "          [-0.0934, -0.1124, -0.1095],\n",
      "          [-0.0657, -0.1206, -0.1118]],\n",
      "\n",
      "         [[ 0.0796,  0.0808,  0.0958],\n",
      "          [ 0.0977, -0.0994,  0.0175],\n",
      "          [ 0.1028,  0.0696,  0.0615]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0934, -0.1178, -0.0305],\n",
      "          [ 0.0244, -0.0543, -0.1131],\n",
      "          [-0.0684,  0.0046, -0.1162]],\n",
      "\n",
      "         [[ 0.0201, -0.0513, -0.0630],\n",
      "          [ 0.1024,  0.1193, -0.0570],\n",
      "          [ 0.1031, -0.0644,  0.0702]],\n",
      "\n",
      "         [[-0.0220, -0.0935,  0.0648],\n",
      "          [-0.0999, -0.0978,  0.1103],\n",
      "          [ 0.0376, -0.0209, -0.1159]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0701,  0.1112, -0.0710],\n",
      "          [ 0.0735,  0.1063, -0.1122],\n",
      "          [-0.0954, -0.0462, -0.0890]],\n",
      "\n",
      "         [[ 0.0751, -0.0771, -0.0718],\n",
      "          [ 0.0834,  0.0597, -0.1112],\n",
      "          [ 0.0747, -0.0542,  0.0432]],\n",
      "\n",
      "         [[ 0.0646, -0.0238, -0.1035],\n",
      "          [ 0.1117, -0.0481, -0.1131],\n",
      "          [-0.1234,  0.0138, -0.0920]],\n",
      "\n",
      "         [[ 0.0483,  0.0583,  0.1151],\n",
      "          [-0.1037,  0.0902, -0.1013],\n",
      "          [ 0.0740,  0.0017, -0.0491]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0504,  0.0193, -0.0623],\n",
      "          [ 0.0091, -0.0651, -0.0943],\n",
      "          [ 0.0969,  0.0947,  0.1175]],\n",
      "\n",
      "         [[-0.1029,  0.0206, -0.0862],\n",
      "          [ 0.0302,  0.1208,  0.0736],\n",
      "          [ 0.0387, -0.0477,  0.1137]],\n",
      "\n",
      "         [[-0.1035,  0.1125,  0.0544],\n",
      "          [-0.1024, -0.0801,  0.0523],\n",
      "          [-0.0488,  0.0010, -0.0444]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1138,  0.0620, -0.0012],\n",
      "          [-0.0515, -0.0581,  0.1173],\n",
      "          [-0.0280, -0.0572,  0.1005]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1145, -0.0331,  0.0691],\n",
      "          [ 0.0977, -0.0113,  0.0129],\n",
      "          [-0.0987, -0.0826, -0.0967]],\n",
      "\n",
      "         [[-0.1040,  0.1117,  0.0899],\n",
      "          [-0.0533,  0.0062,  0.0584],\n",
      "          [-0.0742,  0.1089,  0.0734]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0465,  0.0962,  0.0330],\n",
      "          [ 0.0759, -0.0880,  0.0197],\n",
      "          [ 0.1229, -0.1094, -0.0022]],\n",
      "\n",
      "         [[ 0.0655,  0.0917,  0.1118],\n",
      "          [-0.1005,  0.0095,  0.0582],\n",
      "          [-0.1138,  0.0610,  0.0733]],\n",
      "\n",
      "         [[-0.0997, -0.0877,  0.0607],\n",
      "          [ 0.1121,  0.0672, -0.0815],\n",
      "          [ 0.0929, -0.0559,  0.0448]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0794,  0.1001,  0.1110],\n",
      "          [ 0.1066,  0.0078,  0.0650],\n",
      "          [ 0.0721, -0.0998, -0.0391]],\n",
      "\n",
      "         [[ 0.0500, -0.0745, -0.0893],\n",
      "          [-0.0790, -0.0187,  0.0853],\n",
      "          [-0.0678,  0.0989,  0.0739]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1160, -0.1107,  0.0546],\n",
      "          [-0.0649, -0.0938, -0.0671],\n",
      "          [ 0.0944,  0.1155, -0.0758]],\n",
      "\n",
      "         [[ 0.0936, -0.0793,  0.0725],\n",
      "          [ 0.1063,  0.0673, -0.0969],\n",
      "          [ 0.0558, -0.0134, -0.0450]],\n",
      "\n",
      "         [[-0.0458,  0.0902, -0.0967],\n",
      "          [-0.0203, -0.0275,  0.0431],\n",
      "          [-0.1032,  0.1111,  0.0845]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0953, -0.0664, -0.0538],\n",
      "          [-0.0241, -0.1034, -0.0748],\n",
      "          [ 0.1152,  0.0949, -0.1090]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1222,  0.0837, -0.0541],\n",
      "          [ 0.0493, -0.1000,  0.0830],\n",
      "          [ 0.1203, -0.0244, -0.0663]],\n",
      "\n",
      "         [[-0.1114,  0.0319,  0.1160],\n",
      "          [-0.0902, -0.0991,  0.1105],\n",
      "          [ 0.0023,  0.1117,  0.0064]],\n",
      "\n",
      "         [[ 0.0977, -0.0738,  0.1194],\n",
      "          [ 0.0371,  0.0213,  0.0625],\n",
      "          [-0.0961, -0.0834,  0.0301]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0908, -0.0470, -0.0573],\n",
      "          [ 0.1179, -0.0089,  0.0730],\n",
      "          [-0.0559, -0.0241, -0.1187]],\n",
      "\n",
      "         [[-0.0286, -0.0826, -0.1060],\n",
      "          [ 0.1109, -0.0510, -0.0664],\n",
      "          [-0.0385,  0.0939,  0.0811]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for a in light.model.decent3.filter_list:\n",
    "    print(a.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2797f-8a72-46db-86ae-41868e90828e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
