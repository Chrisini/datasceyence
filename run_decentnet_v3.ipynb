{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ùîªùïñùïîùïñùïüùï•‚Ñïùïñùï•: ùïïùïöùï§ùïñùïüùï•ùïíùïüùïòùïùùïñùïï ùïüùïñùï•\n",
    "\n",
    "Goal: create a sparse and modular ConvNet\n",
    "\n",
    "Todos: \n",
    "* [ ] delete node (filter) if either no input or no output edges\n",
    "* [ ] AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
    "* [ ] cuda error, if one of the decent1x1 has no kernels left - we need at least one input for each 1x1 filter\n",
    "* [ ] can we keep training if filter gets removed (e.g. at reloading model)\n",
    "* [ ] need some working filter removing in general - only at reload rn\n",
    "\n",
    "\n",
    "Notes:\n",
    "* additionally needed: position, activated channels, connection between channels\n",
    "* within this layer, a whole filter can be deactivated\n",
    "* within a filter, single channels can be deactivated\n",
    "* within this layer, filters can be swapped\n",
    "* the 'value' in the csv file is random if the CI metric is 'random'\n",
    "     \n",
    "* pruning actually doesn't work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf\n",
    "\n",
    "pip install:\n",
    "* pytorch_lightning\n",
    "\n",
    "preprocessing possible:\n",
    "* flatten layers\n",
    "* denoise\n",
    "* crop background\n",
    "\n",
    "\n",
    "warnings:\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73a26-f239-466f-901d-559d192f61de",
   "metadata": {},
   "source": [
    "![uml of code](examples/example_vis/uml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba244e3-3e6a-47e7-aa4b-a22df6054b8a",
   "metadata": {},
   "source": [
    "# conventions\n",
    "\n",
    "* entry image: entry_id5_0_0_0_mo3_gt2.png\n",
    "* hidden layer: hid_id5_3_8_2.png\n",
    "* last layer (global pooling - connected to class n): pool_2_3_4_gp2.png\n",
    "* activated image: cam_id5_mo3_gt2.png\n",
    "* activated image gray: camgray_id5_mo3_gt2.png\n",
    "\n",
    "\n",
    "* circle in: in_2_3_4_ep65.png\n",
    "* circle out: out_2_3_4_ep65.png\n",
    "\n",
    "id may be image id if available, else batch id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# sys\n",
    "# =============================================================================\n",
    "import sys \n",
    "sys.path.insert(0, \"helper\")\n",
    "# =============================================================================\n",
    "# alphabetic order misc\n",
    "# =============================================================================\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import scipy.io\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "# =============================================================================\n",
    "# torch\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torchvision\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "# from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "# =============================================================================\n",
    "# datasceyence\n",
    "# =============================================================================\n",
    "from helper.model.decentnet import DecentNet\n",
    "from helper.visualisation import filter_activation\n",
    "from helper.data.mnist import DataLoaderMNIST\n",
    "from helper.data.retinamnist import DataLoaderRetinaMNIST\n",
    "from helper.data.octmnist import DataLoaderOCTMNIST\n",
    "from helper.data.octa500 import DataLoaderOCTA500\n",
    "from helper.data.organmnist3D import DataLoaderOrganMNIST3D\n",
    "from data.transform.octa500_resize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397b450-c5c6-4da1-b630-7bf80e46891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch 2.0.0 == False\n",
      "tl 2.1.0 == False\n"
     ]
    }
   ],
   "source": [
    "seed = 1997 # was 19 before\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "debug_model = False\n",
    "\n",
    "print('torch 2.0.0 ==', torch.__version__=='2.0.0')\n",
    "print('tl 2.1.0 ==', pl.__version__=='2.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b0be-245d-49c0-88b4-d94167f7da90",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97670e82-0d27-4b7f-91df-874acf9974a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'input_data_csv': ['data_prep/data_octa500.csv'], 'result_path': 'examples/example_results', 'exp_name': 'first_ever_experiment', 'load_ckpt_file': 'version_0/checkpoints/epoch=9-unpruned=1155-val_f1=0.00.ckpt', 'dataset': 'octa500', 'load_mode': True, 'epochs': 10, 'img_size': 28, 'batch_size': 2, 'log_every_n_steps': 50, 'device': 'cuda', 'num_workers': 0, 'train_size': 0, 'val_size': 100, 'test_size': -1, 'octa500_id': 199, 'xai_done': False}\n",
      "model kwargs {'in_channels': 1, 'n_classes': None, 'out_dim': [1, 4, 8, 16], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 0.5, 'cc_metric': 'l2', 'ci_metric': 'l2', 'cm_metric': 'not implemented yet', 'update_every_nth_epoch': 1, 'pretrain_epochs': 1, 'prune_keep': 0.97, 'prune_keep_total': 0.4}\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {\n",
    "    'in_channels' : 1, # not in use yet\n",
    "    'n_classes': None, # filled in the dataset\n",
    "    'out_dim' :  [1, 4, 8, 16], # [1, 8, 16, 32], #[1, 16, 24, 32] # entry, decent1, decent2, decent3\n",
    "    'grid_size' : 18*18,\n",
    "    'criterion': torch.nn.CrossEntropyLoss(),# torch.nn.BCEWithLogitsLoss(),\n",
    "    'optimizer': \"sgd\", # sgd adamw\n",
    "    'base_lr': 0.001,\n",
    "    'min_lr' : 0.00001,\n",
    "    'momentum' : 0.9,\n",
    "    'lr_update' : 100,\n",
    "    # decentnet\n",
    "    'cc_weight': 0.5,\n",
    "    'cc_metric' : 'l2', # connection cost metric (for loss) - distance metric\n",
    "    'ci_metric' : 'l2', # channel importance metric (for pruning)\n",
    "    'cm_metric' : 'not implemented yet', # 'count', # crossing minimisation \n",
    "    'update_every_nth_epoch' : 1, # 5\n",
    "    'pretrain_epochs' : 1, # 20\n",
    "    'prune_keep' : 0.97, # 0.97, # in each epoch\n",
    "    'prune_keep_total' : 0.4, # this number is not exact, depends on the prune_keep value\n",
    "}\n",
    "\n",
    "train_kwargs = {\n",
    "    'input_data_csv': [\"data_prep/data_octa500.csv\"],\n",
    "    'result_path': \"examples/example_results\", # \"example_results/lightning_logs\", # not in use??\n",
    "    'exp_name': \"first_ever_experiment\", # must include dataset name, otherwise mnist is used\n",
    "    'load_ckpt_file' : \"version_0/checkpoints/epoch=9-unpruned=1155-val_f1=0.00.ckpt\", # \"version_0/checkpoints/epoch=94-unpruned=1600-val_f1=0.67.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "    'dataset' : 'octa500',\n",
    "    'load_mode' : True, # True, False\n",
    "    'epochs': 10, # including the pretrain epochs - no adding up\n",
    "    'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ... # MNIST/MedMNIST 28 √ó 28 Pixel\n",
    "    'batch_size': 2, # 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "    'log_every_n_steps' : 50, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "    'device': \"cuda\",\n",
    "    'num_workers' : 0, # 18, # 18 for seri computer, 0 or 8 for my laptop # make sure smaller than activate dataset sizes\n",
    "    'train_size' : 0, # total, none = 0, all = -1  (batch size * forward passes per epoch) # set 0 to skip training and just do testing\n",
    "    'val_size' : 100, # total, none = 0, all = -1 (batch size * forward passes per epoch) \n",
    "    'test_size' : -1, # total, none = 0, all = -1 (batch size * forward passes per epoch)\n",
    "    'octa500_id' : 200-1, # todo\n",
    "    'xai_done' : False, # DO NOT CHANGE, WILL BE CHANGED IN CODE\n",
    "}\n",
    "\n",
    "print(\"train kwargs\", train_kwargs)\n",
    "print(\"model kwargs\", model_kwargs)\n",
    "\n",
    "kwargs = {'train_kwargs':train_kwargs, 'model_kwargs':model_kwargs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ee3d0-9c79-4917-a355-093f1daf4855",
   "metadata": {},
   "source": [
    "## check the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3d5ff36-c015-4bd6-8b12-c4d0e3b64b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights that stay\n",
      "0 6000\n",
      "1 5820\n",
      "2 5645\n",
      "3 5475\n",
      "4 5310\n",
      "5 5150\n",
      "6 4995\n",
      "7 4845\n",
      "8 4699\n",
      "9 4558\n"
     ]
    }
   ],
   "source": [
    "breaking = 6000*model_kwargs['prune_keep_total']\n",
    "weights = 6000 # this value is an estimate for a model [1, 8, 16, 32]\n",
    "# 'unpruned' is the logger variable for the value\n",
    "\n",
    "print(\"weights that stay\")\n",
    "for i in range(train_kwargs['epochs']):\n",
    "    \n",
    "    if (weights < breaking): # weights*model_kwargs['prune_keep']\n",
    "        print(\"stop:\", breaking)\n",
    "        print('you need at least this many epochs:', i)\n",
    "        print('you currently have this many epochs:', train_kwargs['epochs'])\n",
    "        print(\"recommended to add 2*update_every_nth_epoch\")\n",
    "        break\n",
    "    \n",
    "    # not sure whether -1 is correct, have to check\n",
    "    if i >= model_kwargs['pretrain_epochs'] and ((i-1)%model_kwargs['update_every_nth_epoch'] == 0):\n",
    "        weights = int(weights*model_kwargs['prune_keep'])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(i, weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b3283-2825-4f35-9716-78ccf5bb8b1c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "* the dataset name needs to be part of the experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Empty DataFrame\n",
      "Columns: [img_id, img_path, msk_path, mode, lbl_disease, sex, os-od, age]\n",
      "Index: []\n",
      "0\n",
      "val\n",
      "Empty DataFrame\n",
      "Columns: [img_id, img_path, msk_path, mode, lbl_disease, sex, os-od, age]\n",
      "Index: []\n",
      "0\n",
      "test\n",
      "     img_id                                           img_path  \\\n",
      "0     10001  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "1     10004  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "2     10005  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "3     10007  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "4     10008  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "..      ...                                                ...   \n",
      "175   10288  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "176   10290  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "177   10291  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "178   10297  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "179   10300  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "\n",
      "                                              msk_path  mode  lbl_disease sex  \\\n",
      "0    C://Users/Prinzessin/projects/decentnet/datasc...  test            3   M   \n",
      "1    C://Users/Prinzessin/projects/decentnet/datasc...  test            2   M   \n",
      "2    C://Users/Prinzessin/projects/decentnet/datasc...  test            1   M   \n",
      "3    C://Users/Prinzessin/projects/decentnet/datasc...  test            3   M   \n",
      "4    C://Users/Prinzessin/projects/decentnet/datasc...  test            0   F   \n",
      "..                                                 ...   ...          ...  ..   \n",
      "175  C://Users/Prinzessin/projects/decentnet/datasc...  test            2   M   \n",
      "176  C://Users/Prinzessin/projects/decentnet/datasc...  test            3   F   \n",
      "177  C://Users/Prinzessin/projects/decentnet/datasc...  test            0   F   \n",
      "178  C://Users/Prinzessin/projects/decentnet/datasc...  test            1   M   \n",
      "179  C://Users/Prinzessin/projects/decentnet/datasc...  test            1   M   \n",
      "\n",
      "    os-od  age  \n",
      "0      OD   55  \n",
      "1      OD   53  \n",
      "2      OS   27  \n",
      "3      OD   24  \n",
      "4      OD   51  \n",
      "..    ...  ...  \n",
      "175    OD   64  \n",
      "176    OD   49  \n",
      "177    OD   29  \n",
      "178    OD   48  \n",
      "179    OD   59  \n",
      "\n",
      "[180 rows x 8 columns]\n",
      "180\n",
      "********** DECENT INFO: DataLoader infos **********\n",
      "python_class : OCTA500\n",
      "description : The OCTA500 is based on optical coherence tomography (OCT) images for retinal diseases. The dataset is comprised of 4 diagnosis categories, leading to a multi-class classification task. We split the dataset as needed based on a CSV file into training, validation and testset. Possible split for validation of the decentnet: 0:0:180\n",
      "task : multi-class\n",
      "label : {'0': 'choroidal neovascularization', '1': 'diabetic retinopathy', '2': 'amd', '3': 'normal'}\n",
      "n_channels : 1\n",
      "n_samples : {'train': 0, 'val': 0, 'test': 180}\n",
      "n_classes: 4\n"
     ]
    }
   ],
   "source": [
    "if 'octmnist' in train_kwargs['dataset']:\n",
    "    # OCTMINST\n",
    "    dataloader = DataLoaderOCTMNIST(train_kwargs, model_kwargs)   \n",
    "elif 'retinamnist' in train_kwargs['dataset']:\n",
    "    # RetinaMNIST\n",
    "    dataloader = DataLoaderRetinaMNIST(train_kwargs, model_kwargs)\n",
    "elif 'octa500' in train_kwargs['dataset']:\n",
    "    # OCTA-500\n",
    "    dataloader = DataLoaderOCTA500(train_kwargs, model_kwargs)\n",
    "elif '3d' in train_kwargs['dataset']:\n",
    "    dataloader = DataLoaderOrganMNIST3D(train_kwargs, model_kwargs)\n",
    "else:\n",
    "    print(\"select a valid dataset\")\n",
    "\n",
    "assert model_kwargs['n_classes'] != None, \"DECENT ERROR: make sure you set the n_classes with the dataset\"  \n",
    "print(\"n_classes:\", model_kwargs['n_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X (Datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # an object with image representations and their positions\n",
    "    # amout of channels need to have same length as m and n lists\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        self.data = data # list of tensors (image representations)\n",
    "        self.ms_x = ms_x # list of integers (m position of each image representation)\n",
    "        self.ns_x = ns_x # list of integers (n position of each image representation)\n",
    "                \n",
    "    def setter(self, data, ms_x, ns_x):\n",
    "        self.data = data\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        \n",
    "    def getter(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint\n",
    "        # Save a checkpoint at the end of the training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "            print(\"DECENT NOTE: callback on_train_epoch_end\", monitor_candidates[\"epoch\"].item())\n",
    "            if monitor_candidates[\"epoch\"] > 0:\n",
    "                if monitor_candidates[\"unpruned_state\"] != -1:\n",
    "                    print(\"DECENT NOTE: save model\", monitor_candidates[\"epoch\"].item())\n",
    "                    if self._every_n_epochs >= 1 and ((trainer.current_epoch + 1) % self._every_n_epochs) == 0:\n",
    "                        self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                    self._save_last_checkpoint(trainer, monitor_candidates)\n",
    "                    \n",
    "                    pl_module.model.get_everything(current_epoch=trainer.current_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "## LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # Lightning Module consists of functions that define the training routine\n",
    "    # train, val, test: before epoch, step, after epoch, ...\n",
    "    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n",
    "    # order for the instance methods:\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\n",
    "    # \n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, kwargs, log_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        if train_kwargs[\"load_mode\"]: # True, False\n",
    "            self.ckpt_path = os.path.join(log_dir, train_kwargs[\"load_ckpt_file\"])\n",
    "            if os.path.isfile(ckpt_path):\n",
    "                print(f\"Found pretrained model at {ckpt_path}, loading...\")\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir, ckpt_path=ckpt_path).to(\"cuda\")\n",
    "            else:\n",
    "                # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        else:\n",
    "            # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "            self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "            \n",
    "        # print(self.model)\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        self.pretrain_epochs = model_kwargs[\"pretrain_epochs\"]\n",
    "        self.image_size = kwargs['train_kwargs'][\"img_size\"]\n",
    "        \n",
    "        # needed for hparams.yaml file\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if False:\n",
    "            self.metric = { \"train_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"train_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"val_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"val_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "                       }\n",
    "        elif False:\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            \n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            \n",
    "            self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "        else:\n",
    "            self.train_metrics = torchmetrics.MetricCollection(\n",
    "                {\n",
    "                \"acc\": torchmetrics.classification.MulticlassAccuracy(num_classes=self.n_classes),\n",
    "                \"f1_macro\": torchmetrics.classification.MulticlassF1Score(num_classes=self.n_classes),\n",
    "                \"f1_micro\" : torchmetrics.classification.MulticlassF1Score(num_classes=self.n_classes, average='micro'),\n",
    "                \"prec\": torchmetrics.classification.MulticlassPrecision(num_classes=self.n_classes),\n",
    "                \"rec\": torchmetrics.classification.MulticlassRecall(num_classes=self.n_classes),\n",
    "                # \"cm\": torchmetrics.classification.MulticlassConfusionMatrix(num_classes=self.n_classes)\n",
    "                }, prefix=\"train_\",)\n",
    "            self.val_metrics = self.train_metrics.clone(prefix=\"val_\")\n",
    "            self.test_metrics = self.train_metrics.clone(prefix=\"test_\")\n",
    "        \n",
    "        print(\"init\")\n",
    "        #print(self.train_metrics)\n",
    "        #print(self.val_metrics)\n",
    "            \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        # =============================================================================\n",
    "        # we make it possible to use model_output = self(image)\n",
    "        # =============================================================================\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # =============================================================================\n",
    "        # returns:\n",
    "        #    optimiser and lr scheduler\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimiser = optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        else:\n",
    "            optimiser = optim.SGD(self.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        # initial plot of circular layer\n",
    "        # updates model every nth epoch\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: on_train_epoch_start\", self.current_epoch)\n",
    "        \n",
    "        # plot random layer (the circular plot)\n",
    "        if self.current_epoch == 0:\n",
    "            self.model.plot_incoming_connections(current_epoch=0)\n",
    "            self.model.plot_outgoing_connections(current_epoch=0)\n",
    "\n",
    "        # update model\n",
    "         # don't update unless pretrain epochs is reached\n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch >= self.pretrain_epochs:\n",
    "            print(\"DECENT NOTE: update model\", self.current_epoch)        \n",
    "            if debug_model:\n",
    "                print(\"DECENT NOTE: before update\")\n",
    "                print(\"DECENT NOTE: print model ...\")\n",
    "                print(self.model)\n",
    "            self.model.update(current_epoch = self.current_epoch)\n",
    "            if True: \n",
    "                print(\"DECENT NOTE: after update\")\n",
    "                print(\"DECENT NOTE: print model ...\")\n",
    "                print(self.model)\n",
    "                \n",
    "            print(\"DECENT NOTE: model updated\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculates loss for a batch # 1\n",
    "        # parameters:\n",
    "        #    batch\n",
    "        #    batch id\n",
    "        # returns:\n",
    "        #    loss\n",
    "        # notes:\n",
    "        #    calling gradcam like self.gradcam(batch) is dangerous cause changes gradients\n",
    "        # =============================================================================     \n",
    "        if False: # batch_idx < 2: # print first two steps\n",
    "            print(\"DECENT NOTE: training_step\", batch_idx)\n",
    "\n",
    "        # calculate loss\n",
    "        # loss = torch.tensor(1)\n",
    "        loss = self.run_loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging # 2\n",
    "        # =============================================================================\n",
    "        if False: # batch_idx < 2:\n",
    "            print(\"DECENT NOTE: validation_step\", batch_idx)\n",
    "        \n",
    "        self.run_loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing # 3\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_validation_epoch_end\")\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # save model if next iteration model is pruned # 4 \n",
    "        # this needs to be called before callback \n",
    "        # - if internal pytorch lightning convention changes, this will stop working\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_train_epoch_end\", self.current_epoch)\n",
    "               \n",
    "        if False:\n",
    "            print(\"current epoch\")\n",
    "            print(((self.current_epoch+1) % self.update_every_nth_epoch) == 0)\n",
    "            print(self.current_epoch+1)\n",
    "            print(self.current_epoch)\n",
    "            print(self.update_every_nth_epoch)\n",
    "        \n",
    "        # numel: returns the total number of elements in the input tensor\n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.log(f'unpruned', unpruned, on_step=False, on_epoch=True) \n",
    "        \n",
    "        if ((self.current_epoch+1) % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            # if next epoch is an update, set unpruned flag            \n",
    "            self.log(f'unpruned_state', 1, on_step=False, on_epoch=True)\n",
    "            \n",
    "            # save file\n",
    "            with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "                f.write(\"\\n# parameter requires grad shape #\\n\")\n",
    "                for p in self.model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        f.write(str(p.shape))\n",
    "            \n",
    "        else:\n",
    "            # else set unpruned flag to -1, then model won't be saved\n",
    "            self.log(f'unpruned_state', -1, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.model.get_everything(current_epoch='final_test')\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging, plot gradcam\n",
    "        # =============================================================================\n",
    "        if batch_idx < 2:\n",
    "            print(\"DECENT NOTE: test_step\", batch_idx)\n",
    "\n",
    "        # we update mo and gt here\n",
    "        self.run_loss_n_metrics(batch, mode=\"test\")\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.enable_grad():\n",
    "            grad_preds = preds.requires_grad_()\n",
    "            preds2 = self.layer2(grad_preds)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # save image\n",
    "        \n",
    "        if len(batch) == 4:\n",
    "            img, _, msks, img_id = batch\n",
    "            img_id = img_id.detach().cpu().item()\n",
    "            tmp_b4 = True\n",
    "        else:\n",
    "            img, _ = batch # image and mask come out of this\n",
    "            img_id = batch_idx\n",
    "            msks = None\n",
    "            tmp_b4 = False             \n",
    "        \n",
    "        # print(img.shape)\n",
    "        \n",
    "        # save image\n",
    "        tmp_file_name = f'entry_id{img_id}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        # tmp_img = self.feature_maps.squeeze()[i_map].cpu().detach().numpy()\n",
    "        tmp_img = img.squeeze().cpu().detach().numpy()\n",
    "        tmp_path = os.path.join(self.log_dir, \"img_choice\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_img)\n",
    "        \n",
    "        if tmp_b4:\n",
    "            msks = msks.detach().cpu().numpy().squeeze()\n",
    "            \n",
    "            tmp_msk = msks[0] # 28\n",
    "            \n",
    "            # save mask\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            for o, boundary in enumerate(tmp_msk): # skip last one\n",
    "                # plt.plot(list(range(len(layer))), layer)\n",
    "                plt.plot(boundary[:,1]-0.5, boundary[:,0])\n",
    "            plt.ylim(0, 28 - 1)\n",
    "            plt.gca().invert_yaxis()\n",
    "            #plt.axis('off')\n",
    "            # Save the plot\n",
    "            # plt.savefig('plot_without_axes.png', bbox_inches='tight', pad_inches=0)\n",
    "            tmp_file_name = f'entry_id{img_id}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "            tmp_path = os.path.join(self.log_dir, \"msk_choice\")\n",
    "            os.makedirs(tmp_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(tmp_path, tmp_file_name), bbox_inches='tight', pad_inches=0)\n",
    "            #plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_msk) # todo\n",
    "\n",
    "            # save image + mask (todo)\n",
    "            plt.imshow(tmp_img, cmap=\"gray\")\n",
    "            for o, boundary in enumerate(tmp_msk):\n",
    "                plt.plot(boundary[:,1]-0.5, boundary[:,0])\n",
    "            tmp_file_name = f'entry_id{img_id}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "            tmp_path = os.path.join(self.log_dir, \"img_with_msk\")\n",
    "            os.makedirs(tmp_path, exist_ok=True)\n",
    "            plt.savefig(os.path.join(tmp_path, tmp_file_name), bbox_inches='tight', pad_inches=0)\n",
    "            \n",
    "            # save mat file\n",
    "            for msk, msk_size in zip(msks, [28,26,24,22]):\n",
    "                tmp_mat = {'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Fri May 06 15:17:37 2022',\n",
    "                     '__version__': '1.0',\n",
    "                     '__globals__': [],\n",
    "                     'Layer': msk\n",
    "                            }\n",
    "\n",
    "                tmp_file_name = f'mat_id{img_id}_size{msk_size}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.mat'\n",
    "                tmp_path = os.path.join(self.log_dir, \"mat_transformed_choice\")\n",
    "                os.makedirs(tmp_path, exist_ok=True)\n",
    "                scipy.io.savemat(file_name=os.path.join(tmp_path, tmp_file_name), mdict=tmp_mat)\n",
    "            \n",
    "            \n",
    "        \n",
    "            # plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_img)\n",
    "        \n",
    "        # save feature maps of hidden layers and the layer that gets globally pooled\n",
    "        try:\n",
    "            with torch.set_grad_enabled(True): # torch.set_grad_enabled(True):\n",
    "                self.run_xai_gradcam(batch, batch_idx, mode='explain')\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: batch size has to be 1\")\n",
    "            print(e)\n",
    "            \n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            layer = self.model.decent1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            #filter_list.extend(tmp)\n",
    "            \n",
    "            layer = self.model.decent2\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent2' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent3\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent3' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent1x1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1x1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "        # get filter list            \n",
    "        filter_list = []\n",
    "        for l in self.model.decent1.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{1}\")\n",
    "        for l in self.model.decent2.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{2}\")\n",
    "        for l in self.model.decent3.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{3}\")\n",
    "        for l in self.model.decent1x1.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{4}\")\n",
    "        df = pd.DataFrame(filter_list, columns=['filter'])\n",
    "        df.to_csv(os.path.join(self.log_dir, \"all_filters.csv\"), index=False)\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_test_epoch_end\", self.current_epoch)\n",
    "        pass\n",
    "    \n",
    "    def run_xai_feature_map(self, batch, batch_idx, layer, layer_str, device='cuda'):\n",
    "        # https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5\n",
    " \n",
    "        # img, label = testset.__getitem__(0) # batch x channel x width x height, class\n",
    "\n",
    "        # img = X(img.to(device).unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "                    \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, msk, img_id = batch\n",
    "            img_id = img_id.detach().cpu().item()\n",
    "        else:\n",
    "            img, ground_truth = batch # image and mask come out of this\n",
    "            img_id = batch_idx\n",
    "            msk = None\n",
    "            \n",
    "        \n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        # print(img.data.shape)\n",
    "\n",
    "        # run feature map\n",
    "        # model, layer, layer_str, log_dir, device=\"cpu\"\n",
    "        fm = filter_activation.DecentFilterActivation(model=self.model, layer=layer, layer_str=layer_str, log_dir=self.log_dir, device=device)\n",
    "        fm.run(tmp_img, img_id)\n",
    "        \n",
    "        filter_list = fm.log()\n",
    "        \n",
    "        return filter_list\n",
    "        \n",
    "        \n",
    "    \n",
    "    def run_xai_gradcam(self, batch, batch_idx, mode='explain'):\n",
    "        # =============================================================================\n",
    "        # grad cam - or just cam?? idk\n",
    "        # todo error: RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n",
    "        # BATCH SIZE HAS TO BE ONE!!!\n",
    "        # grad enable in test mode:\n",
    "        # https://github.com/Project-MONAI/MONAI/discussions/1598\n",
    "        # https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "        # =============================================================================\n",
    "    \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, msk, img_id = batch\n",
    "            img_id = img_id.detach().cpu().item()\n",
    "        else:\n",
    "            img, ground_truth = batch # image and mask come out of this\n",
    "            img_id = batch_idx\n",
    "            msk = None\n",
    "\n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)]) # .requires_grad_()\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        #print(\"nooooooooooo grad, whyyyyy\")\n",
    "        #print(tmp_img1)\n",
    "        #print(img)\n",
    "\n",
    "        #print('b1', tmp_img1)\n",
    "        #print('b2', tmp_img2)\n",
    "\n",
    "        model_output = self(tmp_img1, mode)\n",
    "\n",
    "        #print('c1', tmp_img1)\n",
    "        #print('c2', tmp_img2)\n",
    "\n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "\n",
    "        # get prediction value\n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "\n",
    "        #print('d1', tmp_img1)\n",
    "\n",
    "        #print(\"mo\", model_output)\n",
    "        #print(\"max\", pred_max)\n",
    "        #print(\"backprop\", model_output[:, pred_max])\n",
    "\n",
    "        # backpropagate for gradient tracking\n",
    "        model_output[:, pred_max].backward()\n",
    "\n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        #print('e2', tmp_img2)\n",
    "\n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # draw the heatmap\n",
    "        # plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "        # fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_l2_plot[0])}_n{int(self.n_l2_plot[0])}_{str(current_epoch)}.png\"))\n",
    "        \n",
    "        tmp_path = os.path.join(self.log_dir, \"gradcam\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.imsave(os.path.join(tmp_path, \n",
    "                                f\"cam_id{img_id}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\"\n",
    "                               ), heatmap.detach().cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "        heatmap *= 255.0 / heatmap.max()\n",
    "        pil_heatmap = Image.fromarray(heatmap.detach().cpu().numpy().squeeze()).convert('RGB')\n",
    "       \n",
    "        tmp_path = os.path.join(self.log_dir, \"gradcam\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        pil_heatmap.save(os.path.join(tmp_path, \n",
    "                                      f\"camgray_id{img_id}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" \n",
    "                                     )) \n",
    "            \n",
    "    def run_loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        # =============================================================================\n",
    "        # put image through model, calculate loss and metrics\n",
    "        # use cc term that has been calculated previously\n",
    "        # =============================================================================\n",
    "        \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, mask, img_id = batch\n",
    "        else:\n",
    "            img, ground_truth = batch\n",
    "            \n",
    "        #print(img)\n",
    "        #print(ground_truth)\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # for test routine\n",
    "        self.mo = model_output.argmax(dim=1).squeeze().detach().cpu().numpy()\n",
    "        self.gt = ground_truth.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        #print('self mo', self.mo)\n",
    "        #print('self gt', self.gt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"gt\", ground_truth)\n",
    "        print(\"gt shape\", ground_truth.shape)\n",
    "        print(\"gt type\", ground_truth.type())\n",
    "        print(torch.zeros(ground_truth.size(0), self.n_classes))\n",
    "        \n",
    "        if len(ground_truth.shape) < 2:\n",
    "            ground_truth_tmp_tmp = ground_truth.unsqueeze(1)\n",
    "        else:\n",
    "            ground_truth = ground_truth.transpose(1, 0)\n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth_tmp.size(0), self.n_classes).scatter_(1, ground_truth_tmp.to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \"\"\"\n",
    "\n",
    "        ground_truth = ground_truth.squeeze()\n",
    "        if len(ground_truth.shape) < 1:\n",
    "            ground_truth = ground_truth.unsqueeze(0)\n",
    "        loss = self.criterion(model_output, ground_truth.long()) # ground_truth_multi_hot)\n",
    "        cc = torch.mean(self.model.cc) * self.cc_weight\n",
    "        \n",
    "        #print(\"loss\", loss)\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "        \n",
    "        try:\n",
    "            pass # print('pred i', pred_i.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics pred\")\n",
    "            print(e)\n",
    "        try:\n",
    "            pass # print('gt', ground_truth.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics gt\")\n",
    "            print(e)\n",
    "        \n",
    "        \n",
    "        if mode == \"train\":\n",
    "            value = self.train_metrics(preds=pred_i, target=ground_truth)\n",
    "            self.log_dict(value, on_step=False, on_epoch=True)\n",
    "            \"\"\"\n",
    "            try:\n",
    "                ta = self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "                tf = self.train_f1(preds=pred_i, target=ground_truth) \n",
    "                tp = self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            except Exception as e:\n",
    "                print(\"DECENT ERROR: we are experiencing this CUDA ERROR most likely, because our decent1x1 has too little filters.\")\n",
    "                print(\"We need the same number as classes. It can happen, that all in-connections to a filter in decent1x1 got pruned and hence it is gone.\")\n",
    "                print(\"preds\", pred_i)\n",
    "                print(\"target\", ground_truth)\n",
    "                print(e)\n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"train info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", ta)\n",
    "                print(\"f\", tf)\n",
    "                print(\"p\", tp)\n",
    "                print(\"l\", loss)\n",
    "            \"\"\"\n",
    "                \n",
    "        elif mode == \"val\":\n",
    "            value = self.val_metrics(preds=pred_i, target=ground_truth)\n",
    "            self.log_dict(value, on_step=False, on_epoch=True)\n",
    "            \"\"\"\n",
    "            va = self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            vf = self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            vp = self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"val info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", va)\n",
    "                print(\"f\", vf)\n",
    "                print(\"p\", vp)\n",
    "                print(\"l\", loss)\n",
    "            \"\"\"\n",
    "                \n",
    "        else:\n",
    "            value = self.test_metrics(preds=pred_i, target=ground_truth)\n",
    "            self.log_dict(value, on_step=False, on_epoch=True)\n",
    "            \"\"\"\n",
    "            print(pred_i)\n",
    "            print(ground_truth)\n",
    "            ta = self.test_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            tf = self.test_f1(preds=pred_i, target=ground_truth) \n",
    "            tp = self.test_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.test_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.test_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.test_prec, on_step=False, on_epoch=True)\n",
    "            \"\"\"\n",
    "            \n",
    "        \n",
    "        #print(value) \n",
    "            \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2227f9c-6913-40b6-a822-a3167b68fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cc373f4-517d-4f3e-88fe-9699ecf2759b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2906-61b6-4f3a-b2fa-7aeeaa12e7e1",
   "metadata": {},
   "source": [
    "## run dev routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_kwargs[\"xai_done\"] = False\n",
    "\n",
    "if train_kwargs[\"train_size\"] > 0:\n",
    "    \n",
    "    # =============================================================================\n",
    "    # train model and run test/xAI routine\n",
    "\n",
    "    # logger - save logs in \"examples/example_results/lightning_logs\"\n",
    "    # light - DecentLightning model\n",
    "    # trainer - pl.Trainer\n",
    "    # trainer.fit\n",
    "    # explainer - pl.Trainer\n",
    "    # explainer.test\n",
    "    # =============================================================================\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "    \n",
    "\n",
    "    # THE LOGGER\n",
    "    logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"])\n",
    "\n",
    "    # THE LIGHTNING MODEL\n",
    "    # Initialize the LightningModule\n",
    "    light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "    # THE LIGHTNING TRAINER (for training)\n",
    "    trainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         # inference_mode=False, # do grad manually\n",
    "                         log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                         logger=logger,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         max_epochs=train_kwargs[\"epochs\"],\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1_macro\",\n",
    "                                                   filename='{epoch}-{val_f1:.2f}-{unpruned:.0f}'),\n",
    "                                    DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                                    filename='{epoch}-{unpruned:.0f}-{val_f1:.2f}'),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # THE TRAIN-RUN\n",
    "    # Train the model using a Trainer\n",
    "    trainer.fit(light, dataloader.train_dataloader, dataloader.val_dataloader)\n",
    "    \n",
    "\n",
    "    # THE LIGHTNING TRAINER (for testing)\n",
    "    # we want the grad to work in test, hence: inference_mode=False\n",
    "    explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         logger=logger,\n",
    "                         inference_mode=False)\n",
    "\n",
    "    # THE TEST-RUN\n",
    "    # including test\n",
    "    test_result = explainer.test(light, dataloader.xai_dataloader, verbose=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    train_kwargs[\"xai_done\"] = True\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "058c7763-9a1c-479d-9d74-d09946aee7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66262489-1c4a-439e-9673-32a40c5c52f1",
   "metadata": {},
   "source": [
    "## run test routine ****************************\n",
    "\n",
    "we need this with the OCTA-500 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f7aa2-96ed-4518-b129-5751bd5e39a8",
   "metadata": {},
   "source": [
    "torch.load(ckpt_path)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "891e1fef-8dab-44fd-908d-e9f2a901dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 19\n",
      "Missing logger folder: examples/example_results\\lightning_logs\\first_ever_experiment_xAI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT INFO: be aware, that you have to manually check, whether every output node has an input. otherwise an error may be triggered by cuda\n",
      "DECENT INFO: You are using checkpoint file:  examples/example_results\\lightning_logs\\first_ever_experiment\\version_0/checkpoints/epoch=9-unpruned=1155-val_f1=0.00.ckpt\n",
      "Found pretrained model at examples/example_results\\lightning_logs\\first_ever_experiment\\version_0/checkpoints/epoch=9-unpruned=1155-val_f1=0.00.ckpt, loading...\n",
      "DECENT INFO: dimensions are entry, decent1, decent2, decent3, decent1x1 == out [1, 4, 8, 16, 4]\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 2, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 7, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 7, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 5, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 7, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 7, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 7, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 7, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 7, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 6, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 3, 3, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 11, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 13, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 11, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 11, 1, 1])\n",
      "[10.]\n",
      "[5.]\n",
      "init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d3d51e5df24633a128c7f0284183ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: test_step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\AppData\\Local\\Temp\\ipykernel_21536\\406094672.py:258: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure(figsize=(5, 5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: test_step 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Optimal parameters not found: The maximum number of function evaluations is exceeded.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21536\\3946818899.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# THE TEST-RUN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# only test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mtest_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxai_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_maybe_unwrap_optimized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m         return call._call_and_handle_interrupt(\n\u001b[0m\u001b[0;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_provided\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m         )\n\u001b[1;32m--> 778\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m         \u001b[1;31m# remove the tensors from the test results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_tensors_to_scalars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    971\u001b[0m         \u001b[1;31m# RUN THE TRAINER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[1;31m# ----------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m         \u001b[1;31m# ----------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1009\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluation_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mprevious_dataloader_idx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;31m# this will run only when no pre-fetching was done.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[1;31m# consume the batch we just fetched\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py\u001b[0m in \u001b[0;36m_fetch_next_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_profiler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop_profiler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Sequential\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_idx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\decentnet\\datasceyence\\helper\\data\\octa500.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mflt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mocta500_flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# flattened\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mocta500_crop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# should probably save this mask too ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\decentnet\\datasceyence\\helper\\data\\transform\\octa500_flatten.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m         modelCoeffs, pcov = curve_fit(self.fittingQuadraticModel, x, markers, \\\n\u001b[0m\u001b[0;32m    352\u001b[0m                                     method = 'dogbox', loss = 'soft_l1')\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\scipy\\optimize\\_minpack_py.py\u001b[0m in \u001b[0;36mcurve_fit\u001b[1;34m(f, xdata, ydata, p0, sigma, absolute_sigma, check_finite, bounds, method, jac, **kwargs)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Optimal parameters not found: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[0mysize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Optimal parameters not found: The maximum number of function evaluations is exceeded."
     ]
    }
   ],
   "source": [
    "if train_kwargs[\"load_mode\"] and not train_kwargs[\"xai_done\"]:\n",
    "\n",
    "    # =============================================================================\n",
    "    # load model and run test/xAI routine\n",
    "\n",
    "    # logger - save logs in \"dumpster\"\n",
    "    # light - DecentLightning model\n",
    "    # explainer - pl.Trainer\n",
    "    # explainer.test\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"DECENT INFO: be aware, that you have to manually check, whether every output node has an input. otherwise an error may be triggered by cuda\")\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "\n",
    "    # train_kwargs[\"load_ckpt_file\"] = \"version_7/checkpoints/epoch=0-val_f1=0.62-unpruned=1560.ckpt\"\n",
    "    \n",
    "    # Check whether pretrained model exists. If yes, load it.\n",
    "    # ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\\debug_oct_no_fc\", 'version_13', 'checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt'])\n",
    "    ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "    print(\"DECENT INFO: You are using checkpoint file: \", ckpt_path)\n",
    "    \n",
    "    if os.path.isfile(ckpt_path):\n",
    "\n",
    "        # THE LOGGER\n",
    "        logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"]+\"_xAI\") # the xAI routine for an experiment\n",
    "\n",
    "        # THE LIGHTNING MODEL\n",
    "        # load from checkpoint doesn't work, since our architecture is 'messed up' through pruning\n",
    "        # light = DecentLightning.load_from_checkpoint(state_dict, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n",
    "        # use this line instead:\n",
    "        light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "        # THE LIGHTNING TRAINER (for testing)\n",
    "        # we want the grad to work in test, hence: inference_mode=False\n",
    "        explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                             accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                             #devices=[0],\n",
    "                             logger=logger,\n",
    "                             inference_mode=False)\n",
    "\n",
    "        # THE TEST-RUN\n",
    "        # only test\n",
    "        test_result = explainer.test(light, dataloader.xai_dataloader, verbose=False)\n",
    "\n",
    "    else:\n",
    "        print('DECENT ERROR: not a file - may have been resetted in dev routine, check the load_ckpt_file, set dev routine to False and run everything')\n",
    "\n",
    "    \n",
    "print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696de4a-76ea-4b95-9d11-e1cf8a733a42",
   "metadata": {},
   "source": [
    "# random nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecentNet(\n",
       "  (decent1): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([10.]), n_this=Parameter containing:\n",
       "      tensor([10.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([1.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([2.]), n_this=Parameter containing:\n",
       "      tensor([1.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([11.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "    )\n",
       "  )\n",
       "  (decent2): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([10.]), n_this=Parameter containing:\n",
       "      tensor([5.]))\n",
       "       with inputs: ms_in= 10, 2, ns_in= 10, 1)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([0.]))\n",
       "       with inputs: ms_in= 1, 2, ns_in= 6, 1)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 10, 1, 2, ns_in= 10, 6, 1)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([9.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 10, 1, 2, ns_in= 10, 6, 1)\n",
       "      (4): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([0.]), n_this=Parameter containing:\n",
       "      tensor([16.]))\n",
       "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
       "      (5): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([12.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 10, 2, 13, ns_in= 10, 1, 11)\n",
       "      (6): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([14.]), n_this=Parameter containing:\n",
       "      tensor([16.]))\n",
       "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
       "      (7): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([8.]))\n",
       "       with inputs: ms_in= 1, 2, ns_in= 6, 1)\n",
       "    )\n",
       "  )\n",
       "  (decent3): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 7, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([3.]))\n",
       "       with inputs: ms_in= 10, 13, 9, 0, 12, 14, 7, ns_in= 5, 0, 7, 16, 6, 16, 8)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 7, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([17.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 13, 13, 9, 0, 12, 14, 7, ns_in= 0, 6, 7, 16, 6, 16, 8)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 6, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([8.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, 14, 7, ns_in= 5, 0, 6, 7, 16, 8)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([0.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 10, 0, 12, 14, 7, ns_in= 5, 16, 6, 16, 8)\n",
       "      (4): DecentFilter(weights: torch.Size([1, 7, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([12.]), n_this=Parameter containing:\n",
       "      tensor([12.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, 0, 12, 7, ns_in= 5, 0, 6, 7, 16, 6, 8)\n",
       "      (5): DecentFilter(weights: torch.Size([1, 7, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, 12, 14, 7, ns_in= 5, 0, 6, 7, 6, 16, 8)\n",
       "      (6): DecentFilter(weights: torch.Size([1, 6, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([12.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 10, 13, 9, 12, 14, 7, ns_in= 5, 6, 7, 6, 16, 8)\n",
       "      (7): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([14.]), n_this=Parameter containing:\n",
       "      tensor([13.]))\n",
       "       with inputs: ms_in= 9, 12, 7, ns_in= 7, 6, 8)\n",
       "      (8): DecentFilter(weights: torch.Size([1, 6, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([15.]), n_this=Parameter containing:\n",
       "      tensor([8.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 0, 14, 7, ns_in= 5, 0, 6, 16, 16, 8)\n",
       "      (9): DecentFilter(weights: torch.Size([1, 7, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([14.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, 0, 14, 7, ns_in= 5, 0, 6, 7, 16, 16, 8)\n",
       "      (10): DecentFilter(weights: torch.Size([1, 7, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([1.]), n_this=Parameter containing:\n",
       "      tensor([11.]))\n",
       "       with inputs: ms_in= 10, 13, 9, 0, 12, 14, 7, ns_in= 5, 6, 7, 16, 6, 16, 8)\n",
       "      (11): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([11.]), n_this=Parameter containing:\n",
       "      tensor([16.]))\n",
       "       with inputs: ms_in= 10, 9, 0, 12, ns_in= 5, 7, 16, 6)\n",
       "      (12): DecentFilter(weights: torch.Size([1, 7, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([0.]), n_this=Parameter containing:\n",
       "      tensor([13.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 0, 12, 14, 7, ns_in= 5, 0, 6, 16, 6, 16, 8)\n",
       "      (13): DecentFilter(weights: torch.Size([1, 6, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([12.]), n_this=Parameter containing:\n",
       "      tensor([0.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 0, 12, 14, ns_in= 5, 0, 6, 16, 6, 16)\n",
       "      (14): DecentFilter(weights: torch.Size([1, 6, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([6.]), n_this=Parameter containing:\n",
       "      tensor([2.]))\n",
       "       with inputs: ms_in= 13, 13, 9, 0, 12, 7, ns_in= 0, 6, 7, 16, 6, 8)\n",
       "      (15): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([0.]), n_this=Parameter containing:\n",
       "      tensor([2.]))\n",
       "       with inputs: ms_in= 13, 0, 12, ns_in= 0, 16, 6)\n",
       "    )\n",
       "  )\n",
       "  (decent1x1): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 11, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 5, 12, 14, 15, 14, 11, 12, 6, ns_in= 3, 14, 8, 14, 14, 13, 8, 7, 16, 0, 2)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 13, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([9.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 0, 12, 5, 14, 15, 14, 1, 0, 12, 0, ns_in= 3, 14, 8, 7, 12, 14, 13, 8, 7, 11, 13, 0, 2)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 11, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([8.]), n_this=Parameter containing:\n",
       "      tensor([16.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 0, 12, 12, 14, 14, 11, 12, 6, ns_in= 3, 14, 8, 7, 12, 14, 13, 7, 16, 0, 2)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 11, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([2.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 17, 5, 12, 5, 12, 14, 15, 1, 11, 6, 0, ns_in= 14, 8, 12, 14, 14, 13, 8, 11, 16, 2, 2)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (mish1): Mish()\n",
       "  (mish2): Mish()\n",
       "  (mish3): Mish()\n",
       "  (mish1x1): Mish()\n",
       "  (bias1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias2): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias3): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias1x1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.1332,  0.0426,  0.0269],\n",
      "          [-0.0547,  0.0660,  0.0907],\n",
      "          [-0.0951, -0.1076,  0.0876]],\n",
      "\n",
      "         [[-0.0801,  0.0602, -0.0629],\n",
      "          [ 0.0570, -0.0621, -0.0849],\n",
      "          [-0.0205, -0.0831, -0.0922]],\n",
      "\n",
      "         [[-0.1177, -0.0839, -0.1119],\n",
      "          [ 0.0746, -0.0375, -0.1074],\n",
      "          [-0.0884,  0.0457, -0.1312]],\n",
      "\n",
      "         [[-0.1067,  0.0128, -0.0696],\n",
      "          [-0.0403,  0.0543,  0.0822],\n",
      "          [-0.0621,  0.1015, -0.0531]],\n",
      "\n",
      "         [[-0.0492, -0.0432,  0.1350],\n",
      "          [ 0.1356,  0.0510, -0.0540],\n",
      "          [ 0.0637,  0.1224,  0.0663]],\n",
      "\n",
      "         [[ 0.0236,  0.0377, -0.1391],\n",
      "          [ 0.0923,  0.0050, -0.1032],\n",
      "          [ 0.0033, -0.1072, -0.0350]],\n",
      "\n",
      "         [[ 0.1476,  0.1660,  0.1275],\n",
      "          [-0.0007,  0.0250, -0.0197],\n",
      "          [-0.0648,  0.1567,  0.0448]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1139,  0.1430, -0.0070],\n",
      "          [ 0.0095,  0.1536,  0.1085],\n",
      "          [ 0.0939,  0.0283, -0.0542]],\n",
      "\n",
      "         [[ 0.1492,  0.1158,  0.0521],\n",
      "          [-0.1362, -0.0961, -0.0202],\n",
      "          [-0.1258,  0.0153, -0.0305]],\n",
      "\n",
      "         [[ 0.0862, -0.0929,  0.0054],\n",
      "          [ 0.0356, -0.0799, -0.0103],\n",
      "          [ 0.0776,  0.1368, -0.0194]],\n",
      "\n",
      "         [[-0.0194,  0.0889,  0.0332],\n",
      "          [-0.0839, -0.0034,  0.0747],\n",
      "          [-0.1124,  0.0407, -0.1238]],\n",
      "\n",
      "         [[-0.0616,  0.1185,  0.0406],\n",
      "          [ 0.0709,  0.1224,  0.1028],\n",
      "          [-0.0578,  0.1506, -0.0412]],\n",
      "\n",
      "         [[-0.0433, -0.0881,  0.0424],\n",
      "          [ 0.0051, -0.0434,  0.0823],\n",
      "          [ 0.0481,  0.1337,  0.0427]],\n",
      "\n",
      "         [[ 0.1185, -0.1083,  0.0322],\n",
      "          [-0.0237, -0.0416,  0.0343],\n",
      "          [ 0.1106,  0.0864, -0.0927]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0794,  0.1137, -0.0893],\n",
      "          [-0.0173, -0.0431,  0.0209],\n",
      "          [-0.0949,  0.0591,  0.0350]],\n",
      "\n",
      "         [[ 0.0947,  0.1483,  0.0716],\n",
      "          [-0.0683,  0.0890,  0.0871],\n",
      "          [-0.1152,  0.0169, -0.0467]],\n",
      "\n",
      "         [[-0.0737, -0.0886,  0.1085],\n",
      "          [ 0.0789, -0.1002, -0.0964],\n",
      "          [-0.0342,  0.1103, -0.1433]],\n",
      "\n",
      "         [[ 0.1202, -0.1101,  0.0577],\n",
      "          [ 0.1504,  0.1030,  0.0097],\n",
      "          [ 0.0244,  0.0500,  0.0742]],\n",
      "\n",
      "         [[ 0.0117,  0.1607,  0.0654],\n",
      "          [ 0.0967, -0.0338,  0.0231],\n",
      "          [-0.0268,  0.0700,  0.1097]],\n",
      "\n",
      "         [[ 0.1052,  0.0886,  0.0846],\n",
      "          [ 0.0664, -0.0591, -0.1066],\n",
      "          [-0.0800,  0.0099, -0.0226]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0950, -0.0720,  0.1055],\n",
      "          [ 0.0424, -0.0935, -0.0370],\n",
      "          [ 0.0162, -0.1079,  0.1049]],\n",
      "\n",
      "         [[-0.0601,  0.0879,  0.0501],\n",
      "          [ 0.0365,  0.0527,  0.0747],\n",
      "          [ 0.1021, -0.1166, -0.1134]],\n",
      "\n",
      "         [[-0.0897,  0.0604,  0.0865],\n",
      "          [ 0.0515,  0.0838,  0.0800],\n",
      "          [ 0.0592,  0.1149, -0.1014]],\n",
      "\n",
      "         [[ 0.1027, -0.1170, -0.0366],\n",
      "          [-0.1003, -0.0907, -0.1017],\n",
      "          [-0.0382, -0.0188,  0.0933]],\n",
      "\n",
      "         [[-0.0548,  0.1291,  0.0559],\n",
      "          [ 0.1174, -0.0064,  0.1153],\n",
      "          [-0.0550, -0.0929,  0.0397]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0655,  0.0402,  0.0195],\n",
      "          [-0.0890, -0.1023, -0.1107],\n",
      "          [-0.0753, -0.1407, -0.1300]],\n",
      "\n",
      "         [[-0.0426,  0.0631,  0.0593],\n",
      "          [ 0.0216,  0.0280, -0.0597],\n",
      "          [-0.1007, -0.1349,  0.0391]],\n",
      "\n",
      "         [[-0.0740,  0.0551,  0.0521],\n",
      "          [-0.0224, -0.0764, -0.0967],\n",
      "          [ 0.0590, -0.0949, -0.0528]],\n",
      "\n",
      "         [[ 0.1123,  0.1019,  0.1138],\n",
      "          [ 0.1182, -0.0797,  0.0227],\n",
      "          [ 0.1071,  0.0610,  0.0441]],\n",
      "\n",
      "         [[ 0.0637, -0.0980,  0.0522],\n",
      "          [-0.0430,  0.0924, -0.0053],\n",
      "          [-0.0304,  0.0434, -0.1038]],\n",
      "\n",
      "         [[-0.0976, -0.0307,  0.0855],\n",
      "          [-0.0178, -0.0807, -0.0373],\n",
      "          [ 0.0217, -0.0132, -0.1388]],\n",
      "\n",
      "         [[-0.1207,  0.0282, -0.0753],\n",
      "          [ 0.0031, -0.0717, -0.1137],\n",
      "          [-0.1341, -0.0377, -0.0694]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1241, -0.0849, -0.0056],\n",
      "          [ 0.0271, -0.0484, -0.1159],\n",
      "          [-0.0403,  0.0209, -0.0874]],\n",
      "\n",
      "         [[-0.0342,  0.1372, -0.0235],\n",
      "          [-0.0568,  0.0821, -0.0380],\n",
      "          [ 0.0141,  0.1077,  0.1390]],\n",
      "\n",
      "         [[-0.1412, -0.0396,  0.0591],\n",
      "          [ 0.1166,  0.0778,  0.0527],\n",
      "          [-0.0971, -0.0827, -0.0635]],\n",
      "\n",
      "         [[ 0.0061, -0.0684, -0.0934],\n",
      "          [ 0.1056,  0.1236, -0.0557],\n",
      "          [ 0.1464, -0.0277,  0.0982]],\n",
      "\n",
      "         [[-0.0186, -0.0866,  0.0673],\n",
      "          [-0.1079, -0.1093,  0.0992],\n",
      "          [ 0.0096, -0.0660, -0.1568]],\n",
      "\n",
      "         [[ 0.0824, -0.0277, -0.0009],\n",
      "          [-0.0742, -0.0839,  0.0613],\n",
      "          [ 0.1523, -0.0035,  0.0841]],\n",
      "\n",
      "         [[ 0.0570,  0.0937, -0.0328],\n",
      "          [ 0.0762, -0.0599, -0.0486],\n",
      "          [-0.1213,  0.0171,  0.0492]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0811,  0.1398, -0.0194],\n",
      "          [ 0.0764,  0.1267, -0.1026],\n",
      "          [-0.1128, -0.0843, -0.1202]],\n",
      "\n",
      "         [[ 0.0539, -0.1286, -0.1412],\n",
      "          [ 0.0292,  0.0050, -0.1174],\n",
      "          [ 0.1162, -0.0446, -0.0080]],\n",
      "\n",
      "         [[ 0.1343, -0.0491, -0.0284],\n",
      "          [ 0.0491,  0.0789,  0.0731],\n",
      "          [ 0.1450,  0.0594, -0.0906]],\n",
      "\n",
      "         [[ 0.0608, -0.0249, -0.1045],\n",
      "          [ 0.0865, -0.0762, -0.1421],\n",
      "          [-0.1548, -0.0226, -0.1328]],\n",
      "\n",
      "         [[ 0.0082,  0.0684,  0.1448],\n",
      "          [ 0.0351,  0.1693, -0.0334],\n",
      "          [ 0.0128, -0.0246, -0.0466]],\n",
      "\n",
      "         [[ 0.0149,  0.0193,  0.0825],\n",
      "          [-0.1609,  0.0399, -0.1619],\n",
      "          [ 0.0302, -0.0497, -0.1045]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.1631, -0.0353, -0.1410],\n",
      "          [-0.0040,  0.0823,  0.0385],\n",
      "          [ 0.0228, -0.0648,  0.0922]],\n",
      "\n",
      "         [[ 0.0805,  0.1273,  0.1480],\n",
      "          [ 0.0842,  0.1196,  0.0753],\n",
      "          [-0.0484,  0.0760, -0.0732]],\n",
      "\n",
      "         [[-0.0346,  0.1768,  0.1068],\n",
      "          [-0.0461, -0.0361,  0.0930],\n",
      "          [-0.0318,  0.0170, -0.0254]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0285, -0.0716,  0.0054],\n",
      "          [ 0.0462,  0.0670, -0.0658],\n",
      "          [-0.0639, -0.0965,  0.0968]],\n",
      "\n",
      "         [[-0.0359,  0.1238,  0.0798],\n",
      "          [ 0.1185, -0.0244,  0.1426],\n",
      "          [ 0.0979,  0.1156,  0.1443]],\n",
      "\n",
      "         [[-0.0499, -0.0652, -0.1382],\n",
      "          [ 0.1308, -0.0570,  0.0752],\n",
      "          [-0.0498,  0.0206,  0.0902]],\n",
      "\n",
      "         [[ 0.0349,  0.1297,  0.0595],\n",
      "          [-0.0396, -0.0437, -0.0198],\n",
      "          [ 0.1442,  0.0702,  0.0179]],\n",
      "\n",
      "         [[ 0.1035,  0.0199, -0.0019],\n",
      "          [-0.0633, -0.0707,  0.1082],\n",
      "          [ 0.0362,  0.0004,  0.1624]],\n",
      "\n",
      "         [[-0.0225,  0.0655, -0.0468],\n",
      "          [-0.0747, -0.0606, -0.1109],\n",
      "          [-0.1128, -0.1067, -0.0351]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.1372, -0.1239, -0.0662],\n",
      "          [-0.0304,  0.0122,  0.0132],\n",
      "          [-0.0654,  0.0423, -0.0096]],\n",
      "\n",
      "         [[-0.0872, -0.1148,  0.0856],\n",
      "          [-0.0138, -0.0687, -0.0812],\n",
      "          [ 0.0317, -0.0032, -0.0708]],\n",
      "\n",
      "         [[ 0.0948, -0.0109,  0.0693],\n",
      "          [ 0.1317,  0.0618,  0.0643],\n",
      "          [-0.1243, -0.1100, -0.0665]],\n",
      "\n",
      "         [[-0.0375, -0.1265, -0.0306],\n",
      "          [-0.0160,  0.0320,  0.0533],\n",
      "          [ 0.1078, -0.1145,  0.0202]],\n",
      "\n",
      "         [[-0.0810, -0.1493, -0.1284],\n",
      "          [ 0.0250, -0.0282, -0.0160],\n",
      "          [ 0.1228,  0.1177,  0.0934]],\n",
      "\n",
      "         [[-0.0915, -0.0587, -0.0247],\n",
      "          [-0.1174, -0.1122, -0.1028],\n",
      "          [-0.1010, -0.0281, -0.0631]],\n",
      "\n",
      "         [[-0.1281,  0.1213,  0.1056],\n",
      "          [-0.0736,  0.0027,  0.0816],\n",
      "          [-0.0955,  0.1060,  0.0726]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0562, -0.0442,  0.0845],\n",
      "          [ 0.1256,  0.0885, -0.0694],\n",
      "          [ 0.0569,  0.0743,  0.0296]],\n",
      "\n",
      "         [[ 0.0006,  0.1258,  0.0323],\n",
      "          [ 0.0746, -0.0890,  0.0199],\n",
      "          [ 0.1322, -0.0822,  0.0680]],\n",
      "\n",
      "         [[ 0.0853,  0.1120,  0.1477],\n",
      "          [-0.1269, -0.0165,  0.0173],\n",
      "          [-0.1679, -0.0016,  0.0146]],\n",
      "\n",
      "         [[-0.1024, -0.0853,  0.0592],\n",
      "          [ 0.1465,  0.0698, -0.0851],\n",
      "          [ 0.1346, -0.0300,  0.0912]],\n",
      "\n",
      "         [[ 0.0765, -0.0651,  0.0756],\n",
      "          [-0.0683, -0.0941,  0.0400],\n",
      "          [ 0.0121,  0.0343,  0.0814]],\n",
      "\n",
      "         [[ 0.0571,  0.0194, -0.0187],\n",
      "          [-0.0267, -0.0123, -0.1244],\n",
      "          [ 0.0140, -0.0972, -0.1339]],\n",
      "\n",
      "         [[-0.0161, -0.0452,  0.0737],\n",
      "          [-0.0140, -0.1026,  0.0716],\n",
      "          [ 0.0134,  0.1516, -0.0085]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0330, -0.1209, -0.0606],\n",
      "          [-0.0852, -0.0672, -0.0643],\n",
      "          [ 0.0123, -0.0220,  0.0840]],\n",
      "\n",
      "         [[ 0.0142,  0.1284,  0.0526],\n",
      "          [ 0.0555,  0.0048,  0.1071],\n",
      "          [ 0.0579,  0.0363,  0.0561]],\n",
      "\n",
      "         [[-0.1146,  0.0600,  0.0556],\n",
      "          [ 0.1053,  0.0003,  0.0559],\n",
      "          [ 0.0372, -0.1371, -0.0682]],\n",
      "\n",
      "         [[ 0.0208, -0.1027, -0.1147],\n",
      "          [-0.1184, -0.0556,  0.0454],\n",
      "          [-0.0973,  0.0777,  0.0502]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.4566e-02,  8.3138e-05, -1.4302e-01],\n",
      "          [ 1.2615e-01,  4.8005e-02, -9.4577e-02],\n",
      "          [ 2.5895e-02,  9.3814e-02,  4.4412e-02]],\n",
      "\n",
      "         [[-1.3844e-01, -3.7984e-02,  2.5353e-02],\n",
      "          [ 8.5713e-02, -4.0649e-02,  6.6414e-02],\n",
      "          [ 5.4454e-02,  5.0275e-02, -5.9939e-02]],\n",
      "\n",
      "         [[ 1.4533e-01, -6.5438e-02,  6.8999e-02],\n",
      "          [-6.2367e-02, -6.9610e-02, -4.2433e-02],\n",
      "          [ 7.2726e-02,  1.0597e-01, -3.7344e-02]],\n",
      "\n",
      "         [[ 4.2662e-02, -1.3399e-01,  2.1126e-02],\n",
      "          [ 1.4914e-01,  1.0208e-01, -6.7313e-02],\n",
      "          [ 1.2062e-01,  3.1132e-02,  1.9812e-02]],\n",
      "\n",
      "         [[-4.3135e-02,  8.4752e-02, -1.0741e-01],\n",
      "          [-1.9881e-03, -1.1405e-02,  3.2073e-02],\n",
      "          [-9.3823e-02,  1.4201e-01,  1.1027e-01]],\n",
      "\n",
      "         [[-4.9719e-02, -1.0883e-01, -3.7535e-03],\n",
      "          [ 5.0022e-02, -1.2356e-01, -1.1884e-01],\n",
      "          [-1.0712e-01,  6.9421e-04, -1.0792e-01]],\n",
      "\n",
      "         [[ 5.8822e-03,  1.2224e-01,  3.7283e-02],\n",
      "          [-6.5522e-02,  2.1334e-02,  2.7529e-02],\n",
      "          [ 5.3443e-02,  1.0919e-01,  1.1189e-01]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.1388, -0.0821, -0.0561],\n",
      "          [-0.0373, -0.0815, -0.0468],\n",
      "          [ 0.1537,  0.1418, -0.0570]],\n",
      "\n",
      "         [[-0.0576,  0.0061,  0.0901],\n",
      "          [ 0.0309, -0.0836, -0.0533],\n",
      "          [ 0.0967,  0.1373, -0.0418]],\n",
      "\n",
      "         [[-0.0470,  0.0914,  0.0050],\n",
      "          [-0.0660,  0.0515,  0.0188],\n",
      "          [-0.0701, -0.0895, -0.1242]],\n",
      "\n",
      "         [[ 0.0564, -0.0586, -0.0520],\n",
      "          [ 0.0828,  0.0470,  0.0410],\n",
      "          [ 0.0233,  0.0384,  0.1674]],\n",
      "\n",
      "         [[-0.1505, -0.1715, -0.0685],\n",
      "          [ 0.0455, -0.0272,  0.0282],\n",
      "          [ 0.0398, -0.0252, -0.0799]],\n",
      "\n",
      "         [[-0.0238, -0.0961, -0.0156],\n",
      "          [-0.1299, -0.0825, -0.0172],\n",
      "          [ 0.0209, -0.0218, -0.0946]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0808,  0.0393, -0.1030],\n",
      "          [-0.0407, -0.1745,  0.0040],\n",
      "          [ 0.0791, -0.0748, -0.1073]],\n",
      "\n",
      "         [[-0.0680,  0.0749,  0.1627],\n",
      "          [-0.0973, -0.0917,  0.1239],\n",
      "          [ 0.0027,  0.0986, -0.0675]],\n",
      "\n",
      "         [[ 0.0136, -0.0856, -0.1686],\n",
      "          [ 0.0431, -0.1230,  0.0433],\n",
      "          [-0.1246,  0.0219, -0.0698]],\n",
      "\n",
      "         [[ 0.0593, -0.1558,  0.0066],\n",
      "          [-0.1241, -0.0278, -0.0594],\n",
      "          [-0.0279, -0.0581, -0.0471]],\n",
      "\n",
      "         [[ 0.0850, -0.0062,  0.0530],\n",
      "          [ 0.0789, -0.0164,  0.1039],\n",
      "          [ 0.0241, -0.0795,  0.1032]],\n",
      "\n",
      "         [[ 0.1673, -0.0102,  0.1785],\n",
      "          [ 0.0962,  0.0613,  0.0923],\n",
      "          [-0.0574, -0.0559,  0.0591]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.1107, -0.1121, -0.1069],\n",
      "          [ 0.0314, -0.0412, -0.0739],\n",
      "          [-0.1240, -0.0302, -0.0752]],\n",
      "\n",
      "         [[-0.0304, -0.0983, -0.0349],\n",
      "          [-0.0581, -0.1100, -0.0491],\n",
      "          [-0.1186,  0.0358, -0.0650]],\n",
      "\n",
      "         [[ 0.0023, -0.0612, -0.1020],\n",
      "          [ 0.1428, -0.0415, -0.0820],\n",
      "          [ 0.0007,  0.1303,  0.0874]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for a in light.model.decent3.filter_list:\n",
    "    print(a.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2797f-8a72-46db-86ae-41868e90828e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "617dcfd1-2827-4b69-8b31-fe8cd1f677ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(range(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f2c63-9855-4e48-aec5-5d4b1a971392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
