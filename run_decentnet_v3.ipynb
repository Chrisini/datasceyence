{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ùîªùïñùïîùïñùïüùï•‚Ñïùïñùï•: ùïïùïöùï§ùïñùïüùï•ùïíùïüùïòùïùùïñùïï ùïüùïñùï•\n",
    "\n",
    "Goal: create a sparse and modular ConvNet\n",
    "\n",
    "Todos: \n",
    "* [ ] delete node (filter) if either no input or no output edges\n",
    "* [ ] AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
    "* [ ] cuda error, if one of the decent1x1 has no kernels left - we need at least one input for each 1x1 filter\n",
    "* [ ] can we keep training if filter gets removed (e.g. at reloading model)\n",
    "* [ ] need some working filter removing in general - only at reload rn\n",
    "\n",
    "\n",
    "Notes:\n",
    "* additionally needed: position, activated channels, connection between channels\n",
    "* within this layer, a whole filter can be deactivated\n",
    "* within a filter, single channels can be deactivated\n",
    "* within this layer, filters can be swapped\n",
    "* the 'value' in the csv file is random if the CI metric is 'random'\n",
    "     \n",
    "* pruning actually doesn't work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf\n",
    "\n",
    "pip install:\n",
    "* pytorch_lightning\n",
    "\n",
    "preprocessing possible:\n",
    "* flatten layers\n",
    "* denoise\n",
    "* crop background\n",
    "\n",
    "\n",
    "warnings:\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73a26-f239-466f-901d-559d192f61de",
   "metadata": {},
   "source": [
    "![uml of code](examples/example_vis/uml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba244e3-3e6a-47e7-aa4b-a22df6054b8a",
   "metadata": {},
   "source": [
    "# conventions\n",
    "\n",
    "* entry image: entry_id5_0_0_0_mo3_gt2.png\n",
    "* hidden layer: hid_id5_3_8_2.png\n",
    "* last layer (global pooling - connected to class n): pool_2_3_4_gp2.png\n",
    "* activated image: cam_id5_mo3_gt2.png\n",
    "* activated image gray: camgray_id5_mo3_gt2.png\n",
    "\n",
    "\n",
    "* circle in: in_2_3_4_ep65.png\n",
    "* circle out: out_2_3_4_ep65.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# sys\n",
    "# =============================================================================\n",
    "import sys \n",
    "sys.path.insert(0, \"helper\")\n",
    "# =============================================================================\n",
    "# alphabetic order misc\n",
    "# =============================================================================\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "# =============================================================================\n",
    "# torch\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torchvision\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "# from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "# =============================================================================\n",
    "# datasceyence\n",
    "# =============================================================================\n",
    "from helper.model.decentnet import DecentNet\n",
    "from helper.visualisation import filter_activation\n",
    "from helper.data.mnist import DataLoaderMNIST\n",
    "from helper.data.retinamnist import DataLoaderRetinaMNIST\n",
    "from helper.data.octmnist import DataLoaderOCTMNIST\n",
    "from helper.data.octa500 import DataLoaderOCTA500\n",
    "from helper.data.organmnist3D import DataLoaderOrganMNIST3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397b450-c5c6-4da1-b630-7bf80e46891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch 2.0.0 == False\n",
      "tl 2.1.0 == False\n"
     ]
    }
   ],
   "source": [
    "seed = 1997 # was 19 before\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "debug_model = False\n",
    "\n",
    "print('torch 2.0.0 ==', torch.__version__=='2.0.0')\n",
    "print('tl 2.1.0 ==', pl.__version__=='2.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b0be-245d-49c0-88b4-d94167f7da90",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97670e82-0d27-4b7f-91df-874acf9974a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'input_data_csv': ['data_prep/data_octa500.csv'], 'result_path': 'examples/example_results', 'exp_name': 'debug_octmnist_no_fc', 'dataset': 'octa500', 'load_ckpt_file': '', 'epochs': 1, 'img_size': 28, 'batch_size': 2, 'log_every_n_steps': 4, 'device': 'cuda', 'num_workers': 0, 'train_size': 0, 'val_size': 0, 'test_size': 1, 'octa500_id': 199}\n",
      "model kwargs {'in_channels': 1, 'n_classes': None, 'out_dim': [1, 4, 4, 4], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 10, 'cc_metric': 'l2', 'ci_metric': 'l2', 'cm_metric': 'not implemented yet', 'update_every_nth_epoch': 1, 'pretrain_epochs': 1, 'prune_keep': 0.7, 'prune_keep_total': 0.4}\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {\n",
    "    'in_channels' : 1, # not in use yet\n",
    "    'n_classes': None, # filled in the dataset\n",
    "    'out_dim' :  [1, 4, 4, 4], # [1, 8, 16, 32], #[1, 16, 24, 32] # entry, decent1, decent2, decent3\n",
    "    'grid_size' : 18*18,\n",
    "    'criterion': torch.nn.CrossEntropyLoss(),# torch.nn.BCEWithLogitsLoss(),\n",
    "    'optimizer': \"sgd\", # sgd adamw\n",
    "    'base_lr': 0.001,\n",
    "    'min_lr' : 0.00001,\n",
    "    'momentum' : 0.9,\n",
    "    'lr_update' : 100,\n",
    "    # decentnet\n",
    "    'cc_weight': 10,\n",
    "    'cc_metric' : 'l2', # connection cost metric (for loss) - distance metric\n",
    "    'ci_metric' : 'l2', # channel importance metric (for pruning)\n",
    "    'cm_metric' : 'not implemented yet', # 'count', # crossing minimisation \n",
    "    'update_every_nth_epoch' : 1, # 5\n",
    "    'pretrain_epochs' : 1, # 20\n",
    "    'prune_keep' : 0.7, # 0.97, # in each epoch\n",
    "    'prune_keep_total' : 0.4, # this number is not exact, depends on the prune_keep value\n",
    "}\n",
    "\n",
    "train_kwargs = {\n",
    "    'input_data_csv': [\"data_prep/data_octa500.csv\"],\n",
    "    'result_path': \"examples/example_results\", # \"example_results/lightning_logs\", # not in use??\n",
    "    'exp_name': \"debug_octmnist_no_fc\", # must include dataset name, otherwise mnist is used\n",
    "    'dataset' : 'octa500',\n",
    "    'load_ckpt_file' : '', # \"version_0/checkpoints/epoch=94-unpruned=1600-val_f1=0.67.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "    'epochs': 1, # including the pretrain epochs - no adding up\n",
    "    'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ... # MNIST/MedMNIST 28 √ó 28 Pixel\n",
    "    'batch_size': 2, # 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "    'log_every_n_steps' : 4, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "    'device': \"cuda\",\n",
    "    'num_workers' : 0, # 18, # 18 for computer, 0 for laptop # make sure smaller than activate dataset sizes\n",
    "    'train_size' : 0, # total or percentage (batch size * forward passes per epoch)\n",
    "    'val_size' : 0, # total or percentage (batch size * forward passes per epoch)\n",
    "    'test_size' : 1, # total or percentage - 0 for all\n",
    "    'octa500_id' : 200-1, # todo\n",
    "}\n",
    "\n",
    "print(\"train kwargs\", train_kwargs)\n",
    "print(\"model kwargs\", model_kwargs)\n",
    "\n",
    "kwargs = {'train_kwargs':train_kwargs, 'model_kwargs':model_kwargs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ee3d0-9c79-4917-a355-093f1daf4855",
   "metadata": {},
   "source": [
    "## check the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d5ff36-c015-4bd6-8b12-c4d0e3b64b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "breaking = 6000*model_kwargs['prune_keep_total']\n",
    "weights = 6000 # this value is an estimate for a model [1, 8, 16, 32]\n",
    "# 'unpruned' is the logger variable for the value\n",
    "\n",
    "for i in range(train_kwargs['epochs']):\n",
    "    \n",
    "    if (weights < breaking): # weights*model_kwargs['prune_keep']\n",
    "        print(\"stop:\", breaking)\n",
    "        print('you need at least this many epochs:', i)\n",
    "        print('you currently have this many epochs:', train_kwargs['epochs'])\n",
    "        print(\"recommended to add 2*update_every_nth_epoch\")\n",
    "        break\n",
    "    \n",
    "    # not sure whether -1 is correct, have to check\n",
    "    if i > model_kwargs['pretrain_epochs'] and ((i-1)%model_kwargs['update_every_nth_epoch'] == 0):\n",
    "        weights = int(weights*model_kwargs['prune_keep'])\n",
    "    \n",
    "        print(i, weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b3283-2825-4f35-9716-78ccf5bb8b1c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "* the dataset name needs to be part of the experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "     img_id                                           img_path  \\\n",
      "0     10001  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "1     10004  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "2     10005  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "3     10007  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "4     10008  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "..      ...                                                ...   \n",
      "175   10288  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "176   10290  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "177   10291  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "178   10297  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "179   10300  C://Users/Prinzessin/projects/decentnet/datasc...   \n",
      "\n",
      "                                              msk_path  mode  lbl_disease sex  \\\n",
      "0    C://Users/Prinzessin/projects/decentnet/datasc...  test            3   M   \n",
      "1    C://Users/Prinzessin/projects/decentnet/datasc...  test            2   M   \n",
      "2    C://Users/Prinzessin/projects/decentnet/datasc...  test            1   M   \n",
      "3    C://Users/Prinzessin/projects/decentnet/datasc...  test            3   M   \n",
      "4    C://Users/Prinzessin/projects/decentnet/datasc...  test            0   F   \n",
      "..                                                 ...   ...          ...  ..   \n",
      "175  C://Users/Prinzessin/projects/decentnet/datasc...  test            2   M   \n",
      "176  C://Users/Prinzessin/projects/decentnet/datasc...  test            3   F   \n",
      "177  C://Users/Prinzessin/projects/decentnet/datasc...  test            0   F   \n",
      "178  C://Users/Prinzessin/projects/decentnet/datasc...  test            1   M   \n",
      "179  C://Users/Prinzessin/projects/decentnet/datasc...  test            1   M   \n",
      "\n",
      "    os-od  age  \n",
      "0      OD   55  \n",
      "1      OD   53  \n",
      "2      OS   27  \n",
      "3      OD   24  \n",
      "4      OD   51  \n",
      "..    ...  ...  \n",
      "175    OD   64  \n",
      "176    OD   49  \n",
      "177    OD   29  \n",
      "178    OD   48  \n",
      "179    OD   59  \n",
      "\n",
      "[180 rows x 8 columns]\n",
      "python_class : OCTA500\n",
      "description : The OCTA500 is based on optical coherence tomography (OCT) images for retinal diseases. The dataset is comprised of 4 diagnosis categories, leading to a multi-class classification task. We split the dataset with a ratio of n:n:n into training, validation and testset\n",
      "task : multi-class\n",
      "label : {'0': 'choroidal neovascularization', '1': 'diabetic retinopathy', '2': 'amd', '3': 'normal'}\n",
      "n_channels : 1\n",
      "n_samples : {'train': 0, 'val': 0, 'test': 0}\n",
      "n_classes: 4\n"
     ]
    }
   ],
   "source": [
    "if 'octmnist' in train_kwargs['dataset']:\n",
    "    # OCTMINST\n",
    "    data = DataLoaderOCTMNIST(train_kwargs, model_kwargs)   \n",
    "elif 'retinamnist' in train_kwargs['dataset']:\n",
    "    # RetinaMNIST\n",
    "    data = DataLoaderRetinaMNIST(train_kwargs, model_kwargs)\n",
    "elif 'octa500' in train_kwargs['dataset']:\n",
    "    # OCTA-500\n",
    "    data = DataLoaderOCTA500(train_kwargs, model_kwargs)\n",
    "elif '3d' in train_kwargs['dataset']:\n",
    "    data = DataLoaderOrganMNIST3D(train_kwargs, model_kwargs)\n",
    "else:\n",
    "    # MNIST\n",
    "    data = DataLoaderMNIST(train_kwargs, model_kwargs)\n",
    "\n",
    "assert model_kwargs['n_classes'] != None, \"DECENT ERROR: make sure you set the n_classes with the dataset\"  \n",
    "print(\"n_classes:\", model_kwargs['n_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X (Datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # an object with image representations and their positions\n",
    "    # amout of channels need to have same length as m and n lists\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        self.data = data # list of tensors (image representations)\n",
    "        self.ms_x = ms_x # list of integers (m position of each image representation)\n",
    "        self.ns_x = ns_x # list of integers (n position of each image representation)\n",
    "                \n",
    "    def setter(self, data, ms_x, ns_x):\n",
    "        self.data = data\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        \n",
    "    def getter(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint\n",
    "        # Save a checkpoint at the end of the training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "            print(\"DECENT NOTE: callback on_train_epoch_end\", monitor_candidates[\"epoch\"].item())\n",
    "            if monitor_candidates[\"epoch\"] > 0:\n",
    "                if monitor_candidates[\"unpruned_state\"] != -1:\n",
    "                    print(\"DECENT NOTE: save model\", monitor_candidates[\"epoch\"].item())\n",
    "                    if self._every_n_epochs >= 1 and ((trainer.current_epoch + 1) % self._every_n_epochs) == 0:\n",
    "                        self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                    self._save_last_checkpoint(trainer, monitor_candidates)\n",
    "                    \n",
    "                    pl_module.model.get_everything(current_epoch=trainer.current_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "## LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # Lightning Module consists of functions that define the training routine\n",
    "    # train, val, test: before epoch, step, after epoch, ...\n",
    "    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n",
    "    # order for the instance methods:\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\n",
    "    # \n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, kwargs, log_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        if train_kwargs[\"load_ckpt_file\"] != '':\n",
    "            self.ckpt_path = os.path.join(log_dir, train_kwargs[\"load_ckpt_file\"])\n",
    "            if os.path.isfile(ckpt_path):\n",
    "                print(f\"Found pretrained model at {ckpt_path}, loading...\")\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir, ckpt_path=ckpt_path).to(\"cuda\")\n",
    "            else:\n",
    "                # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        else:\n",
    "            # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "            self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "            \n",
    "        # print(self.model)\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        self.pretrain_epochs = model_kwargs[\"pretrain_epochs\"]\n",
    "        \n",
    "        # needed for hparams.yaml file\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if False:\n",
    "            self.metric = { \"train_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"train_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"val_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"val_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "                       }\n",
    "        else:\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            \n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            \n",
    "            self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "\n",
    "            \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        # =============================================================================\n",
    "        # we make it possible to use model_output = self(image)\n",
    "        # =============================================================================\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # =============================================================================\n",
    "        # returns:\n",
    "        #    optimiser and lr scheduler\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimiser = optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        else:\n",
    "            optimiser = optim.SGD(self.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        # initial plot of circular layer\n",
    "        # updates model every nth epoch\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: on_train_epoch_start\", self.current_epoch)\n",
    "        \n",
    "        # plot random layer (the circular plot)\n",
    "        if self.current_epoch == 0:\n",
    "            self.model.plot_incoming_connections(current_epoch=0)\n",
    "            self.model.plot_outgoing_connections(current_epoch=0)\n",
    "\n",
    "        # update model\n",
    "         # don't update unless pretrain epochs is reached\n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch >= self.pretrain_epochs:\n",
    "            print(\"DECENT NOTE: update model\", self.current_epoch)        \n",
    "            if debug_model:\n",
    "                print(\"DECENT NOTE: before update\")\n",
    "                print(\"DECENT NOTE: print model ...\")\n",
    "                print(self.model)\n",
    "            self.model.update(current_epoch = self.current_epoch)\n",
    "            if True: \n",
    "                print(\"DECENT NOTE: after update\")\n",
    "                print(\"DECENT NOTE: print model ...\")\n",
    "                print(self.model)\n",
    "                \n",
    "            print(\"DECENT NOTE: model updated\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculates loss for a batch # 1\n",
    "        # parameters:\n",
    "        #    batch\n",
    "        #    batch id\n",
    "        # returns:\n",
    "        #    loss\n",
    "        # notes:\n",
    "        #    calling gradcam like self.gradcam(batch) is dangerous cause changes gradients\n",
    "        # =============================================================================     \n",
    "        if False: # batch_idx < 2: # print first two steps\n",
    "            print(\"DECENT NOTE: training_step\", batch_idx)\n",
    "\n",
    "        # calculate loss\n",
    "        # loss = torch.tensor(1)\n",
    "        loss = self.run_loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging # 2\n",
    "        # =============================================================================\n",
    "        if False: # batch_idx < 2:\n",
    "            print(\"DECENT NOTE: validation_step\", batch_idx)\n",
    "        \n",
    "        self.run_loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing # 3\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_validation_epoch_end\")\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # save model if next iteration model is pruned # 4 \n",
    "        # this needs to be called before callback \n",
    "        # - if internal pytorch lightning convention changes, this will stop working\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_train_epoch_end\", self.current_epoch)\n",
    "               \n",
    "        if False:\n",
    "            print(\"current epoch\")\n",
    "            print(((self.current_epoch+1) % self.update_every_nth_epoch) == 0)\n",
    "            print(self.current_epoch+1)\n",
    "            print(self.current_epoch)\n",
    "            print(self.update_every_nth_epoch)\n",
    "        \n",
    "        # numel: returns the total number of elements in the input tensor\n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.log(f'unpruned', unpruned, on_step=False, on_epoch=True) \n",
    "        \n",
    "        if ((self.current_epoch+1) % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            # if next epoch is an update, set unpruned flag            \n",
    "            self.log(f'unpruned_state', 1, on_step=False, on_epoch=True)\n",
    "            \n",
    "            # save file\n",
    "            with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "                f.write(\"\\n# parameter requires grad shape #\\n\")\n",
    "                for p in self.model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        f.write(str(p.shape))\n",
    "            \n",
    "        else:\n",
    "            # else set unpruned flag to -1, then model won't be saved\n",
    "            self.log(f'unpruned_state', -1, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.model.get_everything(current_epoch='final_test')\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging, plot gradcam\n",
    "        # =============================================================================\n",
    "        if batch_idx < 2:\n",
    "            print(\"DECENT NOTE: test_step\", batch_idx)\n",
    "\n",
    "        # we update mo and gt here\n",
    "        self.run_loss_n_metrics(batch, mode=\"test\")\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.enable_grad():\n",
    "            grad_preds = preds.requires_grad_()\n",
    "            preds2 = self.layer2(grad_preds)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # save image\n",
    "        \n",
    "        if len(batch) == 4:\n",
    "            img, _, msk, img_id = batch\n",
    "        else:\n",
    "            img, _ = batch # image and mask come out of this\n",
    "        \n",
    "        img_id = img_id.detach().cpu().item()\n",
    "        \n",
    "        # print(img.shape)\n",
    "        \n",
    "        # save image\n",
    "        tmp_file_name = f'entry_id{batch_idx}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        # tmp_img = self.feature_maps.squeeze()[i_map].cpu().detach().numpy()\n",
    "        tmp_img = img.squeeze().cpu().detach().numpy()\n",
    "        tmp_path = os.path.join(self.log_dir, \"img_choice\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_img)\n",
    "        \n",
    "        \n",
    "        # save mask (todo)\n",
    "        tmp_file_name = f'entry_img{img_id}_id{batch_idx}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        msk = msk.detach().cpu().numpy().squeeze()\n",
    "        \n",
    "        \n",
    "        print(\"********** mask **********\")\n",
    "        print(msk)\n",
    "        print(msk.shape)\n",
    "        \n",
    "        dummy_mask = []\n",
    "        dummy_mask.append(np.random.randint(8, 11, 28).tolist())\n",
    "        dummy_mask.append(np.random.randint(12, 14, 28).tolist())\n",
    "        dummy_mask.append(np.random.randint(15, 18, 28).tolist())\n",
    "        dummy_mask.append(np.random.randint(19, 21, 28).tolist())\n",
    "        dummy_mask.append(np.random.randint(21, 23, 28).tolist())\n",
    "        dummy_mask = np.array(dummy_mask)\n",
    "        \n",
    "        print(\"********** dummy mask **********\")\n",
    "        print(dummy_mask)\n",
    "        print(dummy_mask.shape)\n",
    "        \n",
    "        plt.figure(figsize=(5, 5))\n",
    "        for o, boundary in enumerate(msk): # skip last one\n",
    "            # plt.plot(list(range(len(layer))), layer)\n",
    "            plt.plot(boundary[:,1]-0.5, boundary[:,0])\n",
    "        plt.ylim(0, 28 - 1)\n",
    "        plt.gca().invert_yaxis()\n",
    "        #plt.axis('off')\n",
    "        # Save the plot\n",
    "        # plt.savefig('plot_without_axes.png', bbox_inches='tight', pad_inches=0)\n",
    "        # tmp_msk = dummy_msak\n",
    "        tmp_path = os.path.join(self.log_dir, \"msk_choice\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.savefig(os.path.join(tmp_path, tmp_file_name), bbox_inches='tight', pad_inches=0)\n",
    "        #plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_msk) # todo\n",
    "        \n",
    "        # save image + mask (todo)\n",
    "        tmp_file_name = f'entry_img{img_id}_id{batch_idx}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        plt.imshow(tmp_img, cmap=\"gray\")\n",
    "        for o, boundary in enumerate(msk):\n",
    "            plt.plot(boundary[:,1]-0.5, boundary[:,0])\n",
    "        tmp_path = os.path.join(self.log_dir, \"img_with_msk\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.savefig(os.path.join(tmp_path, tmp_file_name), bbox_inches='tight', pad_inches=0)\n",
    "        \n",
    "        # plt.imsave(os.path.join(tmp_path, tmp_file_name), tmp_img)\n",
    "        \n",
    "        # save feature maps of hidden layers and the layer that gets globally pooled\n",
    "        try:\n",
    "            with torch.set_grad_enabled(True): # torch.set_grad_enabled(True):\n",
    "                self.run_xai_gradcam(batch, batch_idx, mode='explain')\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: batch size has to be 1\")\n",
    "            print(e)\n",
    "            \n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            layer = self.model.decent1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            #filter_list.extend(tmp)\n",
    "            \n",
    "            layer = self.model.decent2\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent2' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent3\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent3' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent1x1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1x1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "        # get filter list            \n",
    "        filter_list = []\n",
    "        for l in self.model.decent1.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{1}\")\n",
    "        for l in self.model.decent2.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{2}\")\n",
    "        for l in self.model.decent3.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{3}\")\n",
    "        for l in self.model.decent1x1.filter_list:\n",
    "            filter_list.append(f\"filter_{int(l.m_this)}_{int(l.n_this)}_{4}\")\n",
    "        df = pd.DataFrame(filter_list, columns=['filter'])\n",
    "        df.to_csv(os.path.join(self.log_dir, \"all_filters.csv\"), index=False)\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_test_epoch_end\", self.current_epoch)\n",
    "        pass\n",
    "    \n",
    "    def run_xai_feature_map(self, batch, batch_idx, layer, layer_str, device='cuda'):\n",
    "        # https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5\n",
    " \n",
    "        # img, label = testset.__getitem__(0) # batch x channel x width x height, class\n",
    "\n",
    "        # img = X(img.to(device).unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, mask, img_id = batch\n",
    "        else:\n",
    "            img, ground_truth = batch\n",
    "            \n",
    "        \n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        # print(img.data.shape)\n",
    "\n",
    "        # run feature map\n",
    "        # model, layer, layer_str, log_dir, device=\"cpu\"\n",
    "        fm = filter_activation.DecentFilterActivation(model=self.model, layer=layer, layer_str=layer_str, log_dir=self.log_dir, device=device)\n",
    "        fm.run(tmp_img, batch_idx)\n",
    "        \n",
    "        filter_list = fm.log()\n",
    "        \n",
    "        return filter_list\n",
    "        \n",
    "        \n",
    "    \n",
    "    def run_xai_gradcam(self, batch, batch_idx, mode='explain'):\n",
    "        # =============================================================================\n",
    "        # grad cam - or just cam?? idk\n",
    "        # todo error: RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n",
    "        # BATCH SIZE HAS TO BE ONE!!!\n",
    "        # grad enable in test mode:\n",
    "        # https://github.com/Project-MONAI/MONAI/discussions/1598\n",
    "        # https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "        # =============================================================================\n",
    "    \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, _, _ = batch\n",
    "        else:\n",
    "            img, ground_truth = batch\n",
    "\n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)]) # .requires_grad_()\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        #print(\"nooooooooooo grad, whyyyyy\")\n",
    "        #print(tmp_img1)\n",
    "        #print(img)\n",
    "\n",
    "        #print('b1', tmp_img1)\n",
    "        #print('b2', tmp_img2)\n",
    "\n",
    "        model_output = self(tmp_img1, mode)\n",
    "\n",
    "        #print('c1', tmp_img1)\n",
    "        #print('c2', tmp_img2)\n",
    "\n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "\n",
    "        # get prediction value\n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "\n",
    "        #print('d1', tmp_img1)\n",
    "\n",
    "        #print(\"mo\", model_output)\n",
    "        #print(\"max\", pred_max)\n",
    "        #print(\"backprop\", model_output[:, pred_max])\n",
    "\n",
    "        # backpropagate for gradient tracking\n",
    "        model_output[:, pred_max].backward()\n",
    "\n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        #print('e2', tmp_img2)\n",
    "\n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # draw the heatmap\n",
    "        # plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "        # fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_l2_plot[0])}_n{int(self.n_l2_plot[0])}_{str(current_epoch)}.png\"))\n",
    "        \n",
    "        tmp_path = os.path.join(self.log_dir, \"gradcam\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        plt.imsave(os.path.join(tmp_path, \n",
    "                                f\"cam_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\"\n",
    "                               ), heatmap.detach().cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "        heatmap *= 255.0 / heatmap.max()\n",
    "        pil_heatmap = Image.fromarray(heatmap.detach().cpu().numpy().squeeze()).convert('RGB')\n",
    "       \n",
    "        tmp_path = os.path.join(self.log_dir, \"gradcam\")\n",
    "        os.makedirs(tmp_path, exist_ok=True)\n",
    "        pil_heatmap.save(os.path.join(tmp_path, \n",
    "                                      f\"camgray_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" \n",
    "                                     )) \n",
    "            \n",
    "    def run_loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        # =============================================================================\n",
    "        # put image through model, calculate loss and metrics\n",
    "        # use cc term that has been calculated previously\n",
    "        # =============================================================================\n",
    "        \n",
    "        if len(batch) == 4:\n",
    "            img, ground_truth, mask, img_id = batch\n",
    "        else:\n",
    "            img, ground_truth = batch\n",
    "            \n",
    "        #print(img)\n",
    "        #print(ground_truth)\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # for test routine\n",
    "        self.mo = model_output.argmax(dim=1).squeeze().detach().cpu().numpy()\n",
    "        self.gt = ground_truth.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        print('self mo', self.mo)\n",
    "        print('self gt', self.gt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"gt\", ground_truth)\n",
    "        print(\"gt shape\", ground_truth.shape)\n",
    "        print(\"gt type\", ground_truth.type())\n",
    "        print(torch.zeros(ground_truth.size(0), self.n_classes))\n",
    "        \n",
    "        if len(ground_truth.shape) < 2:\n",
    "            ground_truth_tmp_tmp = ground_truth.unsqueeze(1)\n",
    "        else:\n",
    "            ground_truth = ground_truth.transpose(1, 0)\n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth_tmp.size(0), self.n_classes).scatter_(1, ground_truth_tmp.to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \"\"\"\n",
    "\n",
    "        ground_truth = ground_truth.squeeze()\n",
    "        if len(ground_truth.shape) < 1:\n",
    "            ground_truth = ground_truth.unsqueeze(0)\n",
    "        loss = self.criterion(model_output, ground_truth.long()) # ground_truth_multi_hot)\n",
    "        cc = torch.mean(self.model.cc) * self.cc_weight\n",
    "        \n",
    "        #print(\"loss\", loss)\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "        \n",
    "        try:\n",
    "            pass # print('pred i', pred_i.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics pred\")\n",
    "            print(e)\n",
    "        try:\n",
    "            pass # print('gt', ground_truth.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics gt\")\n",
    "            print(e)\n",
    "        \n",
    "        \n",
    "        if mode == \"train\":\n",
    "            try:\n",
    "                ta = self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "                tf = self.train_f1(preds=pred_i, target=ground_truth) \n",
    "                tp = self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            except Exception as e:\n",
    "                print(\"DECENT ERROR: we are experiencing this CUDA ERROR most likely, because our decent1x1 has too little filters.\")\n",
    "                print(\"We need the same number as classes. It can happen, that all in-connections to a filter in decent1x1 got pruned and hence it is gone.\")\n",
    "                print(\"preds\", pred_i)\n",
    "                print(\"target\", ground_truth)\n",
    "                print(e)\n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"train info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", ta)\n",
    "                print(\"f\", tf)\n",
    "                print(\"p\", tp)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        elif mode == \"val\":\n",
    "            va = self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            vf = self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            vp = self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"val info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", va)\n",
    "                print(\"f\", vf)\n",
    "                print(\"p\", vp)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        else:\n",
    "            print(pred_i)\n",
    "            print(ground_truth)\n",
    "            ta = self.test_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            tf = self.test_f1(preds=pred_i, target=ground_truth) \n",
    "            tp = self.test_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.test_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.test_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.test_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2227f9c-6913-40b6-a822-a3167b68fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc373f4-517d-4f3e-88fe-9699ecf2759b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2906-61b6-4f3a-b2fa-7aeeaa12e7e1",
   "metadata": {},
   "source": [
    "## run dev routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    \n",
    "    # =============================================================================\n",
    "    # train model and run test/xAI routine\n",
    "\n",
    "    # logger - save logs in \"examples/example_results/lightning_logs\"\n",
    "    # light - DecentLightning model\n",
    "    # trainer - pl.Trainer\n",
    "    # trainer.fit\n",
    "    # explainer - pl.Trainer\n",
    "    # explainer.test\n",
    "    # =============================================================================\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "    train_kwargs[\"load_ckpt_file\"] = \"\" # needed\n",
    "\n",
    "    # THE LOGGER\n",
    "    logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"])\n",
    "\n",
    "    # THE LIGHTNING MODEL\n",
    "    # Initialize the LightningModule\n",
    "    light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "    # THE LIGHTNING TRAINER (for training)\n",
    "    trainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         # inference_mode=False, # do grad manually\n",
    "                         log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                         logger=logger,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         max_epochs=train_kwargs[\"epochs\"],\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1\",\n",
    "                                                   filename='{epoch}-{val_f1:.2f}-{unpruned:.0f}'),\n",
    "                                    DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                                    filename='{epoch}-{unpruned:.0f}-{val_f1:.2f}'),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # THE TRAIN-RUN\n",
    "    # Train the model using a Trainer\n",
    "    trainer.fit(light, data.train_dataloader, data.val_dataloader)\n",
    "    \n",
    "\n",
    "    # THE LIGHTNING TRAINER (for testing)\n",
    "    # we want the grad to work in test, hence: inference_mode=False\n",
    "    explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         logger=logger,\n",
    "                         inference_mode=False)\n",
    "\n",
    "    # THE TEST-RUN\n",
    "    # including test\n",
    "    test_result = explainer.test(light, data.xai_dataloader, verbose=False)\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66262489-1c4a-439e-9673-32a40c5c52f1",
   "metadata": {},
   "source": [
    "## run test routine ****************************\n",
    "\n",
    "we need this with the OCTA-500 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f7aa2-96ed-4518-b129-5751bd5e39a8",
   "metadata": {},
   "source": [
    "torch.load(ckpt_path)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891e1fef-8dab-44fd-908d-e9f2a901dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT INFO: be aware, that you have to manually check, whether every output node has an input. otherwise an error may be triggered by cuda\n",
      "DECENT INFO: You are using checkpoint file:  examples/example_results\\lightning_logs\\debug_octmnist_no_fc\\version_7/checkpoints/epoch=0-val_f1=0.62-unpruned=1560.ckpt\n",
      "Found pretrained model at examples/example_results\\lightning_logs\\debug_octmnist_no_fc\\version_7/checkpoints/epoch=0-val_f1=0.62-unpruned=1560.ckpt, loading...\n",
      "DECENT INFO: dimensions are entry, decent1, decent2, decent3, decent1x1 == out [1, 4, 4, 4, 4]\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 1, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 4, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 8, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 8, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 8, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 8, 3, 3])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 16, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 16, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 16, 1, 1])\n",
      "DECENT INFO: ckpt_weight.shape torch.Size([1, 16, 1, 1])\n",
      "[10.]\n",
      "[5.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd11d333b2c448b921b2c91943b3c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: test_step 0\n",
      "self mo 3\n",
      "self gt 3\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([3], device='cuda:0')\n",
      "********** mask **********\n",
      "[[[ 4.62  0.  ]\n",
      "  [ 4.48  0.07]\n",
      "  [ 4.48  0.14]\n",
      "  ...\n",
      "  [ 4.9  27.79]\n",
      "  [ 5.04 27.86]\n",
      "  [ 5.04 27.93]]\n",
      "\n",
      " [[ 8.12  0.  ]\n",
      "  [ 7.98  0.07]\n",
      "  [ 8.12  0.14]\n",
      "  ...\n",
      "  [ 8.82 27.79]\n",
      "  [ 8.96 27.86]\n",
      "  [ 8.96 27.93]]\n",
      "\n",
      " [[10.64  0.  ]\n",
      "  [10.64  0.07]\n",
      "  [10.64  0.14]\n",
      "  ...\n",
      "  [11.06 27.79]\n",
      "  [11.2  27.86]\n",
      "  [11.06 27.93]]\n",
      "\n",
      " [[14.14  0.  ]\n",
      "  [14.14  0.07]\n",
      "  [14.14  0.14]\n",
      "  ...\n",
      "  [14.42 27.79]\n",
      "  [14.56 27.86]\n",
      "  [14.56 27.93]]\n",
      "\n",
      " [[16.52  0.  ]\n",
      "  [16.38  0.07]\n",
      "  [16.38  0.14]\n",
      "  ...\n",
      "  [16.24 27.79]\n",
      "  [16.38 27.86]\n",
      "  [16.38 27.93]]\n",
      "\n",
      " [[17.22  0.  ]\n",
      "  [17.08  0.07]\n",
      "  [17.08  0.14]\n",
      "  ...\n",
      "  [17.36 27.79]\n",
      "  [17.64 27.86]\n",
      "  [17.64 27.93]]]\n",
      "(6, 400, 2)\n",
      "********** dummy mask **********\n",
      "[[ 9 10  9 10  8  8 10 10  8 10 10  9  9 10 10 10  8  8  9  9  8  9  9  8\n",
      "   9  9 10  9]\n",
      " [13 13 13 12 12 13 13 12 13 12 12 12 12 13 12 13 13 13 12 13 13 13 13 13\n",
      "  13 13 13 13]\n",
      " [16 15 17 15 17 15 16 17 15 15 17 17 15 17 16 16 17 16 15 15 17 16 16 15\n",
      "  17 16 15 17]\n",
      " [19 19 19 19 20 19 20 20 19 20 19 19 19 19 20 19 19 20 20 20 20 19 19 20\n",
      "  19 20 19 20]\n",
      " [22 22 22 22 21 21 22 22 22 21 22 21 21 22 21 22 22 21 22 22 22 22 21 21\n",
      "  22 22 21 22]]\n",
      "(5, 28)\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 4, 24, 24])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "DECENT NOTE: on_test_epoch_end 0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "\n",
    "    # =============================================================================\n",
    "    # load model and run test/xAI routine\n",
    "\n",
    "    # logger - save logs in \"dumpster\"\n",
    "    # light - DecentLightning model\n",
    "    # explainer - pl.Trainer\n",
    "    # explainer.test\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"DECENT INFO: be aware, that you have to manually check, whether every output node has an input. otherwise an error may be triggered by cuda\")\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "\n",
    "    train_kwargs[\"load_ckpt_file\"] = \"version_7/checkpoints/epoch=0-val_f1=0.62-unpruned=1560.ckpt\"\n",
    "    \n",
    "    # Check whether pretrained model exists. If yes, load it.\n",
    "    # ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\\debug_oct_no_fc\", 'version_13', 'checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt'])\n",
    "    ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "    print(\"DECENT INFO: You are using checkpoint file: \", ckpt_path)\n",
    "    \n",
    "    if os.path.isfile(ckpt_path):\n",
    "\n",
    "        # THE LOGGER\n",
    "        logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "\n",
    "        # THE LIGHTNING MODEL\n",
    "        # load from checkpoint doesn't work, since our architecture is 'messed up' through pruning\n",
    "        # light = DecentLightning.load_from_checkpoint(state_dict, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n",
    "        # use this line instead:\n",
    "        light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "        # THE LIGHTNING TRAINER (for testing)\n",
    "        # we want the grad to work in test, hence: inference_mode=False\n",
    "        explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                             accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                             #devices=[0],\n",
    "                             logger=logger,\n",
    "                             inference_mode=False)\n",
    "\n",
    "        # THE TEST-RUN\n",
    "        # only test\n",
    "        test_result = explainer.test(light, data.xai_dataloader, verbose=False)\n",
    "\n",
    "    else:\n",
    "        print('DECENT ERROR: not a file - may have been resetted in dev routine, check the load_ckpt_file, set dev routine to False and run everything')\n",
    "        print('current load_ckpt_file is:', train_kwargs[\"load_ckpt_file\"])\n",
    "\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696de4a-76ea-4b95-9d11-e1cf8a733a42",
   "metadata": {},
   "source": [
    "# random nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecentNet(\n",
       "  (decent1): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([10.]), n_this=Parameter containing:\n",
       "      tensor([10.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([1.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([2.]), n_this=Parameter containing:\n",
       "      tensor([1.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([11.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "    )\n",
       "  )\n",
       "  (decent2): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([10.]), n_this=Parameter containing:\n",
       "      tensor([5.]))\n",
       "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([0.]))\n",
       "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([9.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
       "    )\n",
       "  )\n",
       "  (decent3): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([3.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, 0, 12, 14, 7, ns_in= 5, 0, 6, 7, 16, 6, 16, 8)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([17.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, 0, 12, 14, 7, ns_in= 5, 0, 6, 7, 16, 6, 16, 8)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([8.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, 0, 12, 14, 7, ns_in= 5, 0, 6, 7, 16, 6, 16, 8)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([0.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 10, 13, 13, 9, 0, 12, 14, 7, ns_in= 5, 0, 6, 7, 16, 6, 16, 8)\n",
       "    )\n",
       "  )\n",
       "  (decent1x1): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 16, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 0, 12, 5, 12, 14, 15, 14, 1, 11, 0, 12, 6, 0, ns_in= 3, 14, 8, 7, 12, 14, 14, 13, 8, 7, 11, 16, 13, 0, 2, 2)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 16, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([9.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 0, 12, 5, 12, 14, 15, 14, 1, 11, 0, 12, 6, 0, ns_in= 3, 14, 8, 7, 12, 14, 14, 13, 8, 7, 11, 16, 13, 0, 2, 2)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 16, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([8.]), n_this=Parameter containing:\n",
       "      tensor([16.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 0, 12, 5, 12, 14, 15, 14, 1, 11, 0, 12, 6, 0, ns_in= 3, 14, 8, 7, 12, 14, 14, 13, 8, 7, 11, 16, 13, 0, 2, 2)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 16, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([2.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 7, 17, 5, 0, 12, 5, 12, 14, 15, 14, 1, 11, 0, 12, 6, 0, ns_in= 3, 14, 8, 7, 12, 14, 14, 13, 8, 7, 11, 16, 13, 0, 2, 2)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (mish1): Mish()\n",
       "  (mish2): Mish()\n",
       "  (mish3): Mish()\n",
       "  (mish1x1): Mish()\n",
       "  (bias1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias2): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias3): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias1x1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 1.1904e-01,  4.2941e-02,  4.3113e-02],\n",
      "          [-6.6414e-02,  7.8661e-02,  1.0530e-01],\n",
      "          [-9.7719e-02, -9.8189e-02,  1.1255e-01]],\n",
      "\n",
      "         [[-4.9038e-02,  8.0040e-02, -3.0326e-02],\n",
      "          [ 8.2556e-02, -2.6775e-02, -5.4588e-02],\n",
      "          [-3.4844e-02, -8.8669e-02, -8.9346e-02]],\n",
      "\n",
      "         [[-2.1224e-02, -8.9769e-02, -1.0740e-01],\n",
      "          [-2.5721e-02,  8.1673e-05,  9.1710e-02],\n",
      "          [ 9.4190e-02, -7.8222e-02, -4.3863e-02]],\n",
      "\n",
      "         [[-6.5934e-02, -5.1868e-02, -1.0727e-01],\n",
      "          [ 1.1127e-01, -9.4682e-03, -8.8849e-02],\n",
      "          [-3.6629e-02,  8.3328e-02, -9.9786e-02]],\n",
      "\n",
      "         [[-1.1947e-01,  1.3452e-02, -6.2635e-02],\n",
      "          [-2.6653e-02,  6.4831e-02,  8.5248e-02],\n",
      "          [-5.2336e-02,  1.1670e-01, -2.2338e-02]],\n",
      "\n",
      "         [[-8.6815e-02, -7.7134e-02,  1.1293e-01],\n",
      "          [ 7.5371e-02,  1.5938e-02, -7.3930e-02],\n",
      "          [ 1.1225e-02,  7.2744e-02,  3.3453e-02]],\n",
      "\n",
      "         [[ 7.4076e-02,  9.4882e-02, -1.0749e-01],\n",
      "          [ 1.1937e-01,  3.5953e-02, -8.3216e-02],\n",
      "          [-2.0658e-03, -1.0886e-01, -3.0938e-02]],\n",
      "\n",
      "         [[ 9.2911e-02,  1.0526e-01,  8.9568e-02],\n",
      "          [-7.7529e-02, -2.8792e-02, -5.8904e-02],\n",
      "          [-1.1697e-01,  1.0308e-01, -1.7895e-03]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0091,  0.0773,  0.0806],\n",
      "          [-0.0613, -0.0382, -0.0731],\n",
      "          [ 0.0318, -0.0317,  0.0524]],\n",
      "\n",
      "         [[ 0.0741,  0.0982, -0.0508],\n",
      "          [-0.0468,  0.1043,  0.0617],\n",
      "          [ 0.0076, -0.0469, -0.1167]],\n",
      "\n",
      "         [[ 0.0957,  0.0586, -0.0263],\n",
      "          [-0.0955, -0.0437,  0.0207],\n",
      "          [-0.1108,  0.0251, -0.0155]],\n",
      "\n",
      "         [[ 0.0837, -0.0989,  0.0044],\n",
      "          [ 0.0317, -0.0769,  0.0043],\n",
      "          [ 0.0472,  0.1102, -0.0454]],\n",
      "\n",
      "         [[-0.0224,  0.0880,  0.0356],\n",
      "          [-0.1123, -0.0258,  0.0637],\n",
      "          [-0.0949,  0.0549, -0.0974]],\n",
      "\n",
      "         [[-0.0884,  0.0885,  0.0143],\n",
      "          [ 0.0522,  0.1053,  0.0667],\n",
      "          [-0.1077,  0.1058, -0.0967]],\n",
      "\n",
      "         [[-0.0637, -0.0943,  0.0281],\n",
      "          [-0.0538, -0.0865,  0.0360],\n",
      "          [-0.0419,  0.0526, -0.0307]],\n",
      "\n",
      "         [[ 0.1092, -0.1108,  0.0321],\n",
      "          [-0.0335, -0.0449,  0.0372],\n",
      "          [ 0.0845,  0.0582, -0.1296]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 6.2044e-02,  1.0510e-01, -9.7192e-02],\n",
      "          [-2.2387e-02, -5.1015e-02,  1.8102e-02],\n",
      "          [-7.1404e-02,  8.4338e-02,  5.9652e-02]],\n",
      "\n",
      "         [[ 4.4069e-02,  1.0682e-01,  2.8645e-02],\n",
      "          [-6.9703e-02,  7.0718e-02,  8.2435e-02],\n",
      "          [-4.6633e-02,  9.2354e-02,  1.8521e-02]],\n",
      "\n",
      "         [[-7.5223e-02, -9.7158e-02,  1.1849e-01],\n",
      "          [ 1.0875e-01, -6.5306e-02, -7.4562e-02],\n",
      "          [-4.4564e-02,  9.8188e-02, -1.1305e-01]],\n",
      "\n",
      "         [[ 9.2506e-02, -1.3159e-01,  6.0529e-02],\n",
      "          [ 7.9856e-02,  4.1147e-02, -5.5201e-02],\n",
      "          [-9.1036e-03, -1.1545e-04,  3.6474e-02]],\n",
      "\n",
      "         [[ 2.2274e-02, -1.1416e-03, -5.0352e-03],\n",
      "          [-3.9236e-02, -1.7631e-02,  6.5191e-03],\n",
      "          [ 7.5830e-03,  9.6639e-02,  1.0314e-01]],\n",
      "\n",
      "         [[-9.4013e-02, -3.2598e-02, -3.5932e-02],\n",
      "          [-1.7914e-02,  4.1833e-02, -1.4957e-02],\n",
      "          [ 9.9042e-02,  4.5147e-03,  3.5400e-03]],\n",
      "\n",
      "         [[-6.8404e-02,  5.7515e-02, -1.6448e-02],\n",
      "          [ 4.9231e-02, -9.1131e-02, -4.6363e-02],\n",
      "          [-3.9345e-02,  5.9259e-02,  1.1013e-01]],\n",
      "\n",
      "         [[ 1.0207e-01,  9.2882e-02,  8.0606e-02],\n",
      "          [ 6.6112e-02, -6.0244e-02, -1.0406e-01],\n",
      "          [-7.2117e-02,  3.4060e-02,  2.7056e-03]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0856, -0.0795,  0.1019],\n",
      "          [ 0.0331, -0.0970, -0.0346],\n",
      "          [ 0.0147, -0.1081,  0.1078]],\n",
      "\n",
      "         [[-0.0358,  0.0090,  0.0960],\n",
      "          [ 0.0293,  0.0940,  0.0173],\n",
      "          [ 0.0291, -0.0213,  0.0873]],\n",
      "\n",
      "         [[ 0.0123,  0.0334,  0.0648],\n",
      "          [ 0.0811,  0.0644,  0.1093],\n",
      "          [-0.0422,  0.0709, -0.0660]],\n",
      "\n",
      "         [[ 0.0463, -0.0088,  0.0331],\n",
      "          [ 0.0078, -0.0214, -0.1017],\n",
      "          [ 0.1114, -0.0457, -0.0317]],\n",
      "\n",
      "         [[-0.0632,  0.0885,  0.0519],\n",
      "          [ 0.0446,  0.0603,  0.0883],\n",
      "          [ 0.1045, -0.1136, -0.1127]],\n",
      "\n",
      "         [[-0.1104,  0.0407,  0.0701],\n",
      "          [ 0.0308,  0.0674,  0.0701],\n",
      "          [ 0.0471,  0.1037, -0.1070]],\n",
      "\n",
      "         [[ 0.1136, -0.1063, -0.0283],\n",
      "          [-0.0924, -0.0823, -0.0943],\n",
      "          [-0.0356, -0.0164,  0.0964]],\n",
      "\n",
      "         [[-0.0744,  0.1075,  0.0348],\n",
      "          [ 0.0942, -0.0281,  0.0962],\n",
      "          [-0.0675, -0.1026,  0.0321]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for a in light.model.decent3.filter_list:\n",
    "    print(a.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2797f-8a72-46db-86ae-41868e90828e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
