{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ùîªùïñùïîùïñùïüùï•‚Ñïùïñùï•: ùïïùïöùï§ùïñùïüùï•ùïíùïüùïòùïùùïñùïï ùïüùïñùï•\n",
    "\n",
    "Goal: create a sparse and modular ConvNet\n",
    "\n",
    "Todos: \n",
    "* [ ] delete node (filter) if either no input or no output edges\n",
    "* [ ] AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
    "* [ ] cuda error, if one of the decent1x1 has no kernels left - we need at least one input for each 1x1 filter\n",
    "* [ ] can we keep training if filter gets removed (e.g. at reloading model)\n",
    "* [ ] need some working filter removing in general - only at reload rn\n",
    "\n",
    "\n",
    "Notes:\n",
    "* additionally needed: position, activated channels, connection between channels\n",
    "* within this layer, a whole filter can be deactivated\n",
    "* within a filter, single channels can be deactivated\n",
    "* within this layer, filters can be swapped\n",
    "* the 'value' in the csv file is random if the CI metric is 'random'\n",
    "     \n",
    "* pruning actually doesn't work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf\n",
    "\n",
    "pip install:\n",
    "* pytorch_lightning\n",
    "\n",
    "\n",
    "warnings:\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73a26-f239-466f-901d-559d192f61de",
   "metadata": {},
   "source": [
    "![uml of code](examples/example_vis/uml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba244e3-3e6a-47e7-aa4b-a22df6054b8a",
   "metadata": {},
   "source": [
    "# conventions\n",
    "\n",
    "* entry image: entry_id5_0_0_0_mo3_gt2.png\n",
    "* hidden layer: hid_id5_3_8_2.png\n",
    "* last layer (global pooling - connected to class n): pool_2_3_4_gp2.png\n",
    "* activated image: cam_id5_mo3_gt2.png\n",
    "* activated image gray: camgray_id5_mo3_gt2.png\n",
    "\n",
    "\n",
    "* circle in: in_2_3_4_ep65.png\n",
    "* circle out: out_2_3_4_ep65.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# sys\n",
    "# =============================================================================\n",
    "import sys \n",
    "sys.path.insert(0, \"helper\")\n",
    "# =============================================================================\n",
    "# alphabetic order misc\n",
    "# =============================================================================\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "# =============================================================================\n",
    "# torch\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torchvision\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "# from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "# =============================================================================\n",
    "# datasceyence\n",
    "# =============================================================================\n",
    "from helper.model.decentnet import DecentNet\n",
    "from helper.visualisation import feature_map\n",
    "from helper.data.mnist import DataMNIST\n",
    "from helper.data.retinamnist import DataRetinaMNIST\n",
    "from helper.data.octmnist import DataOCTMNIST\n",
    "from helper.data.octa500 import DataOCTA500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397b450-c5c6-4da1-b630-7bf80e46891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch 2.0.0 == False\n",
      "tl 2.1.0 == False\n"
     ]
    }
   ],
   "source": [
    "seed = 1997 # was 19 before\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "debug_model = False\n",
    "\n",
    "print('torch 2.0.0 ==', torch.__version__=='2.0.0')\n",
    "print('tl 2.1.0 ==', pl.__version__=='2.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b0be-245d-49c0-88b4-d94167f7da90",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97670e82-0d27-4b7f-91df-874acf9974a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'result_path': 'examples/example_results', 'exp_name': 'debug_octmnist_no_fc', 'load_ckpt_file': 'version_18/checkpoints/epoch=4-unpruned=192-val_f1=0.12.ckpt', 'epochs': 3, 'img_size': 28, 'batch_size': 2, 'log_every_n_steps': 4, 'device': 'cuda', 'num_workers': 0, 'train_size': 8, 'val_size': 8, 'test_size': 8}\n",
      "model kwargs {'in_channels': 1, 'n_classes': None, 'out_dim': [1, 4, 8, 8], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 10, 'cc_metric': 'l2', 'ci_metric': 'l2', 'cm_metric': 'not implemented yet', 'update_every_nth_epoch': 1, 'pretrain_epochs': 1, 'prune_keep': 0.7, 'prune_keep_total': 0.4}\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {\n",
    "    'in_channels' : 1, # not in use yet\n",
    "    'n_classes': None, # filled in the dataset\n",
    "    'out_dim' :  [1, 4, 8, 8], # [1, 8, 16, 32], #[1, 16, 24, 32] # entry, decent1, decent2, decent3\n",
    "    'grid_size' : 18*18,\n",
    "    'criterion': torch.nn.CrossEntropyLoss(),# torch.nn.BCEWithLogitsLoss(),\n",
    "    'optimizer': \"sgd\", # sgd adamw\n",
    "    'base_lr': 0.001,\n",
    "    'min_lr' : 0.00001,\n",
    "    'momentum' : 0.9,\n",
    "    'lr_update' : 100,\n",
    "    # decentnet\n",
    "    'cc_weight': 10,\n",
    "    'cc_metric' : 'l2', # connection cost metric (for loss) - distance metric\n",
    "    'ci_metric' : 'l2', # channel importance metric (for pruning)\n",
    "    'cm_metric' : 'not implemented yet', # 'count', # crossing minimisation \n",
    "    'update_every_nth_epoch' : 1, # 5\n",
    "    'pretrain_epochs' : 1, # 20\n",
    "    'prune_keep' : 0.7, # 0.97, # in each epoch\n",
    "    'prune_keep_total' : 0.4, # this number is not exact, depends on the prune_keep value\n",
    "}\n",
    "\n",
    "train_kwargs = {\n",
    "    'result_path': \"examples/example_results\", # \"example_results/lightning_logs\", # not in use??\n",
    "    'exp_name': \"debug_octmnist_no_fc\", # must include dataset name, otherwise mnist is used\n",
    "    'load_ckpt_file' : 'version_18/checkpoints/epoch=4-unpruned=192-val_f1=0.12.ckpt', # \"version_0/checkpoints/epoch=94-unpruned=1600-val_f1=0.67.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "    'epochs': 3, # including the pretrain epochs - no adding up\n",
    "    'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ... # MNIST/MedMNIST 28 √ó 28 Pixel\n",
    "    'batch_size': 2, # 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "    'log_every_n_steps' : 4, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "    'device': \"cuda\",\n",
    "    'num_workers' : 0, # 18, # 18 for computer, 0 for laptop\n",
    "    'train_size' : (2 * 4), # total or percentage\n",
    "    'val_size' : (2 * 4), # total or percentage\n",
    "    'test_size' : 8, # total or percentage - 0 for all\n",
    "}\n",
    "\n",
    "print(\"train kwargs\", train_kwargs)\n",
    "print(\"model kwargs\", model_kwargs)\n",
    "\n",
    "kwargs = {'train_kwargs':train_kwargs, 'model_kwargs':model_kwargs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ee3d0-9c79-4917-a355-093f1daf4855",
   "metadata": {},
   "source": [
    "## check the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d5ff36-c015-4bd6-8b12-c4d0e3b64b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4200\n"
     ]
    }
   ],
   "source": [
    "breaking = 6000*model_kwargs['prune_keep_total']\n",
    "weights = 6000 # this value is an estimate for a model [1, 8, 16, 32]\n",
    "# 'unpruned' is the logger variable for the value\n",
    "\n",
    "for i in range(train_kwargs['epochs']):\n",
    "    \n",
    "    if (weights < breaking): # weights*model_kwargs['prune_keep']\n",
    "        print(\"stop:\", breaking)\n",
    "        print('you need at least this many epochs:', i)\n",
    "        print('you currently have this many epochs:', train_kwargs['epochs'])\n",
    "        print(\"recommended to add 2*update_every_nth_epoch\")\n",
    "        break\n",
    "    \n",
    "    # not sure whether -1 is correct, have to check\n",
    "    if i > model_kwargs['pretrain_epochs'] and ((i-1)%model_kwargs['update_every_nth_epoch'] == 0):\n",
    "        weights = int(weights*model_kwargs['prune_keep'])\n",
    "    \n",
    "        print(i, weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b3283-2825-4f35-9716-78ccf5bb8b1c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "* the dataset name needs to be part of the experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "python_class : OCTMNIST\n",
      "description : The OCTMNIST is based on a prior dataset of 109,309 valid optical coherence tomography (OCT) images for retinal diseases. The dataset is comprised of 4 diagnosis categories, leading to a multi-class classification task. We split the source training set with a ratio of 9:1 into training and validation set, and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384‚àí1,536)√ó(277‚àí512). We center-crop the images and resize them into 1√ó28√ó28.\n",
      "url : https://zenodo.org/record/6496656/files/octmnist.npz?download=1\n",
      "MD5 : c68d92d5b585d8d81f7112f81e2d0842\n",
      "task : multi-class\n",
      "label : {'0': 'choroidal neovascularization', '1': 'diabetic macular edema', '2': 'drusen', '3': 'normal'}\n",
      "n_channels : 1\n",
      "n_samples : {'train': 97477, 'val': 10832, 'test': 1000}\n",
      "license : CC BY 4.0\n",
      "n_classes: 4\n"
     ]
    }
   ],
   "source": [
    "if 'octmnist' in train_kwargs['exp_name']:\n",
    "    # OCTMINST\n",
    "    data = DataOCTMNIST(train_kwargs, model_kwargs)   \n",
    "elif 'retinamnist' in train_kwargs['exp_name']:\n",
    "    # RetinaMNIST\n",
    "    data = DataRetinaMNIST(train_kwargs, model_kwargs)\n",
    "elif 'octa500' in train_kwargs['exp_name']:\n",
    "    # OCTA-500\n",
    "    data = DataOCT500(train_kwargs, model_kwargs)\n",
    "else:\n",
    "    # MNIST\n",
    "    data = DataMNIST(train_kwargs, model_kwargs)\n",
    "\n",
    "assert model_kwargs['n_classes'] != None, \"DECENT ERROR: make sure you set the n_classes with the dataset\"  \n",
    "print(\"n_classes:\", model_kwargs['n_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X (Datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # an object with image representations and their positions\n",
    "    # amout of channels need to have same length as m and n lists\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        self.data = data # list of tensors (image representations)\n",
    "        self.ms_x = ms_x # list of integers (m position of each image representation)\n",
    "        self.ns_x = ns_x # list of integers (n position of each image representation)\n",
    "                \n",
    "    def setter(self, data, ms_x, ns_x):\n",
    "        self.data = data\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        \n",
    "    def getter(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint\n",
    "        # Save a checkpoint at the end of the training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "            print(\"DECENT NOTE: callback on_train_epoch_end\", monitor_candidates[\"epoch\"].item())\n",
    "            if monitor_candidates[\"epoch\"] > 0:\n",
    "                if monitor_candidates[\"unpruned_state\"] != -1:\n",
    "                    print(\"DECENT NOTE: save model\", monitor_candidates[\"epoch\"].item())\n",
    "                    if self._every_n_epochs >= 1 and ((trainer.current_epoch + 1) % self._every_n_epochs) == 0:\n",
    "                        self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                    self._save_last_checkpoint(trainer, monitor_candidates)\n",
    "                    \n",
    "                    pl_module.model.get_everything(current_epoch=trainer.current_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "## LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # Lightning Module consists of functions that define the training routine\n",
    "    # train, val, test: before epoch, step, after epoch, ...\n",
    "    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n",
    "    # order for the instance methods:\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\n",
    "    # \n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, kwargs, log_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        if train_kwargs[\"load_ckpt_file\"] != '':\n",
    "            self.ckpt_path = os.path.join(log_dir, train_kwargs[\"load_ckpt_file\"])\n",
    "            if os.path.isfile(ckpt_path):\n",
    "                print(f\"Found pretrained model at {ckpt_path}, loading...\")\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir, ckpt_path=ckpt_path).to(\"cuda\")\n",
    "            else:\n",
    "                # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "                self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        else:\n",
    "            # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "            self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "            \n",
    "        # print(self.model)\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        self.pretrain_epochs = model_kwargs[\"pretrain_epochs\"]\n",
    "        \n",
    "        # needed for hparams.yaml file\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if False:\n",
    "            self.metric = { \"train_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"train_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"val_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                         \"val_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "                       }\n",
    "        else:\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            \n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            \n",
    "            self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.test_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "\n",
    "            \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        # =============================================================================\n",
    "        # we make it possible to use model_output = self(image)\n",
    "        # =============================================================================\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # =============================================================================\n",
    "        # returns:\n",
    "        #    optimiser and lr scheduler\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimiser = optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        else:\n",
    "            optimiser = optim.SGD(self.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        # initial plot of circular layer\n",
    "        # updates model every nth epoch\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: on_train_epoch_start\", self.current_epoch)\n",
    "        \n",
    "        # plot random layer (the circular plot)\n",
    "        if self.current_epoch == 0:\n",
    "            self.model.plot_incoming_connections(current_epoch=0)\n",
    "            self.model.plot_outgoing_connections(current_epoch=0)\n",
    "\n",
    "        # update model\n",
    "         # don't update unless pretrain epochs is reached\n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch >= self.pretrain_epochs:\n",
    "            print(\"DECENT NOTE: update model\", self.current_epoch)        \n",
    "            if debug_model:\n",
    "                print(\"DECENT NOTE: before update\")\n",
    "                print(\"DECENT NOTE: print model ...\")\n",
    "                print(self.model)\n",
    "            self.model.update(current_epoch = self.current_epoch)\n",
    "            if True: \n",
    "                print(\"DECENT NOTE: after update\")\n",
    "                print(\"DECENT NOTE: print model ...\")\n",
    "                print(self.model)\n",
    "                \n",
    "            print(\"DECENT NOTE: model updated\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculates loss for a batch # 1\n",
    "        # parameters:\n",
    "        #    batch\n",
    "        #    batch id\n",
    "        # returns:\n",
    "        #    loss\n",
    "        # notes:\n",
    "        #    calling gradcam like self.gradcam(batch) is dangerous cause changes gradients\n",
    "        # =============================================================================     \n",
    "        if False: # batch_idx < 2: # print first two steps\n",
    "            print(\"DECENT NOTE: training_step\", batch_idx)\n",
    "\n",
    "        # calculate loss\n",
    "        # loss = torch.tensor(1)\n",
    "        loss = self.run_loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging # 2\n",
    "        # =============================================================================\n",
    "        if False: # batch_idx < 2:\n",
    "            print(\"DECENT NOTE: validation_step\", batch_idx)\n",
    "        \n",
    "        self.run_loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing # 3\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_validation_epoch_end\")\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # save model if next iteration model is pruned # 4 \n",
    "        # this needs to be called before callback \n",
    "        # - if internal pytorch lightning convention changes, this will stop working\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_train_epoch_end\", self.current_epoch)\n",
    "               \n",
    "        if False:\n",
    "            print(\"current epoch\")\n",
    "            print(((self.current_epoch+1) % self.update_every_nth_epoch) == 0)\n",
    "            print(self.current_epoch+1)\n",
    "            print(self.current_epoch)\n",
    "            print(self.update_every_nth_epoch)\n",
    "        \n",
    "        # numel: returns the total number of elements in the input tensor\n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.log(f'unpruned', unpruned, on_step=False, on_epoch=True) \n",
    "        \n",
    "        if ((self.current_epoch+1) % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            # if next epoch is an update, set unpruned flag            \n",
    "            self.log(f'unpruned_state', 1, on_step=False, on_epoch=True)\n",
    "            \n",
    "            # save file\n",
    "            with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "                f.write(\"\\n# parameter requires grad shape #\\n\")\n",
    "                for p in self.model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        f.write(str(p.shape))\n",
    "            \n",
    "        else:\n",
    "            # else set unpruned flag to -1, then model won't be saved\n",
    "            self.log(f'unpruned_state', -1, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.model.get_everything(current_epoch='final_test')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging, plot gradcam\n",
    "        # =============================================================================\n",
    "        if batch_idx < 2:\n",
    "            print(\"DECENT NOTE: test_step\", batch_idx)\n",
    "\n",
    "        self.run_loss_n_metrics(batch, mode=\"test\")\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.enable_grad():\n",
    "            grad_preds = preds.requires_grad_()\n",
    "            preds2 = self.layer2(grad_preds)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # save image\n",
    "        \n",
    "        img, _ = batch\n",
    "        \n",
    "        print(img.shape)\n",
    "        \n",
    "        tmp_file_name = f'entry_id{batch_idx}_{0}_{0}_{0}_mo{self.mo}_gt{self.gt}.png'\n",
    "        # tmp_img = self.feature_maps.squeeze()[i_map].cpu().detach().numpy()\n",
    "        \n",
    "        tmp_img = img.squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        plt.imsave(os.path.join(self.log_dir, tmp_file_name), tmp_img)\n",
    "        \n",
    "        \n",
    "        # save feature maps of hidden layers and the layer that gets globally pooled\n",
    "        try:\n",
    "            with torch.set_grad_enabled(True): # torch.set_grad_enabled(True):\n",
    "                self.run_xai_gradcam(batch, batch_idx, mode='explain')\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: batch size has to be 1\")\n",
    "            print(e)\n",
    "            \n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            layer = self.model.decent1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent2\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent2' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent3\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent3' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent1x1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1x1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_test_epoch_end\", self.current_epoch)\n",
    "        pass\n",
    "    \n",
    "    def run_xai_feature_map(self, batch, batch_idx, layer, layer_str, device='cuda'):\n",
    "        # https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5\n",
    " \n",
    "        # img, label = testset.__getitem__(0) # batch x channel x width x height, class\n",
    "\n",
    "        # img = X(img.to(device).unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        \n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        # print(img.data.shape)\n",
    "\n",
    "        # run feature map\n",
    "        # model, layer, layer_str, log_dir, device=\"cpu\"\n",
    "        fm = feature_map.DecentFeatureMap(model=self.model, layer=layer, layer_str=layer_str, log_dir=self.log_dir, device=device)\n",
    "        fm.run(tmp_img, batch_idx)\n",
    "        fm.log()\n",
    "    \n",
    "    def run_xai_gradcam(self, batch, batch_idx, mode='explain'):\n",
    "        # =============================================================================\n",
    "        # grad cam - or just cam?? idk\n",
    "        # todo error: RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n",
    "        # BATCH SIZE HAS TO BE ONE!!!\n",
    "        # grad enable in test mode:\n",
    "        # https://github.com/Project-MONAI/MONAI/discussions/1598\n",
    "        # https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "        # =============================================================================\n",
    "    \n",
    "        img, ground_truth = batch\n",
    "\n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)]) # .requires_grad_()\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        #print(\"nooooooooooo grad, whyyyyy\")\n",
    "        #print(tmp_img1)\n",
    "        #print(img)\n",
    "\n",
    "        #print('b1', tmp_img1)\n",
    "        #print('b2', tmp_img2)\n",
    "\n",
    "        model_output = self(tmp_img1, mode)\n",
    "\n",
    "        #print('c1', tmp_img1)\n",
    "        #print('c2', tmp_img2)\n",
    "\n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "\n",
    "        # get prediction value\n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "\n",
    "        #print('d1', tmp_img1)\n",
    "\n",
    "        #print(\"mo\", model_output)\n",
    "        #print(\"max\", pred_max)\n",
    "        #print(\"backprop\", model_output[:, pred_max])\n",
    "\n",
    "        # backpropagate for gradient tracking\n",
    "        model_output[:, pred_max].backward()\n",
    "\n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        #print('e2', tmp_img2)\n",
    "\n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # draw the heatmap\n",
    "        # plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "        # fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_l2_plot[0])}_n{int(self.n_l2_plot[0])}_{str(current_epoch)}.png\"))\n",
    "\n",
    "        plt.imsave(os.path.join( self.log_dir, f\"cam_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" ), heatmap.detach().cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "        heatmap *= 255.0 / heatmap.max()\n",
    "        pil_heatmap = Image.fromarray(heatmap.detach().cpu().numpy().squeeze()).convert('RGB')\n",
    "        pil_heatmap.save(os.path.join( self.log_dir, f\"camgray_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" ) ) \n",
    "            \n",
    "    def run_loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        # =============================================================================\n",
    "        # put image through model, calculate loss and metrics\n",
    "        # use cc term that has been calculated previously\n",
    "        # =============================================================================\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # for test routine\n",
    "        self.mo = model_output.argmax(dim=1).squeeze().detach().cpu().numpy()\n",
    "        self.gt = ground_truth.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        print('self mo', self.mo)\n",
    "        print('self gt', self.gt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"gt\", ground_truth)\n",
    "        print(\"gt shape\", ground_truth.shape)\n",
    "        print(\"gt type\", ground_truth.type())\n",
    "        print(torch.zeros(ground_truth.size(0), self.n_classes))\n",
    "        \n",
    "        if len(ground_truth.shape) < 2:\n",
    "            ground_truth_tmp_tmp = ground_truth.unsqueeze(1)\n",
    "        else:\n",
    "            ground_truth = ground_truth.transpose(1, 0)\n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth_tmp.size(0), self.n_classes).scatter_(1, ground_truth_tmp.to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \"\"\"\n",
    "\n",
    "        ground_truth = ground_truth.squeeze()\n",
    "        if len(ground_truth.shape) < 1:\n",
    "            ground_truth = ground_truth.unsqueeze(0)\n",
    "        loss = self.criterion(model_output, ground_truth.long()) # ground_truth_multi_hot)\n",
    "        cc = torch.mean(self.model.cc) * self.cc_weight\n",
    "        \n",
    "        #print(\"loss\", loss)\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "        \n",
    "        try:\n",
    "            pass # print('pred i', pred_i.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics pred\")\n",
    "            print(e)\n",
    "        try:\n",
    "            pass # print('gt', ground_truth.squeeze().detach().cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(\"DECENT EXCEPTION: loss n metrics gt\")\n",
    "            print(e)\n",
    "        \n",
    "        \n",
    "        if mode == \"train\":\n",
    "            try:\n",
    "                ta = self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "                tf = self.train_f1(preds=pred_i, target=ground_truth) \n",
    "                tp = self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            except Exception as e:\n",
    "                print(\"DECENT ERROR: we are experiencing this CUDA ERROR most likely, because our decent1x1 has too little filters. We need the same number as classes. It can happen, that all in-connections to a filter in decent1x1 got pruned and hence it is gone\")\n",
    "                print(\"preds\", pred_i)\n",
    "                print(\"target\", ground_truth)\n",
    "                print(e)\n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"train info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", ta)\n",
    "                print(\"f\", tf)\n",
    "                print(\"p\", tp)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        elif mode == \"val\":\n",
    "            va = self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            vf = self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            vp = self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"val info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", va)\n",
    "                print(\"f\", vf)\n",
    "                print(\"p\", vp)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        else:\n",
    "            print(pred_i)\n",
    "            print(ground_truth)\n",
    "            ta = self.test_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            tf = self.test_f1(preds=pred_i, target=ground_truth) \n",
    "            tp = self.test_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.test_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.test_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.test_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2906-61b6-4f3a-b2fa-7aeeaa12e7e1",
   "metadata": {},
   "source": [
    "## run dev routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT INFO: dimensions are entry, decent1, decent2, decent3, decent1x1 == out [1, 4, 8, 8, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name       | Type                | Params\n",
      "----------------------------------------------------\n",
      "0  | model      | DecentNet           | 1.3 K \n",
      "1  | criterion  | CrossEntropyLoss    | 0     \n",
      "2  | train_acc  | MulticlassAccuracy  | 0     \n",
      "3  | train_f1   | MulticlassF1Score   | 0     \n",
      "4  | train_prec | MulticlassPrecision | 0     \n",
      "5  | val_acc    | MulticlassAccuracy  | 0     \n",
      "6  | val_f1     | MulticlassF1Score   | 0     \n",
      "7  | val_prec   | MulticlassPrecision | 0     \n",
      "8  | test_acc   | MulticlassAccuracy  | 0     \n",
      "9  | test_f1    | MulticlassF1Score   | 0     \n",
      "10 | test_prec  | MulticlassPrecision | 0     \n",
      "----------------------------------------------------\n",
      "952       Trainable params\n",
      "312       Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.]\n",
      "[5.]\n",
      "DECENT NOTE: configure_optimizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self mo [2 1]\n",
      "self gt [3 0]\n",
      "self mo [1 1]\n",
      "self gt [3 3]\n",
      "DECENT NOTE: on_validation_epoch_end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4402e48c3a044b689b5f5d8e4c54adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: on_train_epoch_start 0\n",
      "self mo [1 2]\n",
      "self gt [1 3]\n",
      "self mo [1 1]\n",
      "self gt [3 0]\n",
      "self mo [2 2]\n",
      "self gt [0 3]\n",
      "self mo [0 2]\n",
      "self gt [3 0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self mo [1 0]\n",
      "self gt [3 0]\n",
      "self mo [2 1]\n",
      "self gt [3 3]\n",
      "self mo [3 1]\n",
      "self gt [0 1]\n",
      "self mo [0 1]\n",
      "self gt [3 0]\n",
      "DECENT NOTE: on_validation_epoch_end\n",
      "DECENT NOTE: on_train_epoch_end 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: UserWarning: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: UserWarning: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: callback on_train_epoch_end 0\n",
      "DECENT NOTE: on_train_epoch_start 1\n",
      "DECENT NOTE: update model 1\n",
      "DECENT INFO: filter list length:  8 -> 8\n",
      "DECENT INFO: filter list length:  8 -> 8\n",
      "DECENT INFO: filter list length:  4 -> 4\n",
      "DECENT NOTE: after update\n",
      "DECENT NOTE: print model ...\n",
      "DecentNet(\n",
      "  (decent1): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([10.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([10.], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([1.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([6.], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([2.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([1.], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([13.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([11.], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "    )\n",
      "  )\n",
      "  (decent2): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([10.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([5.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, ns_in= 10)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([13.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([0.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 2, ns_in= 10, 1)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([13.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([6.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 1, 2, ns_in= 10, 6, 1)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([9.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 2, 13, ns_in= 6, 1, 11)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([0.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([16.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([12.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([6.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 13, ns_in= 10, 11)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([16.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 1, 2, 13, ns_in= 10, 6, 1, 11)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([8.], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 2, 13, ns_in= 6, 1, 11)\n",
      "    )\n",
      "  )\n",
      "  (decent3): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 7, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([3.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 13, 9, 0, 12, 14, 7, ns_in= 5, 6, 7, 16, 6, 16, 8)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 6, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([17.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'))\n",
      "       with inputs: ms_in= 13, 13, 9, 0, 12, 7, ns_in= 0, 6, 7, 16, 6, 8)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([5.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([8.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 13, 13, 9, 7, ns_in= 5, 0, 6, 7, 8)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([0.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 0, 12, 14, 7, ns_in= 5, 16, 6, 16, 8)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 6, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([12.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([12.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 13, 13, 9, 0, 7, ns_in= 5, 0, 6, 7, 16, 8)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 7, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([5.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 13, 13, 9, 12, 14, 7, ns_in= 5, 0, 6, 7, 6, 16, 8)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([12.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 13, 9, 12, 7, ns_in= 5, 6, 7, 6, 8)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([13.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 9, 7, ns_in= 5, 7, 8)\n",
      "    )\n",
      "  )\n",
      "  (decent1x1): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 6, 1, 1]) at position: m_this=Parameter containing:\n",
      "      tensor([5.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'))\n",
      "       with inputs: ms_in= 17, 0, 12, 5, 12, 14, ns_in= 14, 7, 12, 14, 14, 13)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 7, 1, 1]) at position: m_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([9.], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 17, 5, 0, 12, 5, 12, ns_in= 3, 14, 8, 7, 12, 14, 14)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 5, 1, 1]) at position: m_this=Parameter containing:\n",
      "      tensor([8.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([16.], device='cuda:0'))\n",
      "       with inputs: ms_in= 17, 12, 5, 12, 14, ns_in= 14, 12, 14, 14, 13)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
      "      tensor([2.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 0, 5, 14, ns_in= 3, 7, 14, 13)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (mish1): Mish()\n",
      "  (mish2): Mish()\n",
      "  (mish3): Mish()\n",
      "  (mish1x1): Mish()\n",
      "  (bias1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias2): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias3): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias1x1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      ")\n",
      "DECENT NOTE: model updated\n",
      "self mo [1 2]\n",
      "self gt [3 3]\n",
      "self mo [2 1]\n",
      "self gt [0 3]\n",
      "self mo [1 0]\n",
      "self gt [1 3]\n",
      "self mo [1 2]\n",
      "self gt [0 0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self mo [0 0]\n",
      "self gt [3 0]\n",
      "self mo [2 2]\n",
      "self gt [3 3]\n",
      "self mo [1 0]\n",
      "self gt [0 1]\n",
      "\n",
      "val info at random intervals\n",
      "p tensor([1, 0], device='cuda:0')\n",
      "g tensor([0, 1], device='cuda:0', dtype=torch.int32)\n",
      "a tensor(0., device='cuda:0')\n",
      "f tensor(0., device='cuda:0')\n",
      "p tensor(0., device='cuda:0')\n",
      "l tensor(1.2416, device='cuda:0')\n",
      "self mo [0 1]\n",
      "self gt [3 0]\n",
      "DECENT NOTE: on_validation_epoch_end\n",
      "DECENT NOTE: on_train_epoch_end 1\n",
      "DECENT NOTE: callback on_train_epoch_end 1\n",
      "DECENT NOTE: save model 1\n",
      "DECENT NOTE: on_train_epoch_start 2\n",
      "DECENT NOTE: update model 2\n",
      "DECENT INFO: filter list length:  8 -> 8\n",
      "DECENT INFO: filter list length:  8 -> 8\n",
      "DECENT INFO: filter list length:  4 -> 4\n",
      "DECENT NOTE: after update\n",
      "DECENT NOTE: print model ...\n",
      "DecentNet(\n",
      "  (decent1): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([10.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([10.], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([1.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([6.], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([2.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([1.], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([13.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([11.], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "    )\n",
      "  )\n",
      "  (decent2): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([10.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([5.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, ns_in= 10)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([13.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([0.], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, ns_in= 1)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([13.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([6.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 1, ns_in= 10, 6)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([9.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 2, 13, ns_in= 6, 1, 11)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([0.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([16.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 2, ns_in= 10, 1)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([12.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([6.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 13, ns_in= 10, 11)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([16.], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 13, ns_in= 1, 11)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([8.], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 13, ns_in= 1, 11)\n",
      "    )\n",
      "  )\n",
      "  (decent3): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([3.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 9, 0, 14, 7, ns_in= 5, 7, 16, 16, 8)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([17.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'))\n",
      "       with inputs: ms_in= 13, 12, 7, ns_in= 0, 6, 8)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([5.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([8.], device='cuda:0'))\n",
      "       with inputs: ms_in= 13, 7, ns_in= 6, 8)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([0.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 0, 12, 14, 7, ns_in= 5, 16, 6, 16, 8)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([12.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([12.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 9, 0, ns_in= 5, 7, 16)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([5.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 13, 9, 12, 7, ns_in= 5, 6, 7, 6, 8)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([12.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 13, 12, 7, ns_in= 5, 6, 6, 8)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([13.], device='cuda:0'))\n",
      "       with inputs: ms_in= 10, 9, 7, ns_in= 5, 7, 8)\n",
      "    )\n",
      "  )\n",
      "  (decent1x1): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 3, 1, 1]) at position: m_this=Parameter containing:\n",
      "      tensor([5.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([14.], device='cuda:0'))\n",
      "       with inputs: ms_in= 12, 12, 14, ns_in= 12, 14, 13)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([9.], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 5, 12, 12, ns_in= 3, 8, 12, 14)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
      "      tensor([8.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([16.], device='cuda:0'))\n",
      "       with inputs: ms_in= 12, 5, 12, 14, ns_in= 12, 14, 14, 13)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
      "      tensor([2.], device='cuda:0'), n_this=Parameter containing:\n",
      "      tensor([7.], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 0, 5, 14, ns_in= 3, 7, 14, 13)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (mish1): Mish()\n",
      "  (mish2): Mish()\n",
      "  (mish3): Mish()\n",
      "  (mish1x1): Mish()\n",
      "  (bias1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias2): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias3): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias1x1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      ")\n",
      "DECENT NOTE: model updated\n",
      "self mo [1 0]\n",
      "self gt [3 3]\n",
      "self mo [1 0]\n",
      "self gt [0 3]\n",
      "self mo [1 1]\n",
      "self gt [0 1]\n",
      "self mo [2 1]\n",
      "self gt [3 0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self mo [1 1]\n",
      "self gt [3 0]\n",
      "self mo [2 1]\n",
      "self gt [3 3]\n",
      "self mo [1 1]\n",
      "self gt [0 1]\n",
      "self mo [0 1]\n",
      "self gt [3 0]\n",
      "DECENT NOTE: on_validation_epoch_end\n",
      "DECENT NOTE: on_train_epoch_end 2\n",
      "DECENT NOTE: callback on_train_epoch_end 2\n",
      "DECENT NOTE: save model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171cc4535b284ab2a0bc4169f0691829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: test_step 0\n",
      "self mo 2\n",
      "self gt 3\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 8, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "DECENT NOTE: test_step 1\n",
      "self mo 1\n",
      "self gt 2\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([2], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 8, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 3\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 8, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 3\n",
      "self gt 3\n",
      "tensor([3], device='cuda:0')\n",
      "tensor([3], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 8, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 1\n",
      "self gt 0\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 8, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 1\n",
      "self gt 2\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([2], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 8, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 0\n",
      "self gt 1\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([1], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 8, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "self mo 1\n",
      "self gt 0\n",
      "tensor([1], device='cuda:0')\n",
      "tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 1, 28, 28])\n",
      "feature map shape torch.Size([1, 4, 26, 26])\n",
      "feature map shape torch.Size([1, 8, 24, 24])\n",
      "feature map shape torch.Size([1, 8, 22, 22])\n",
      "feature map shape torch.Size([1, 4, 22, 22])\n",
      "DECENT NOTE: on_test_epoch_end 0\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    \n",
    "    train_kwargs['load_ckpt_file'] = \"\"\n",
    "    \n",
    "    # =============================================================================\n",
    "    # this is the main function, run this cell!!!\n",
    "\n",
    "    # dataset\n",
    "    # logger\n",
    "    # trainer\n",
    "    # trainer.fit\n",
    "    # trainer.test\n",
    "    # =============================================================================\n",
    "\n",
    "    # \"examples/example_results/lightning_logs\"\n",
    "    logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"])\n",
    "    trainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         # inference_mode=False, # do grad manually\n",
    "                         log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                         logger=logger,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         max_epochs=train_kwargs[\"epochs\"],\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1\",\n",
    "                                                   filename='{epoch}-{val_f1:.2f}-{unpruned:.0f}'),\n",
    "                                    DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                                    filename='{epoch}-{unpruned:.0f}-{val_f1:.2f}'),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "\n",
    "    # Initialize the LightningModule and LightningDataModule\n",
    "    light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "    # Train the model using a Trainer\n",
    "    trainer.fit(light, data.train_dataloader, data.val_dataloader)\n",
    "\n",
    "    # we don't save the positions here ...\n",
    "    # light = DecentLightning.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, kwargs=kwargs) # Load best checkpoint after training\n",
    "\n",
    "    # we want the grad to work in test, hence: inference_mode=False\n",
    "    # logger_x = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "    explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         logger=logger,\n",
    "                         inference_mode=False)\n",
    "\n",
    "    test_result = explainer.test(light, data.xai_dataloader, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66262489-1c4a-439e-9673-32a40c5c52f1",
   "metadata": {},
   "source": [
    "## run test routine ****************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f7aa2-96ed-4518-b129-5751bd5e39a8",
   "metadata": {},
   "source": [
    "torch.load(ckpt_path)['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "891e1fef-8dab-44fd-908d-e9f2a901dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples/example_results\\lightning_logs\\debug_octmnist_no_fc\\\n",
      "not a dir\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # load model and run test/xAI routine\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "\n",
    "\n",
    "    # ckpt_path = ''\n",
    "    # ckpt_path = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\\debug_oct_no_fc\", 'version_13', 'checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt'])\n",
    "    print(ckpt_path)\n",
    "    if os.path.isfile(ckpt_path):\n",
    "        # save the logs somewhere else\n",
    "        logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "\n",
    "        light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "        # we want the grad to work in test, hence: inference_mode=False\n",
    "        # logger_x = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "        explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                             accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                             #devices=[0],\n",
    "                             logger=logger,\n",
    "                             inference_mode=False)\n",
    "\n",
    "        test_result = explainer.test(light, data.xai_dataloader, verbose=False)\n",
    "\n",
    "    else:\n",
    "        print('not a dir')\n",
    "\n",
    "\n",
    "        #print(state_dict)\n",
    "\n",
    "\n",
    "        #light = DecentLightning.load_from_checkpoint(state_dict, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39a83b94-b458-4390-8bfd-cd98ffc9d109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecentNet(\n",
       "  (decent1): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([10.]), n_this=Parameter containing:\n",
       "      tensor([10.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([1.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([2.]), n_this=Parameter containing:\n",
       "      tensor([1.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([11.]))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "    )\n",
       "  )\n",
       "  (decent2): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([10.]), n_this=Parameter containing:\n",
       "      tensor([5.]))\n",
       "       with inputs: ms_in= 10, ns_in= 10)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([0.]))\n",
       "       with inputs: ms_in= 2, ns_in= 1)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([13.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 10, 1, ns_in= 10, 6)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([9.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 1, 2, 13, ns_in= 6, 1, 11)\n",
       "      (4): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([0.]), n_this=Parameter containing:\n",
       "      tensor([16.]))\n",
       "       with inputs: ms_in= 10, 2, ns_in= 10, 1)\n",
       "      (5): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([12.]), n_this=Parameter containing:\n",
       "      tensor([6.]))\n",
       "       with inputs: ms_in= 10, 13, ns_in= 10, 11)\n",
       "      (6): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([14.]), n_this=Parameter containing:\n",
       "      tensor([16.]))\n",
       "       with inputs: ms_in= 2, 13, ns_in= 1, 11)\n",
       "      (7): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([8.]))\n",
       "       with inputs: ms_in= 2, 13, ns_in= 1, 11)\n",
       "    )\n",
       "  )\n",
       "  (decent3): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([3.]))\n",
       "       with inputs: ms_in= 10, 9, 0, 14, 7, ns_in= 5, 7, 16, 16, 8)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([17.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 13, 12, 7, ns_in= 0, 6, 8)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 2, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([8.]))\n",
       "       with inputs: ms_in= 13, 7, ns_in= 6, 8)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([0.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 10, 0, 12, 14, 7, ns_in= 5, 16, 6, 16, 8)\n",
       "      (4): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([12.]), n_this=Parameter containing:\n",
       "      tensor([12.]))\n",
       "       with inputs: ms_in= 10, 9, 0, ns_in= 5, 7, 16)\n",
       "      (5): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 10, 13, 9, 12, 7, ns_in= 5, 6, 7, 6, 8)\n",
       "      (6): DecentFilter(weights: torch.Size([1, 4, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([12.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 10, 13, 12, 7, ns_in= 5, 6, 6, 8)\n",
       "      (7): DecentFilter(weights: torch.Size([1, 3, 3, 3]) at position: m_this=Parameter containing:\n",
       "      tensor([14.]), n_this=Parameter containing:\n",
       "      tensor([13.]))\n",
       "       with inputs: ms_in= 10, 9, 7, ns_in= 5, 7, 8)\n",
       "    )\n",
       "  )\n",
       "  (decent1x1): DecentLayer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 3, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([5.]), n_this=Parameter containing:\n",
       "      tensor([14.]))\n",
       "       with inputs: ms_in= 12, 12, 14, ns_in= 12, 14, 13)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([7.]), n_this=Parameter containing:\n",
       "      tensor([9.]))\n",
       "       with inputs: ms_in= 7, 5, 12, 12, ns_in= 3, 8, 12, 14)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([8.]), n_this=Parameter containing:\n",
       "      tensor([16.]))\n",
       "       with inputs: ms_in= 12, 5, 12, 14, ns_in= 12, 14, 14, 13)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 4, 1, 1]) at position: m_this=Parameter containing:\n",
       "      tensor([2.]), n_this=Parameter containing:\n",
       "      tensor([7.]))\n",
       "       with inputs: ms_in= 7, 0, 5, 14, ns_in= 3, 7, 14, 13)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (mish1): Mish()\n",
       "  (mish2): Mish()\n",
       "  (mish3): Mish()\n",
       "  (mish1x1): Mish()\n",
       "  (bias1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias2): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias3): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (bias1x1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f306c6d-6155-4af0-b913-42b25857b745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.1120,  0.0359,  0.0384],\n",
      "          [-0.0689,  0.0823,  0.1093],\n",
      "          [-0.0925, -0.0965,  0.1136]],\n",
      "\n",
      "         [[-0.0705, -0.0586, -0.1164],\n",
      "          [ 0.1089, -0.0156, -0.0937],\n",
      "          [-0.0375,  0.0847, -0.1003]],\n",
      "\n",
      "         [[-0.1164,  0.0155, -0.0606],\n",
      "          [-0.0176,  0.0735,  0.0926],\n",
      "          [-0.0519,  0.1130, -0.0256]],\n",
      "\n",
      "         [[ 0.0731,  0.0974, -0.1084],\n",
      "          [ 0.1173,  0.0332, -0.0819],\n",
      "          [-0.0020, -0.1078, -0.0292]],\n",
      "\n",
      "         [[ 0.0957,  0.1058,  0.0913],\n",
      "          [-0.0797, -0.0297, -0.0592],\n",
      "          [-0.1087,  0.1140,  0.0102]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0677,  0.0885, -0.0587],\n",
      "          [-0.0575,  0.0986,  0.0617],\n",
      "          [ 0.0120, -0.0422, -0.1122]],\n",
      "\n",
      "         [[-0.0895,  0.0876,  0.0136],\n",
      "          [ 0.0500,  0.1062,  0.0700],\n",
      "          [-0.1033,  0.1148, -0.0827]],\n",
      "\n",
      "         [[ 0.1096, -0.1123,  0.0312],\n",
      "          [-0.0358, -0.0471,  0.0361],\n",
      "          [ 0.0862,  0.0653, -0.1201]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0780, -0.0993,  0.1079],\n",
      "          [ 0.0993, -0.0778, -0.0730],\n",
      "          [-0.0387,  0.1052, -0.1074]],\n",
      "\n",
      "         [[ 0.1088,  0.0951,  0.0870],\n",
      "          [ 0.0668, -0.0554, -0.1003],\n",
      "          [-0.0716,  0.0341,  0.0032]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0933, -0.0719,  0.1100],\n",
      "          [ 0.0367, -0.0929, -0.0299],\n",
      "          [ 0.0128, -0.1099,  0.1057]],\n",
      "\n",
      "         [[-0.0634,  0.0901,  0.0548],\n",
      "          [ 0.0459,  0.0608,  0.0882],\n",
      "          [ 0.0986, -0.1188, -0.1177]],\n",
      "\n",
      "         [[-0.1141,  0.0364,  0.0656],\n",
      "          [ 0.0259,  0.0623,  0.0647],\n",
      "          [ 0.0451,  0.1015, -0.1099]],\n",
      "\n",
      "         [[ 0.1220, -0.0973, -0.0167],\n",
      "          [-0.0928, -0.0828, -0.0942],\n",
      "          [-0.0387, -0.0189,  0.0921]],\n",
      "\n",
      "         [[-0.0797,  0.1007,  0.0281],\n",
      "          [ 0.0905, -0.0318,  0.0920],\n",
      "          [-0.0673, -0.1022,  0.0327]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0604,  0.0320,  0.0094],\n",
      "          [-0.0937, -0.1107, -0.1062],\n",
      "          [-0.0641, -0.1177, -0.1083]],\n",
      "\n",
      "         [[ 0.0854,  0.0869,  0.1010],\n",
      "          [ 0.0981, -0.0981,  0.0207],\n",
      "          [ 0.1046,  0.0727,  0.0610]],\n",
      "\n",
      "         [[ 0.0582, -0.0996,  0.0739],\n",
      "          [-0.0541,  0.1058,  0.0155],\n",
      "          [ 0.0103,  0.0811, -0.1011]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 8.3588e-02, -1.2592e-01, -3.9456e-02],\n",
      "          [ 2.4520e-02, -5.5967e-02, -1.1657e-01],\n",
      "          [-6.8253e-02, -1.1120e-04, -1.1948e-01]],\n",
      "\n",
      "         [[-9.4842e-02,  1.2035e-02,  1.0980e-01],\n",
      "          [ 7.8618e-02,  5.8726e-02,  1.2236e-02],\n",
      "          [-9.6724e-02, -7.7756e-02, -4.3953e-02]],\n",
      "\n",
      "         [[ 1.1033e-02, -6.3005e-02, -7.0964e-02],\n",
      "          [ 9.9804e-02,  1.0856e-01, -6.8904e-02],\n",
      "          [ 9.7434e-02, -6.8217e-02,  6.8290e-02]],\n",
      "\n",
      "         [[-1.6555e-02, -8.8796e-02,  6.9168e-02],\n",
      "          [-9.5391e-02, -9.4876e-02,  1.1259e-01],\n",
      "          [ 4.1625e-02, -1.7344e-02, -1.1248e-01]],\n",
      "\n",
      "         [[ 5.7544e-02,  1.0598e-01, -2.2759e-02],\n",
      "          [ 9.5236e-02, -5.0186e-02, -2.9936e-02],\n",
      "          [-6.1652e-02,  7.6830e-02,  1.0014e-01]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.0537,  0.1012, -0.0750],\n",
      "          [ 0.0804,  0.1148, -0.1059],\n",
      "          [-0.1021, -0.0584, -0.1054]],\n",
      "\n",
      "         [[ 0.0926, -0.0632, -0.0725],\n",
      "          [ 0.0684,  0.0403, -0.1210],\n",
      "          [ 0.0716, -0.0535,  0.0486]],\n",
      "\n",
      "         [[ 0.0649, -0.0253, -0.1068],\n",
      "          [ 0.1136, -0.0476, -0.1140],\n",
      "          [-0.1143,  0.0253, -0.0813]],\n",
      "\n",
      "         [[ 0.0521,  0.0597,  0.1115],\n",
      "          [-0.1084,  0.0902, -0.1014],\n",
      "          [ 0.0826,  0.0123, -0.0368]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0510,  0.0175, -0.0655],\n",
      "          [ 0.0017, -0.0689, -0.0970],\n",
      "          [ 0.0960,  0.0903,  0.1143]],\n",
      "\n",
      "         [[-0.1091,  0.0136, -0.0922],\n",
      "          [ 0.0267,  0.1150,  0.0664],\n",
      "          [ 0.0296, -0.0548,  0.1077]],\n",
      "\n",
      "         [[-0.1060,  0.1113,  0.0518],\n",
      "          [-0.1023, -0.0809,  0.0517],\n",
      "          [-0.0467,  0.0050, -0.0389]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for a in light.model.decent3.filter_list:\n",
    "    print(a.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c44cd3-15c4-4344-a6be-4c45ffea2070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0696de4a-76ea-4b95-9d11-e1cf8a733a42",
   "metadata": {},
   "source": [
    "# random nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14953a49-823e-46c6-b48b-affde35f45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    tmp_listi = [nn.Parameter(torch.tensor([10.0, 15.0, 3.5])), \n",
    "                nn.Parameter(torch.tensor([4.0, 15.0])),\n",
    "                nn.Parameter(torch.tensor([5.0, 15.0])),\n",
    "                nn.Parameter(torch.tensor([6.0, 12.0, 4.5]))\n",
    "                              ]\n",
    "    # nn.ModuleList(\n",
    "\n",
    "    remove = []\n",
    "    for i, aaaa in enumerate(tmp_listi):\n",
    "        #print(aaaa)\n",
    "        if aaaa.shape[0] == 3:\n",
    "            print(aaaa)\n",
    "            remove.append(i)\n",
    "\n",
    "            tmp_listi.pop(i)\n",
    "\n",
    "    print('listi')\n",
    "    print(tmp_listi)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        state_dict = torch.load(ckpt_path)\n",
    "        print(state_dict['state_dict'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    x = []\n",
    "    x.extend([324, 23]) # Nothing is printed because the return value is None\n",
    "    x.extend([324, 23])\n",
    "    x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7899e827-6fa5-467a-9089-a6df73d92eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT EXCEPTION: layer not working, run not defined\n",
      "name 'run_explain' is not defined\n",
      "{'test accuracy on valset': 0.125}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    result = {\"test accuracy on valset\": test_result[0][\"test_acc\"]}\n",
    "except:\n",
    "    result = 0\n",
    "\n",
    "try:     \n",
    "    layer = light.model.decent2 # .filter_list[7]weights\n",
    "    run_explain(light, layer, device='cuda')\n",
    "except Exception as e:\n",
    "    print(\"DECENT EXCEPTION: layer not working, run not defined\" )\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67ccb396-da0e-4942-998b-676d1fb8234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7212581\n",
      "0.7065036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7212581, 0.7065036]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[np.array(0.7212581)], [np.array(0.7065036)]]\n",
    "\n",
    "for e in a:\n",
    "    for i in e:\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "flattened = [val.item() for tmp in a for val in tmp] \n",
    " \n",
    "\n",
    "a\n",
    "a\n",
    "\n",
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ee6f3b-11df-4ef3-9fd9-a110fa996159",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [[0.2847180664539337, 0.29257622361183167, 0.2561589181423187, 0.2416810840368271, 0.2306821644306183], [0.24937507510185242, 0.294414222240448, 0.2743309736251831, 0.2431085854768753, 0.23851336538791656], [0.30998995900154114, 0.2402188777923584, 0.22844929993152618, 0.2568821907043457], [0.28166717290878296, 0.22486495971679688, 0.2940433919429779, 0.26794517040252686]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d48391f-1f3e-4d78-aa61-79dc7eeac5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    a=np.array([np.array(0.69074845)])\n",
    "\n",
    "    print(a.flatten())\n",
    "\n",
    "    flattened = [val for tmp in a for val in tmp] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c1d301c-cbad-4154-a053-137a3263c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# useless, always the same filters\n",
    "\n",
    "for i_filter in range(100):\n",
    "    try:\n",
    "        layer = model.model.decent2.filter_list[i_filter] # i_filter] # .filter_list[7]weights\n",
    "        run_explain(model, layer, device='cuda')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2568e15-95ba-4c8c-9a51-b8cb5050ae51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_70184\\1227141380.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecent2\u001b[0m \u001b[1;31m# .filter_list[7]weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrun_explain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "layer = model.model.decent2 # .filter_list[7]weights\n",
    "run_explain(model, layer, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f4513-419a-433b-97ff-6f54c5a6d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b09e4-644a-4cca-9da3-a47986de1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca471c2c-819f-44aa-a8f9-e318aacf3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"examples/example_results/lightning_logs/debug_oct_no_fc/version_13/checkpoints/epoch=2-unpruned=269-val_f1=0.25.ckpt\") # .keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['loops'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['state_dict'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
