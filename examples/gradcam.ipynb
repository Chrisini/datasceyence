{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6a08a7-d96f-4a30-a51d-cde9bbdeccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess an image\n",
    "def load_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img_tensor = preprocess(img)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "# Function to perform Grad-CAM\n",
    "class GradCam:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "\n",
    "        # Register a hook to capture gradients during backward pass\n",
    "        self.hook = self.register_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        def hook_fn(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        target_layer = self.model._modules.get(self.target_layer)\n",
    "        hook = target_layer.register_forward_hook(hook_fn)\n",
    "        return hook\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def backward(self, output):\n",
    "        self.model.zero_grad()\n",
    "        output.backward()\n",
    "\n",
    "    def generate_heatmap(self, input_tensor, class_idx):\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Perform forward and backward pass\n",
    "        output = self.forward(input_tensor)\n",
    "        target = output[0][class_idx]\n",
    "        self.backward(target)\n",
    "\n",
    "        # Calculate the importance weights (gradients)\n",
    "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "        # Get the activations from the target layer\n",
    "        target_layer_output = self.hook.output[0]\n",
    "\n",
    "        # Weighted sum of activations to get the Grad-CAM heatmap\n",
    "        grad_cam = torch.sum(weights * target_layer_output, dim=1, keepdim=True)\n",
    "        grad_cam = F.relu(grad_cam)\n",
    "\n",
    "        # Resize the heatmap to match the input image size\n",
    "        grad_cam = F.interpolate(grad_cam, size=(input_tensor.shape[2], input_tensor.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dfa9036-abd4-4543-ae80-66ce362b1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86441eb9-7c25-4154-ac8e-a46b310cd6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "# disect the network to access its last convolutional layer\n",
    "vgg.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c710b5c-16f0-4cd4-987d-f8ebc49d9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resi(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resi, self).__init__()\n",
    "        \n",
    "        # get the pretrained VGG19 network\n",
    "        self.vgg = torchvision.models.vgg19(pretrained=True)\n",
    "        \n",
    "        # disect the network to access its last convolutional layer\n",
    "        self.features_conv = self.vgg.features[:36]\n",
    "        \n",
    "        # get the max pool of the features stem\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # get the classifier of the vgg19\n",
    "        self.classifier = self.vgg.classifier\n",
    "        \n",
    "        \n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        # register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # apply the remaining pooling\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view((1, -1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37533ea5-1966-442d-96e1-881f7ff3cda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([117])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained ResNet model\n",
    "model = Resi()\n",
    "model.eval()\n",
    "\n",
    "# get the image from the dataloader\n",
    "img = load_image(\"C:/Users/Prinzessin/projects/decentnet/datasceyence/examples/example_data/eye/AMD/A0001.jpg\")\n",
    "\n",
    "# get the most likely prediction of the model\n",
    "pred = model(img) # .argmax(dim=1)\n",
    "\n",
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20b589b0-5b75-41ba-b7f6-0c3bc3faf8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12c0ccc1d60>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the gradient of the output with respect to the parameters of the model\n",
    "#pred[:, 386].backward()\n",
    "pred[:, 386].backward()\n",
    "#  \n",
    "# pull the gradients out of the model\n",
    "gradients = model.get_activations_gradient()\n",
    "\n",
    "# pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "# get the activations of the last convolutional layer\n",
    "activations = model.get_activations(img).detach()\n",
    "\n",
    "# weight the channels by corresponding gradients\n",
    "for i in range(512):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    \n",
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# relu on top of the heatmap\n",
    "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc103405-99b8-41bd-a872-d812b253e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
