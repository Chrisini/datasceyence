{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Chrisini/DecentNet/blob/master/weak_vessel_map.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iGzdlVOnoVa"
   },
   "source": [
    "# ð•Žð•–ð•’ð•œ ð•§ð•–ð•¤ð•¤ð•–ð• ð•žð•’ð•¤ð•œ ð•˜ð•–ð•Ÿð•–ð•£ð•’ð•¥ð•šð• ð•Ÿ\n",
    "\n",
    "* In order to understand our concept clusters and neural network results, we want to get vessel information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Yio7ZmBhXqN"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkR8-O1MP6l5",
    "outputId": "c788e2bf-1754-4107-950e-9976dcca94ac"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor, ToPILImage, CenterCrop\n",
    "\n",
    "if True:\n",
    "    pass\n",
    "  #pip install bunch\n",
    "  #from bunch import Bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52_3EpIIP_Fo",
    "outputId": "31879b66-a666-45b2-8162-f5850527aa0f"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "if False:\n",
    "    train_path =     r\"/content/drive/MyDrive/decentnet/data/images/train/AMD\"\n",
    "elif False:\n",
    "    train_path =     r\"/content/drive/MyDrive/decentnet/data/images/train/nonAMD\"\n",
    "elif True:\n",
    "    train_path =     r\"C:/Users/Prinzessin/projects/image_data/RAVIR_Dataset/train/training_images\"\n",
    "    #train_path = r\"C:\\snec_data\\Result_Data\\cirrus\\enface\"\n",
    "    #train_path = r\"C:\\snec_data\\Result_Data\\cirrus\\superenface\"\n",
    "\n",
    "ckpt_vessel_path =   r\"example_ckpt/vessel_unet_w_liu\"\n",
    "\n",
    "vessel_path =        r\"example_results/Vessel_Masks\" # results\n",
    "vessel_path_thresh = r\"example_results/Vessel_Masks_Thresholded\" # results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghulTfTQULjt"
   },
   "source": [
    "# FR Unet\n",
    "\n",
    "This code is stolen and adapted from: https://github.com/lseventeen/FR-UNet\n",
    "Liu, Wentao, et al. \"Full-resolution network and dual-threshold iteration for retinal vessel and coronary angiograph segmentation.\" IEEE Journal of Biomedical and Health Informatics 26.9 (2022): 4623-4634."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JgQKJ0OSbLSB"
   },
   "outputs": [],
   "source": [
    "class vessel_dataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "\n",
    "        self.data_path = path\n",
    "        self.data_file = os.listdir(self.data_path)\n",
    "        \n",
    "        self.transforms = Compose([\n",
    "            # Resize((592, 592)), # 592\n",
    "            Resize((256, 256)),\n",
    "            CenterCrop((96, 96)),\n",
    "            ToTensor(),\n",
    "            # torchvision.transforms.RandomInvert(p=1),\n",
    "            # Normalize((0.7, ), (0.7, )) # grayscale\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.data_path, self.data_file[idx])\n",
    "        \n",
    "        img = Image.open(path).convert(\"L\")\n",
    "        if True:\n",
    "            mask = Image.open(path.replace(\"training_images\", \"training_masks\")).convert(\"L\")\n",
    "        #red, img, blue = img.split()\n",
    "        size = img.size\n",
    "        img = self.transforms(img)\n",
    "        mask = self.transforms(mask)\n",
    "        return img, mask, self.data_file[idx], size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "592 * 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NnqKROkZWlW-"
   },
   "outputs": [],
   "source": [
    "class conv(nn.Module):\n",
    "    def __init__(self, in_c, out_c, dp=0):\n",
    "        super(conv, self).__init__()\n",
    "        self.in_c = in_c\n",
    "        self.out_c = out_c\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.Dropout2d(dp),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.Dropout2d(dp),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.relu = nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.conv(x)\n",
    "        out = x + res\n",
    "        out = self.relu(out)\n",
    "        return x\n",
    "\n",
    "\n",
    "class feature_fuse(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(feature_fuse, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(\n",
    "            in_c, out_c, kernel_size=1, padding=0, bias=False)\n",
    "        self.conv33 = nn.Conv2d(\n",
    "            in_c, out_c, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv33_di = nn.Conv2d(\n",
    "            in_c, out_c, kernel_size=3, padding=2, bias=False, dilation=2)\n",
    "        self.norm = nn.BatchNorm2d(out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv11(x)\n",
    "        x2 = self.conv33(x)\n",
    "        x3 = self.conv33_di(x)\n",
    "        out = self.norm(x1+x2+x3)\n",
    "        return out\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_c, out_c, dp=0):\n",
    "        super(up, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, kernel_size=2,\n",
    "                               padding=0, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.1, inplace=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_c, out_c, dp=0):\n",
    "        super(down, self).__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=2,\n",
    "                      padding=0, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class block(nn.Module):\n",
    "    def __init__(self, in_c, out_c,  dp=0, is_up=False, is_down=False, fuse=False):\n",
    "        super(block, self).__init__()\n",
    "        self.in_c = in_c\n",
    "        self.out_c = out_c\n",
    "        if fuse == True:\n",
    "            self.fuse = feature_fuse(in_c, out_c)\n",
    "        else:\n",
    "            self.fuse = nn.Conv2d(in_c, out_c, kernel_size=1, stride=1)\n",
    "\n",
    "        self.is_up = is_up\n",
    "        self.is_down = is_down\n",
    "        self.conv = conv(out_c, out_c, dp=dp)\n",
    "        if self.is_up == True:\n",
    "            self.up = up(out_c, out_c//2)\n",
    "        if self.is_down == True:\n",
    "            self.down = down(out_c, out_c*2)\n",
    "\n",
    "    def forward(self,  x):\n",
    "        if self.in_c != self.out_c:\n",
    "            x = self.fuse(x)\n",
    "        x = self.conv(x)\n",
    "        if self.is_up == False and self.is_down == False:\n",
    "            return x\n",
    "        elif self.is_up == True and self.is_down == False:\n",
    "            x_up = self.up(x)\n",
    "            return x, x_up\n",
    "        elif self.is_up == False and self.is_down == True:\n",
    "            x_down = self.down(x)\n",
    "            return x, x_down\n",
    "        else:\n",
    "            x_up = self.up(x)\n",
    "            x_down = self.down(x)\n",
    "            return x, x_up, x_down\n",
    "\n",
    "\n",
    "class FR_UNet(nn.Module):\n",
    "    def __init__(self,  num_classes=1, num_channels=1, feature_scale=2,  dropout=0.2, fuse=True, out_ave=True):\n",
    "        super(FR_UNet, self).__init__()\n",
    "        self.out_ave = out_ave\n",
    "        filters = [64, 128, 256, 512, 1024]\n",
    "        filters = [int(x / feature_scale) for x in filters]\n",
    "        self.block1_3 = block(\n",
    "            num_channels, filters[0],  dp=dropout, is_up=False, is_down=True, fuse=fuse)\n",
    "        self.block1_2 = block(\n",
    "            filters[0], filters[0],  dp=dropout, is_up=False, is_down=True, fuse=fuse)\n",
    "        self.block1_1 = block(\n",
    "            filters[0]*2, filters[0],  dp=dropout, is_up=False, is_down=True, fuse=fuse)\n",
    "        self.block10 = block(\n",
    "            filters[0]*2, filters[0],  dp=dropout, is_up=False, is_down=True, fuse=fuse)\n",
    "        self.block11 = block(\n",
    "            filters[0]*2, filters[0],  dp=dropout, is_up=False, is_down=True, fuse=fuse)\n",
    "        self.block12 = block(\n",
    "            filters[0]*2, filters[0],  dp=dropout, is_up=False, is_down=False, fuse=fuse)\n",
    "        self.block13 = block(\n",
    "            filters[0]*2, filters[0],  dp=dropout, is_up=False, is_down=False, fuse=fuse)\n",
    "        self.block2_2 = block(\n",
    "            filters[1], filters[1],  dp=dropout, is_up=True, is_down=True, fuse=fuse)\n",
    "        self.block2_1 = block(\n",
    "            filters[1]*2, filters[1],  dp=dropout, is_up=True, is_down=True, fuse=fuse)\n",
    "        self.block20 = block(\n",
    "            filters[1]*3, filters[1],  dp=dropout, is_up=True, is_down=True, fuse=fuse)\n",
    "        self.block21 = block(\n",
    "            filters[1]*3, filters[1],  dp=dropout, is_up=True, is_down=False, fuse=fuse)\n",
    "        self.block22 = block(\n",
    "            filters[1]*3, filters[1],  dp=dropout, is_up=True, is_down=False, fuse=fuse)\n",
    "        self.block3_1 = block(\n",
    "            filters[2], filters[2],  dp=dropout, is_up=True, is_down=True, fuse=fuse)\n",
    "        self.block30 = block(\n",
    "            filters[2]*2, filters[2],  dp=dropout, is_up=True, is_down=False, fuse=fuse)\n",
    "        self.block31 = block(\n",
    "            filters[2]*3, filters[2],  dp=dropout, is_up=True, is_down=False, fuse=fuse)\n",
    "        self.block40 = block(filters[3], filters[3],\n",
    "                             dp=dropout, is_up=True, is_down=False, fuse=fuse)\n",
    "        self.final1 = nn.Conv2d(\n",
    "            filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "        self.final2 = nn.Conv2d(\n",
    "            filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "        self.final3 = nn.Conv2d(\n",
    "            filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "        self.final4 = nn.Conv2d(\n",
    "            filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "        self.final5 = nn.Conv2d(\n",
    "            filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "        self.fuse = nn.Conv2d(\n",
    "            5, num_classes, kernel_size=1, padding=0, bias=True)\n",
    "        # self.apply(InitWeights_He)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_3, x_down1_3 = self.block1_3(x)\n",
    "        x1_2, x_down1_2 = self.block1_2(x1_3)\n",
    "        x2_2, x_up2_2, x_down2_2 = self.block2_2(x_down1_3)\n",
    "        x1_1, x_down1_1 = self.block1_1(torch.cat([x1_2, x_up2_2], dim=1))\n",
    "        x2_1, x_up2_1, x_down2_1 = self.block2_1(\n",
    "            torch.cat([x_down1_2, x2_2], dim=1))\n",
    "        x3_1, x_up3_1, x_down3_1 = self.block3_1(x_down2_2)\n",
    "        x10, x_down10 = self.block10(torch.cat([x1_1, x_up2_1], dim=1))\n",
    "        x20, x_up20, x_down20 = self.block20(\n",
    "            torch.cat([x_down1_1, x2_1, x_up3_1], dim=1))\n",
    "        x30, x_up30 = self.block30(torch.cat([x_down2_1, x3_1], dim=1))\n",
    "        _, x_up40 = self.block40(x_down3_1)\n",
    "        x11, x_down11 = self.block11(torch.cat([x10, x_up20], dim=1))\n",
    "        x21, x_up21 = self.block21(torch.cat([x_down10, x20, x_up30], dim=1))\n",
    "        _, x_up31 = self.block31(torch.cat([x_down20, x30, x_up40], dim=1))\n",
    "        x12 = self.block12(torch.cat([x11, x_up21], dim=1))\n",
    "        _, x_up22 = self.block22(torch.cat([x_down11, x21, x_up31], dim=1))\n",
    "        x13 = self.block13(torch.cat([x12, x_up22], dim=1))\n",
    "        if self.out_ave == True:\n",
    "            output = (self.final1(x1_1)+self.final2(x10) +\n",
    "                      self.final3(x11)+self.final4(x12)+self.final5(x13))/5\n",
    "        else:\n",
    "            output = self.final5(x13)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class Fix_RandomRotation(object):\n",
    "\n",
    "    def __init__(self, degrees=360, resample=False, expand=False, center=None):\n",
    "        self.degrees = degrees\n",
    "        self.resample = resample\n",
    "        self.expand = expand\n",
    "        self.center = center\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params():\n",
    "        p = torch.rand(1)\n",
    "\n",
    "        if p >= 0 and p < 0.25:\n",
    "            angle = -180\n",
    "        elif p >= 0.25 and p < 0.5:\n",
    "            angle = -90\n",
    "        elif p >= 0.5 and p < 0.75:\n",
    "            angle = 90\n",
    "        else:\n",
    "            angle = 0\n",
    "        return angle\n",
    "\n",
    "    def __call__(self, img):\n",
    "        angle = self.get_params()\n",
    "        return F.rotate(img, angle, self.resample, self.expand, self.center)\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + \\\n",
    "            '(degrees={0}'.format(self.degrees)\n",
    "        format_string += ', resample={0}'.format(self.resample)\n",
    "        format_string += ', expand={0}'.format(self.expand)\n",
    "        if self.center is not None:\n",
    "            format_string += ', center={0}'.format(self.center)\n",
    "        format_string += ')'\n",
    "        return format_string\n",
    "\n",
    "\n",
    "def dir_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def remove_files(path):\n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "\n",
    "\n",
    "def read_pickle(path, type):\n",
    "    with open(file=path + f\"/{type}.pkl\", mode='rb') as file:\n",
    "        img = pickle.load(file)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_pickle(path, type, img_list):\n",
    "    with open(file=path + f\"/{type}.pkl\", mode='wb') as file:\n",
    "        pickle.dump(img_list, file)\n",
    "\n",
    "\n",
    "def double_threshold_iteration(index,img, h_thresh, l_thresh, save=True):\n",
    "    h, w = img.shape\n",
    "    img = np.array(torch.sigmoid(img).cpu().detach()*255, dtype=np.uint8)\n",
    "    bin = np.where(img >= h_thresh*255, 255, 0).astype(np.uint8)\n",
    "    gbin = bin.copy()\n",
    "    gbin_pre = gbin-1\n",
    "    while(gbin_pre.all() != gbin.all()):\n",
    "        gbin_pre = gbin\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                if gbin[i][j] == 0 and img[i][j] < h_thresh*255 and img[i][j] >= l_thresh*255:\n",
    "                    if gbin[i-1][j-1] or gbin[i-1][j] or gbin[i-1][j+1] or gbin[i][j-1] or gbin[i][j+1] or gbin[i+1][j-1] or gbin[i+1][j] or gbin[i+1][j+1]:\n",
    "                        gbin[i][j] = 255\n",
    "\n",
    "    if save:\n",
    "        cv2.imwrite(f\"save_picture/bin{index}.png\", bin)\n",
    "        cv2.imwrite(f\"save_picture/gbin{index}.png\", gbin)\n",
    "    return gbin/255\n",
    "\n",
    "\n",
    "def recompone_overlap(preds, img_h, img_w, stride_h, stride_w):\n",
    "    assert (len(preds.shape) == 4)\n",
    "    assert (preds.shape[1] == 1 or preds.shape[1] == 3)\n",
    "    patch_h = preds.shape[2]\n",
    "    patch_w = preds.shape[3]\n",
    "    N_patches_h = (img_h - patch_h) // stride_h + 1\n",
    "    N_patches_w = (img_w - patch_w) // stride_w + 1\n",
    "    N_patches_img = N_patches_h * N_patches_w\n",
    "    assert (preds.shape[0] % N_patches_img == 0)\n",
    "    N_full_imgs = preds.shape[0] // N_patches_img\n",
    "    full_prob = np.zeros((N_full_imgs, preds.shape[1], img_h, img_w))\n",
    "    full_sum = np.zeros((N_full_imgs, preds.shape[1], img_h, img_w))\n",
    "    k = 0\n",
    "    for i in range(N_full_imgs):\n",
    "        for h in range((img_h - patch_h) // stride_h + 1):\n",
    "            for w in range((img_w - patch_w) // stride_w + 1):\n",
    "                full_prob[i, :, h * stride_h:(h * stride_h) + patch_h, w * stride_w:(w * stride_w) + patch_w] += preds[\n",
    "                    k]\n",
    "                full_sum[i, :, h * stride_h:(h * stride_h) + patch_h,\n",
    "                         w * stride_w:(w * stride_w) + patch_w] += 1\n",
    "                k += 1\n",
    "    assert (k == preds.shape[0])\n",
    "    assert (np.min(full_sum) >= 1.0)\n",
    "    final_avg = full_prob / full_sum\n",
    "    return final_avg\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.val = None\n",
    "        self.avg = None\n",
    "        self.sum = None\n",
    "        self.count = None\n",
    "\n",
    "    def initialize(self, val, weight):\n",
    "        self.val = val\n",
    "        self.avg = val\n",
    "        self.sum = np.multiply(val, weight)\n",
    "        self.count = weight\n",
    "        self.initialized = True\n",
    "\n",
    "    def update(self, val, weight=1):\n",
    "        if not self.initialized:\n",
    "            self.initialize(val, weight)\n",
    "        else:\n",
    "            self.add(val, weight)\n",
    "\n",
    "    def add(self, val, weight):\n",
    "        self.val = val\n",
    "        self.sum = np.add(self.sum, np.multiply(val, weight))\n",
    "        self.count = self.count + weight\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return np.round(self.val, 4)\n",
    "\n",
    "    @property\n",
    "    def average(self):\n",
    "        return np.round(self.avg, 4)\n",
    "\n",
    "\n",
    "def get_metrics(predict, target, threshold=None, predict_b=None):\n",
    "    predict = torch.sigmoid(predict).cpu().detach().numpy().flatten()\n",
    "    if predict_b is not None:\n",
    "        predict_b = predict_b.flatten()\n",
    "    else:\n",
    "        predict_b = np.where(predict >= threshold, 1, 0)\n",
    "    if torch.is_tensor(target):\n",
    "        target = target.cpu().detach().numpy().flatten()\n",
    "    else:\n",
    "        target = target.flatten()\n",
    "    tp = (predict_b * target).sum()\n",
    "    tn = ((1 - predict_b) * (1 - target)).sum()\n",
    "    fp = ((1 - target) * predict_b).sum()\n",
    "    fn = ((1 - predict_b) * target).sum()\n",
    "    auc = roc_auc_score(target, predict)\n",
    "    acc = (tp + tn) / (tp + fp + fn + tn)\n",
    "    pre = tp / (tp + fp)\n",
    "    sen = tp / (tp + fn)\n",
    "    spe = tn / (tn + fp)\n",
    "    iou = tp / (tp + fp + fn)\n",
    "    f1 = 2 * pre * sen / (pre + sen)\n",
    "    return {\n",
    "        \"AUC\": np.round(auc, 4),\n",
    "        \"F1\": np.round(f1, 4),\n",
    "        \"Acc\": np.round(acc, 4),\n",
    "        \"Sen\": np.round(sen, 4),\n",
    "        \"Spe\": np.round(spe, 4),\n",
    "        \"pre\": np.round(pre, 4),\n",
    "        \"IOU\": np.round(iou, 4),\n",
    "    }\n",
    "\n",
    "\n",
    "def count_connect_component(predict, target, threshold=None, connectivity=8):\n",
    "    if threshold != None:\n",
    "        predict = torch.sigmoid(predict).cpu().detach().numpy()\n",
    "        predict = np.where(predict >= threshold, 1, 0)\n",
    "    if torch.is_tensor(target):\n",
    "        target = target.cpu().detach().numpy()\n",
    "    pre_n, _, _, _ = cv2.connectedComponentsWithStats(np.asarray(\n",
    "        predict, dtype=np.uint8)*255, connectivity=connectivity)\n",
    "    gt_n, _, _, _ = cv2.connectedComponentsWithStats(np.asarray(\n",
    "        target, dtype=np.uint8)*255, connectivity=connectivity)\n",
    "    return pre_n/gt_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    # Source: https://github.com/HiLab-git/SSL4MIS/blob/master/code/utils/losses.py\n",
    "    def __init__(self, n_output_neurons, softmax=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.n_output_neurons = n_output_neurons\n",
    "        self.softmax=softmax\n",
    "\n",
    "    def _one_hot_encoder(self, input_tensor):\n",
    "        tensor_list = []\n",
    "        for i in range(self.n_output_neurons):\n",
    "            temp_prob = input_tensor == i * torch.ones_like(input_tensor)\n",
    "            tensor_list.append(temp_prob)\n",
    "        output_tensor = torch.cat(tensor_list, dim=1)\n",
    "        return output_tensor.float()\n",
    "\n",
    "    def _dice_loss(self, model_output, ground_truth):\n",
    "        ground_truth = ground_truth.float()\n",
    "        smooth = 1e-5\n",
    "        intersect = torch.sum(model_output * ground_truth)\n",
    "        y_sum = torch.sum(ground_truth * ground_truth)\n",
    "        z_sum = torch.sum(model_output * model_output)\n",
    "        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
    "        loss = 1 - loss\n",
    "        return loss\n",
    "\n",
    "    def forward(self, model_output, ground_truth, weight=None):\n",
    "        if self.softmax:\n",
    "            model_output = torch.softmax(model_output, dim=1)\n",
    "        ground_truth = self._one_hot_encoder(ground_truth)\n",
    "        \n",
    "        if weight is None:\n",
    "            weight = [1] * self.n_output_neurons\n",
    "        assert model_output.size() == ground_truth.size(), 'predict & ground_truth shape do not match'\n",
    "        class_wise_dice = []\n",
    "        loss = 0.0\n",
    "        for i in range(0, self.n_output_neurons):\n",
    "            dice = self._dice_loss(model_output[:, i], ground_truth[:, i])\n",
    "            class_wise_dice.append(1.0 - dice.item())\n",
    "            loss += dice * weight[i]\n",
    "        return loss / self.n_output_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "from loguru import logger\n",
    "from torch.utils import tensorboard\n",
    "from tqdm import tqdm\n",
    "# from utils.helpers import dir_exists, get_instance, remove_files, double_threshold_iteration\n",
    "# from utils.metrics import AverageMeter, get_metrics, get_metrics, count_connect_component\n",
    "import ttach as tta\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loss=None, train_loader=None, val_loader=None):\n",
    "\n",
    "        self.epochs= 40\n",
    "        self.save_dir= \"example_results/\"\n",
    "        self.val_per_epochs= 1\n",
    "        self.save_period= 1\n",
    "        # self.tensorboard= true\n",
    "        self.threshold= 0.5\n",
    "        self.threshold_low= 0.3\n",
    "        #self.DTI= false\n",
    "        #self.amp= true\n",
    "        #self.tta= false\n",
    "        #self.CCC= false\n",
    "        \n",
    "        self.loss = DiceLoss(3)\n",
    "        self.model = nn.DataParallel(model.cuda())\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=0.00001) # get_instance(torch.optim, \"optimizer\", CFG, self.model.parameters())\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optim, \n",
    "                                                                              T_0 = 5, # number of iterations for the first restart.\n",
    "                                                                              eta_min = 0.00001\n",
    "                                                                               ) # get_instance(torch.optim.lr_scheduler, \"lr_scheduler\", CFG, self.optimizer)\n",
    "\n",
    "\n",
    "        start_time = datetime.now().strftime('%y%m%d%H%M%S')\n",
    "        self.checkpoint_dir = os.path.join(self.save_dir, start_time)\n",
    "        self.writer = tensorboard.SummaryWriter(self.checkpoint_dir)\n",
    "        dir_exists(self.checkpoint_dir)\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self._train_epoch(epoch)\n",
    "            if self.val_loader is not None and epoch % self.val_per_epochs == 0:\n",
    "                results = self._valid_epoch(epoch)\n",
    "                logger.info(f'## Info for epoch {epoch} ## ')\n",
    "                for k, v in results.items():\n",
    "                    logger.info(f'{str(k):15s}: {v}')\n",
    "            if epoch % 10 == 0:\n",
    "                self._save_checkpoint(epoch)\n",
    "\n",
    "    def _train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        wrt_mode = 'train'\n",
    "        self._reset_metrics()\n",
    "        tbar = tqdm(self.train_loader, ncols=160)\n",
    "        tic = time.time()\n",
    "        for img, gt, _, _ in tbar:\n",
    "            self.data_time.update(time.time() - tic)\n",
    "            img = img.cuda(non_blocking=True)\n",
    "            gt = gt.cuda(non_blocking=True)\n",
    "            self.optim.zero_grad()\n",
    "            \n",
    "            pre = self.model(img)\n",
    "            \n",
    "            #print(pre.shape)\n",
    "            #print(gt.shape)\n",
    "            \n",
    "            loss = self.loss(pre, gt)\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            \n",
    "            self.total_loss.update(loss.item())\n",
    "            self.batch_time.update(time.time() - tic)\n",
    "            \n",
    "            \"\"\"\n",
    "            self._metrics_update(\n",
    "                *get_metrics(pre, gt, threshold=self.threshold).values())\n",
    "            tbar.set_description(\n",
    "                'TRAIN ({}) | Loss: {:.4f} | AUC {:.4f} F1 {:.4f} Acc {:.4f}  Sen {:.4f} Spe {:.4f} Pre {:.4f} IOU {:.4f} |B {:.2f} D {:.2f} |'.format(\n",
    "                    epoch, self.total_loss.average, *self._metrics_ave().values(), self.batch_time.average, self.data_time.average))\n",
    "            \"\"\"\n",
    "            tic = time.time()\n",
    "        self.writer.add_scalar(\n",
    "            f'{wrt_mode}/loss', self.total_loss.average, epoch)\n",
    "        \"\"\"\n",
    "        for k, v in list(self._metrics_ave().items())[:-1]:\n",
    "            self.writer.add_scalar(f'{wrt_mode}/{k}', v, epoch)\n",
    "        \"\"\"\n",
    "        for i, opt_group in enumerate(self.optim.param_groups):\n",
    "            self.writer.add_scalar(\n",
    "                f'{wrt_mode}/Learning_rate_{i}', opt_group['lr'], epoch)\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def _valid_epoch(self, epoch):\n",
    "        logger.info('\\n###### EVALUATION ######')\n",
    "        self.model.eval()\n",
    "        wrt_mode = 'val'\n",
    "        \"\"\"\n",
    "        self._reset_metrics()\n",
    "        \"\"\"\n",
    "        tbar = tqdm(self.val_loader, ncols=160)\n",
    "        with torch.no_grad():\n",
    "            for img, gt, _, _ in tbar:\n",
    "                img = img.cuda(non_blocking=True)\n",
    "                gt = gt.cuda(non_blocking=True)\n",
    "                \n",
    "                predict = self.model(img)\n",
    "                loss = self.loss(predict, gt)\n",
    "                self.total_loss.update(loss.item())\n",
    "                \"\"\"\n",
    "                self._metrics_update(\n",
    "                    *get_metrics(predict, gt, threshold=self.threshold).values())\n",
    "                tbar.set_description(\n",
    "                    'EVAL ({})  | Loss: {:.4f} | AUC {:.4f} F1 {:.4f} Acc {:.4f} Sen {:.4f} Spe {:.4f} Pre {:.4f} IOU {:.4f} |'.format(\n",
    "                        epoch, self.total_loss.average, *self._metrics_ave().values()))\n",
    "                \"\"\"\n",
    "                self.writer.add_scalar(\n",
    "                    f'{wrt_mode}/loss', self.total_loss.average, epoch)\n",
    "\n",
    "        self.writer.add_scalar(\n",
    "            f'{wrt_mode}/loss', self.total_loss.average, epoch)\n",
    "        \"\"\"\n",
    "        for k, v in list(self._metrics_ave().items())[:-1]:\n",
    "            self.writer.add_scalar(f'{wrt_mode}/{k}', v, epoch)\n",
    "        \"\"\"\n",
    "        log = {\n",
    "            'val_loss': self.total_loss.average,\n",
    "            #**self._metrics_ave()\n",
    "        }\n",
    "        \n",
    "        return log\n",
    "\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        state = {\n",
    "            'arch': type(self.model).__name__,\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optim': self.optim.state_dict()\n",
    "        }\n",
    "        filename = os.path.join(self.checkpoint_dir,\n",
    "                                f'checkpoint-epoch{epoch}.pth')\n",
    "        logger.info(f'Saving a checkpoint: {filename} ...')\n",
    "        torch.save(state, filename)\n",
    "        return filename\n",
    "\n",
    "    def _reset_metrics(self):\n",
    "        self.batch_time = AverageMeter()\n",
    "        self.data_time = AverageMeter()\n",
    "        self.total_loss = AverageMeter()\n",
    "        self.auc = AverageMeter()\n",
    "        self.f1 = AverageMeter()\n",
    "        self.acc = AverageMeter()\n",
    "        self.sen = AverageMeter()\n",
    "        self.spe = AverageMeter()\n",
    "        self.pre = AverageMeter()\n",
    "        self.iou = AverageMeter()\n",
    "        self.CCC = AverageMeter()\n",
    "    def _metrics_update(self, auc, f1, acc, sen, spe, pre, iou):\n",
    "        self.auc.update(auc)\n",
    "        self.f1.update(f1)\n",
    "        self.acc.update(acc)\n",
    "        self.sen.update(sen)\n",
    "        self.spe.update(spe)\n",
    "        self.pre.update(pre)\n",
    "        self.iou.update(iou)\n",
    "\n",
    "    def _metrics_ave(self):\n",
    "\n",
    "        return {\n",
    "            \"AUC\": self.auc.average,\n",
    "            \"F1\": self.f1.average,\n",
    "            \"Acc\": self.acc.average,\n",
    "            \"Sen\": self.sen.average,\n",
    "            \"Spe\": self.spe.average,\n",
    "            \"pre\": self.pre.average,\n",
    "            \"IOU\": self.iou.average\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jXjhzMRwXIYl"
   },
   "outputs": [],
   "source": [
    "import PIL.ImageOps   \n",
    "class Tester():\n",
    "    def __init__(self, model, checkpoint, test_loader, dataset_path):\n",
    "\n",
    "        # test loader has to have batch size = 1\n",
    "\n",
    "        self.test_loader = test_loader\n",
    "        self.model = model\n",
    "\n",
    "    def test(self):\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _, (img, _, image_path, size) in enumerate(self.test_loader):\n",
    "\n",
    "                cur_img_name = os.path.splitext(os.path.basename(os.path.normpath(image_path[0])))[0]\n",
    "\n",
    "                pred = self.model(img)\n",
    "\n",
    "                # predict = torch.sigmoid(pred).cpu().detach().numpy()\n",
    "                \n",
    "                # predict = pred.cpu().detach().numpy()\n",
    "                \n",
    "                #print(pred.shape)\n",
    "                \n",
    "                pred, classes = torch.max(pred, dim=1)\n",
    "                \n",
    "                \n",
    "                predict = classes.cpu().detach().numpy()\n",
    "                \n",
    "                #print(predict.shape)\n",
    "                \n",
    "                # predict = predict.transpose(2,3,1,0).squeeze()\n",
    "                predict = predict.transpose(1, 2, 0).squeeze()\n",
    "                \n",
    "                #print(predict.shape)\n",
    "                \n",
    "                print(np.unique(predict))\n",
    "                \n",
    "                im1 = Image.fromarray(np.uint8(predict*100))\n",
    "                \n",
    "                #print(im1)\n",
    "                \n",
    "                # im1 = PIL.ImageOps.invert(im1)\n",
    "                im1 = im1.resize(size)\n",
    "                im1 = im1.save(os.path.join( vessel_path, f\"{cur_img_name}.png\" )) # _pre_orig_{i}\n",
    "\n",
    "                \"\"\"\n",
    "                predict_b = np.where(predict >= 0.15, 1, 0) # first value is the threshold\n",
    "                pb = predict_b.transpose(2,3,1,0).squeeze()\n",
    "                im2 = Image.fromarray(np.uint8(pb*255))\n",
    "                # im2 = PIL.ImageOps.invert(im2)\n",
    "                im2 = im2.resize(size)\n",
    "                im2 = im2.save(os.path.join( vessel_path_thresh, f\"{cur_img_name}.png\" ))\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzqK_4obhDq0"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DEmHVraRV6FV",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    data_path = train_path\n",
    "    weight_path = os.path.join(ckpt_vessel_path, \"drive-epoch40.pth\")\n",
    "\n",
    "    result_path = vessel_path\n",
    "\n",
    "    checkpoint = torch.load(weight_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "    test_dataset = vessel_dataset(data_path)\n",
    "    test_loader = DataLoader(test_dataset, 1, shuffle=False,  num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = FR_UNet(num_classes=1, num_channels=1, feature_scale=2,  dropout=0.2, fuse=True, out_ave=True)\n",
    "    for key in list(checkpoint['state_dict'].keys()):\n",
    "                checkpoint['state_dict'][key.replace('module.', '')] = checkpoint['state_dict'].pop(key)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "    filters = [int(64/2)]\n",
    "    num_classes = 3 # bg, vessel1, vessel2\n",
    "    model.final1 = nn.Conv2d(\n",
    "        filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "    model.final2 = nn.Conv2d(\n",
    "        filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "    model.final3 = nn.Conv2d(\n",
    "        filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "    model.final4 = nn.Conv2d(\n",
    "        filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "    model.final5 = nn.Conv2d(\n",
    "        filters[0], num_classes, kernel_size=1, padding=0, bias=True)\n",
    "    model.fuse = nn.Conv2d(\n",
    "        5, num_classes, kernel_size=1, padding=0, bias=True)\n",
    "\n",
    "\n",
    "    train = Trainer(model, train_loader=test_loader, val_loader=test_loader)\n",
    "    train.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[0 1]\n",
      "[0 1 2]\n",
      "[0 1]\n",
      "[0 1 2]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = train_path\n",
    "weight_path = r\"C:\\Users\\Prinzessin\\projects\\decentnet\\datasceyence\\examples\\example_results\\230713102104\\checkpoint-epoch40.pth\"\n",
    "result_path = vessel_path\n",
    "\n",
    "checkpoint = torch.load(weight_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "test_dataset = vessel_dataset(data_path)\n",
    "test_loader = DataLoader(test_dataset, 1, shuffle=False,  num_workers=0, pin_memory=True)\n",
    "\n",
    "model = FR_UNet(num_classes=3, num_channels=1, feature_scale=2,  dropout=0.2, fuse=True, out_ave=True)\n",
    "for key in list(checkpoint['state_dict'].keys()):\n",
    "            checkpoint['state_dict'][key.replace('module.', '')] = checkpoint['state_dict'].pop(key)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "test = Tester(model, checkpoint, test_loader, data_path)\n",
    "test.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "18432/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "id": "oFaRc2khXH6Z",
    "outputId": "07f414a4-ff1e-44f9-d013-52885bee57ee"
   },
   "outputs": [],
   "source": [
    "data_file = os.listdir(train_path)[0]\n",
    "path = os.path.join(train_path, data_file)\n",
    "\n",
    "img = Image.open(path) # .convert(\"L\")\n",
    "\n",
    "# red, green, blue = img.split()\n",
    "\n",
    "t = ToTensor()\n",
    "a = Normalize((0.5, ), (0.5, ))\n",
    "p = ToPILImage()\n",
    " \n",
    "plt.figure()\n",
    "plt.imshow(p((t(img))), cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(p((t(img))), cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(p((t(img))), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7RMK0Z1KTjy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMwxit8KzkRQzMfmLculUkK",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
