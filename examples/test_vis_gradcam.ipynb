{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec24d12-30ca-4840-84b1-e3f15df6d86e",
   "metadata": {},
   "source": [
    "Example code for something that could be gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6a08a7-d96f-4a30-a51d-cde9bbdeccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load and preprocess an image\n",
    "def load_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img_tensor = preprocess(img)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "# Function to perform Grad-CAM\n",
    "class GradCam:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "\n",
    "        # Register a hook to capture gradients during backward pass\n",
    "        self.hook = self.register_hooks()\n",
    "\n",
    "    def register_hooks(self):\n",
    "        def hook_fn(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        target_layer = self.model._modules.get(self.target_layer)\n",
    "        hook = target_layer.register_forward_hook(hook_fn)\n",
    "        return hook\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def backward(self, output):\n",
    "        self.model.zero_grad()\n",
    "        output.backward()\n",
    "\n",
    "    def generate_heatmap(self, input_tensor, class_idx):\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Perform forward and backward pass\n",
    "        output = self.forward(input_tensor)\n",
    "        target = output[0][class_idx]\n",
    "        self.backward(target)\n",
    "\n",
    "        # Calculate the importance weights (gradients)\n",
    "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "\n",
    "        # Get the activations from the target layer\n",
    "        target_layer_output = self.hook.output[0]\n",
    "\n",
    "        # Weighted sum of activations to get the Grad-CAM heatmap\n",
    "        grad_cam = torch.sum(weights * target_layer_output, dim=1, keepdim=True)\n",
    "        grad_cam = F.relu(grad_cam)\n",
    "\n",
    "        # Resize the heatmap to match the input image size\n",
    "        grad_cam = F.interpolate(grad_cam, size=(input_tensor.shape[2], input_tensor.shape[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dfa9036-abd4-4543-ae80-66ce362b1bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86441eb9-7c25-4154-ac8e-a46b310cd6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg = torchvision.models.vgg19(pretrained=True)\n",
    "\n",
    "# disect the network to access its last convolutional layer\n",
    "vgg.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c710b5c-16f0-4cd4-987d-f8ebc49d9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resi(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resi, self).__init__()\n",
    "        \n",
    "        # get the pretrained VGG19 network\n",
    "        self.vgg = torchvision.models.vgg19(pretrained=True)\n",
    "        \n",
    "        # disect the network to access its last convolutional layer\n",
    "        self.features_conv = self.vgg.features[:36]\n",
    "        \n",
    "        print(self.features_conv)\n",
    "        \n",
    "        # get the max pool of the features stem\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # get the classifier of the vgg19\n",
    "        self.classifier = self.vgg.classifier\n",
    "        \n",
    "        \n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        # register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # apply the remaining pooling\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view((1, -1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37533ea5-1966-442d-96e1-881f7ff3cda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (17): ReLU(inplace=True)\n",
      "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (24): ReLU(inplace=True)\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): ReLU(inplace=True)\n",
      "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (31): ReLU(inplace=True)\n",
      "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (33): ReLU(inplace=True)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU(inplace=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([117])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained ResNet model\n",
    "model = Resi()\n",
    "model.eval()\n",
    "\n",
    "# get the image from the dataloader\n",
    "try:\n",
    "    img = load_image(\"C:/Users/Prinzessin/projects/decentnet/datasceyence/examples/example_data/eye/AMD/A0001.jpg\")\n",
    "except:\n",
    "    img = load_image(\"C:/Users/Christina/Documents/datasceyence/examples/example_data/eye/fundus/A0001.jpg\")\n",
    "\n",
    "# get the most likely prediction of the model\n",
    "pred = model(img) # .argmax(dim=1)\n",
    "\n",
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b589b0-5b75-41ba-b7f6-0c3bc3faf8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b8b93c2eb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the gradient of the output with respect to the parameters of the model\n",
    "#pred[:, 386].backward()\n",
    "pred[:, 386].backward()\n",
    "#  \n",
    "# pull the gradients out of the model\n",
    "gradients = model.get_activations_gradient()\n",
    "\n",
    "# pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "# get the activations of the last convolutional layer\n",
    "activations = model.get_activations(img).detach()\n",
    "\n",
    "# weight the channels by corresponding gradients\n",
    "for i in range(512):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    \n",
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# relu on top of the heatmap\n",
    "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc103405-99b8-41bd-a872-d812b253e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
