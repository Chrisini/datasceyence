{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DecentNet from conv layer\n",
    "\n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "     \n",
    "* pruning actually doesn\"t work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46441962-88b7-4cd6-bd4a-a71a6fbbd427",
   "metadata": {},
   "source": [
    "pip install:\n",
    "    pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "# from torch.nn import init\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "import os\n",
    "import torchmetrics\n",
    "\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from typing import Optional, List, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys \n",
    "sys.path.insert(0, \"../helper\")\n",
    "from visualisation.feature_map import *\n",
    "\n",
    "# Turn interactive plotting off\n",
    "plt.ioff()\n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "sys.path.insert(0, \"/helper\")\n",
    "sys.path.insert(0, \"./helper\")\n",
    "sys.path.insert(0, \"../helper\")\n",
    "print(sys.path)\n",
    "\"\"\"\n",
    "\n",
    "# own module\n",
    "#from visualisation.feature_map import *\n",
    "\n",
    "import random\n",
    "\n",
    "print()\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "debug_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7f1ab-1129-436d-aed0-5643651c84a0",
   "metadata": {},
   "source": [
    "# Conv experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38836e4-6905-4e0c-b344-bcc3b8094388",
   "metadata": {},
   "source": [
    "## conv2d layer (slightly adapted original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b972ff66-723c-43a1-a9d9-80c43b450efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConvNd(torch.nn.Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    in_channels: int\n",
    "    _reversed_padding_repeated_twice: List[int]\n",
    "    out_channels: int\n",
    "    kernel_size: Tuple[int, ...]\n",
    "    stride: Tuple[int, ...]\n",
    "    padding: Union[str, Tuple[int, ...]]\n",
    "    dilation: Tuple[int, ...]\n",
    "    transposed: bool\n",
    "    output_padding: Tuple[int, ...]\n",
    "    groups: int\n",
    "    padding_mode: str\n",
    "    weight: Tensor\n",
    "    bias: Optional[Tensor]\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Tuple[int, ...],\n",
    "                 stride: Tuple[int, ...],\n",
    "                 padding: Tuple[int, ...],\n",
    "                 dilation: Tuple[int, ...],\n",
    "                 transposed: bool,\n",
    "                 output_padding: Tuple[int, ...],\n",
    "                 groups: int,\n",
    "                 bias: bool,\n",
    "                 padding_mode: str,\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        print(factory_kwargs)\n",
    "        super().__init__()\n",
    "        if groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            #self.importance = Parameter(torch.empty(\n",
    "            #    (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            #self.importance = Parameter(torch.empty(\n",
    "            #    (out_channels, in_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "class CustomConv2d(_ConvNd):\n",
    "    \n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = stride #_pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super().__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        print(self.stride)\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369256cb-1ea4-4ff2-8ce5-a820511e0779",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2823e218-5eb3-44d9-a277-2cf17b772c84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CustomConv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv2 = CustomConv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv3 = CustomConv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv1x1 = CustomConv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "        self.K = 100 \n",
    "        self.L = 10 # last one\n",
    "        self.num_of_bases = 1 # 3rd dim\n",
    "        \n",
    "        if False:\n",
    "            self.conv1 = Conv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv2 = Conv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv3 = Conv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv1x1 = Conv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "        # 4x16384\n",
    "        # self.fc1 = nn.Linear(10*10*10, 10)\n",
    "        #self.fc2 = nn.Linear(10, 10)\n",
    "        \n",
    "        #self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        #self.sub_concept_pooling = nn.modules.MaxPool2d((self.K, 1), stride=(1,1))\n",
    "        #self.instance_pooling = nn.modules.MaxPool2d((opt.num_of_bases, 1), stride=(1,1))\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.mish1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.mish2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.mish3(x)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        #print(x.size())\n",
    "        #print(x.size()[2:])\n",
    "        \n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        #x = x.view(-1, self.L, self.K, 10)\n",
    "        \n",
    "        # input, kernel_size, stride, padding, dilation, ceil_mode\n",
    "        #x = self.sub_concept_pooling(x).view(-1, self.L, self.num_of_bases).permute(0,2,1).unsqueeze(1)\n",
    "        \n",
    "        # output = F.sigmoid(x)\n",
    "        # x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        #x = torch.flatten(x, 1)\n",
    "        # x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.fc2(x)\n",
    "        #output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76fa05-b47e-4264-85cf-766d8ca060d3",
   "metadata": {},
   "source": [
    "## normal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0819306e-ff93-4f38-898b-4c1cd2221b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for i_batch, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if i_batch == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i_batch * len(data), len(train_loader.dataset),\n",
    "                100. * i_batch / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            \"\"\"\n",
    "            if i == 0 and epoch % args.log_interval == 0:\n",
    "            # if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot(path=f\"example_results/feature_map_{epoch}.png\")\n",
    "                \"\"\"\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.1\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 5\n",
    "        self.save_model = True\n",
    "        \n",
    "\n",
    "def main_train():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader, epoch)\n",
    "        #scheduler.step()\n",
    "        \n",
    "        \n",
    "        if args.save_model and epoch % args.log_interval == 0:\n",
    "            torch.save(model.state_dict(), f\"example_results/mnist_cnn_{epoch}.ckpt\")\n",
    "\n",
    "\n",
    "def main_test():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "\n",
    "    if True:\n",
    "        model.load_state_dict(torch.load(\"example_results/mnist_cnn_5.ckpt\"))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(\"example_results/pruned_model.ckpt\"))\n",
    "    \n",
    "\n",
    "    # model = torch.load(model.state_dict(), \"example_results/mnist_cnn_30.ckpt\")\n",
    "    if False:\n",
    "        test(args, model, device, test_loader, 0)\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38eb5e24-3f20-4788-8344-66a98d6791bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1242e1a-c691-4cb6-a279-9904cedcd806",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_to_prune= main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "270319e6-3827-47cd-a579-476c89217968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(model_to_prune.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {},
   "source": [
    "# DecentNet trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        \n",
    "        self.ms_x = ms_x # list of integers\n",
    "        self.ns_x = ns_x # list of integers\n",
    "        self.data = data # list of tensors\n",
    "                \n",
    "    def set(self, data, ms_x, ns_x):\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        self.data = data\n",
    "    \n",
    "    def get(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        # amout of channels need to have same length as m and n lists\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    \n",
    "    \n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfa123-54e1-4053-94c3-52b45ec0859c",
   "metadata": {},
   "source": [
    "## DecentFilter\n",
    "* conv2d problem: https://stackoverflow.com/questions/61269421/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # convolution happens in here\n",
    "    \n",
    "    def __init__(self, ms_in, ns_in, m_this, n_this,\n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 padding_mode=\"zeros\",\n",
    "                 dilation=3, \n",
    "                 # transposed=None, \n",
    "                 device=None, \n",
    "                 dtype=None):\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        \n",
    "        # padding\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        \n",
    "         \n",
    "        # convolution\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding_mode = padding_mode\n",
    "        self.padding = padding\n",
    "        self.dilation = _pair(dilation)\n",
    "        #self.transposed = transposed\n",
    "        \n",
    "        \n",
    "        # weights\n",
    "        assert len(ms_in) == len(ns_in), \"ms_in and ns_in are not of same length\"\n",
    "        self.n_weights = len(ms_in)\n",
    "        \n",
    "        # position\n",
    "        # self.non_trainable_param = nn.Parameter(torch.Tensor([1.0]), requires_grad=False)\n",
    "        # todo\n",
    "        self.ms_in = nn.Parameter(torch.Tensor(ms_in), requires_grad=False) # ms_in # list\n",
    "        self.ns_in = nn.Parameter(torch.Tensor(ns_in), requires_grad=False) # ns_in # list\n",
    "        self.m_this = nn.Parameter(torch.Tensor([m_this]), requires_grad=False) # m_this # single integer\n",
    "        self.n_this = nn.Parameter(torch.Tensor([n_this]), requires_grad=False) # n_this # single integer\n",
    "        \n",
    "        # weight\n",
    "        # filters x channels x kernel x kernel\n",
    "        # self.weights = torch.autograd.Variable(torch.randn(1,n_weights,*self.kernel_size)).to(\"cuda\")\n",
    "        # self.weights = torch.nn.Parameter(torch.randn(1,n_weights,*self.kernel_size))\n",
    "        self.weights = torch.nn.Parameter(torch.empty((1, self.n_weights, *self.kernel_size), **factory_kwargs))\n",
    "        \n",
    "        #print(\"weight shape init\")\n",
    "        #print(self.weights.shape)\n",
    "            \n",
    "        # bias    \n",
    "        if False: \n",
    "            # bias:\n",
    "            # where should the bias be???\n",
    "            self.bias = Parameter(torch.empty(1, **factory_kwargs))\n",
    "        else:\n",
    "            #self.bias = False\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # reset weights and bias in filter\n",
    "        self.reset_parameters()\n",
    "            \n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*self.kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        torch.nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))        \n",
    "        \n",
    "    def forward(self, x:X) -> Tensor:\n",
    "        \n",
    "        # weights = 1 filter x channels x kernel x kernel\n",
    "        # x = batch x channels x width x height\n",
    "\n",
    "        # Find the indices (IDs) of pairs that exist in both lists\n",
    "        common_pairs = [[i_in, i_x] for i_in, (m_in, n_in) in enumerate(zip(self.ms_in, self.ns_in)) for i_x, (m_x, n_x) in enumerate(zip(x.ms_x, x.ns_x)) if (m_in==m_x and n_in==n_x)]\n",
    "        \n",
    "        if False:\n",
    "            print(common_pairs)\n",
    "            print(len(self.ms_in))\n",
    "            print(len(self.ns_in))\n",
    "            print(len(x.ms_x))\n",
    "            print(len(x.ns_x))\n",
    "\n",
    "            for pair in common_pairs:\n",
    "                print(f\"Common pair at indices {pair}: {self.ms_in[pair[0]], tmp_ms[pair[1]]}, {self.ns_in[pair[0]], tmp_ns[pair[1]]}\")\n",
    "        \n",
    "        common_pairs_a = np.array(common_pairs)\n",
    "        try:\n",
    "            f_ids = common_pairs_a[:,0]\n",
    "            x_ids = common_pairs_a[:,1]\n",
    "        except Exception as e:\n",
    "            print(\"error: no common pairs\")\n",
    "            print(\"pairs\", common_pairs_a)\n",
    "            print(\"pairs shape\", common_pairs_a.shape)\n",
    "            print(\"len ms in\", len(self.ms_in))\n",
    "            print(\"len ns in\", len(self.ns_in))\n",
    "            print(\"len ms x\", len(x.ms_x))\n",
    "            print(\"len ns x\", len(x.ns_x))\n",
    "            print(e)\n",
    "        \n",
    "        # filter data and weights based on common pairs of data and weights\n",
    "        tmp_x = x.data[:, x_ids, :, :]\n",
    "        tmp_w = self.weights[:, f_ids, :, :]\n",
    "        \n",
    "        if self.padding_mode != 'zeros':\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(F.pad(tmp_x, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            tmp_w, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, groups=1)\n",
    "        else:\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(tmp_x, tmp_w, self.bias, self.stride, self.padding, self.dilation, groups=1)\n",
    "        \n",
    "        #print(\"tmp_w\", tmp_w.shape)\n",
    "        \n",
    "        # print(x_data.shape, \"- batch x filters x width x height\")        \n",
    "        return x_data\n",
    "    \n",
    "    \"\"\"\n",
    "    def set_position_and_value(self, value, m_this, n_this):\n",
    "        self.weights = value # weights in this filter\n",
    "        self.m_this = m_this # single integer\n",
    "        self.n_this = n_this # single integer\n",
    "    \n",
    "    def get_position_and_value(self):\n",
    "        return self.weights, self.m_this, self.n_this\n",
    "    \"\"\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'DecentFilter(weights: ' + str(self.weights.shape) + ' at position: m_this=' + str(self.m_this) + ', n_this=' + str(self.n_this) + ')' + \\\n",
    "    '\\n with inputs: ms_in= ' + ', '.join(str(int(m.item())) for m in self.ms_in) + ', ns_in= ' + ', '.join(str(int(n.item())) for n in self.ns_in) + ')'\n",
    "    __repr__ = __str__\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdab4-e0b0-4523-882f-dad3ebf06090",
   "metadata": {},
   "source": [
    "## DecentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fddb74c7-5940-424d-aab8-2c75efd914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLayer(torch.nn.Module):\n",
    "    __constants__ = ['stride', 'padding', 'dilation', # 'groups',\n",
    "                     'padding_mode', # 'n_channels', #  'output_padding', # 'n_filters',\n",
    "                     'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "                \n",
    "    def __init__(self, ms_in:list, ns_in:list, n_filters:int,\n",
    "                 kernel_size: _size_2_t,  \n",
    "                 stride: _size_2_t = 1,  \n",
    "                 padding: Union[str, _size_2_t] = 0,  \n",
    "                 dilation: _size_2_t = 1,\n",
    "                 model_kwargs=None,\n",
    "                 #prune_keep:float = 0.9,\n",
    "                 #prune_keep_total:float = 0.5,\n",
    "                 #transposed: bool = False, \n",
    "                 #grid_size:int=81,\n",
    "                 #cc_metric=\"l2\",\n",
    "                 #output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 #groups: int = 1,\n",
    "                 bias: bool = True,  # not in use\n",
    "                 padding_mode: str = \"zeros\",  # not in use\n",
    "                 device=None,  # not in use\n",
    "                 dtype=None) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # prune numbers\n",
    "        self.prune_keep = model_kwargs[\"prune_keep\"] # in each update [0.0:1.0]\n",
    "        self.prune_keep_total = model_kwargs[\"prune_keep_total\"] # total [0.0:1.0]\n",
    "        \n",
    "        # connection cost\n",
    "        self.cc_metric = model_kwargs[\"cc_metric\"]\n",
    "        \n",
    "        # from prev layer\n",
    "        self.ms_in = ms_in\n",
    "        self.ns_in = ns_in\n",
    "        \n",
    "        self.original_size = len(self.ms_in) * n_filters\n",
    "        \n",
    "        \n",
    "        self.grid_size = model_kwargs[\"grid_size\"]\n",
    "        self.grid_sqrt = math.sqrt(self.grid_size)\n",
    "        assert self.grid_sqrt == int(self.grid_sqrt), f\"square root ({self.grid_sqrt}) from grid size {self.grid_size} not possible; possible exampes: 81 (9*9), 144 (12*12)\"\n",
    "        self.grid_sqrt = int(self.grid_sqrt)\n",
    "        \n",
    "        # use techniques from coo matrix\n",
    "        self.geometry_array = np.full(self.grid_size, np.nan)\n",
    "        # plus 1 here cause of to_sparse array\n",
    "        self.geometry_array[0:n_filters] = range(1,n_filters+1)\n",
    "        np.random.shuffle(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.reshape((self.grid_sqrt,self.grid_sqrt), order='C')\n",
    "        self.geometry_array = torch.tensor(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.to_sparse(sparse_dim=2).to(\"cuda\")\n",
    "\n",
    "        #print(self.geometry_array)\n",
    "        #print(self.geometry_array.values())\n",
    "\n",
    "        self.filter_list = torch.nn.ModuleList([])\n",
    "        for i_filter in range(n_filters):\n",
    "            # minus 1 here cause of to_sparse array\n",
    "            index = (self.geometry_array.values()-1 == i_filter).nonzero(as_tuple=True)[0]\n",
    "            m_this = self.geometry_array.indices()[0][index]\n",
    "            n_this = self.geometry_array.indices()[1][index]\n",
    "            f = DecentFilter(ms_in, ns_in, m_this, n_this, \n",
    "                             kernel_size=kernel_size, \n",
    "                             stride=stride, padding=padding, dilation=dilation)\n",
    "            self.filter_list.append(f)\n",
    "            # self.register_parameter(f\"filter {i_filter}\", f.weights)\n",
    "            \n",
    "            #torch.nn.Parameter(torch.empty((1, n_channels, *kernel_size), **factory_kwargs))\n",
    "    \n",
    "    def compute_layer_connection_cost(self) -> Tensor:\n",
    "        # compute connection cost for a layer\n",
    "        # based on previous layer (cause I only have input ms_in, n_in information)\n",
    "        # mean( sum( of connection cost between this filter and all incoming filters\n",
    "        # need it for loss - aka all layers, all filters together\n",
    "        # need it for swapping - this layer, all filters\n",
    "        # only the active ones (we need to use the indices for that)\n",
    "        # for swapping i need ??\n",
    "        # adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        # def get_cc(self, weight_factor=2.0, bias_penalize=True, no_penalize_last=False):\n",
    "        # https://stackoverflow.com/questions/74086766/how-to-find-total-cost-of-each-path-in-graph-using-dictionary-in-python\n",
    "        \"\"\"\n",
    "        num_linear = len(self.linears)\n",
    "        for i in range(num_linear):\n",
    "            if i == num_linear - 1 and no_penalize_last:\n",
    "                weight_factor = 0.\n",
    "            biolinear = self.linears[i]\n",
    "            dist = torch.sum(torch.abs(biolinear.out_coordinates.unsqueeze(dim=1) - biolinear.in_coordinates.unsqueeze(dim=0)),dim=2)\n",
    "            cc += torch.mean(torch.abs(biolinear.linear.weight)*(weight_factor*dist+self.l0))\n",
    "            if bias_penalize == True:\n",
    "                cc += torch.mean(torch.abs(biolinear.linear.bias)*(self.l0))\n",
    "        if self.token_embedding:\n",
    "            cc += torch.mean(torch.abs(self.embedding)*(self.l0))\n",
    "            #pass\n",
    "        \"\"\"\n",
    "            \n",
    "        cc = []\n",
    "        for f in self.filter_list:\n",
    "\n",
    "            #mn = torch.cat([torch.tensor(f.m_this), torch.tensor(f.n_this)])\n",
    "            #print(mn.shape)\n",
    "            #msns = torch.cat([torch.tensor(f.ms_in), torch.tensor(f.ns_in)]) # .transpose(1,0)\n",
    "            #print(msns.shape)\n",
    "            #cc.append(torch.cdist(mn.unsqueeze(dim=0), msns.transpose(1,0), 'euclidean') / 8) # number comes from 9*9 = 81 [0-8]\n",
    "            \n",
    "            mn = torch.cat([f.m_this.unsqueeze(0), f.n_this.unsqueeze(0)]).transpose(1,0)\n",
    "            #print(mn)\n",
    "            msns = torch.cat([f.ms_in.unsqueeze(0), f.ns_in.unsqueeze(0)]).transpose(1,0)\n",
    "            #print(msns)\n",
    "            # mean ( l2 norm as distance metric / normalisation term for l2 norm)\n",
    "            # mean of distances\n",
    "            # normalise with max=grid square root, min=0\n",
    "            cc.append(torch.mean( torch.cdist(mn.float(), msns.float()) / self.grid_sqrt )) \n",
    "        \n",
    "        # mean connection cost of a layer\n",
    "        return torch.mean(torch.tensor(cc))\n",
    "    \n",
    "    def compute_channel_importance(self, i_f:int) -> list:\n",
    "        # channel importance metric for pruning\n",
    "        # based on l2 norm = magnitude = euclidean distance\n",
    "        \n",
    "        ci = []\n",
    "        \n",
    "        #print(self.filter_list[i_f].weights.shape)\n",
    "        \n",
    "        for i_w in range(self.filter_list[i_f].weights.shape[1]):\n",
    "            # importance of a kernel in a layer\n",
    "            #print(self.filter_list[i_f].weights[:,i_w].shape)\n",
    "            # maybe the kernel trigger todo\n",
    "            if self.cc_metric == 'l2':\n",
    "                ci.append(self.filter_list[i_f].weights[:,i_w].norm(2).detach().cpu().numpy()) # .detach().cpu().numpy()\n",
    "            \n",
    "        return ci # channel importance list of a filter\n",
    "    \n",
    "    def swap_filter(self):\n",
    "        # we swap filters within the layer\n",
    "        # based on connection cost\n",
    "        # filter can move a maximum of two positions per swap\n",
    "    \n",
    "        # change positions\n",
    "        # change\n",
    "        \n",
    "        print(\"swap here\")\n",
    "        \n",
    "        self.m_this = self.m_this # single integer\n",
    "        self.n_this = self.n_this # single integer\n",
    "    \n",
    "    def grow_filter(self) -> None:\n",
    "        # introduce new filters in a layer\n",
    "        # based on \n",
    "        # algorithmic growth process \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def grow_channel(self) -> None:\n",
    "        # introduce new channel in a layer\n",
    "        # based on connection cost??\n",
    "        # algorithmic growth process \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def prune_filter(self) -> None:\n",
    "        # delete filter in a layer\n",
    "        pass\n",
    "    \n",
    "    def prune_channel(self, i_f:int, keep_ids:list) -> None:\n",
    "        # delete channels in a filter\n",
    "        # based on importance score\n",
    "        \n",
    "        #print(\"prune here\")\n",
    "        \n",
    "        #for f in self.filter_list:\n",
    "            # f.update()\n",
    "            \n",
    "        # only keep \"the best\" weights\n",
    "        \n",
    "        if False:\n",
    "            for i in keep_ids:\n",
    "                print(i)\n",
    "                print(self.filter_list[i_f].ms_in[i])\n",
    "                print( torch.nn.Parameter(self.filter_list[i_f].ms_in[keep_ids]) )\n",
    "        \n",
    "        if random.randint(1, 100) == 5:\n",
    "            print()\n",
    "            print(\"info at random intervals\")\n",
    "            print(keep_ids)\n",
    "            print(self.filter_list[i_f].weights[:, keep_ids, :, :].shape)\n",
    "            print(self.filter_list[i_f].weights.shape)        \n",
    "        \n",
    "        self.filter_list[i_f].weights = torch.nn.Parameter(self.filter_list[i_f].weights[:, keep_ids, :, :])\n",
    "        self.filter_list[i_f].ms_in = torch.nn.Parameter(self.filter_list[i_f].ms_in[keep_ids])\n",
    "        #[self.filter_list[i_f].ms_in[i] for i in keep_ids] # self.ms_in[remove_ids]\n",
    "        self.filter_list[i_f].ns_in = torch.nn.Parameter(self.filter_list[i_f].ns_in[keep_ids])\n",
    "        # [self.filter_list[i_f].ns_in[i] for i in keep_ids] # self.ns_in[remove_ids]\n",
    "\n",
    "        \n",
    "        # pruning based on a metric\n",
    "        \n",
    "        # delete layer with id\n",
    "        # delete channels in each layer with id\n",
    "        \n",
    "        # channel deactivation\n",
    "        # require_grad = False/True for each channel\n",
    "        #deactivate_ids = [1, 2, 6]\n",
    "        #self.active[deactivate_ids] = False\n",
    "        #print(\"weight\")\n",
    "        #print(self.weight.shape)\n",
    "        #print(self.weight[:,self.active,:,:].shape)\n",
    "        # this is totally wrong - iterative will break after first iteration\n",
    "        #print()\n",
    "        # Good to hear it’s working, although I would think you’ll get an error at some point in your code, as the cuda() call creates a non-leaf tensor.\n",
    "        #self.weight = torch.nn.Parameter(  self.weight[:,self.active,:,:] ) # .detach().cpu().numpy()\n",
    "        #self.weight = self.weight.cuda()\n",
    "        #print(self.weight.shape)\n",
    "        #print(self.active)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def reset_parameters(self) -> None:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def extra_repr(self):\n",
    "        \n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        # return s.format(**self.__dict__)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "    \"\"\"       \n",
    "\n",
    "        \n",
    "    def forward(self, x: X) -> Tensor:\n",
    "        \n",
    "        # calculate output for each filter\n",
    "        output_list = []\n",
    "        m_list = []\n",
    "        n_list = []\n",
    "        for f in self.filter_list:\n",
    "            # output = filter(input)\n",
    "            output_list.append(f(x))\n",
    "            m_list.append(f.m_this)\n",
    "            n_list.append(f.n_this)\n",
    "        x.ms_x = m_list\n",
    "        x.ns_x = n_list\n",
    "        x.data = torch.cat(output_list, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def update(self):\n",
    "        # 1: deactivate channels based on importance metric\n",
    "        '''\n",
    "        n = 256\n",
    "        n_tmp = n * 0.5\n",
    "\n",
    "        for i in range (100):\n",
    "\n",
    "            n = n*0.97\n",
    "            if n <= n_tmp:\n",
    "                break\n",
    "            print(int(n))\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        amout_remove = 1\n",
    "        \n",
    "        all_ci = []\n",
    "        all_len = 0\n",
    "        for i_f in range(len(self.filter_list)):\n",
    "            all_len += len(self.filter_list[i_f].ms_in)\n",
    "            # list of lists\n",
    "            all_ci.append(self.compute_channel_importance(i_f))\n",
    "            #tmp_ids = sorted(range(len(all_ci)), key=lambda sub: all_ci[sub])\n",
    "          \n",
    "        print(\"=\"*1000)\n",
    "        print(all_len) # this is the size of the previous pruning\n",
    "        print(self.original_size)\n",
    "        print(self.prune_keep_total)\n",
    "        print(int(self.original_size * self.prune_keep_total))\n",
    "        \n",
    "        #self.log(f'{self.original_size}_active_channels', all_len, on_step=True, on_epoch=True)\n",
    "        \n",
    "        if all_len < int(self.original_size * self.prune_keep_total):\n",
    "            # if n percent have been pruned, stop this layer\n",
    "            print(\"pruning done for this layer\")\n",
    "        else:\n",
    "            n = int(all_len*self.prune_keep)\n",
    "            all_ci_flatten = [item for row in all_ci for item in row] # don't have equal lengths, so no numpy possible\n",
    "            index = sorted(range(all_len), key=lambda sub: all_ci_flatten[sub])[-n]\n",
    "            threshold_value = all_ci_flatten[index]\n",
    "\n",
    "            for i_f in range(len(self.filter_list)):\n",
    "\n",
    "                # channel importance list for this filter\n",
    "                ci = all_ci[i_f] # self.compute_channel_importance(i_f)\n",
    "\n",
    "                #print(ci)\n",
    "                #print(threshold_value)\n",
    "                # torch.where()\n",
    "                indices = np.where(ci >= threshold_value)[0] # just need the x axis\n",
    "\n",
    "\n",
    "                # indices should be list/np/detached\n",
    "                self.prune_channel(i_f, indices)\n",
    "                \n",
    "                #print(\"prune done\")\n",
    "\n",
    "                # ci = ci[indices] # probably not useful\n",
    "            \n",
    "            \n",
    "            # print(\"channel importance ci\", ci)\n",
    "            # keep_ids = random.sample(range(0, 8), 5)\n",
    "            #keep_ids = sorted(range(len(ci)), key=lambda sub: ci[sub])[amout_remove:]\n",
    "            #print(keep_ids)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "                print()\n",
    "                print(\"update\")\n",
    "                print()\n",
    "                n = len(test_list)\n",
    "                n = int(n*0.97)\n",
    "\n",
    "                print(\"keep\", n)\n",
    "\n",
    "                index = sorted(range(len(test_list)), key=lambda sub: test_list[sub])[-n]\n",
    "                threshold_value = test_list[index]\n",
    "                print(\"t\", threshold_value)\n",
    "\n",
    "                # get indices from array where condition\n",
    "                indices = np.where((test_list >= threshold_value))\n",
    "\n",
    "                print(\"v\", test_list)\n",
    "                print(\"s\", sorted(test_list))\n",
    "\n",
    "                test_list = np.array(test_list)[indices]\n",
    "\n",
    "                print(\"v\", test_list) # i want this to not be ordered\n",
    "                print(\"si\", index)\n",
    "\n",
    "\n",
    "                #test_list = random.sample(range(100, 200), len(test_list))\n",
    "\n",
    "                if n <= n_tmp:\n",
    "                    break\n",
    "\n",
    "\n",
    "                pass\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_filter_positions(self):\n",
    "        # in use for next layer input\n",
    "        \n",
    "        ms_this = []\n",
    "        ns_this = []\n",
    "        for f in self.filter_list:\n",
    "            ms_this.append(f.m_this)\n",
    "            ns_this.append(f.n_this)\n",
    "        \n",
    "        return ms_this, ns_this\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d0510096-312f-4ae3-8192-8165b649199c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c186cf1-c7db-45da-b779-f7ab88f3896f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentNet(nn.Module):\n",
    "    def __init__(self, model_kwargs, log_dir=\"\") -> None:\n",
    "        super(DecentNet, self).__init__()\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        out_dim = model_kwargs[\"out_dim\"]\n",
    "        out_dim.append(self.n_classes) # out_dim = [1, 32, 48, 64, 10]     \n",
    "        \n",
    "        grid_size = model_kwargs[\"grid_size\"]\n",
    "        assert not any(i > grid_size for i in out_dim), f\"filters need to be less than {grid_size}\"\n",
    "        self.grid_sqrt = int(math.sqrt(grid_size))\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        \n",
    "        # out_dim:list=[1, 16, 32, 64], n_classes:int=10, grid_size:int=81, prune_keep:float=0.9, prune_keep_total:float= 0.5, log_dir=None, , cc_metric='l2'\n",
    "\n",
    "        # backbone\n",
    "        \n",
    "        ms_in_1 = [torch.tensor(0)]\n",
    "        ns_in_1 = [torch.tensor(0)]\n",
    "        assert out_dim[0] == len(ms_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ms_in_1={len(ms_in_1)})\"\n",
    "        assert out_dim[0] == len(ns_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ns_in_1={len(ns_in_1)})\"\n",
    "        self.decent1 = DecentLayer(ms_in=ms_in_1, ns_in=ns_in_1, n_filters=out_dim[1], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        ms_in_2,ns_in_2 = self.decent1.get_filter_positions()\n",
    "        assert out_dim[1] == len(ms_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ms_in_2={len(ms_in_2)})\"\n",
    "        assert out_dim[1] == len(ns_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ns_in_2={len(ns_in_2)})\"\n",
    "        self.decent2 = DecentLayer(ms_in=ms_in_2, ns_in=ns_in_2, n_filters=out_dim[2], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        ms_in_3,ns_in_3 = self.decent2.get_filter_positions()\n",
    "        assert out_dim[2] == len(ms_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ms_in_3={len(ms_in_3)})\"\n",
    "        assert out_dim[2] == len(ns_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ns_in_3={len(ns_in_3)})\"\n",
    "        self.decent3 = DecentLayer(ms_in=ms_in_3, ns_in=ns_in_3, n_filters=out_dim[3], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        ms_in_1x1,ns_in_1x1 = self.decent3.get_filter_positions()\n",
    "        assert out_dim[3] == len(ms_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ms_in_1x1={len(ms_in_1x1)})\"\n",
    "        assert out_dim[3] == len(ns_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ns_in_1x1={len(ns_in_1x1)})\"\n",
    "        self.decent1x1 = DecentLayer(ms_in=ms_in_1x1, ns_in=ns_in_1x1, n_filters=out_dim[-1], kernel_size=1, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        #self.tmp = torchvision.models.squeezenet1_0(torchvision.models.SqueezeNet1_0_Weights.IMAGENET1K_V1)\n",
    "        #self.tmp.classifier[1] = torch.nn.Conv2d(512, 10, kernel_size=(3,3))\n",
    "        \n",
    "        # head\n",
    "        self.fc = torch.nn.Linear(out_dim[-1], out_dim[-1])\n",
    "    \n",
    "        # activation\n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        # bias\n",
    "        self.bias1 = torch.nn.InstanceNorm2d(out_dim[1])\n",
    "        self.bias2 = torch.nn.InstanceNorm2d(out_dim[2])\n",
    "        self.bias3 = torch.nn.InstanceNorm2d(out_dim[3])\n",
    "        self.bias1x1 = torch.nn.InstanceNorm2d(out_dim[-1])\n",
    "        \n",
    "        # activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # init cc\n",
    "        self.cc = []\n",
    "        self.update_connection_cost()\n",
    "        \n",
    "        # get a position in filter list\n",
    "        self.m_plot = self.decent2.filter_list[0].m_this.detach().cpu().numpy()\n",
    "        self.n_plot = self.decent2.filter_list[0].n_this.detach().cpu().numpy()  \n",
    "        # self.plot_layer_of_1_channel(current_epoch=0) - not working here, dir not created yet\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        # hook on the data (for gradcam or something similar)\n",
    "        # https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
    "        if mode != 'val' and mode != 'test':\n",
    "            output = x.data.register_hook(self.activations_hook)\n",
    "        \n",
    "        \n",
    "        # global max pooling for MIL\n",
    "        x.data = F.max_pool2d(x.data, kernel_size=x.data.size()[2:])\n",
    "        \n",
    "        x.data = x.data.reshape(x.data.size(0), -1)\n",
    "        x.data = self.fc(x.data) \n",
    "        \n",
    "        # x.data = self.sigmoid(x.data)\n",
    "        \n",
    "        # x.data = self.tmp(x.data)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    \n",
    "    def activations_hook(self, grad):\n",
    "        # hook for the gradients of the activations\n",
    "        self.gradients = grad\n",
    "    def get_activations_gradient(self):\n",
    "        # method for the gradient extraction\n",
    "        return self.gradients\n",
    "    def get_activations(self, x):\n",
    "        # method for the activation exctraction\n",
    "        \n",
    "        print('0', x)\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        \n",
    "        print('1', x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        \n",
    "        print('2', x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        \n",
    "        print('3', x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        \n",
    "        print('1x1', x)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    def update_connection_cost(self):\n",
    "        self.cc = []\n",
    "        # self.cc.append(self.decent1.compute_layer_connection_cost()) # maybe not even needed ...\n",
    "        self.cc.append(self.decent2.compute_layer_connection_cost())\n",
    "        self.cc.append(self.decent3.compute_layer_connection_cost())\n",
    "        self.cc.append(self.decent1x1.compute_layer_connection_cost())\n",
    "        self.cc = torch.mean(torch.tensor(self.cc))\n",
    "        \n",
    "    def plot_layer_of_1_channel(self, current_epoch=0):\n",
    "        \n",
    "        # get each filter position that has a channel that matches\n",
    "        ms = []; ns = []\n",
    "        \n",
    "        #print(self.decent2.filter_list)\n",
    "        #print(\"**********************\")\n",
    "        #print(self.decent3.filter_list)\n",
    "\n",
    "        \n",
    "        # go through all filters in this layer\n",
    "        for f in self.decent3.filter_list:\n",
    "            \n",
    "            # if filter position in prev layer matches any channel in this layer\n",
    "            if any(pair == (self.m_plot, self.n_plot) for pair in zip(f.ms_in.detach().cpu().numpy(), f.ns_in.detach().cpu().numpy())):\n",
    "                \n",
    "                #print('match', f.m_this, f.n_this)\n",
    "                \n",
    "                # save position of each filter in this layer\n",
    "                ms.append(f.m_this.detach().cpu().numpy())\n",
    "                ns.append(f.n_this.detach().cpu().numpy())\n",
    "              \n",
    "            if False:\n",
    "                    print(\"nooooooooooooooo\")\n",
    "                    print(f.ms_in)\n",
    "                    print(self.m_plot)\n",
    "                    print(f.ns_in)\n",
    "                    print(self.n_plot)\n",
    "\n",
    "                    print((self.m_plot, self.n_plot))\n",
    "                \n",
    "        # visualising the previous and current layer neurons\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=100000, color='blue', alpha=0.1) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=50000, color='blue',alpha=0.2) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=25000, color='blue',alpha=0.3) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=500, color='blue') # previous layer\n",
    "        ax.scatter(ms, ns, color='red') # next layer\n",
    "        plt.xlim(0, self.grid_sqrt) # m coordinate of grid_size field\n",
    "        plt.ylim(0, self.grid_sqrt) # n coordinate of grid_size field\n",
    "        ax.grid() # enable grid line\n",
    "        fig.savefig(os.path.join(self.log_dir, f\"l2_m{int(self.m_plot[0])}_n{int(self.n_plot[0])}_{str(current_epoch)}.png\"))\n",
    "        \n",
    "\n",
    "    def update(self, current_epoch):\n",
    "        \n",
    "        # adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        #self.decent1.update()\n",
    "        self.decent2.update()\n",
    "        self.decent3.update()\n",
    "        #self.decent1x1.update()\n",
    "        \n",
    "        # measurement for updating\n",
    "        \n",
    "        # update layer by layer\n",
    "        \n",
    "\n",
    "        self.plot_layer_of_1_channel(current_epoch)\n",
    "        \n",
    "        \n",
    "        # connection cost has to be calculated after pruning\n",
    "        # self.cc which is updated is used for loss function\n",
    "        self.update_connection_cost()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## DecentModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "\n",
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "\n",
    "    \"\"\"\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # as we advance one step at end of training, we use `global_step - 1` to avoid saving duplicates\n",
    "        #trainer.fit_loop.global_step -= 1\n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer)\n",
    "            and self._save_on_train_epoch_end\n",
    "            and self._every_n_epochs > 0\n",
    "            and (trainer.current_epoch + 1) % self._every_n_epochs == 0\n",
    "            and self.current_score != -1\n",
    "        ):\n",
    "            self.save_checkpoint(trainer)\n",
    "        else:\n",
    "            print(self.current_score)\n",
    "        #trainer.fit_loop.global_step += 1\n",
    "    \"\"\"\n",
    "        \n",
    "        \n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        \"\"\"Save a checkpoint at the end of the training epoch.\"\"\"\n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]-1\n",
    "            if monitor_candidates[\"unpruned\"] != -1:\n",
    "                if self._every_n_epochs >= 1 and (trainer.current_epoch + 1) % self._every_n_epochs == 0:\n",
    "                    self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                self._save_last_checkpoint(trainer, monitor_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6be2dce9-a8d6-42c2-aed5-e25199d0a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(ModelCheckpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81d937-90ac-4b68-b255-5cbddabe624b",
   "metadata": {},
   "source": [
    "## explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a78f1c9b-367f-4bf1-9af8-ad5856cbd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation.hook import Hook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class FeatureMap1():\n",
    "    # =============================================================================\n",
    "    # ??? https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030\n",
    "    # ??? https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c\n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, model, layer, device=\"cpu\", ckpt_net_path=None, iterations=200, lr=1):\n",
    "        # =============================================================================\n",
    "        # Initialise iter, lr, model, layer\n",
    "        # =============================================================================\n",
    "\n",
    "        # settings for dreams\n",
    "        self.iterations=iterations\n",
    "        self.lr=lr\n",
    "        self.device = device\n",
    "\n",
    "        # model\n",
    "        if ckpt_net_path is not None:\n",
    "            model.load_state_dict(torch.load(ckpt_net_path)[\"model\"]) # 'dir/decentnet_epoch_19_0.3627.ckpt'\n",
    "        self.model = model.eval()\n",
    "        \n",
    "        # the (conv) layer to be visualised\n",
    "        self.layer = layer\n",
    "        print(\"\")\n",
    "        print(\"Layer:\", self.layer)\n",
    "        print(\"\")\n",
    "\n",
    "    def run(self, img_tensor):\n",
    "        # =============================================================================\n",
    "        # Feature map visualisation using hooks       \n",
    "        # A high activation means a certain feature was found. \n",
    "        # A feature map is called the activations of a layer after the convolutional operation.\n",
    "        # =============================================================================\n",
    "        \n",
    "        #print(\"i\", img_tensor.data.shape)\n",
    "        self.ii = img_tensor.data\n",
    "            \n",
    "        hook = Hook(self.layer)\n",
    "        output = self.model(img_tensor)\n",
    "        self.feature_maps = hook.output.data.squeeze()\n",
    "        \n",
    "        #print('o', output.shape)\n",
    "        #print('i', self.ii.shape)\n",
    "        #print('f', self.feature_maps.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def plot(self, path=None):\n",
    "        # =============================================================================\n",
    "        # plot 15 random feature maps + original image\n",
    "        # =============================================================================\n",
    "        fig, axarr = plt.subplots(4, 4)\n",
    "        # plt.figure(figsize=(100,100))\n",
    "        amount = self.feature_maps.shape[0]\n",
    "        print(\"amount of feature maps:\", amount)\n",
    "        if amount < 16:\n",
    "            sample_amount = amount\n",
    "        else:\n",
    "            sample_amount = 16\n",
    "        random_samples = random.sample(range(0, amount), sample_amount)\n",
    "        counter = 0  \n",
    "        idx,idx2 = [0, 0]\n",
    "        for idx in range(0, 4):\n",
    "            for idx2 in range(0, 4):\n",
    "                axarr[idx, idx2].axis('off')\n",
    "                try:\n",
    "                    axarr[idx, idx2].imshow(self.feature_maps[random_samples[counter]].cpu().detach().numpy())\n",
    "                    counter += 1\n",
    "                except:\n",
    "                    try:\n",
    "                        axarr[idx, idx2].imshow(self.feature_maps.cpu().detach().numpy())\n",
    "                        counter += 1\n",
    "                    except Exception as e:\n",
    "                        print(\"not possible to show feature maps image\")\n",
    "                        print(self.feature_maps.shape)\n",
    "                        print(e)\n",
    "\n",
    "        # overwrite first image with original image\n",
    "        try:\n",
    "            axarr[idx,idx2].imshow(self.ii.cpu().detach().numpy().transpose(1, 2, 0))\n",
    "        except:\n",
    "            try:\n",
    "                axarr[idx,idx2].imshow(self.ii.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "            except:\n",
    "                try: \n",
    "                    axarr[idx,idx2].imshow(self.ii.squeeze(1).cpu().detach().numpy().transpose(1, 2, 0))\n",
    "                except Exception as e:\n",
    "                    print(\"not possible to show original image\")\n",
    "                    print(e)\n",
    "        \n",
    "        if path is not None:\n",
    "            fig.savefig(path)\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "        # print(self.ii.shape)\n",
    "        # plt.imshow(self.ii.squeeze(0).cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4cd9fbda-b8d3-43ba-9b86-f28c9eaedaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_explain(model, layer, device='cuda'):\n",
    "\n",
    "    ichallenge_data = torchvision.datasets.ImageFolder('example_data/eye')\n",
    "    img, label = ichallenge_data.__getitem__(1)\n",
    "\n",
    "    # tensor preparation\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    resize = transforms.Resize(28)\n",
    "    img = to_tensor(img)\n",
    "    img = resize(img)\n",
    "    \n",
    "    import torch\n",
    "\n",
    "    tmp = torch.autograd.Variable(torch.randn(1, 1, 28, 28)) # batch x channel x width x height\n",
    "    # dense_input.shape\n",
    "\n",
    "    # todo: ms need to have same size as channel\n",
    "\n",
    "    img = X(img.unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "    \n",
    "    print(img.data.shape)\n",
    "    \n",
    "    # run feature map\n",
    "    dd = FeatureMap1(model=model, layer=layer, device=device, iterations=10, lr=0.1)\n",
    "    dd.run(img)\n",
    "    dd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3988da38-6bbe-4665-b6a1-dd682020435f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X(data: torch.Size([5, 2, 30, 30]) at positions: ms_x= 5, 7, ns_x= 3, 12)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tmp = torch.autograd.Variable(torch.randn(5, 2, 30, 30)) # batch x channel x width x height\n",
    "# dense_input.shape\n",
    "\n",
    "# todo: ms need to have same size as channel\n",
    "\n",
    "X(tmp, [torch.tensor(5), torch.tensor(7)], [torch.tensor(3), torch.tensor(12)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "### DecentLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    def __init__(self, kwargs, log_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "    \n",
    "        # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "        self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if False:\n",
    "            self.metric = { \"train_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"train_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"val_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"val_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "                   }\n",
    "        else:\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "\n",
    "            \n",
    "\n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Always use self for the first argument to instance methods.\n",
    "    Always use cls for the first argument to class methods.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def load_from_checkpoint(cls, checkpoint_path, **kwargs):\n",
    "        loaded = cls._load_from_checkpoint(\n",
    "            cls,  # type: ignore[arg-type]\n",
    "            checkpoint_path,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return cast(Self, loaded)\n",
    "    \n",
    "    @classmethod\n",
    "    def _load_from_checkpoint(\n",
    "        cls,\n",
    "        checkpoint_path,\n",
    "        **kwargs):\n",
    "        \n",
    "        map_location = None\n",
    "        with pl_legacy_patch():\n",
    "            checkpoint = pl_load(checkpoint_path, map_location=map_location)\n",
    "\n",
    "        # convert legacy checkpoints to the new format\n",
    "        checkpoint = _pl_migrate_checkpoint(\n",
    "            checkpoint, checkpoint_path=(checkpoint_path if isinstance(checkpoint_path, (str, Path)) else None)\n",
    "        )\n",
    "\n",
    "        if hparams_file is not None:\n",
    "            extension = str(hparams_file).split(\".\")[-1]\n",
    "            if extension.lower() == \"csv\":\n",
    "                hparams = load_hparams_from_tags_csv(hparams_file)\n",
    "            elif extension.lower() in (\"yml\", \"yaml\"):\n",
    "                hparams = load_hparams_from_yaml(hparams_file)\n",
    "            else:\n",
    "                raise ValueError(\".csv, .yml or .yaml is required for `hparams_file`\")\n",
    "\n",
    "            # overwrite hparams by the given file\n",
    "            checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY] = hparams\n",
    "\n",
    "        # TODO: make this a migration:\n",
    "        # for past checkpoint need to add the new key\n",
    "        checkpoint.setdefault(cls.CHECKPOINT_HYPER_PARAMS_KEY, {})\n",
    "        # override the hparams with values that were passed in\n",
    "        checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)\n",
    "\n",
    "        if issubclass(cls, pl.LightningDataModule):\n",
    "            return _load_state(cls, checkpoint, **kwargs)\n",
    "        if issubclass(cls, pl.LightningModule):\n",
    "            model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "            if not state_dict:\n",
    "                rank_zero_warn(f\"The state dict in {checkpoint_path!r} contains no parameters.\")\n",
    "                return model\n",
    "\n",
    "            device = next((t for t in state_dict.values() if isinstance(t, torch.Tensor)), torch.tensor(0)).device\n",
    "            assert isinstance(model, pl.LightningModule)\n",
    "            return model.to(device)\n",
    "\n",
    "        raise NotImplementedError(f\"Unsupported {cls}\")\n",
    "\n",
    "           \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        print(\"configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimiser = optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        else:\n",
    "            optimiser = optim.SGD(self.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # UPDATE\n",
    "        \n",
    "        print(\"* EPOCH START * \" * 10)\n",
    "        \n",
    "        if self.current_epoch == 0:\n",
    "            # should something like on training start ...\n",
    "            self.model.plot_layer_of_1_channel(current_epoch=0)\n",
    "        \n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            \n",
    "            # torch.save(model.state_dict(), PATH)\n",
    "            # torch.save(self.model.state_dict(), os.path.join(self.log_dir, f\"checkpoints/before_update_at_{self.current_epoch}.ckpt\"))\n",
    "\n",
    "            # self.save_checkpoint(f\"before_update_at_{self.current_epoch}.ckpt\")\n",
    "            \n",
    "            if debug_model:\n",
    "                print(\"before update\")\n",
    "                print(self.model)\n",
    "                \n",
    "            self.model.update(current_epoch = self.current_epoch)\n",
    "            \n",
    "            if debug_model: \n",
    "                print(\"after update\")\n",
    "                print(self.model)\n",
    "                \n",
    "            print(\"* MODEL UPDATED * \" * 10)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # 1       \n",
    "        loss = self._loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        self.gradcam(batch)\n",
    "        \n",
    "        \n",
    "        if batch_idx < 2:\n",
    "            print(\"training_step\", batch_idx)\n",
    "        #loss = torch.tensor(1)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # 2\n",
    "        self._loss_n_metrics(batch, mode=\"val\")\n",
    "        if batch_idx < 2:\n",
    "            print(\"validation_step\", batch_idx)\n",
    "        #self._loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        print(\"on_validation_epoch_end\")\n",
    "        # 3\n",
    "        \n",
    "        \"\"\"\n",
    "        for parameter in self.parameters():\n",
    "            print(\"parameter\")\n",
    "            print(parameter)\n",
    "        \"\"\" \n",
    "        #return\n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        print(\"on_train_epoch_end\")\n",
    "        # 4        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # model before the update\n",
    "        if (self.current_epoch % self.update_every_nth_epoch-1) == 0 and self.current_epoch != 0:\n",
    "            print('*'*100)\n",
    "            unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "\n",
    "            self.log(f'unpruned', unpruned, on_step=False, on_epoch=True)\n",
    "            #    \n",
    "            \n",
    "            return\n",
    "        \n",
    "        self.log(f'unpruned', -1, on_step=False, on_epoch=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # torch.save(model.state_dict(), PATH)\n",
    "            # torch.save(self.model.state_dict(), os.path.join(self.log_dir, f\"checkpoints/before_update_at_{self.current_epoch}.ckpt\"))\n",
    "\n",
    "            # self.save_checkpoint(f\"before_update_at_{self.current_epoch}.ckpt\")\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._loss_n_metrics(batch, mode=\"test\")\n",
    "        if batch_idx < 3:\n",
    "            print(\"test_step\", batch_idx)\n",
    "        \n",
    "        # torch.set_grad_enabled(True)\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "\n",
    "            self.gradcam(batch)\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        print(\"on_test_epoch_end\")\n",
    "        # 3\n",
    "        \n",
    "        \"\"\"\n",
    "        for parameter in self.parameters():\n",
    "            print(\"parameter\")\n",
    "            print(parameter)\n",
    "        \"\"\" \n",
    "        #return\n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def gradcam(self, batch):\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        # make it an X object\n",
    "        \n",
    "        img = torch.tensor(img, requires_grad=True)\n",
    "        \n",
    "        print('a', img.shape)\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        print('b1', tmp_img1)\n",
    "        print('b2', tmp_img2)\n",
    "        \n",
    "        model_output = self(tmp_img1)\n",
    "        \n",
    "        print('c1', tmp_img1)\n",
    "        print('c2', tmp_img2)\n",
    "        \n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "        \n",
    "        print('d1', tmp_img1)\n",
    "        \n",
    "        print(\"mo\", model_output)\n",
    "        print(\"max\", pred_max)\n",
    "        \n",
    "        model_output[:, pred_max].backward()\n",
    "        #  \n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        print('e2', tmp_img2)\n",
    "        \n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        # what is 512???\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "        \n",
    "        print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        print(\"hm\", heatmap.shape)\n",
    "        \n",
    "        # draw the heatmap\n",
    "        plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "    \n",
    "    def _loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth.unsqueeze(1).size(0), self.n_classes).scatter_(1, ground_truth.unsqueeze(1).to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \n",
    "        loss = self.criterion(model_output, ground_truth) # ground_truth_multi_hot)\n",
    "        cc = torch.mean(self.model.cc) * self.cc_weight\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "        \n",
    "        #print(model_output)\n",
    "        #print(pred_i)\n",
    "        #print(ground_truth)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            self.train_f1(preds=pred_i, target=ground_truth) \n",
    "            self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", self.train_acc)\n",
    "                print(\"f\", self.train_f1)\n",
    "                print(\"p\", self.train_prec)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        else:\n",
    "            self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c4f41c8a-13cc-4c2d-9e93-e99d6b2ea908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8cae21-08fe-4e44-a1ea-a7ae8d12ad0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def dev_routine(**kwargs):\n",
    "    \n",
    "    print(\"train kwargs\", kwargs['train_kwargs'])\n",
    "    print(\"model kwargs\", kwargs['model_kwargs'])\n",
    "    \n",
    "    train_kwargs = kwargs['train_kwargs']\n",
    "    model_kwargs = kwargs['model_kwargs']\n",
    "    \n",
    "    transform=transforms.Compose([\n",
    "        #transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(size=train_kwargs[\"img_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.MNIST('example_data', train=False, download=True, transform=transform)\n",
    "    # val_set = datasets.MNIST('example_data', train=False, transform=transform)\n",
    "    \n",
    "    print(len(dataset))\n",
    "    \n",
    "    # Split the indices in a stratified way\n",
    "    indices = np.arange(len(dataset))\n",
    "    # train_indices, val_indices = train_test_split(indices, train_size=0.8, test_size=0.2, stratify=dataset.targets)\n",
    "    train_indices, val_indices = train_test_split(indices, train_size=10, test_size=10, stratify=dataset.targets)\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_subset, shuffle=True, batch_size=train_kwargs[\"batch_size\"], num_workers=train_kwargs[\"num_workers\"])\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_subset, shuffle=False, batch_size=train_kwargs[\"batch_size\"], num_workers=train_kwargs[\"num_workers\"]) # , persistent_workers=True)\n",
    "    \n",
    "    logger = CSVLogger(\"example_results/lightning_logs\", name=train_kwargs[\"exp_name\"])\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(train_kwargs[\"ckpt_path\"], \"example_results\"),\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                         logger=logger,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         max_epochs=train_kwargs[\"epochs\"],\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1\",\n",
    "                                                   filename='{epoch}-{val_f1:.2f}-{unpruned:.0f}'),\n",
    "                                    DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                                    filename='{epoch}-{unpruned:.0f}-{val_f1:.2f}'),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    \n",
    "    \n",
    "    \n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(*[train_kwargs[\"ckpt_path\"], train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = DecentLightning.load_from_checkpoint(pretrained_filename, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(19) # To be reproducable\n",
    "                \n",
    "        # Initialize the LightningModule and LightningDataModule\n",
    "        model = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "        \n",
    "\n",
    "        # Train the model using a Trainer\n",
    "        trainer.fit(model, train_dataloader, val_dataloader)\n",
    "        \n",
    "        # we don't save the positions here ...\n",
    "        # model = DecentLightning.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, kwargs=kwargs) # Load best checkpoint after training\n",
    "\n",
    "        \n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_dataloader, verbose=False)\n",
    "    # test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    try:\n",
    "        result = {\"test accuracy on valset\": val_result[0][\"test_acc\"]}\n",
    "    except:\n",
    "        result = 0\n",
    "        \n",
    "    try:     \n",
    "        layer = model.model.decent2 # .filter_list[7]weights\n",
    "        run_explain(model, layer, device='cuda')\n",
    "    except Exception as e:\n",
    "        print(\" layer not working \" )\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "    print(result)\n",
    "\n",
    "    return model, result, val_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c1dfe-a818-43a0-848c-d6b5c0efe719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f1c73-2b3b-4c4c-91b2-b0eb2f6e88e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b02da990-f5b1-4cef-86cb-403eb1d6fcc2",
   "metadata": {},
   "source": [
    "### run lightning ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "39ad8b29-48de-4f62-88da-381f60d698f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 19\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'ckpt_path': 'example_results/lightning_logs', 'exp_name': 'tmp', 'load_ckpt_file': 'xversion_22/checkpoints/epoch=0-unpruned=10942-val_f1=0.06.ckpt', 'epochs': 1, 'img_size': 28, 'batch_size': 1, 'log_every_n_steps': 2, 'device': 'cuda', 'num_workers': 0}\n",
      "model kwargs {'n_classes': 10, 'out_dim': [1, 8, 16, 32], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 10, 'cc_metric': 'l2', 'update_every_nth_epoch': 2, 'prune_keep': 0.9, 'prune_keep_total': 0.5}\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type                | Params\n",
      "---------------------------------------------------\n",
      "0 | model      | DecentNet           | 8.3 K \n",
      "1 | criterion  | CrossEntropyLoss    | 0     \n",
      "2 | train_acc  | MulticlassAccuracy  | 0     \n",
      "3 | train_f1   | MulticlassF1Score   | 0     \n",
      "4 | train_prec | MulticlassPrecision | 0     \n",
      "5 | val_acc    | MulticlassAccuracy  | 0     \n",
      "6 | val_f1     | MulticlassF1Score   | 0     \n",
      "7 | val_prec   | MulticlassPrecision | 0     \n",
      "---------------------------------------------------\n",
      "6.3 K     Trainable params\n",
      "2.1 K     Non-trainable params\n",
      "8.3 K     Total params\n",
      "0.033     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configure_optimizers\n",
      "Sanity Checking: |                                                                               | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                              | 0/2 [00:00<?, ?it/s]validation_step 0\n",
      "Sanity Checking DataLoader 0:  50%|███████████████████████████                           | 1/2 [00:01<00:01,  0.59it/s]validation_step 1\n",
      "Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████| 2/2 [00:03<00:00,  0.65it/s]on_validation_epoch_end\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                  | 0/10 [00:00<?, ?it/s]* EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_31828\\3718186858.py:243: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(img, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a torch.Size([1, 1, 28, 28])\n",
      "b1 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "b2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "c1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "c2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "d1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "mo tensor([[ 1.4749,  3.9971, -3.2353,  0.1693,  0.0678, -5.7743, -1.6334, -1.4086,\n",
      "          1.8770,  0.9981]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "max tensor([1], device='cuda:0')\n",
      "e2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "0 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "1 X(data: torch.Size([1, 8, 26, 26]) at positions: ms_x= 10.0, 1.0, 2.0, 13.0, 14.0, 12.0, 2.0, 8.0, ns_x= 10.0, 6.0, 1.0, 11.0, 1.0, 2.0, 3.0, 15.0)\n",
      "2 X(data: torch.Size([1, 16, 24, 24]) at positions: ms_x= 10.0, 13.0, 13.0, 9.0, 0.0, 12.0, 14.0, 7.0, 16.0, 3.0, 7.0, 9.0, 9.0, 16.0, 6.0, 11.0, ns_x= 5.0, 0.0, 6.0, 7.0, 16.0, 6.0, 16.0, 8.0, 3.0, 3.0, 7.0, 13.0, 10.0, 14.0, 8.0, 10.0)\n",
      "3 X(data: torch.Size([1, 32, 22, 22]) at positions: ms_x= 7.0, 17.0, 5.0, 0.0, 12.0, 5.0, 12.0, 14.0, 15.0, 14.0, 1.0, 11.0, 0.0, 12.0, 6.0, 0.0, 6.0, 7.0, 11.0, 14.0, 5.0, 16.0, 13.0, 4.0, 13.0, 1.0, 2.0, 11.0, 9.0, 1.0, 2.0, 4.0, ns_x= 3.0, 14.0, 8.0, 7.0, 12.0, 14.0, 14.0, 13.0, 8.0, 7.0, 11.0, 16.0, 13.0, 0.0, 2.0, 2.0, 9.0, 14.0, 13.0, 10.0, 13.0, 15.0, 8.0, 1.0, 3.0, 2.0, 15.0, 10.0, 9.0, 0.0, 7.0, 9.0)\n",
      "1x1 X(data: torch.Size([1, 10, 22, 22]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "hm torch.Size([22, 22])\n",
      "hm torch.Size([22, 22])\n",
      "training_step 0\n",
      "Epoch 0:  10%|██████▍                                                         | 1/10 [00:04<00:38,  0.23it/s, v_num=48]a torch.Size([1, 1, 28, 28])\n",
      "b1 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "b2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "c1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "c2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "d1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "mo tensor([[ 3.1065,  3.0798, -3.8114,  1.4464,  0.1809, -5.8809, -1.1617, -1.5890,\n",
      "          2.1885,  2.6510]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "max tensor([0], device='cuda:0')\n",
      "e2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "0 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "1 X(data: torch.Size([1, 8, 26, 26]) at positions: ms_x= 10.0, 1.0, 2.0, 13.0, 14.0, 12.0, 2.0, 8.0, ns_x= 10.0, 6.0, 1.0, 11.0, 1.0, 2.0, 3.0, 15.0)\n",
      "2 X(data: torch.Size([1, 16, 24, 24]) at positions: ms_x= 10.0, 13.0, 13.0, 9.0, 0.0, 12.0, 14.0, 7.0, 16.0, 3.0, 7.0, 9.0, 9.0, 16.0, 6.0, 11.0, ns_x= 5.0, 0.0, 6.0, 7.0, 16.0, 6.0, 16.0, 8.0, 3.0, 3.0, 7.0, 13.0, 10.0, 14.0, 8.0, 10.0)\n",
      "3 X(data: torch.Size([1, 32, 22, 22]) at positions: ms_x= 7.0, 17.0, 5.0, 0.0, 12.0, 5.0, 12.0, 14.0, 15.0, 14.0, 1.0, 11.0, 0.0, 12.0, 6.0, 0.0, 6.0, 7.0, 11.0, 14.0, 5.0, 16.0, 13.0, 4.0, 13.0, 1.0, 2.0, 11.0, 9.0, 1.0, 2.0, 4.0, ns_x= 3.0, 14.0, 8.0, 7.0, 12.0, 14.0, 14.0, 13.0, 8.0, 7.0, 11.0, 16.0, 13.0, 0.0, 2.0, 2.0, 9.0, 14.0, 13.0, 10.0, 13.0, 15.0, 8.0, 1.0, 3.0, 2.0, 15.0, 10.0, 9.0, 0.0, 7.0, 9.0)\n",
      "1x1 X(data: torch.Size([1, 10, 22, 22]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "hm torch.Size([22, 22])\n",
      "hm torch.Size([22, 22])\n",
      "training_step 1\n",
      "Epoch 0:  20%|████████████▊                                                   | 2/10 [00:08<00:33,  0.24it/s, v_num=48]a torch.Size([1, 1, 28, 28])\n",
      "b1 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "b2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "c1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "c2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "d1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "mo tensor([[ 2.1195,  2.4246, -2.2514,  0.8789,  0.2707, -4.1037, -2.0507, -0.8640,\n",
      "          0.9475,  1.7275]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "max tensor([1], device='cuda:0')\n",
      "e2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "0 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "1 X(data: torch.Size([1, 8, 26, 26]) at positions: ms_x= 10.0, 1.0, 2.0, 13.0, 14.0, 12.0, 2.0, 8.0, ns_x= 10.0, 6.0, 1.0, 11.0, 1.0, 2.0, 3.0, 15.0)\n",
      "2 X(data: torch.Size([1, 16, 24, 24]) at positions: ms_x= 10.0, 13.0, 13.0, 9.0, 0.0, 12.0, 14.0, 7.0, 16.0, 3.0, 7.0, 9.0, 9.0, 16.0, 6.0, 11.0, ns_x= 5.0, 0.0, 6.0, 7.0, 16.0, 6.0, 16.0, 8.0, 3.0, 3.0, 7.0, 13.0, 10.0, 14.0, 8.0, 10.0)\n",
      "3 X(data: torch.Size([1, 32, 22, 22]) at positions: ms_x= 7.0, 17.0, 5.0, 0.0, 12.0, 5.0, 12.0, 14.0, 15.0, 14.0, 1.0, 11.0, 0.0, 12.0, 6.0, 0.0, 6.0, 7.0, 11.0, 14.0, 5.0, 16.0, 13.0, 4.0, 13.0, 1.0, 2.0, 11.0, 9.0, 1.0, 2.0, 4.0, ns_x= 3.0, 14.0, 8.0, 7.0, 12.0, 14.0, 14.0, 13.0, 8.0, 7.0, 11.0, 16.0, 13.0, 0.0, 2.0, 2.0, 9.0, 14.0, 13.0, 10.0, 13.0, 15.0, 8.0, 1.0, 3.0, 2.0, 15.0, 10.0, 9.0, 0.0, 7.0, 9.0)\n",
      "1x1 X(data: torch.Size([1, 10, 22, 22]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "hm torch.Size([22, 22])\n",
      "hm torch.Size([22, 22])\n",
      "Epoch 0:  30%|███████████████████▏                                            | 3/10 [00:13<00:30,  0.23it/s, v_num=48]a torch.Size([1, 1, 28, 28])\n",
      "b1 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "b2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "c1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "c2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "d1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "mo tensor([[ 2.7785,  2.2749, -2.5246,  1.2119,  0.7954, -3.8224, -2.7273, -0.3591,\n",
      "         -0.0509,  2.0090]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "max tensor([0], device='cuda:0')\n",
      "e2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "0 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "1 X(data: torch.Size([1, 8, 26, 26]) at positions: ms_x= 10.0, 1.0, 2.0, 13.0, 14.0, 12.0, 2.0, 8.0, ns_x= 10.0, 6.0, 1.0, 11.0, 1.0, 2.0, 3.0, 15.0)\n",
      "2 X(data: torch.Size([1, 16, 24, 24]) at positions: ms_x= 10.0, 13.0, 13.0, 9.0, 0.0, 12.0, 14.0, 7.0, 16.0, 3.0, 7.0, 9.0, 9.0, 16.0, 6.0, 11.0, ns_x= 5.0, 0.0, 6.0, 7.0, 16.0, 6.0, 16.0, 8.0, 3.0, 3.0, 7.0, 13.0, 10.0, 14.0, 8.0, 10.0)\n",
      "3 X(data: torch.Size([1, 32, 22, 22]) at positions: ms_x= 7.0, 17.0, 5.0, 0.0, 12.0, 5.0, 12.0, 14.0, 15.0, 14.0, 1.0, 11.0, 0.0, 12.0, 6.0, 0.0, 6.0, 7.0, 11.0, 14.0, 5.0, 16.0, 13.0, 4.0, 13.0, 1.0, 2.0, 11.0, 9.0, 1.0, 2.0, 4.0, ns_x= 3.0, 14.0, 8.0, 7.0, 12.0, 14.0, 14.0, 13.0, 8.0, 7.0, 11.0, 16.0, 13.0, 0.0, 2.0, 2.0, 9.0, 14.0, 13.0, 10.0, 13.0, 15.0, 8.0, 1.0, 3.0, 2.0, 15.0, 10.0, 9.0, 0.0, 7.0, 9.0)\n",
      "1x1 X(data: torch.Size([1, 10, 22, 22]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "hm torch.Size([22, 22])\n",
      "hm torch.Size([22, 22])\n",
      "Epoch 0:  40%|█████████████████████████▌                                      | 4/10 [00:17<00:26,  0.22it/s, v_num=48]a torch.Size([1, 1, 28, 28])\n",
      "b1 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "b2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "c1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "c2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "d1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "mo tensor([[ 1.4830,  3.3942, -2.0390,  1.3896,  0.3612, -4.5172, -1.1930, -2.1363,\n",
      "          1.6467,  1.6656]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "max tensor([1], device='cuda:0')\n",
      "e2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "0 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "1 X(data: torch.Size([1, 8, 26, 26]) at positions: ms_x= 10.0, 1.0, 2.0, 13.0, 14.0, 12.0, 2.0, 8.0, ns_x= 10.0, 6.0, 1.0, 11.0, 1.0, 2.0, 3.0, 15.0)\n",
      "2 X(data: torch.Size([1, 16, 24, 24]) at positions: ms_x= 10.0, 13.0, 13.0, 9.0, 0.0, 12.0, 14.0, 7.0, 16.0, 3.0, 7.0, 9.0, 9.0, 16.0, 6.0, 11.0, ns_x= 5.0, 0.0, 6.0, 7.0, 16.0, 6.0, 16.0, 8.0, 3.0, 3.0, 7.0, 13.0, 10.0, 14.0, 8.0, 10.0)\n",
      "3 X(data: torch.Size([1, 32, 22, 22]) at positions: ms_x= 7.0, 17.0, 5.0, 0.0, 12.0, 5.0, 12.0, 14.0, 15.0, 14.0, 1.0, 11.0, 0.0, 12.0, 6.0, 0.0, 6.0, 7.0, 11.0, 14.0, 5.0, 16.0, 13.0, 4.0, 13.0, 1.0, 2.0, 11.0, 9.0, 1.0, 2.0, 4.0, ns_x= 3.0, 14.0, 8.0, 7.0, 12.0, 14.0, 14.0, 13.0, 8.0, 7.0, 11.0, 16.0, 13.0, 0.0, 2.0, 2.0, 9.0, 14.0, 13.0, 10.0, 13.0, 15.0, 8.0, 1.0, 3.0, 2.0, 15.0, 10.0, 9.0, 0.0, 7.0, 9.0)\n",
      "1x1 X(data: torch.Size([1, 10, 22, 22]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "hm torch.Size([22, 22])\n",
      "hm torch.Size([22, 22])\n",
      "Epoch 0:  50%|████████████████████████████████                                | 5/10 [00:22<00:22,  0.22it/s, v_num=48]a torch.Size([1, 1, 28, 28])\n",
      "b1 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "b2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "c1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "c2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "d1 X(data: torch.Size([1, 10]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "mo tensor([[ 1.5938,  2.9105, -1.4894,  0.8089,  0.4103, -3.5185, -2.2989,  0.1664,\n",
      "          0.6289,  1.2715]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "max tensor([1], device='cuda:0')\n",
      "e2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "0 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "1 X(data: torch.Size([1, 8, 26, 26]) at positions: ms_x= 10.0, 1.0, 2.0, 13.0, 14.0, 12.0, 2.0, 8.0, ns_x= 10.0, 6.0, 1.0, 11.0, 1.0, 2.0, 3.0, 15.0)\n",
      "2 X(data: torch.Size([1, 16, 24, 24]) at positions: ms_x= 10.0, 13.0, 13.0, 9.0, 0.0, 12.0, 14.0, 7.0, 16.0, 3.0, 7.0, 9.0, 9.0, 16.0, 6.0, 11.0, ns_x= 5.0, 0.0, 6.0, 7.0, 16.0, 6.0, 16.0, 8.0, 3.0, 3.0, 7.0, 13.0, 10.0, 14.0, 8.0, 10.0)\n",
      "3 X(data: torch.Size([1, 32, 22, 22]) at positions: ms_x= 7.0, 17.0, 5.0, 0.0, 12.0, 5.0, 12.0, 14.0, 15.0, 14.0, 1.0, 11.0, 0.0, 12.0, 6.0, 0.0, 6.0, 7.0, 11.0, 14.0, 5.0, 16.0, 13.0, 4.0, 13.0, 1.0, 2.0, 11.0, 9.0, 1.0, 2.0, 4.0, ns_x= 3.0, 14.0, 8.0, 7.0, 12.0, 14.0, 14.0, 13.0, 8.0, 7.0, 11.0, 16.0, 13.0, 0.0, 2.0, 2.0, 9.0, 14.0, 13.0, 10.0, 13.0, 15.0, 8.0, 1.0, 3.0, 2.0, 15.0, 10.0, 9.0, 0.0, 7.0, 9.0)\n",
      "1x1 X(data: torch.Size([1, 10, 22, 22]) at positions: ms_x= 5.0, 7.0, 8.0, 2.0, 3.0, 16.0, 5.0, 2.0, 14.0, 10.0, ns_x= 14.0, 9.0, 16.0, 7.0, 2.0, 13.0, 6.0, 1.0, 3.0, 14.0)\n",
      "hm torch.Size([22, 22])\n",
      "hm torch.Size([22, 22])\n",
      "Epoch 0:  60%|██████████████████████████████████████▍                         | 6/10 [00:27<00:18,  0.22it/s, v_num=48]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|                                                                     | 0/10 [00:00<?, ?it/s]test_step 0\n",
      "a torch.Size([1, 1, 28, 28])\n",
      "b1 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "b2 X(data: torch.Size([1, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot register a hook on a tensor that doesn't require gradient",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, results, v_res \u001b[38;5;241m=\u001b[39m \u001b[43mdev_routine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mout_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# [1, 8, 16, 32], #[1, 16, 24, 32]\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrid_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcriterion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# torch.nn.BCEWithLogitsLoss(),\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# sgd adamw\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_lr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin_lr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr_update\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcc_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcc_metric\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupdate_every_nth_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 10\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprune_keep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 0.97, # in each epoch\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprune_keep_total\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# this number is not exact, depends on the prune_keep value\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrain_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mckpt_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexample_results/lightning_logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# \"example_results/lightning_logs\", # not in use??\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexp_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtmp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mload_ckpt_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxversion_22/checkpoints/epoch=0-unpruned=10942-val_f1=0.06.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#168, # keep mnist at original size, training didn't work when i increased the size ...\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlog_every_n_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;66;43;03m# 'test_batch_size': 1,\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 18, # 18 for computer, 0 for laptop\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[143], line 73\u001b[0m, in \u001b[0;36mdev_routine\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloader, val_dataloader)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# we don't save the positions here ...\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# model = DecentLightning.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, kwargs=kwargs) # Load best checkpoint after training\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m     \n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Test best model on validation and test set\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m val_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# test_result = trainer.test(model, test_loader, verbose=False)\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:755\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:795\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    794\u001b[0m )\n\u001b[1;32m--> 795\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    797\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 990\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    995\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1029\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mzero_grad(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mzero_grad_kwargs)\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[1;32m-> 1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:416\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtest_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[141], line 222\u001b[0m, in \u001b[0;36mDecentLightning.test_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# torch.set_grad_enabled(True)\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[141], line 256\u001b[0m, in \u001b[0;36mDecentLightning.gradcam\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, tmp_img1)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m, tmp_img2)\n\u001b[1;32m--> 256\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtmp_img1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc1\u001b[39m\u001b[38;5;124m'\u001b[39m, tmp_img1)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc2\u001b[39m\u001b[38;5;124m'\u001b[39m, tmp_img2)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[141], line 44\u001b[0m, in \u001b[0;36mDecentLightning.forward\u001b[1;34m(self, x, mode)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[111], line 106\u001b[0m, in \u001b[0;36mDecentNet.forward\u001b[1;34m(self, x, mode)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m#print(x)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# hook on the data (for gradcam or something similar)\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations_hook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# global max pooling for MIL\u001b[39;00m\n\u001b[0;32m    110\u001b[0m x\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x\u001b[38;5;241m.\u001b[39mdata, kernel_size\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\_tensor.py:527\u001b[0m, in \u001b[0;36mTensor.register_hook\u001b[1;34m(self, hook)\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39mregister_hook, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, hook)\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m--> 527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    528\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot register a hook on a tensor that \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt require gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    529\u001b[0m     )\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cannot register a hook on a tensor that doesn't require gradient"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGyCAYAAAB9ZmrWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABszUlEQVR4nO2deZRU5Zn/v7durb1v0AvdTaNBNhEMIFEW4UQUJAr2tJhgDAnnjDGDEzrMYRwTUfRnYjQzDsR4NM75JcZjMHFIw5igZPgZEHBBgQAujSBbN72vVd1dXUvfur8/Hm9X71R33aq71PM5p0513a6uevre932/93ne531eQZZlGQzDMAxjYixaG8AwDMMwsYbFjmEYhjE9LHYMwzCM6WGxYxiGYUwPix3DMAxjeljsGIZhGNPDYscwDMOYHhY7hmEYxvSw2DEMwzCmh8WOYRiGMT2jFruDBw/ijjvuQEFBAQRBwO7du/v9vrOzEw8++CAKCwvhcrkwffp0vPjii2rZyzAMwzCjZtRi19XVhVmzZuH5558f8vebNm3C3r178eqrr6KyshLl5eV48MEH8cYbb0RtLMMwDMOMBSGaQtCCIGDXrl1YvXp177Frr70W99xzD7Zs2dJ7bM6cOVixYgWefPLJqIxlGIZhmLFgVfsDb7rpJrzxxhtYv349CgoKcODAAZw5cwb/+Z//OeT7/X4//H5/7+tQKITW1lZkZ2dDEAS1zWMYhmEMgizL6OjoQEFBASyWKFNM5CgAIO/atavfMZ/PJ3/nO9+RAchWq1W22+3y7373u2E/47HHHpMB8IMf/OAHP/gx5KO6ujoaqZJlWZZV9+yee+45fPDBB3jjjTcwceJEHDx4EBs2bEBBQQFuueWWQe9/+OGHsWnTpt7XbrcbxcXFOHPmDLKystQ2T3WCwSD279+PpUuXwmazaW3OiLCtsYFtjQ1sa2wwkq2tra245pprkJqaGvVnqSp23d3d+PGPf4xdu3Zh5cqVAIDrrrsOJ06cwL//+78PKXYOhwMOh2PQ8aysLGRnZ6tpXkwIBoNISkpCdna27hsO2xob2NbYwLbGBiPZqqDGlJaq6+yCwSCCweCg2KooigiFQmp+FcMwDMNEzKg9u87OTnzxxRe9ry9cuIATJ04gKysLxcXFuPnmm7F582a4XC5MnDgR77zzDl555RU8++yzqhrOMAzDMJEyarE7evQoli5d2vtamW9bt24dXn75ZfzhD3/Aww8/jHvvvRetra2YOHEifvrTn+KBBx5Qz2qGYRiGGQWjFrslS5ZAHmFpXl5eHn77299GZRTDMAzDqAnXxmQYhmFMD4sdwzAMY3pY7BiGYRjTw2LHMAzDmB4WO4ZhGMb0sNgxDMMwpofFjmEYhjE9LHYMwzCM6WGxYxiGYUwPix3DMAxjeljsGIZhGNPDYscwDMOYHhY7hmEYxvSw2DEMwzCmh8WOYRiGMT0sdgzDMIzpYbFjGIZhTA+LHcMwDGN6WOwYhmEY08NixzAMw5geFjuGYRjG9LDYMQzDMKaHxY5hGIYxPSx2DMMwjOlhsWMYhmFMD4sdwzAMY3pY7BiGYRjTw2LHMAzDmB4WO4ZhGMb0sNgxDMMwpofFjmEYhjE9Vq0NYBjGwEgScOgQUFcH5OcDixYBoqi1VQwziFF7dgcPHsQdd9yBgoICCIKA3bt3D3pPZWUl7rzzTqSnpyM5ORnz5s1DVVWVGvYyDKMXKiqAkhJg6VJg7Vp6Limh4wyjM0Ytdl1dXZg1axaef/75IX9/7tw5LFy4EFOnTsWBAwdw6tQpbNmyBU6nM2pjGYbRCRUVQFkZcPly/+M1NXScBY/RGaMOY65YsQIrVqwY9vc/+clPcPvtt+OZZ57pPXb11VePzTqGYfSHJAEbNwKyPPh3sgwIAlBeDqxaxSFNRjeoOmcXCoWwZ88e/Ou//ituu+02/P3vf8ekSZPw8MMPY/Xq1UP+jd/vh9/v733t8XgAAMFgEMFgUE3zYoJiI9uqLmxrbFDF1sOHgZYWwOUa/j3NzcDBg8DChWP+moQ7r3HCiLaqgSDLQ92eRfjHgoBdu3b1Cll9fT3y8/ORlJSEJ598EkuXLsXevXvx4x//GPv378fNN9886DO2bt2Kxx9/fNDxHTt2ICkpaaymMQzDMAbH6/Vi7dq1cLvdSEtLi+qzVBW72tpaTJgwAd/61rewY8eO3vfdeeedSE5OxmuvvTboM4by7IqKilBXV4fs7OyxmhY3gsEg9u3bh2XLlsFms2ltzoiwrbEh4Ww9fBhYufLK79uzJ2rPLqHOa5wwkq0tLS3Iz89XRexUDWPm5OTAarVi+vTp/Y5PmzYNhw8fHvJvHA4HHA7HoOM2m033F6IvRrKXbY0NCWPr4sVAdjYlowx1rywIQGEhvU+FObuEOa9xxgi2qmmfqovK7XY75s2bh88//7zf8TNnzmDixIlqfhXDMFohisD27fSzIPT/nfJ62zZOTmF0xag9u87OTnzxxRe9ry9cuIATJ04gKysLxcXF2Lx5M+655x4sXry4d87uz3/+Mw4cOKCm3QzDaElpKbBzJ2Vl9l1+UFhIQldaqplpDDMUoxa7o0ePYunSpb2vN23aBABYt24dXn75Zdx111148cUX8dRTT+GHP/whpkyZgj/96U9YGEXsnmEYHVJaSssLuIIKYwBGLXZLlizBlXJa1q9fj/Xr14/ZKIZhDIIoAkuWaG0Fw1wRLgTNMAzDmB4WO4ZhGMb0sNgxDMMwpofFjmEYhjE9LHYMwzCM6WGxYxiGYUwPix3DMAxjeljsGIZhGNOjaiFohmHUJxSi/VIliX6W5fBz359DocHv7fsZAB0HgKoqwGoNl7IUBFofbrHQQ/lZEOgx8Gfl96I4uDwmw+gRFrtokCTa7gSgZ5WqvDOJgSxTE+rpGSxSgQAdDwTC71PeMxyKGPUVp4G/V/5e+c6+tgwUz+Fs7it2FguJps0G2O10vK8QiiL93sIxJH2QwGMWi91YqaigIrgtLcBrr9H+XtnZVA2ei+AyfVAETXnu6QF8PnooAjdQxAYKisMR/jkaT0r5nqSksY9xfT1IRZiV/yUUIvv6iqIihk4nCaLVSg/ld+wZxokEH7NY7MZCRQVQVkY92uUKH6+poeM7dyZE42H6I8s08AeD9Oz3h0VAETkg7Hkpg77Taayb674e3UjIcn9x7+wMe4zK34siCaAihDYbPa702cwo4TGLxW7USBLdHQ0V55FlGsnKy6kavJFGMGZUyDKJWjAIeL107NIl8mwUUes7oLtciTe/JQhh8RqIcp4kCejqAtxuOqeiGA6J2u30Xp+PPosFcIzwmAWAxW70HDrUf/+ugcgyUF1N7+Nq8KZBmT9TvLXubhK6vnNfskweCofmrozFEhazvoRCdF59PhJAgG4iFM8vKYmeHQ565vMcATxmAWCxGz11deq+j9ElfcWtq4sGX7+fxoW+c1BWa1jsFO+NGTvK/KTDEY62pabSOfb5gI4OugaK95ecTO9TPEEWvyHgMQsAi93oyc9X932MLpAkEjO/n8KS3d1hcVNCcRkZPJhqgcVC59/hCB9Tbkaam0kIFfFLSQl7gSx+X8JjFgAWu9GzaBFQWEgTu0PFwAWBfr9oUfxtYyJGSSbx+chz83rptTJw2mxAejqnzOsVJbknKYleK+LX2Bi+QXE4SPxcLuMlAakKj1kAWOxGjyhSqm5Z2dALmQBg27YE7ln6JRQKz7l1dITn3axW8gLS0ljcjMpA8VMyYhsa6LXTScKXlEQ/D5U0Y1p4zALA5cLGRmkppepOmND/eGFhQqTwGgkl26+5Gbh4kR41NSR4TieQlUUi53Sy0JkJm43m8zIzyUMHaHnZpUvhNuB2h0PVpofHLPbsxkxpKaXqHjwIeDzAnj0JVY1Az0gSeW1dXeTBKVVIlLAWX6LEwmKhUKbLFQ5fd3QAbW3hRKO0NPL6+s4Lmo4EH7NY7KJBFIGFC4E336TnBGk0ekTJ1uvqon6srM1yOunOnhMVGIDagZLtCVC40+8nT89mI8FLSwtneJqOBB6zWOwYwyLL4exJt5u8OWWtG2dOMpHQd9G7UiDA7Q5HAZQEF17Qbnz4EjKGIxCgQcnjoeeeHhqcUlMT6kaVUZm+wuf3A+3tNM/nclHbUtb08dyuMWGxYwyBLJOwdXSQyAUC4bAT33UzaqOEOmWZQuLNzUBTEwleRgY9J1RGpwngYYLRNUpoqb2d5uMAurtOSdHULCZBEIRwcksoRKHyy5dJCNPSyOPrW1eZ0S8sdozuUO6mASrZFwxSsgCHKRktsVjIo0tODnt7ra30mm++9A+LHaMbJCnsxXk8dEwQaK0UJ5swesLppEdPD3l77e10vKWFwpxOp5bWMUPBYsdoTk8P7XXW2kpi17cShsvFQsfoF6u1fyizqYmyOdPSaMkLt1/9wGLHaIayuLe9ne6Onc6wFzdw526G0TOKoGVkUNtta6N2rYhecjKLntaw2DFxx+8Pi5zPR3e/HKpkzIKy40JPD4XjFdFTsjh56YI2sNgxcUPZkNPtJq/O5aLalAxjRqxW8uqU+qweD4ldVhY9c7JVfGGxY8aOJNHuxnV1tBfWokVD9mBlAt/jobvdpCQNstckCfYjhyA21kEan4/A/KFtZXSCJMF25DBgA2xHDiM037g1HEWRPLtQiOakq6upD2RmmiTDOMJxQGtG7VAfPHgQd9xxBwoKCiAIAnbv3j3sex944AEIgoBt27ZFYSKjSyoqgJISYOlSYO1aei4poeNf4vPRFiuXLtEchsNBHTzexXadb1Ygd34Jcu5eiswNa5Fz91Lkzi+B882KK/8xE3eU65X9nZUAgOzvrDTF9bJY6CYvI4Nu+i5fBqqq6CZQ2e3ecEQwDuiFUYtdV1cXZs2aheeff37E9+3atQsffPABCgoKxmwco1MqKmhvrMuX+x+vqQHKyhB8vQJNTdSRm5rCtSq1KKzrfLMCmfeXwVLX31ZLfQ0y7y8z/ABqNhLheglCePuhYJD6SXU1zWMbaruhK4wDehO8UYvdihUr8OSTT+Kuu+4a9j01NTX453/+Z/z+97+HjWvqmAtJAjZuHLpXyjJkACgvR0OtBJuN5ic0qx4vSUh/lGwdmPsifGl/2mPlnPqpFxLseglC2NPz+Ujwampofk/3oneFcQAAUF6uq2ul+pxdKBTCfffdh82bN2PGjBlXfL/f74ff7+997flyNXEwGEQwGFTbPNVRbEwYWw8fDlfHHY72ZuScPYjg/IVjbuuSFOz3PBZsRw4j1N6C0Ei2tjXDcoRsHStq2Bov9GzrwOsVHPAMQJXrFQuiPa9JSaQL7e30SE+P3eL0uI0Dzc20d97CsV8rNcdVQZbHfg8hCAJ27dqF1atX9x576qmnsH//fvz1r3+FIAgoKSlBeXk5ysvLh/yMrVu34vHHHx90fMeOHUhSVhYzDMMwCYfX68XatWvhdruRlpYW1Wep6tkdO3YM27dvx/HjxyFEuGjq4YcfxqZNm3pfezweFBUVYenSpcjOzlbTvJgQDAaxb98+LFu2TPchW1VsPXwYWLnyim9reWVP1N7SyZP7MGvWMoji2Gy1HTncm+QwEnqwNV7o2daB1yvocmHfb36DZevXw9bd3Xs82usVC2JxXgMBCmna7TQdkJamTpJjPMcB7NkTlWfX0tIy5r8diKpid+jQITQ2NqK4uLj3mCRJ+Jd/+Rds27YNFy9eHPQ3DocDjiHS82w2m+7Foy9GsjcaW4M3LoYlMxuWupreeZS+yIIAKb8QofmLIarQM0XRNubBIzR/MSwZ2bDU69/WeKNHW4e7Xrbubti6u1W/XrFAzfOq7LbQ3Q00NtKyhZwc9aqxRDVmLV4MZGfTJONQwUFBAAoL6X1RXCs1x1RV1/Lfd999OHXqFE6cONH7KCgowObNm/HXv/5Vza9i4kwoRIvBq2pE1GzeDoDEoi/Ka8/j2/SxzkYU4X7CILYyfL2GQakw5PdT5mZ9Pf2sKaIIbKdrNUh5ldfbtunqWo1a7Do7O3uFDAAuXLiAEydOoKqqCtnZ2bj22mv7PWw2G/Ly8jBlyhS1bWfihNdLN3DV1SR64t2laHtpJ0J5E/q9T8ovRNtLO+G7vVQjSwfju904tjJ8vYZDEGgBekoKFUyvqqL8EE2THUtLgZ07gQn9rxUKC+l4qb6u1ajDmEePHsXSpUt7XyvzbevWrcPLL7+smmGM9gSDtBi8tZVELj09fKPmu70UvttWGaIqiZFsZcLXy3LkIAAPWl7ZY+gKKmpitZKX191NBUs6OiiamJKiUW3Z0lJg1SpDVFAZtdgtWbIEo0ngHGqejtE3skydqKmJOlVKyjBr5UQRgZuWxNu8sWEkWxlAFCkJ5fibCM5fqNs5Oq1wuWhZQmcneXlZWSR6mqxpFUVgyRINvnh0cG1Mph+BAIVH2trQuyicYRj9oYQ2e3rCe0GOG0fHeAeRwbDYMQD6e3M+H3UYK7cOhtE9Smizq4sqd2Vmaujl6RgezphB3lxmptYWMQwzWpKT2csbCRa7BIa9OYYxF+zlDQ8PbQkKe3MMY17YyxsMi10C0tlJnYC9OYYxLwO9vKwsqsCSqPAwl0AoC1AvXw5vpMowjLlRvLzmZrrBTdQMa1XLhTH6xeejMkMAbSeSkqKtPQzDxA/Fy/P5wnut6n7PPJVhsTM5sgx4PNTAOzroGE9WM0ziIQi0c4IybdHYSFWSEgX9it377+tql1sjIkmUaXn5MoleRob6X2B/7wBcu1+D/b0DfL0SEW4DgCTBduQwANqmSO/nQNlvtbWVxoauLm3tiRf6Fbu77wZKSoCKCq0tMSTd3VS8ubGRwpbJyep+vvPNCuTOL0HO3UuRuWEtcu5eitz5JXC+ydcrUeA2ED4Hyj582d9ZaZhzkJFBuydUV1NmdiiktUWxRb9iB9BoXVbGgjcKZJm24lHu2DIy1A9bOt+sQOb9ZbDUXe533FJfg8z7ywzR0Zno4DZg/HOghDXtdqrhXFdHS5LMir7FTplBLS/XfWhAD4RClHFVU0MNOT0dsKh9hSUJ6Y9uBGQZA5fsKBtupj1WztfLzHAbMNU5cDpprGhro7HD69Xaotigb7EDSPCqq2kLCWZYgkHKtmxooLBlUlJsvsd+5BDEusuDOriCIMuw1lbDfoSvl1nhNmC+cyCK4WzNmhpKajMb+hc7hbo6rS3QLUoDbW2lO7RYZluKjZFdh0jfxxgPbgPmPAdKNEgQaBqkudlc83jGWVSen6+1Bbqko4O8uUCA7sxiXQ5IGh/ZdYj0fYzx4DZg7nOQlESeXn09RYzGjTNHlSX9e3aCABQV0e63TC+yTBlUfZcVxKPuXWD+Ikj5hZCH+TJZENBTUEQ7gTOmhNuA+c+Bw0HJKy0tQG0tZW0aHX2LndKQtm3T5TbvWiFJtKSgro4apdrLCkZEFOF+YjsADOroymvP49v4epkZbgMJcQ6sVrqJ7uykm+rOTq0tig59i11hIbBzJ1BaqrUluiEQoDutpia683I642+D7/ZStL20E6G8Cf2OS/mFaHtpJ3y38/UyO9wGEuMcWCwkeD09lBfQ3m7cMmP6jcT+938DK1YY+s5IbZT6lp2d1ABVX1YwGltuL4XvtlWUldZYB2l8PoVs+HolDNwGwufAcuQgAA9aXtmD0PzFpjsHqalUqKK2loQvO9t42wXpV+xuvNF0DSYavF4KW/r98UlEiQhRROCmJVpbwWgJtwFAFBGcvxA4/iaC8xdCNOm45XLRDXZ9PWVp5uRoe8M9WvQrdkwvnZ3hzCjV61syDMNEiMNBAtfYSLkD48cbxydhsdM5bjcJnbIGhmEYRktstnCmZihEgmezaW3VlWGx0ymyTJPB9fXUkGJVEYVhGGa0KJmabW3k4eXl6X/rMBY7HaKsoWtspLCBsiUHwzCMXlAyNd1u8vDy8rTJDo8UA00vJgahEIlcfT2JHAsdwzB6RRE8ZUsxPReRZrHTEYrQNTVRqq/DobVFDMMwI6PkEwSDtDRBr5vBstjpBEmiGpfNzTT5a4QJX4ZhGIW0NBrH6ur0KXgsdjpAKf/V0kINxgxFVxmGSTxSU2k8q63VX3kxHlY1hoWOiRZZpqoWoVD4IUn0kOXw/qGyHN6ypamJ5luU4gSiSD+LIj0slvDDatVJEQPGEKSm0m4sdXW0WU1KitYWETy0akhfoUtPN87iTCZ+hEJUNcfvJ0ELBuk5EKDycd3d9Lu+QhcKhYWt70bZgkCPrCzg5El6T986h4rICUJ/sbNYwlnBTielmFutFGq3Wul3ymJjhgFI8Do79SV4oxa7gwcP4he/+AWOHTuGuro67Nq1C6tXrwYABINBPPLII3jzzTdx/vx5pKen45ZbbsHPf/5zFBQUqG27oVGSUVjoGCAsaH4/CVl3N90de730uqeHHoJAAqV4XKJIz4r31VegFE+tL7JMn5eXN9hbUzzBgaIpSTRwud1kgyKQshz+brud1oKmppIo2u1hERxTopUkwXbkMGADbEcOm7LepNlJSQl7eAUFcd6dZQhGLXZdXV2YNWsW1q9fj9IBuxF4vV4cP34cW7ZswaxZs9DW1oaNGzfizjvvxNGjR1Uz2uj0Fbq0NO7DiUYwSCLW3U0T+e3t9BwI0O8AEiu7nR7JyeRFxbqdjOXzJYlsDgTo/2hsDIdKbbaw/RkZ9OxykSiOlIDlfLMC6Y9uRKi9BXjtNWR/ZyUsGdlwP7HdFDsJJBIDQ5paCt6oxW7FihVYsWLFkL9LT0/Hvn37+h371a9+hRtuuAFVVVUoLi4em5UmQhE6JeuS5+jMjSSRqHm9JGhuN3V+v59EQgkRKvsSGi0LV5njG2oxsSKCHR3U3kMh+v8cDhoE09Ppf05KIhEURRK6zPvLAFlGqM8iU0t9DTLvLzPN1jmJRF/BKyjQrhpUzIdat9sNQRCQwRWMeyujsNCZF1kmYXO7gdZWevb5aNAXhHB4LzPTeMI2Wmw2evS9mw8G6Xy0tlLhBFmmc+J0AukpEpb8ZCMgyxiYDyPIMmRBQNpj5fDdtorDIQYjNRXweEjwJkzQptJKTIdbn8+Hhx56CN/61reQlpY25Hv8fj/8ffZ893g8AGj+L6jEdHSMYmMktra2kleXlEQDX9/kgXggScF+z3rGSLYGAmRjVVUQLS1hz00UyWNJSxu6bqAWm2DKcrDfc7yxWmkup2/CQiBA50s6fBhCRwt6vvToggOeAQBtzbAcOUhb6ugII7VXrWxNTqZQ9+XLFNKMpJammhogyPLYu5wgCP0SVPoSDAbxD//wD7h8+TIOHDgwrNht3boVjz/++KDjO3bsQBJXP2YYhklYvF4v1q5dC7fbPayGREpMxC4YDGLNmjU4f/48/va3vyE7O3vYzxjKsysqKkJdXd2If6cXgsEg9u3bh2XLlsE2TFxKiVfbbNrWupSkIE6e3IdZs5ZBFPUdQ9Ojrd3dFIppb6dwtFIH0OUKwuXaB5ttGQRBH7YOhywHEQzq09bMTw9jzqMre18HXS7s+81vsGz9eti6u3uPn3hyD4TFC5GWpp/asXpsr8Ohta3Kji4ZGVfeD6+lpQX5+fmqiJ3qYUxF6M6ePYv9+/dfUbAcDgccQ+Qm22y2YcVDjwxnb1cXzdEpWWl6QBRtuu+QClrbqmRLNjbS/Ft3N3XO5ORwR1XS+QXBpjsBGQ492to+bTF6krPhbK6BgPA9uK27G7bubsgQ0J1diM8yFqPnpAiXi5Jcxo8PZ3tqjdbtdTRoaWtWFvUrmw3IzR1+jaaaGjBqsevs7MQXX3zR+/rChQs4ceIEsrKykJ+fj7KyMhw/fhx/+ctfIEkS6uvrAQBZWVmw633DI5Xp7g5PwuthUSUTGZJEHbGhgR4+X/hmJTOTq4nEDFHE6Qe2Y/aTZRiYoqK8/vwH25BbIEKWqX8piS5OJw2aubkkfJy/om8slvAGsKIIjBsX+341arE7evQoli5d2vt606ZNAIB169Zh69ateOONNwAAs2fP7vd3+/fvx5IlS8ZuqcHw+6kT+v3U+Rj9owyeNTUkdgB1SANE001Dw4JSnHhkJ6a+uBHWrpbe475xhTj9/W1oWEDLDgSBEr2UaX2vF6iupuSHjAzK+MvK0k+YkxmM1UpZmo2NJH7Z2bEVvFGL3ZIlSzDSNF8UU4CmIRgkofN6Wej0jjJ/0NRE16yzkwbInBxeGqIVDQtK0fC1VcioPAjAg2NP7EH7tJErqCjCFwzSvOqpUxRNyc0lryEjgz1yPaIsTWlspMubmRm77+LurDLKovGODg556ZlAgLy42lp67umhu8wJE/ia6QJRRNuMhUDgTbTNWAhBiCwuabORhyDLdONy/jxQVUXH8vPJ20uw2RTd43DQuNnQQNcvVlM+LHYqoiwab2ujiXMeNPWHUs2jpoZ+ttvprp8HQHMhCHTzkpoavrGprw/f0OTk0M+MPnC5wnt6Wq2xWXTOYqcibjeFw1JSeIJcT8hy2ItraqJ51ORkutPnSv3mx26nUGYoRCHO06fJmxg3jspXZWXxjakeSEkJJ4ZNmKD+NAKLnUp4vXSRHA72EvREezslLihZsenpdFfPJB4WC3nxGRmUjFRXR+0iLw8oKuL5dT2Qnk6RsaYmmm9VExY7lWhooGfO/tIHHR2UmVdbS/NxWVlj3GqGMSUuFz38fmojjY3k5RUWcnhTSwQhvCTBZlPX42axi5KeHnr2+zlFXQ94vTR4VVfT+jhOP2dGwuGgcHZ3N3DhAnl6RUXaVudPdKzWcIammteAxS4KlIQUgO5GGO3w+Wigqq6mLLz0dL75YCLH5SKvrrMTOHuWQpxFRRTi1KJCf6LjcFDCSlOTep/JYhcFra30ADjRQSsCAboDvHSJEoTS0uiunBMOmLGQkkJeRUcH8NlnlLU7cSLNHxmoeqEpSEqizGm1YLEbI52dNMhyiEwbenro/FdX0w1HcjKJHN90MNGizBulpNAN1Mcfk+gVF1+5cDGjLmrOn7LYjYFAgBJShtuhmYkdskx3e5cuUYjD6aRQEw9AjNpYLFQYIi2NsnpPnKDlChMnUkYvRw9iDyeojBZJAg4dokB8fj6waNGYR8dQiAbZ7m5Kfoj3BqyJTHc3cPEiZVlaLBRa4pJeTKwRRZr/7emhtPgTJ2h+r6SEIztGwvxDRUUFsHEjjZAKhYXA9u1AaemoP66tLVwhhYkPskye9PnzdIedk8MeNRN/rFby7Hw+ytxsawOuukr99WBMbDD3DEdFBVBW1l/oAArAl5XR70dBVxd5dUlJHDaLF14vJQqcPEnh4wkTWOgYbXE6qR0GAtQuP/ssvJEvo1/MK3aSRB7dULswKMfKyyOOQwaDlBAhCDzYxoNQiJ5PnKD5uawsLuvE6AdBCLfJS5eonQLhdsvoD/OK3aFDgz26vsgypfIdOnTFj1Lm6bq6eBPWeNDZCVRW0s+hEN1Fc/UTRo84HNQ+FZGrrKT2y+gP887Z1dWp9j63m9LbeSeD2CJJtDD8/Hm6scjO5n3IGP0jCNROAwGq3uN201weZwnrC/N6dvn5qrzP6w2vp+OGGzu6u2nu49QpcrojvXwMoyfy86n9njpF7bm7W2uLGAXzenaLFlHWZU3N0PN2gkC/X7Ro2I9QytWEQpxiHEuam6lEU3s7ZbvZ7UNfMoYxAhkZlMR2+TKFNCdP5p029IB5PTtRpOUFwOA4mPJ627YR3bW2NiobxHUvY4MkUQr3iRPkQRcU8PZIjDmw26k9e72UsXnhAq/J1Rrzih1A6+h27qQZ5L4UFtLxEdbZdXWRx5GczHNGscDrBT75hCb0k5LIo+PzzJgJQaB27XLRhrGffMJLFLTEvGFMhdJSYNWqUVVQkSQSOlnmLMBY0NREYUu3mwvsMuYnJYXGkdpauomePJlEkIkv5hc7gIRtyZKI366ELzMzY2dSIhIK0TzG2bN018u7EzCJgs1G7b2lhZJXJk+mABMXLo8fiSF2o4DDl7EhGKQlBRcuUCVz3g2aSTQEgRJVlO2DurtpiQJHNuIDi10flPAlwOFLNfF6gTNnKIzDdS2ZRCc1lQTu3DkSvGuu4V3R4wGLXR84fKk+bW00Od/WRotseZcChglvTVVbS4Wlp0zhcSfWcMT4Szh8qT51dTQ/0dlJCbEsdAwTxmqlebyODuon9fVaW2RuWOxAiRMtLZx9qRayTIkon35KP+fm8g0EwwyFsi+jLNPShMuXuaBCrGCxA+Dx0IOTJqInFKIklM8+o1BNVpbWFjGM/snKov7y2WfUf3j3BPVJ+MBSIEDhS6eT04CjpaeHMi7PnaOSScnJWlvEMMYhLY1WSX3+OfWlq67i0L+aJPypbGujCWL2QKIjEKD1c1VVtFsBZ1wyzOhJTibBO3eOBO8rX+ESemqR0GLX1UVb9/AeddHh89HdaE0NMH48d06GiQankyqsXLxI61OnTOGbRzVI2MBdKERCB/DgHA0+Hy0tqKmhVGo+lwmGJCHr1AHkH3gNWacOJGa1Y0lC5qeHAYCeVTgHdjv1p5oa6l8+X9QfmfCMWuwOHjyIO+64AwUFBRAEAbt37+73e1mW8eijjyI/Px8ulwu33HILzp49q5a9qsFJKdGj7EFXW0slR3l+IbHIfbcCN3+3BDc8tBSznl6LGx5aipu/W4Lcdyu0Ni1uKOdgzqMrAQBzHl2p2jmwWqlf1dby3nhqMOrhqaurC7NmzcL69etROsSuAc888wx++ctf4ne/+x0mTZqELVu24LbbbsNnn30Gp0588UCAlho4HJyUMla6u2nHgvp66pC8sW386emhthwMhp+Vn7u7yRvw++m9kkTRDFEEvvY14OBBOmaxhK+dw0HhMpeLPAubjR7Kz3Z7+IYm990KzH6yDED/PHlncw1mP1mGE4/sRMOC4XcVMQN9z0FPnw0v1TwHokj9q66OXk+bxntrjpVRi92KFSuwYsWKIX8nyzK2bduGRx55BKtWrQIAvPLKK8jNzcXu3bvxzW9+MzprVaKtjQYDTkoZGz4fC108CQRofrmrixbou91Ugi0YJMGTJPq57/osi4WEyWKhNY7KOkdFrAIB+luA/k6WSQx7evqnvQsCCZ0o0t/abECyU8KPntsIQMbA5ZMCZMgQMPXX5Wj42irzNg5JwtQX43MOBgre9Ok8hzcWVA08XbhwAfX19bjlllt6j6Wnp2P+/Pl4//33hxQ7v98Pv3L7CcDj8QAAgsEggsGgmuYBoIG6uZnujtSYXpCkYL9nPaOGrT4f1blsaKA5BYslNotgZTnY71nPqGlrIEBC5vWSsLW10bPPF26vNhsJj9VKg54o0iOSKIXFQjZmZgYjWssVCtH3ShIJYTAIpJ8/DGegpZ83MxBrZzMyKg+ibcbCSP7tIdFzG8isPAxrV/gcBAc8A+qcAwWLhfpbQwPdgFxzzdgFz4hjlhoIsjz2oUoQBOzatQurV68GALz33ntYsGABamtrkZ+f3/u+NWvWQBAE/PGPfxz0GVu3bsXjjz8+6PiOHTuQxNVRGYZhEhav14u1a9fC7XYjLS0tqs/SPKXg4YcfxqZNm3pfezweFBUVYenSpcjOzlb1u7xeWgeWkqJeMoUkBXHy5D7MmrUMoqjvvTqisbWnh9bRVVfT8oJYJ6PIchDB4D7YbMsgCPo+r6O1tbsbaG+neePGRmqXkkRzZsojVtu+WCxBzJy5Dx9/vAyh0Ni+pLjqMO7dsfKK73vhG3vwWdZCiCJV9R8/ntZgZmRENu+k5zaQ+enh3qQUgDy6fb/5DZatXw9bn0ySY0/sUcWz60tPD7WboiLaF2+0fdFIY1Z7e4tqn6XqkJWXlwcAaGho6OfZNTQ0YPbs2UP+jcPhgGOIgpQ2mw02FXu8LFP2pdUam/qXomjTfcNRGK2toRBw6RIJ3bhx8d1/SxBsuhvohmM4W0MhKvbb1kaDVEsLCZwg0CLirKzB0zqxLhcVCtnGLHaXChbDa8tGmqcGAgYHhmQI8KQVom3qYuRZREgShWLPnqUQeFISiV5uLglfaurIIVg9toH2aYvRk5wNZ3P/c2Dr7oatuxsyBPjGFaJ92mIIgrrzljYb9cPqakoa+spXxpZoZ4QxS037VM1FnDRpEvLy8vD222/3HvN4PDhy5AhuvPFGNb9q1HR20oDDC8hHhyzT4tZz52iA4nV0kSHL5L2dPQu8+y5lPx49SkkGdjtVuy8oANLTjZfDIVtE7F2+nX4ekJ6hvN67fBtkC/1jokj/p/I/2+10Hj76iM7Lu+/SeWpvN1ARZFHE6QdGPgenv78tZhfXbqf+eO4c3Yga5rxpyKg9u87OTnzxxRe9ry9cuIATJ04gKysLxcXFKC8vx5NPPonJkyf3Lj0oKCjondfTAmUBuTKRz0ROTQ3wxRd0B84ZYFempwdoaqLq9Q0NFLJMSiLvZdw4ra1Tj8pppXh9zU4s37sR6Z7Lvcc9aYXYu3wbKqcNnXKveLNK3VSfjyIu9fUU2szNBQoL6Vzpva82LCjFiUd2YuqLG2HtCofbfOMKcfr722K+9MLppH559ix5e4WFMf06wzNqsTt69CiWLl3a+1qZb1u3bh1efvll/Ou//iu6urpw//33o729HQsXLsTevXs1XWPX0UGPjAzNTDAk9fVUvSEpiYs6R8LFi/Roa6NBPSODdmY3K5XTSnF6yipMrDqElI46dKbm41Lxol6PLhKcTnpkZ9ONweXLFJ7LzARKSsgT1DMNC0rR8LVVyKg8CMCDY0/sQfu0xXFT6uRkmu89fZqmaL6cSWKGYNRit2TJEoyUwCkIAp544gk88cQTURmmFpJEXp3dzgvIR0NbG9W7tFqpGjszGGUfxPp6SgU/eZLusOORwKMXZIuIiyVLVPksl4sePT20lvDkSRK7Tz6hQTw7W6d9WBQpCSXwJtpmLFR9ju5KpKXROfv8c8pH4B3Ph8b0XbKjgxbjcgOIHK+X7hQDAQorMf0JBEjgqqspZKmse8rP57kTNbBaSdiUhfDnz9MjJ4cyELkG62Cysihsfvo0MGsWRWOY/pha7Hp66M7b6eSdsiMlGKSMubY2YMIEra3RF5JEiRXnzpHI2e3hTTcBamMsduqh9Nn8fJrba2mh8z9uHHD11Vy9ZyDjxlEdzTNngBkz4ps1bQRMLXadnTQPwF5dZIRCdAddV0d3z3yDQIRCdNd8/jx5dHY7D7Txxm6n8LAyLXHkCLXRq66i6IMuw5txRqmyUltL4eDJk/m89MW0YidJ7NWNlsuXgQsXKISUKHNOIyHLVFpOuQEAaMDlO2btEEXyYIJB8q6bmmheb9IkCnMmel+3Wuk8XLhAgldcrLVF+sG0Qxp7daOjqYlSmFNTeYkBQGHc8+dp6YUk8RpDvWGzkRcTCNA1qq+n1PtJk7jPO53Uj8+eJcEz05KXaDCl2IVCNFjZ7XynFwleL3UMQeD9/To76a64qormibKyeEsVPaOElLu76brV1ZE3M2lSYheQSE2l9nv2LC1P4IQVk+5U3tlJD77AV0aSqEO43eS9JCqhEAnce+9RCrfTSQk6LHTGwOWi6+V00vV77z26nrEuu6ZncnLCVXwScQP5gZjOs1PKNNlsPDkbCVVVFAbKzU1cL7izk1K2q6pondKECYl7LoyO4sW0tlJ5tqYmYMqUxPTyBIH6dW0trcWbNElri7TFdGKnbHCZ6OG4SGhupjT6jIzETLoIhSgp5/RpKlk1blxsioQz8UUQKErh91Nos7WVBK+wMPFugG02qkt6/jwJXiJHb0x16RWvThA4LfxKdHeH5+kS8a63sxM4fpzu/oNB8uZY6MyF4qUHAnSd//53uu6JhtK/z5yhfp+omErsvF66Q+c6jkMgSbAdOQwAEN8/jC8+l9DertPajZKEzE/J1sxPD6s64dB3bu7iRbrT7VutQw8IIQklFw/g2o9fQ8nFAxBCPOEyVhQvLzubvLz330/MuTxl/u6LLxJ3/s5UYUyPh555jVh/nG9WIP3RjQi1twCvvYZx312JBc5sVH5/O1omxLYy+2jJfbeit4r8m6+9hjmPrkRPcjZOP7A96iryXV1AZaW+5+amVVYM2knAnVaIvcu3D7uTAHNllOvddy5v6tTEuTEWBArTX76cuAXxTePZ+f0kdpyB2R/nmxXIvL8MlrrL/Y4nt9Vg7s/LkPtuhUaWDSb33QrMfrIMzub+tjqbazD7yehsbW6mqht69eYAEro1r5chzdP//0/z1GDN62WYVqmfa2VEBnp5H35IhScSBbudxP3iRa0t0QbTiF1nJ8298MLfPkgS0h/dCMgDt5dE7+7KU39dro+4hiRh6osbMXgrzOhslWUq2PzRR3QzpNe5OSEkYfnekf//5XvLOaSpAg4HVV1xu0nwqqsTp6ZpRgZFOIDEC+WaQuwkieLRXPmjP/YjhyDWXR40eCoIkOFqqkbWp4fiatdQZH16CK5mdW1V9vk6dowGMz3X+5xYdQjpnpH//3RPNSZWaX+tzIBSR1KWqX18/rk+7vnigZKR2dCgrR3xxhRi5/VSlhEvAO6P2FgX0fscrZG9L5ZEakOk7/P5gBMngE8/pWUoWVlRGBcHUjoi+78ifR8TGVlZ1D4++YTai8+ntUWxR4l+XbhAY2eiYHixU5YbWK36vWvXCml8fkTv82dF9r5YEqkNkbzP7aYkhPPnqXCzEZIQOlMj+/8jfR8TOcnJ1E4uXKB243ZrbVF8UErjJUoI1/Bi5/NRDJoTUwYTmL8IUn7hELNAhAwB3eOK0DpjUZwtG0zrjEXozone1vp6modpaKB5GaPM4V4qXgR32sj/vzutCJeKtb9WZkSpsdnQQPO7iRDiy8mh7MxE+F8BE4hdZydt0srLDYZAFNH0yHYAg9MelNenv79NHyvwRRGnHxi7rbJMntzRoxTSLijQx78VKbJFxN7lI///e5dvg2wx0D9lMESR2o3XS4Jndq/HbqdknfPnE2OxuaHFrqeHQg48Vzc0sgx8OqUUb/9gJ3w5/bcd940rxIlHdka9dk1NGhaU4sQjo7c1FKLFsqdOUXmk8eONGdKunFaK19fshCet///vSSvE62t28jq7OCAI4T0LT56kdmXmrMXMTJoGunjR3MIOGHxReVcXhTETff+q4WhupjBF+q2leGflKmRUHgTgwbEn9qB92mJduj4NC0rR8LXIbQ2FqAzSZ59R7T+jlz6rnFaK01NWYWLVIaR01KEzNR+XihexRxdn0tMpY/OTTyhL85przFlXUxDC4cycHHPvfWdYsVMSU3jPuqHp6QEuXaIOSksyRLTNWAgE3kTbjIUQBB0PnmJktoZCtLTg9GlzCJ2CbBFxsWSJ1mYkPKmpNLZ89hmNN1OmmFPwnE5ag3rpEmWn6vAeWBUMe+l8Pooz89q6oWlqoodZvV5JotJflZV0F24WoWP0RUoKta/PPqO2ZtaQZlYWjReNjVpbEjsMK3ZeLyemDEcgQPUfnU5znp9QiBYBnz5NYm6EpQWMcUlOpnZ2+jS1OzMKntVK40VVFVWiMiOGFLtQiNxu9uqGprGRCt6aseCrMkd3+jT9f7zkhIkHSUnU3iorqf2ZUfAyMmjcMOtSBEOKXXc3hzCHw+ejzKrkZPPF3mWZNputrKQ5OvbomHiSnEzt7rPPqB2aLXtRFEnUL10yZyUZQ4qd10sNzYyTxdFSX09eb1qa1paoz4ULVP4rJYXn6BhtSEmhxJVPPzXn7gHp6bScq75ea0vUx3ByIUkcwhwOr5cquKelme9GoL6e7qhdLhpsGEYrUlOpHX76qflCfhYLjR/V1earm2m4IdHrJRebxW4wtbVUUcZsYuB2Ax9/TPMk6elaW8Mw1A5DISpkYLZamqmpQEcHjSdmwnBi19lJa194bV1/Ojrobiw93Vznxu8nofN4zL3glTEe48ZRu/z4Y2qnZkEQKFnl8mUaV8yCocQuGCSx4/Jgg7l8mTxes81lVVYCdXX63ouOSUwEgdplXR21UzORkkJRtMuXtbZEPQy1Cqu7m+6gOAuvP+3tFHKI+55tkoSsTw/B0VoHf1Y+7UigUgqokul26RKQm2u+zFIm/gghSfUybKJI7fPSJWD6dGq3qtyUxbBvRUp2No0r+fnmWMakuthJkoStW7fi1VdfRX19PQoKCvDd734XjzzyCIQoW0FXFw96A5FlCl/29MTX4819twJTX9wIV3P41q87pxCnH9iuSnHp2loKE2VkGGebHka/TKuswPK9G5HuCbdXd1oh9i7fHnWBbbs9LAa1tUBhYVQfF/O+FSkuF91Im2V6RPUw5tNPP40XXngBv/rVr1BZWYmnn34azzzzDJ577rmoPrenh8TO4VDJUJPQ2kqZivH06nLfrcDsJ8vgbO4f43A212D2k2XIfbciqs9vbqbMS4AXjTPRM62yAmteL0Oap397TfPUYM3rZZhWGV17BcLt9LPPgJaWsX9OrPvWaMnKovGltTWuXxsTVBe79957D6tWrcLKlStRUlKCsrIy3Hrrrfjwww+j+lyfj0KYLHb9qa0l7y5u50WSMPXFjRi86xoggGKPU39dTmtExkBXF034BwJRWckwACh0uXzvyO11+d5yCKGxtdeBBAKUodnVNYY/jnHfGgsOB40vZsjMVD2MedNNN+Gll17CmTNncM011+DkyZM4fPgwnn322SHf7/f74e+TyuTxeAAAwWAQwT5F2jo6KNVXb2V6JCnY7zmedHRQ8db09MiqOchysN/zWMisPAxrVwt6RoiZWjubkVF5kHYuGAVKzUu3G5gwgWy0WPRfqE+xkW1VFzVsLb58GEnBkdtrUrAZE2sPoqp4dO21L4qNublB1NRQSbHrrhvdetdY9q2+jHYcSE+naEt7e/yXNak5rgqyrG7Rm1AohB//+Md45plnIIoiJEnCT3/6Uzz88MNDvn/r1q14/PHHBx3fsWMHkjiGxTAMk7B4vV6sXbsWbrcbaVGWhVJd7P7whz9g8+bN+MUvfoEZM2bgxIkTKC8vx7PPPot169YNev9Qnl1RURHq6uqQnZ0NgEKYly5ROqzeElQkKYiTJ/dh1qxlEEVb3L43EACOHRvdQmtZDiIY3AebbRkEYWy2Zn56GHMeXXnF9x17Ys+o7j67uoAjR2h5SVYW3SnPnLkPH3+8DKFQ/M7rWGBbY4MathZXHca9O67cXn+/dk/Unl1fW1tbKXHlhhsizx6PVd8ayFjGAbebvNQ5c+KbMNbe3oLrrstXRexUD2Nu3rwZ//Zv/4ZvfvObAICZM2fi0qVLeOqpp4YUO4fDAccQE042mw02G12Izk46puesPFG0xVXs3G4KY+bnjz5LShBsYxa79mmL0ZOcDWdzTe88Ql9kCPCNK0T7tMURbxCrbMLa1gZMmNA/VB0K2XQ/KCuwrbEhGlsvFSyG15aNNM/w7dWTVohLBYshh6K/k1ZsTU8HamooLH/99ZGFM2PRt0ZiNONAaiolqrjdtLYwXqg5pqqeoOL1emEZcGVFUURojJNtskxVCvQsdPFGmTC22zWogSmKOP3AdrJjwDS68vr097eNygW/fJn20Ro3zvjpzYy+kC0i9i4fub3uXb4t6vV2AxEEas+XLo1iYXYM+pZaiCJgs9ECeqPu9qD6UHnHHXfgpz/9Kfbs2YOLFy9i165dePbZZ3HXXXeN6fMCAQpjchZmmPZ2SgXWqk5kw4JSnHhkJ3w5E/od940rxIlHdo5qLVBnJ3l1DgdfYyY2VE4rxetrdsKT1r+9etIK8fqanVGvsxsOpU1//nk4OnUl1OxbapOeTssqjFoLVPUw5nPPPYctW7bgn/7pn9DY2IiCggJ8//vfx6OPPjqmz/P5aB7HbMWNo6GpidYdauntNiwoRcPXVkVV5UEJX3o8FL5kmFhROa0Up6esUr2CypXIyhp9OFONvhULHA4adxobjVlRRXWxS01NxbZt27Bt2zZVPq+7W/NrrCu6u2lbEV2Ivyii9bolY/5zDl8y8US2iLhYsiSu39k3nDluHFBcHOEfRtm3YkVqKo0/RUXGq1Gs60LQoRBl6fF8XZjWVkpMMXrBZw5fMonCWMKZeiUlhf4HI1ZU0bXYBQL0YLEjJIlCIi6X8T2hCxcofBn34tUMowFZWTTXdeGC1pZEhyDQXqI1NXEt5KIKuhY7v5+8Ow5jEu3t9DD6BqZtbRS+zMw0vmgzTCQIArX3qipq/0YmLS08FhkJXYud16tBar2OaWigZ6uhNmbqjywD589T4hFv1cQkEsnJ1O4vXDBu+j5ASxBkOTweGQXdSkkoRGLHIUyiq4saV5RFBDSnuZlCIBy+ZBKRrCxKzGpu1tqS6EhPp/FoTAWvNUK3Ysfzdf1pb6e7QiOXCw2FyKuTJONlcjGMGrhc1P4vXNBfUfvRkJRE45GRQpm6FbtgkBoFz9cRjY3GF/6GBqrA8GXJU4ZJSJQdwBsbtbYkOux2WvNrFHQrdry+Lkx3N2VyGXmOS5LIqwOML9oMEw12OyWsnDtnvIzGviQnk2fX3a21JZGha7HjQZFwu+l8GDn0V1dHhWTZq2OY8A7gdXVaWzJ2XC4al77cglT36FbsgkEWO4W2NvJyjZqmHwjQXazdTplcDJPo2GzUH86fp/5hRASBxiWjLDDXrdjZj70PETr38SUJtiOHAYCeYxCTCAQoc8vIIcz6eortcwYmY0aEkITiKhoHiqsOQwhFNg5kZdG8XX19LK2LLcnJND4ZQbB1K3bF/3I3cueXwPlmhdamDInzzQrkzi9B9ndos8Xs76yMib0dHbQEw6hZmJJEC2ntdp6DZczHtMoKlG8v6d0g9t4dK1G+vQTTKq88Dogi9YvqauNmZiYl0fjU0aG1JVdGt2IHAJb6GmTeX6Y7wXO+WYHM+8tgqeu/UVUs7G1vpwWcRhWK1la68zNilXSGGYlplRVY83oZ0jz9x4E0Tw3WvF4WkeBlZFD/aGmJkZExRhRpfDLCEgRdi53wZZmBtMfK9ZO2JElIf3QjIA/cXlF9eyWJwhxG9eoASrGWJJ5/ZcyFEJKwfO9GDN5mFb27jC/fW37FkKbdTtvm1NbGxs54kJRE45Rehujh0LXYASQg1tpq2I8c0toUAID9yCGIdZcHNXAFNe3t6KAK40YVu64u6sRGr+XJMAOZWHUI6Z4RxgHISPdUY2LVlceB9HTqJ0aqRtKXpCQap/QeytS92CmIjfrI0Y3UDjXs7ejQfpPWaFDKCRk5uYZhhiKlI7L+Hcn7kpNJLIxWa1JB8U5Z7FRCGp+vtQkAIrcjWntlmUIDRt3rraeHNqw0w3ZEDDOQztTI+nck7xME8o4uXaJ+Y0QcDsq41nOBa92LnSwI6CkoQmD+Iq1NAQAE5i+ClF8IeZgRXC17vV5arGnUTVqbmmh9IIcwGTNyqXgR3GmFQ8zYETIEuNOKcKk4snEgPZ36i5HKb/UlJYWKX3i9WlsyPLoWO0VQPI9v0086oijC/cR2ABgkeGra63ZToVWnM6qP0QRZpsrugmDs7YgYZjhki4i9y78cBwYInvJ67/JtkC2RjQNWK/WXmhp17YwXTme4rKFe0bXYSfmFaHtpJ3y3l2ptSj98t5ei7aWdCOVN6HdcTXtbW40rFG43zT/wcgPGzFROK8Xra3bCk9Z/HPCkFeL1NTtROW1040B6Oi0w17NgjITNpu9qKrodTlt//d/wL12hH49uAL7bS+G7bRUsRw4C8KDllT0IzV+sir2SRA3eqLUwm5roLi8nR2tLGCa2VE4rxekpqzCx9iAy4cHv1+7BpYLFEXt0fUlKovV2jY3GDP+7XDRu6XW3Gt16dsG5N+rzjPVFFBGcvxAA6Fkle7u7jRvCDIXo7tSoyyUYZrTIFhFVxTQOVBUvHJPQKbhc1H+MWFHF6aRxS6+7IOhW7BIZr9e4G9d2dFA1BV5uwDCjJyWF+o/e0/iHwmYD/H79Jqmw2OmQri7jpuu3t1ODN6JXyjBa43RS/zFC+a2BCAJgsbDYMaPA7TamVwdQYgpv48MwY8dmM+4Cc5tNv0LNYqczgkEKYRhxMbnXSxPsRl0byDB6ICWF+pFePaSRcDpp/AoGtbZkMCx2OsPrNW4YsK3N2NsRMYweULbNaWvT2pLRo4Rh9ZikwmKnM7q76a7IiKHA5maK2xt1vpFh9IDSh5qbtbZk9NhsNH7p0StlsdMZXV00yWs0AgGaZ+AsTIaJnuRk6k9G2AF8IBaLPndwMOCwam7a2405X9feTg2c5+sYJnpSUmgnBL0me4yEw6FPu1nsdITfT4JhRLFra9Nv5QSGMRrKDuBGnLdzOGgc8/u1tqQ/LHY6wu837mLylhZjJtUwjF6x26lfGQ27ncaxhBC7mpoafPvb30Z2djZcLhdmzpyJo0ePxuKrTIXfb8zklEDAuMslGEavKGn8Rpu3U5JU9CZ2qheCbmtrw4IFC7B06VK89dZbGDduHM6ePYvMzEy1v8p0GK1RK3R1UU28rCytLWEY8+B00i4CXV3Gi/YIgv7GM9XF7umnn0ZRURF++9vf9h6bNGmS2l9jSrq7jZmJ2dVFOywbzSNlGD2jeEhdXYDRfAVB0N9aO9XF7o033sBtt92Gu+++G++88w4mTJiAf/qnf8I//uM/Dvl+v98Pfx9/1+PxAAAkKQhJ0uEy/AEoNqphq8dDd3Cx2tpeloP9ntWiq4v23lNTqC2WYL9nPcO2xga2lQTP61V3TIjVONAXu51CsJIU3eeoqQGCLKs7tDq/zFLYtGkT7r77bnz00UfYuHEjXnzxRaxbt27Q+7du3YrHH3980PEdO3YgiUtxMAzDJCxerxdr166F2+1GWlpaVJ+lutjZ7XbMnTsX7733Xu+xH/7wh/joo4/w/vvvD3r/UJ5dUVERTp2qQ0ZGtpqmxQRJCuLkyX2YNWsZRHHscbzubuCjj2gxaawSPWQ5iGBwH2y2ZRAEdWKOgQBw6BB5dWqusbNYgpg5cx8+/ngZQiF9x0fZ1tjAttJau1AIWLRIvXm7WIwDA1GWUc2bF90m1O3tLbjuunxVxE71MGZ+fj6mT5/e79i0adPwpz/9acj3OxwOOIYY3UXRFpV4xJto7e3pCc97xbrcliDYVGvkXi816qys2Gw4GQrZdD/QKbCtsSGRbbXZKEnF61X/JljNcWAgNlt4TItm7a2aGqB6OsSCBQvw+eef9zt25swZTJw4Ue2vMhVqNAwt4OQUhokdfZNUjIQohsc0vaC62P3oRz/CBx98gJ/97Gf44osvsGPHDrz00kvYsGGD2l9lKoJBYxZQ7uzU2gKGMTeCYDyxA8huPW31o3oYc968edi1axcefvhhPPHEE5g0aRK2bduGe++9V+2vMhU9PbHLwowlRt5oNhGQJCooXFdHj2AQmDUL+NOfyGvIz6dHbq7xogqJgt1O/cxoyLK+PDvVxQ4AvvGNb+Ab3/hGLD7atAQCxlxj5/XSsgNGX7S3A8eOAUeP0oJ/gNqXMu9z9iwlERw7Rq+dTmDuXGDOHCAjQwuLmeGwWvW5Zc6V0NvCch6mdILPZzzR6OkhT8FodpsZnw/Ytw84fpwGm77RgoEJRH1f+3zAu+8Chw8DX/0qcOutXP5NL1itJBo9Pcbqa1Zr+EZLDxjo1Jmb7m7jhZGUDhhNajGjHufOAbt3h+d3RhsWV97/978DZ84Aq1cDV1+tpoXMWBBFEg2j3VjqTewMGDgzH7JMISUjNWSAOh9v66MPPvwQePVVErpo535lmT7n1VfpcxltsVqpn+kpJBgJViuNa3rJRWCx0wE9PRRSMppoBALGu9s0Ix9+CLz1Fv2s1sCifM5bb7HgaY3VSv3MaGJnsZBI6yVJhcVOB4RC9DDa0oNgkAZFIybWmIVz58JCFyveeou+h9EGi4X6mZ7S+CPBYgmPbXqAhykdoDQIo4mG0Tqf2fD5aI4u9hV36Hv0tj9ZomG0/sZixwzCqGJntLCK2di3T505uiuhzOH97//G9nuY4ZFl4/U3FjtmEJJkzHCgUfffMwPt7bS8IF6T/7JM39feHp/vY/ojisbzrJXwa7Tb/KgFD1U6QJKMOWdnxLWBZuHYsfi3F0EIL0Jn4osRF5YLAnt2zACUux+jZWP6/ezZaYEkUWWUeKd0yzJ9r17u1BMJi8WYnh2LHdMPIw8eRvNGzUBDg3aLdX0+oLFRm+9OZIzYz5QKPix2jOGRJGN2QqNTV6ft99fWavv9iYgSEjQivKic6UWWjSkaRu18RqeuTrvwscWivdgmKtzfooPFjokKI4q00ens1G7gC4V4D0MtGFjU2yjoyW4WO2bM8J2mNmhdfknr709UuL9FB4sdM2Y4E1MbtF7uofX3Jyrc36KDTx8TFXoJUSQSKSnaztmlpGjz3YmMUef19WQ3i50O0FNcezTwnaY25OdrO2eXn6/Ndyc63N+ig08fM2ZE0ZgibXS0FpuCAm2/PxExYjlBBfbsmF6MVjmlLyx28Sc3F3A6tflupxMYP16b705kjNjPlBCmXkRaJ2YkNoJAgme0SioOB2eIaYEoAnPnalMbc+5cY9+cGZVQiPqbkVB2cmGxY3oRxXCFcCPhdHIaulbMmaNNbcw5c+L7nQzR0wMkJWltxehQQq8sdkwvomjMckAul/FsNgsZGcBXvxo/704Q6PsyMuLzfUx/JMmYnp0StdIDLHY6QLn7MZpw2O1aW5DY3HorkJwcn53Kk5Pp+xhtEATj9TcOYzKDMKrY2WxaW5DYOBzA6tXx2al89WrjeRZmw2j9jcWOGYTSIIw2Z2ezGTP8aiauvhpYsSK233H77fQ9jDYo4UAWu+jQiRmJjdVKDcJo2Zh2O3VATlLRlhtuCAueWiFN5XNuvx2YN0+dz2TGRk8P9TMjhjFFUT/l5XRiRmIjCBQiMlo1eZvNmEsmzMgNNwDZ2cDu3UBXV3RRAmWObvVq9uj0QE8P9TOjiV1PD5CayovKmQG4XMYTDbud7trYs9MHV18NbNgAXH89vR7tIKO8//rrgQcfZKHTC5JE/cxoYcyeHu2KHwwFe3Y6wYhr1pQO6PNpbQmj4HQCd9wBLFoEHDsGHD0avj4D5076JkU5nbRgfM4cXl6gN3p66GZYL+HASGGxY4bEbjdmokdSkvHCr4lARgbw9a8DS5YAjY1AbS3tMB4M0u8nT6Yblfx8qnU5frx+1kMx/THignKAQul6Cr3GPIz585//HIIgoLy8PNZfZWisVv3EtkdDejoQCGhtBTMcokiCNmcO8I1vAP/wD3T8H/6BXs+ZQ79nodMvgQD1M6MhCPryRmMqdh999BF+/etf47rrrovl15gCm814Sw8A3tuMYWKNLFPCkNGQZX3NM8ZM7Do7O3Hvvffiv/7rv5CZmRmrrzENVis9jJakkpxMdivhMYZh1CMYJMEwmtgpSTUJ4dlt2LABK1euxC233BKrrzAVDocxRSM5mSahOUmFYdTH56P+ZTSxCwZpPNNT1Z2Y6O4f/vAHHD9+HB999NEV3+v3++H3+3tfezweAIAkBSFJ+h/5FRujtdVqpcncQCB2DUSWg/2e1cBmo/mEtjZ1KyVYLMF+z3qGbY0NbCuJRlaWutMcsRgHBhIIhJcmRROtUlMDBFlWd6aouroac+fOxb59+3rn6pYsWYLZs2dj27Ztg96/detWPP7444OO79ixA0lGTEFiGIZhVMHr9WLt2rVwu91IS0uL6rNUF7vdu3fjrrvugtgnvUuSJAiCAIvFAr/f3+93Q3l2RUVFOHWqDhkZ2WqaFhMkKYiTJ/dh1qxlEMXoZmPPn6dHXp5Kxg1AloMIBvfBZlsGQVBv5ri2ltZz5eer9pGwWIKYOXMfPv54GUIhHc1yDwHbGhvYVupb8+bR8hC1iNU40Jf6euCqq+gRDe3tLbjuunxVxE71MObXv/51fPzxx/2Ofe9738PUqVPx0EMP9RM6AHA4HHAMEbcTRVvU4hFP1LA3KSlc9DWWCIJN1UauOOB+v/rZV6GQTfcDnQLbGhsS1dZgkMaCpKTYjAlqjwN9kWWyO9olLWpqgOpil5qaimuvvbbfseTkZGRnZw86zvRHTwswR0PfJBU9pRozjJExanIKoL8F5QDXxtQVDgeJhdEyMu12KvjaJxrNMEyU+HzUr/QmGldCWS6hp0xMIE7lwg4cOBCPrzE8Dkc4I9NoHlJ2Ns0vMAyjDoEA9SujoWRi6k3s2LPTEQ4HhSyM6CFlZvJ2PwyjFpJE83RGrMfh99M4xmLHjEhGhjHFLiODGjgXhWaY6OnspFJ8RtyBwu/Xp90sdjojOdmYux/Y7UBuLm0cyjBMdHR1UX8y2nwdQOOXHpNqWOx0hstlzCQVAMjJoSwsIxa0Zhi9oPShnBytLRk9SnKKHuuBsNjpjKQkinUbsdZkZibZ7/VqbQnDGBevl/qREefrfD4av1wurS0ZDIudzrDZjJvGn5RE2WM8b8cwY6ezk/qRHr2jK6Esl9BjNjmLnQ4x8oaoubnGDMEyjF4IBqkfGZFgUJ/JKQCLnS5JTjbuvFdGhnHDsAyjNUoYUK+CMRKyTMkpevVIWex0SFJSeHG50UhNpY7KWZkMM3o6O6n/pKZqbcnoCQZJqFnsmIhxuYy7IarFQrs2cJIKw4wen4/6j5p7Q8YLpZanHpNTgDiVC2NGhyjSvF1dHRDlrhaaMG4cNfjubv00fCEkYWLVIaR01KEzNR+XihdBtkRZkp0xFHpvA14vicX48VpbMja6u2mbr2h3OogVLHY6JSsLqK7W2oqxkZ5OE+yXL+tD7KZVVmD53o1I91zuPeZOK8Te5dtROa1UQ8uYeGGENuB2A0VF1H+MiLKrul4xoLOcGKSnGzeUKQhAYSFNWPf0aGvLtMoKrHm9DGl9BjkASPPUYM3rZZhWWaGRZUy8MEIb6Omh/jJhgtaWjA2fj25s9SzULHY6JSmJQphGXbM2bhwtinW7tbNBCElYvncjABkD974UQOmuy/eWQwhx9WqzYpQ24HZTfxk3TlMzxkxnJwmdXpNTAB2LnVFT79VCECh2b8TF5QBgtQITJ1IcX6trObHqENI9lwcNcgoCZKR7qjGx6lBc7WLihxHagCzTfN3EidRvjIjfT0Idix3V1UK3Yqd1+EsPpKZS4zfiEgSA5u2Sk7VbhpDSUafq+xjjYYQ20NVFOxwYdSF5IEDjlN6XS+hW7HhfNGo8KSnGTeNPTgYKCrQLZXam5qv6PsZ4GKENuN3UT/S4U0AkeL00TrHYjREuOUUpvOPHG1fsAOrEoqiNd3qpeBHcaYVDzNYQMgS404pwqXhRnC1j4oXe24DiFRUUaPL1quD10jil1yUHCroVOyPu6RYLMjIoDm5UTzc7m7YqaW+P/3fLFhF7l2+nnwcMdsrrvcu36WqtFaMuem8D7e3UP7KzNfn6qFF2VDdCeTPdih3ASSoAhQaMvG2OxQIUF9MdrBaCXTmtFK+v2QlPWv+cbk9aIV5fs1M3a6yY2KHXNiBJ1C+KioxZMQUIb0ek9xAmoONF5aJISSp63CointjtdOdXXW2MBjUUeXmUqdXaqk1qdeW0UpyeskrX1TOY2KLHNtDaSuG/vDzNTIiari4SayPsqK5bsbPbw7veJjqZmcDFi+Tp6jm1dzjsduDqq4EjR7S7prJFxMWSJfH/YkY36KkNBIPk1V11lTGEYihkmbxTPVdN6YtuneekJOOuMVOb9PRwrUmjkp9Pd7AtLVpbwjDa09pK/SHfwInASu1bo9Tv1a3YORxaW6AflDI8Rt42RxTpLhYw7rpBhlEDv5+8oquv1n8G40h0dVFiih7q30aCbsXObqeUXF6CQIwfb3yRyM2lO1n27phEprWVlhoYdXcDhUDAWOXNdCt2Nht5d0Yf4NUiI4MKQxs1KxOgjLOrrqK7WSOHZBlmrHR3U/ufNMm4GZhAeDsiIyw5UNDt6RYEWpXPnh2RnEyekcejtSXRkZNDld1bW7W2hGHiT2sr7QiSk6O1JdHhdofLARoF3YodQJ4dr7ULo9TOM3LdUEEg787pNPYcJMOMlq4uaveTJhkzq1ohGCT7jVbLU/dix/N2YTIy6KHltjlqkJlJC83b2vhmhkkMZJnae3ExtX8j4/GExyIjoWux43m7/ogihQC13DZHLSZNopRlDmcyiUBrK2VUT5qktSXRIcu0UeuECcbLJNW12PG83WCysqiSilE3dVVISQGmTqU0bF5PyZgZpY1PmULt3sh0dtL/YJSF5H3RtdgBFOOWZeN7MmrhclGsvKNDa0uip7CQNqxsauLry5gTWab2PXEitXej09FB449R1tb1RXWxe+qppzBv3jykpqZi/PjxWL16NT7//PPRf9D77wOSBKeT1txFFcqUJNjfOwDX7tdgf++AcbcQ+JJx44y9qauCxUJ3uwPDmUJIQnHVYQBAcdVhCCFjXy8mcVHCl1OmGHupAUDeqdVq3PWBqp/+d955Bxs2bMAHH3yAffv2IRgM4tZbb0XXaFPv7r4bKCmB9Y0KJCePPdTlfLMCufNLkHP3UmRuWIucu5cid34JnG9WjO0DdUBGBoURjJ6oAgwOZ06rrED59hLcu2MlAODeHStRvr0E0yqNe72YxMRM4UuAxpvsbBJvI6K62O3duxff/e53MWPGDMyaNQsvv/wyqqqqcOzYsdF/WE0NUFaG9LcrxpRu73yzApn3l8FSd7nfcUt9DTLvLzOs4AkCVWAIBMyx758Sziz6qAJrXi9Dmqf/9Urz1GDN62UseIxhMFv4UpIodyI/37jLJmLuWLu/dD+yxjKj+eVETtKPy2GzSKNLVJEkpD+6EZAH71EsfPm5aY+VGzakqSSqGH2ROfBlOPMrEtYe2YjBW2wCAuh6Ld9bziFNxhCYKXwJ0FxdaqoxE1MUYrrFTygUQnl5ORYsWIBrr712yPf4/X74+8QoPV+O3kGXC0HlFqKlGSkfH4TnuoURhwNsRw4j1N6C0EgzqW3NsBw5iOD8hZF96BBIUrDfc7wQRbrLOns28rCCLAf7PeuJwurDSJZa0PPl9QoOeAaApGAzJtYeRFXx2K9XLLBYgv2e9QzbGhv62qpsVDx1KlUY0Vvy1VjGAa8XmDyZxp14+gdqjquCLMfuUvzgBz/AW2+9hcOHD6NwGF9+69atePzxxwcd37FjB5KSkmJlGsMwDKNzvF4v1q5dC7fbjbQo9xKKmdg9+OCD+J//+R8cPHgQk0ZYSTmUZ1dUVIS6zExk+3y9xwMVe3ChkDy7SBYz2o4cRvZ3Vl7xfS2v7Inaszt5ch9mzVoGUYz/rqSffgrU10dWukeWgwgG98FmWwZB0NeuuJmfHsacR8PXK+hyYd9vfoNl69fD1qdq9O/X7tGlZzdz5j58/PEyhEL6Oq8DYVtjg2Lr3/62DMnJNsyZQ3ty6pHRjgMNDRRFmj49DsYNoL29Bdddl6+K2KkexpRlGf/8z/+MXbt24cCBAyMKHQA4HA44hti8ztbdDZvPR7OhhYUQv74YSdUienoi29k3NH8xLBnZsNTX9M7R9bNTECDlFyI0fzFEFUoBiKJNE7GbMIEaYyAQ+R6AgmDTndi1T1uMnuRsOJtreufogC/bQXc3ZAjwpBXiUsFiyCF9lm4IhWy6H5QV2NbYIIo2zJhhM0SB5EjGAb8/nBCnRcUUNcdU1adON2zYgFdffRU7duxAamoq6uvrUV9fj+6x7OmizNlt2waLTURa2iiWIIgi3E9sB0DC1hfltefxbcareTOArCza8djwZbdEEacf+PJ6DUhRUV7vXb4NssXY14sxJ8rWW9OnU3q+WVB2VDdyYoqC6mL3wgsvwO12Y8mSJcjPz+99/PGPfxz9hxUWAjt3AqWlAGjVviBEnm7vu70UbS/tRChvQr/jUn4h2l7aCd/tpaO3SWcIAlBURIs9jb5HXMOCUpx4ZCd8Of2vlzu1EK+v2YnKaca/Xoz5CASA9nb6uaBAU1NUpbubxpWiIuMuN+hLTMKYqvDf/w2sWNHP83K5KFTn80UeD/fdXgrfbatgP3IIYmMdpPH5CMxfZHiPri8ZGdTJLl6ksKaRaVhQioavrUJG5UEAHvzhvj14J7AY+YUizHPFGLMgSUBjI3D11fTaDKKg0NJChauNtrvBcMR06UFU3HjjIEGyWCjNvr5+lJO/oojATUtUNU9vFBbSeVEKtRoaUUTbjIVA4E2krliIvKMi6utJ0M00mDDGRpapz+XlAdOmaW2NunR20hhrhgXxCoZb7picHP+1HkYgNZXCDW63/tb1RIPDAcycSfUzm5q0toZhwjQ10c33zJmRJ4cZAVmmsGxhIY0rZsFwYud0Ujizz6oE5ksKCsirM8OOCH1RBhSLxRz1QBnj43ZTe5w507i1IodDqZZipvlHwIBiJwjUuHgPtMEkJZF35/GYo2ZmX/LygBkzaNLcbGLOGIuODmqHM2ZEtr7VSIRCNH4UFel3neBYMZzYAXQRbDbe1HUo8vLoZsAMNTMHUlJCA0xnp/E3r2WMSWcnid2MGdQezYbbTeNHXp7WlqiPIcXO4SDB41DmYJxOqrTe1WW+eU1BoKy3adNIzEe7axTDRENXF7W7GTOoHZotWUqSaL3gxIk0jpgNQ4odQAkL7NkNTW4uLQJV1v6YCYsFuOYaKrLb3h5ezMswscTrpfY2bRoVRDbDTgYDaW+nccNsoVkFw14yZc0dz90NxmYDiovJ8x3LPoB6x2IhsZs6FWhrYw+PiS1dXdTOpk41z5Y9A+npofGiuJjGDzNi2Mtmt1PmodGrhsSKcePo0damtSWxwWKhu+xp02iegefwmFjQ2Unta/p0amtmFDqAyoKNGweMH6+1JbHD0JdOWQNitsxDNbBaKfYeCpl3blPx8KZPp6QBztJk1KSjIzxHZ1aPDqDxQZZpvDBRYalBGPryJSXRg727ocnJoYWhzc2A3CMh89PDAGg7HbNkryhzeNdeS+EmXofHqIHbTe1p5kxqX2YVOlmm8aGwkMYLM2PoS2ixUN02nrcbGkGg9Ojpn1dg8bqS3v3i5jy6Ejd/twS571Zoa6BKWCzAV74CXHcdJS01NpqrigwTP2SZ2k8wCMyaRe3KbFmXfWlrozG0pMTc/ydgcLEDqHyYUhyaGUzm/grc+O9lSGq93O+4s7kGs58sM43gCQJw1VXAvHmUvFRbaxrnlYkTkkTtJimJ2tGkSeYWgECAHIWrrqI+Y3YML3Y2Gy1D4FDmEEgS0h/diME7xKF3g9Spvy43lSrk5gI33EDPtbXUoRnmSgQCQF0dtZt588ybft8XJXyZCP8rYAKxAyhRxWIxZ5p9NNiPHIJYd3mQ0CkIkOFqqkbWp4fialesSU8H5s6lO9bGRl6awIxMVxe1k0mTqN2YrdblcKSkmN977Yt+t/gZBS4XhTO7u81VpTtaxMa6iN7naI3sfUbC6QRmz6Z2cfo0hWvMsNsyoy6treTVXXstLRY3czaighLtmDTJfPUvR8IUnp0g0CRrMMiJCX2RxudH9D5/VmTvMxqiSCnjc+ZQG6mv5/bBEKEQhS0FgdrHlCmJIXQAbcoKJE74UsEUYgfQHQpv/dOfwPxFkPILIQ8Tp5AhoHtcEVpnLIqzZfFDEKiC+w03UHiqpoazdxMdv5/mczMyqF0UFSVOKK+9naIdgHmXUwyHaf5dq5UGM05U6YMowv3EdgAYJHhKysrp729LiFva7Gwa2EpK6M62pYW9vERDlsPXftIkag/Z2VpbFT8CAZqfNONuDZFgGrEDaL7Obmfvri++20vR9tJOhPIm9D8+rhBv/2An6m8q1ciy+JOcDHz1q5SEYLezl5dI+P10ve12uv7XXx/2cBIBWaad1RMp+3IgpkhQUXA4KDTR2GjOLSrGiu/2UvhuWwXLkYMAPGh5ZQ+6rlsM98civM1UEy9RsFio2G1WFiWuVFVRu8nKSpxQViIhy5SE4veTNzdlCmUhJhrNzTQ2fuUrCRHIGRJTeXYArbmz2Xh91SBEEcH5CwEAwfkL4UoRMXkyDQaJWEQ5JYW9PLMzlDeXiEKn9O9rrkmMxePDYSrPDiCPLj2d7ubsdq2t0Tc5ObQJZWUleTdm3dpjONjLMyfszYUJBqnO59SpiTU/ORSm8+wAEjuLhTd3jYTiYmDChMSuJzmUl8cL0Y1JVxd7cwqyDDQ0AAUF1M8THdN5dgC56mlplGabkaG1NfpGFGkxbVcXZamZvfL5cPT18i5cIC9P2bk5kUM/RqG7m7w5p5M8uUmTElfkFJR5ukRZLH8lTCl2AHl37e1UQsxq2v9SHZKSqEOcOkV7eCVyFZqUFNrWpbAQOH+evIT2dgoBcVhcfwQCdJMmiiRwkyYBmZlaW6U9yt6OkycnVpWUkTCtDCQl0aDd1UVeHjMy48ZRx/jsM5q7S/Rs1sxMCm0WF5Po1X1ZUS07O/HmNvVIMBiuBDJhAolcTg7PtQK09KqjgzY1TqRM6ythWrETBBqwOjqoqD+78VemsJDCQefOAXl57BELAg0W2dk0p3nuHJUcs9spvMltKv5IUrieZV4eJViNH5941UCGo6eHwpdXX039mQlj6uGMvbvRYbHQTgHd3VROacIEvlMG6Lzk5ZHw1dWR6NXVkehlZLAXHA8CAQonBwIkblddBeTn8w1HX0IhuhkrKKDzwzcA/TG12FksYe+O5+4iw2aj9Tjd3eTNJGq1haEQRbpbHj+eBpXqarqLBmhX60TNZo0VyvlUQsg5OVTHMi+P50+HoqmJIg7XXMOh9qEw/fCfnEzJKm43Z2ZGSlISrcs5dYpCRrw1Tn/sdprLKyykeaP6ejpeW0u/S0/nG6to6Omh/hoI0DZNV11FApedzd7KcCjriqdM4YSU4TB90xEEIDNNQsrRA7DtfA329w6YamfuWJGZSR2npwfweLS2Rp9YLBTavPZaej17Ng00jY3kjXBR8tHh9dJ5a2wEkp0SVqQcBgAsEg5jXJbEQjcMHg/10ylTOBN1JGLWfJ5//nmUlJTA6XRi/vz5+PDDD2P1VSNTUYGk6SUoXrcU4zauRc7dS5E7vwTONyu0scdA5OWRh+f18iLrSCgpARYtAm68kcJtXV3hUCcXJx8an4/OT3U1tbOiIuCb9go89tsSrHxhJQBgzqMrcfN3S5D7LvfZgXR10XmbOpX6KzM8MRG7P/7xj9i0aRMee+wxHD9+HLNmzcJtt92GxsbGWHzd8FRUAGVlwOXL/Q5b6muQeX8ZC14ETJhAxWPb23nAjgSrlRIn5s4l4Zs9m8KaHR3UDBsaaIBK1Pk9Wab/v6GBzkdHB52f2bPpfK30V2Dp82VwtvTvs87mGsx+sowFrw8+H/XLyZOpnzIjExOxe/bZZ/GP//iP+N73vofp06fjxRdfRFJSEn7zm9/E4uuGRpKAjRuHHFWEL4+lPVbOIc0rIAjksVx9Nc1PcYHtyElPp4FowQJg8WJg3jwSwkCA5vdqa2luyuxNUJLo/1T+50CAzsO8eXReFiyg85SeImHqixsR3m0xjADqs1N/XW7+ExYBymL6q68GJk7krOlIUH0aPRAI4NixY3j44Yd7j1ksFtxyyy14//33B73f7/fD36fcvNvtBgC0trZGZ8j771N8ZKS88NYmdO1/C8G5N475ayQpCK/Xi/b2FoiivlOgorE1Kwtoa6MyWuPGxT4BQ5aDCAa9CAZbIAj6Pq+R2Gq1UjZhTg7N5bndlFTQ1ETzVLJMGXQOBz1idX4tFmoDgUALQqHYnNeeHirC7PfT4m9BoLnMoiJKMklL61+CTZnbzKh8H97OZni/7LNBpxNerxctTidsyk1rRxNsf38L7dPG3mdjQTzba09PeG+6rKzRz6kbacxyu0kHZDVCIbLK1NTUyADk9957r9/xzZs3yzfccMOg9z/22GMyAH7wgx/84Ac/hnycO3cuam3SPEH64YcfxqZNm3pft7e3Y+LEiaiqqkJ6erqGlkWGx+NBUVERqqurkabzletsa2xgW2MD2xobjGSr2+1GcXExslRY/6S62OXk5EAURTQ0NPQ73tDQgLwh0oUcDgccDseg4+np6bq/EH1JS0szjL1sa2xgW2MD2xobjGSrRYV1J6onqNjtdsyZMwdvv/1277FQKIS3334bN96orzg7wzAMkxjEJIy5adMmrFu3DnPnzsUNN9yAbdu2oaurC9/73vdi8XUMwzAMMyIxEbt77rkHTU1NePTRR1FfX4/Zs2dj7969yI2g0KLD4cBjjz02ZGhTjxjJXrY1NrCtsYFtjQ2Jaqsgy4m6vJVhGIZJFLjaHMMwDGN6WOwYhmEY08NixzAMw5geFjuGYRjG9OhO7HSzNdAIPPXUU5g3bx5SU1Mxfvx4rF69Gp9//rnWZkXEz3/+cwiCgPLycq1NGZKamhp8+9vfRnZ2NlwuF2bOnImjR49qbdYgJEnCli1bMGnSJLhcLlx99dX4P//n/6hTwy9KDh48iDvuuAMFBQUQBAG7d+/u93tZlvHoo48iPz8fLpcLt9xyC86ePauNsRjZ3mAwiIceeggzZ85EcnIyCgoK8J3vfAe1tbW6s3UgDzzwAARBwLZt2+JmX18isbWyshJ33nkn0tPTkZycjHnz5qGqqkp3tnZ2duLBBx9EYWEhXC5X7wYDo0FXYqebrYGuwDvvvIMNGzbggw8+wL59+xAMBnHrrbeiS+ebvn300Uf49a9/jeuuu05rU4akra0NCxYsgM1mw1tvvYXPPvsM//Ef/4FMHe5I+fTTT+OFF17Ar371K1RWVuLpp5/GM888g+eee05r09DV1YVZs2bh+eefH/L3zzzzDH75y1/ixRdfxJEjR5CcnIzbbrsNPo32cBrJXq/Xi+PHj2PLli04fvw4Kioq8Pnnn+POO+/UwNIrn1uFXbt24YMPPkBBQUGcLBvMlWw9d+4cFi5ciKlTp+LAgQM4deoUtmzZAudIxfNjxJVs3bRpE/bu3YtXX30VlZWVKC8vx4MPPog33ngj8i+Jurqmitxwww3yhg0bel9LkiQXFBTITz31lIZWXZnGxkYZgPzOO+9obcqwdHR0yJMnT5b37dsn33zzzfLGjRu1NmkQDz30kLxw4UKtzYiIlStXyuvXr+93rLS0VL733ns1smhoAMi7du3qfR0KheS8vDz5F7/4Re+x9vZ22eFwyK+99poGFvZnoL1D8eGHH8oA5EuXLsXHqGEYztbLly/LEyZMkD/55BN54sSJ8n/+53/G3baBDGXrPffcI3/729/WxqARGMrWGTNmyE888US/Y1/96lfln/zkJxF/rm48O2VroFtuuaX32EhbA+kJZVsiNYqVxooNGzZg5cqV/c6v3njjjTcwd+5c3H333Rg/fjyuv/56/Nd//ZfWZg3JTTfdhLfffhtnzpwBAJw8eRKHDx/GihUrNLZsZC5cuID6+vp+7SA9PR3z58/XfT9TcLvdEAQBGRkZWpsyiFAohPvuuw+bN2/GjBkztDZnWEKhEPbs2YNrrrkGt912G8aPH4/58+ePGJbVkptuuglvvPEGampqIMsy9u/fjzNnzuDWW2+N+DN0I3bNzc2QJGlQlZXc3FzU19drZNWVCYVCKC8vx4IFC3Dttddqbc6Q/OEPf8Dx48fx1FNPaW3KiJw/fx4vvPACJk+ejL/+9a/4wQ9+gB/+8If43e9+p7Vpg/i3f/s3fPOb38TUqVNhs9lw/fXXo7y8HPfee6/Wpo2I0peM1s8UfD4fHnroIXzrW9/SZRHjp59+GlarFT/84Q+1NmVEGhsb0dnZiZ///OdYvnw5/vd//xd33XUXSktL8c4772ht3iCee+45TJ8+HYWFhbDb7Vi+fDmef/55LF68OOLP0HyLH6OzYcMGfPLJJzh8+LDWpgxJdXU1Nm7ciH379mkSix8NoVAIc+fOxc9+9jMAwPXXX49PPvkEL774ItatW6exdf15/fXX8fvf/x47duzAjBkzcOLECZSXl6OgoEB3tpqFYDCINWvWQJZlvPDCC1qbM4hjx45h+/btOH78OASdbx0eCoUAAKtWrcKPfvQjAMDs2bPx3nvv4cUXX8TNN9+spXmDeO655/DBBx/gjTfewMSJE3Hw4EFs2LABBQUFEUerdOPZjXZrID3w4IMP4i9/+Qv279+PwsJCrc0ZkmPHjqGxsRFf/epXYbVaYbVa8c477+CXv/wlrFYrJEnS2sRe8vPzMX369H7Hpk2bpkl22JXYvHlzr3c3c+ZM3HffffjRj36ke+9Z6UtG6mdAWOguXbqEffv26dKrO3ToEBobG1FcXNzb1y5duoR/+Zd/QUlJidbm9SMnJwdWq9UQ/a27uxs//vGP8eyzz+KOO+7AddddhwcffBD33HMP/v3f/z3iz9GN2BlpayBZlvHggw9i165d+Nvf/oZJkyZpbdKwfP3rX8fHH3+MEydO9D7mzp2Le++9FydOnIAoilqb2MuCBQsGLeE4c+YMJk6cqJFFw+P1egftsSWKYu8ds16ZNGkS8vLy+vUzj8eDI0eO6K6fKShCd/bsWfy///f/kJ2drbVJQ3Lffffh1KlT/fpaQUEBNm/ejL/+9a9am9cPu92OefPmGaK/BYNBBIPBqPubrsKYRtkaaMOGDdixYwf+53/+B6mpqb1zHenp6XC5XBpb15/U1NRBc4nJycnIzs7W3Rzjj370I9x000342c9+hjVr1uDDDz/ESy+9hJdeeklr0wZxxx134Kc//SmKi4sxY8YM/P3vf8ezzz6L9evXa20aOjs78cUXX/S+vnDhAk6cOIGsrCwUFxejvLwcTz75JCZPnoxJkyZhy5YtKCgowOrVq3Vnb35+PsrKynD8+HH85S9/gSRJvf0tKysLdrtdN7YWFxcPEmKbzYa8vDxMmTIlrnYCV7Z18+bNuOeee7B48WIsXboUe/fuxZ///GccOHBAd7befPPN2Lx5M1wuFyZOnIh33nkHr7zyCp599tnIvyTqPFGVee655+Ti4mLZbrfLN9xwg/zBBx9obdIgAAz5+O1vf6u1aRGh16UHsizLf/7zn+Vrr71Wdjgc8tSpU+WXXnpJa5OGxOPxyBs3bpSLi4tlp9MpX3XVVfJPfvIT2e/3a22avH///iHb57p162RZpuUHW7ZskXNzc2WHwyF//etflz///HNd2nvhwoVh+9v+/ft1ZetQaLn0IBJb/+///b/yV77yFdnpdMqzZs2Sd+/erUtb6+rq5O9+97tyQUGB7HQ65SlTpsj/8R//IYdCoYi/g7f4YRiGYUyPbubsGIZhGCZWsNgxDMMwpofFjmEYhjE9LHYMwzCM6WGxYxiGYUwPix3DMAxjeljsGIZhGNPDYscwDMOYHhY7hmEYxvSw2DEMwzCmh8WOYRiGMT0sdgzDMIzp+f/9CWFrmHINPAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, results, v_res = dev_routine(model_kwargs={\n",
    "                                'n_classes': 10,\n",
    "                                'out_dim' :  [1, 8, 16, 32], # [1, 8, 16, 32], #[1, 16, 24, 32]\n",
    "                                'grid_size' : 18*18,\n",
    "                                'criterion': torch.nn.CrossEntropyLoss(),# torch.nn.BCEWithLogitsLoss(),\n",
    "                                'optimizer': \"sgd\", # sgd adamw\n",
    "                                'base_lr': 0.001,\n",
    "                                'min_lr' : 0.00001,\n",
    "                                'momentum' : 0.9,\n",
    "                                'lr_update' : 100,\n",
    "                                'cc_weight': 10,\n",
    "                                'cc_metric' : 'l2',\n",
    "                                'update_every_nth_epoch' : 2, # 10\n",
    "                                'prune_keep' : 0.9, # 0.97, # in each epoch\n",
    "                                'prune_keep_total' : 0.5, # this number is not exact, depends on the prune_keep value\n",
    "                            },\n",
    "                            train_kwargs={\n",
    "                                'ckpt_path': \"example_results/lightning_logs\", # \"example_results/lightning_logs\", # not in use??\n",
    "                                'exp_name': \"tmp\",\n",
    "                                'load_ckpt_file' : \"xversion_22/checkpoints/epoch=0-unpruned=10942-val_f1=0.06.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "                                'epochs': 1,\n",
    "                                'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ...\n",
    "                                'batch_size': 1, # 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "                                'log_every_n_steps' : 2, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "                                # 'test_batch_size': 1,\n",
    "                                'device': \"cuda\",\n",
    "                                'num_workers' : 0, # 18, # 18 for computer, 0 for laptop\n",
    "                            }\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43468c27-f130-4118-8d11-275282921c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2c1d301c-cbad-4154-a053-137a3263c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "for l in range(100):\n",
    "    try:\n",
    "        layer = model.model.decent2.filter_list[l] # .filter_list[7]weights\n",
    "        run_explain(model, layer, device='cuda')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2568e15-95ba-4c8c-9a51-b8cb5050ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "layer = model.model.decent2 # .filter_list[7]weights\n",
    "run_explain(model, layer, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f4513-419a-433b-97ff-6f54c5a6d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b09e4-644a-4cca-9da3-a47986de1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca471c2c-819f-44aa-a8f9-e318aacf3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['loops'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a04f0-8c5d-44f1-b0b9-616c64341cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79954f6-112e-4036-aa78-1930828cf4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19929df-c8e1-4548-95e4-cd7c807f1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f4c7f-d650-41fb-b1ef-953fba2e3e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97faa8-afbb-4869-b630-ec3c18936e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd46dc-dcde-48c4-bc34-f9bf5bcad6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb9d4c30-9266-4c29-862b-fffef138cebe",
   "metadata": {},
   "source": [
    "### normal train without lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d89cfe-7561-438c-a0b4-bb3c0c951716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for i_batch, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if i_batch == 5:\n",
    "            \n",
    "            \n",
    "            #model.update()\n",
    "            \n",
    "            #print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            #print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            #print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i_batch * len(data), len(train_loader.dataset),\n",
    "                100. * i_batch / len(train_loader), loss.item()))\n",
    "            \n",
    "            # model.update()\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 1\n",
    "        self.save_model = False\n",
    "        \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = DecentNet().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        model.update() \n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.ckpt\")\n",
    "\n",
    "    return model\n",
    "\n",
    "if False:\n",
    "    model = main()\n",
    "\n",
    "    for i in model.parameters():\n",
    "        print(i.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95870660-6418-4b85-a673-13e266a53960",
   "metadata": {},
   "source": [
    "# conv filter test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7bceb-331c-4f16-be5f-3c5c5d91ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16303538-75ae-4064-ae26-848926552bb7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one filter\n",
    "\n",
    "w_groups = 1 # groups of the channels\n",
    "w_channels = 1024 # input channels\n",
    "w_filters = 10\n",
    "\n",
    "assert w_filters % w_groups == 0\n",
    "assert w_channels % w_groups == 0\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "# w_filters x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "# batch size x w_filters x width x height\n",
    "out = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "print(weights.shape, \"- filters x channels x kernel x kernel\")\n",
    "print(out.shape, \"- batch x filters x width x height\")\n",
    "print()\n",
    "\n",
    "print(\"*\"*50)\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "output_list = []\n",
    "\n",
    "# for each filter, we need different true false vales for our channels\n",
    "active = list(np.random.choice([True, False], size=w_channels, replace=True, p=None))\n",
    "\n",
    "# w_filters x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(1,w_channels,3,3))\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "output_list = []\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    for _ in range(w_filters):\n",
    "        \n",
    "        pass\n",
    "\n",
    "        #print()\n",
    "        #print(active)\n",
    "        #print()\n",
    "        #print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "        #print(weights.shape, \"- filters x channels x kernel x kernel\")\n",
    "\n",
    "\n",
    "        # need to remove weight and input channels according to active list for each filter\n",
    "        #print(inputs.shape)\n",
    "        #print(inputs[:,active,:,:].shape)\n",
    "\n",
    "        #print(weights.shape)\n",
    "        #print(weights[:,active,:,:].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # batch size x w_filters x width x height\n",
    "        \n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            #output_list.append(this_output)\n",
    "        #print(this_output.shape, \"- batch x 1 filter x width x height\")\n",
    "\n",
    "    #out = torch.cat(output_list, dim=1)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    i_tmp = inputs[:,active,:,:]\n",
    "    w_tmp = weights[:,active,:,:]\n",
    "    this_output = F.conv2d(i_tmp, w_tmp, padding=1, groups=w_groups)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    this_output = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print(out.shape, \"- batch x filters x width x height\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# take the mean of all - we can remove all sorts of information from the out tensor\n",
    "mean = torch.mean(out, 1, keepdim=True)\n",
    "print(mean.shape, \"- mean accross the filters (no sense here ...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e3c4e-80a1-448f-9b3c-4ae14cd7bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "#b = torch.empty(w_filters, w_channels, dtype=torch.bool)\n",
    "b = torch.ByteTensor(500, w_channels)\n",
    "print(sys.getsizeof(b.storage())) # 1310776 (bytes)\n",
    "\n",
    "#a = torch.empty(w_filters, w_channels, dtype=torch.uint8)\n",
    "a = torch.ByteTensor(500, w_channels)\n",
    "print(sys.getsizeof(a.storage())) # 1310776 (bytes)\n",
    "\n",
    "active = list(np.random.choice([True, False], size=w_channels, replace=True, p=None))\n",
    "print(sys.getsizeof(active)*500) # 1310776 (bytes)\n",
    "\n",
    "weights = torch.FloatTensor(torch.randn(500,w_channels,3,3))\n",
    "print(sys.getsizeof(weights.storage())) # 1310776 (bytes) 36912\n",
    "print(36912*500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2b255-ccc3-4809-948c-cd4dc5efe661",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dadee83-7300-4844-8284-8d0ef29aaee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(2, size=10)\n",
    "\n",
    "list(np.random.choice([True, False], size=10, replace=True, p=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c72946-baf5-4c90-b84e-14f14c8d949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(1, 82, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2754cb-b071-4267-b53c-5f2190a7baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "9*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95feb489-36f2-4d83-81e3-5e21a530f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(a=4, size=2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5d97e-7408-4c52-8891-864827810c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one filter\n",
    "\n",
    "w_groups = 27 # groups of the channels\n",
    "w_channels = 7 # input channels\n",
    "w_filters = 200\n",
    "batch_size = 27\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(batch_size, w_channels*w_groups, 100,100))\n",
    "#inputs = torch.autograd.Variable(torch.randn(w_channels*w_groups, 100,100))\n",
    "\n",
    "# w_groups x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(1, w_channels,3,3))\n",
    "#weights = torch.autograd.Variable(torch.randn(w_groups*w_channels,3,3))\n",
    "\n",
    "print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "print(weights.shape, \"- filter 1 x channels x kernel x kernel\")\n",
    "\n",
    "try:\n",
    "    o_list = []\n",
    "    for _ in range(w_filters):\n",
    "        # batch size x groups x width x height\n",
    "        out = F.conv2d(inputs, weights, groups=w_groups)\n",
    "        o_list.append(out)\n",
    "        # take the mean of all - we can remove all sorts of information from the out tensor\n",
    "        #mean = torch.mean(out, 1, keepdim=True)\n",
    "    \n",
    "    print(torch.cat(o_list, dim=1).shape, \"- batch x filters x width x height\")\n",
    "    #print(mean.shape, \"- mean accross the groups\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3d443-372f-4548-804e-11efbebe6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = torch.randn(8, 4, 3, 3)\n",
    "inputs = torch.randn(1, 4, 5, 5)\n",
    "F.conv2d(inputs, filters, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bf7b8-35d2-4f68-b045-0ba93bd4c5fc",
   "metadata": {},
   "source": [
    "# Visualise filters and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536067b-6098-4844-9ef4-7a5db5a23156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net() # .to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320d57b-3e22-404f-ad6c-b04515fe2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "def visChannels(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    \n",
    "    plt.figure(figsize=(nrow,rows) )\n",
    "    plt.title(f\"Channels with index {ch}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "def visFilters(tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.title(f\"Filter {filt}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "def visFilters_subplot(subplot, tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    subplot.set_title(f\"Filter {filt+1} with {c} channels\")\n",
    "    subplot.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "    subplot.axis('off')\n",
    "    \n",
    "layer = 1\n",
    "filter = model.conv2.weight.data.clone()\n",
    "\n",
    "print(model.conv2.weight.shape)\n",
    "\n",
    "# need to match the network parameters!!!!\n",
    "in_channels = 5\n",
    "out_filters = 3 # 64\n",
    "\n",
    "\n",
    "fig, subplot = plt.subplots(out_filters, figsize=(10, 10))\n",
    "fig.suptitle(f'Layer with shape {list(model.conv2.weight.shape)} [out, in, kernel, kernel]')\n",
    "\n",
    "for filt in range(0, out_filters):\n",
    "    \n",
    "    visFilters_subplot(subplot[filt], filter, filt=filt, allkernels=False)\n",
    "\n",
    "    #plt.axis('off')\n",
    "    #plt.ioff()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"example_results/filter_with_weights.png\")\n",
    "plt.show()\n",
    "    \n",
    "if False:    \n",
    "    for filt in range(0, out_filters):\n",
    "\n",
    "        visFilters(filter, filt=filt, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"examples/example_results/filter_with_weights.png\")\n",
    "        plt.show()\n",
    "\n",
    "    for ch in range(0, in_channels):\n",
    "\n",
    "        visChannels(filter, ch=ch, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791fa94-5bb8-4d3a-ad97-72cc5a59025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "res = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655d64a-7fc0-42c7-880e-69e7be6acedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.layer1[0].conv1.bias == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3322395-f2a0-40e1-a0e8-86c090586f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.layer1[0].conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d52543-cd67-4d00-963a-5d1effbfc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.extra_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77091394-6e75-4aa3-84c3-a9ebc7ce932e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
