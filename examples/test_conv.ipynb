{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DecentNet from conv layer\n",
    "\n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "     \n",
    "* pruning actually doesn\"t work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df9f06-ef7e-4c73-887b-5b2d129ff9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../helper', './helper', '/helper', 'helper', 'C:\\\\Users\\\\Christina\\\\Documents\\\\datasceyence\\\\examples', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\python39.zip', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\DLLs', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy', '', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\pixelssl-0.1.4-py3.9.egg', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from typing import Optional, List, Tuple, Union\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "sys.path.insert(0, \"/helper\")\n",
    "sys.path.insert(0, \"./helper\")\n",
    "sys.path.insert(0, \"../helper\")\n",
    "print(sys.path)\n",
    "\n",
    "# own module\n",
    "from visualisation.feature_map import *\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38836e4-6905-4e0c-b344-bcc3b8094388",
   "metadata": {},
   "source": [
    "# conv2d layer (slightly adapted original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b972ff66-723c-43a1-a9d9-80c43b450efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConvNd(torch.nn.Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    in_channels: int\n",
    "    _reversed_padding_repeated_twice: List[int]\n",
    "    out_channels: int\n",
    "    kernel_size: Tuple[int, ...]\n",
    "    stride: Tuple[int, ...]\n",
    "    padding: Union[str, Tuple[int, ...]]\n",
    "    dilation: Tuple[int, ...]\n",
    "    transposed: bool\n",
    "    output_padding: Tuple[int, ...]\n",
    "    groups: int\n",
    "    padding_mode: str\n",
    "    weight: Tensor\n",
    "    bias: Optional[Tensor]\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Tuple[int, ...],\n",
    "                 stride: Tuple[int, ...],\n",
    "                 padding: Tuple[int, ...],\n",
    "                 dilation: Tuple[int, ...],\n",
    "                 transposed: bool,\n",
    "                 output_padding: Tuple[int, ...],\n",
    "                 groups: int,\n",
    "                 bias: bool,\n",
    "                 padding_mode: str,\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        print(factory_kwargs)\n",
    "        super().__init__()\n",
    "        if groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            #self.importance = Parameter(torch.empty(\n",
    "            #    (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            #self.importance = Parameter(torch.empty(\n",
    "            #    (out_channels, in_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "class CustomConv2d(_ConvNd):\n",
    "    \n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = stride #_pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super().__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        print(self.stride)\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369256cb-1ea4-4ff2-8ce5-a820511e0779",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2823e218-5eb3-44d9-a277-2cf17b772c84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CustomConv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv2 = CustomConv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv3 = CustomConv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv1x1 = CustomConv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "        self.K = 100 \n",
    "        self.L = 10 # last one\n",
    "        self.num_of_bases = 1 # 3rd dim\n",
    "        \n",
    "        if False:\n",
    "            self.conv1 = Conv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv2 = Conv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv3 = Conv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv1x1 = Conv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "        # 4x16384\n",
    "        # self.fc1 = nn.Linear(10*10*10, 10)\n",
    "        #self.fc2 = nn.Linear(10, 10)\n",
    "        \n",
    "        #self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        #self.sub_concept_pooling = nn.modules.MaxPool2d((self.K, 1), stride=(1,1))\n",
    "        #self.instance_pooling = nn.modules.MaxPool2d((opt.num_of_bases, 1), stride=(1,1))\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.mish1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.mish2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.mish3(x)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        #print(x.size())\n",
    "        #print(x.size()[2:])\n",
    "        \n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        #x = x.view(-1, self.L, self.K, 10)\n",
    "        \n",
    "        # input, kernel_size, stride, padding, dilation, ceil_mode\n",
    "        #x = self.sub_concept_pooling(x).view(-1, self.L, self.num_of_bases).permute(0,2,1).unsqueeze(1)\n",
    "        \n",
    "        # output = F.sigmoid(x)\n",
    "        # x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        #x = torch.flatten(x, 1)\n",
    "        # x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.fc2(x)\n",
    "        #output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76fa05-b47e-4264-85cf-766d8ca060d3",
   "metadata": {},
   "source": [
    "# normal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0819306e-ff93-4f38-898b-4c1cd2221b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for i_batch, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if i_batch == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i_batch * len(data), len(train_loader.dataset),\n",
    "                100. * i_batch / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            \"\"\"\n",
    "            if i == 0 and epoch % args.log_interval == 0:\n",
    "            # if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot(path=f\"example_results/feature_map_{epoch}.png\")\n",
    "                \"\"\"\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.1\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 5\n",
    "        self.save_model = True\n",
    "        \n",
    "\n",
    "def main_train():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader, epoch)\n",
    "        #scheduler.step()\n",
    "        \n",
    "        \n",
    "        if args.save_model and epoch % args.log_interval == 0:\n",
    "            torch.save(model.state_dict(), f\"example_results/mnist_cnn_{epoch}.ckpt\")\n",
    "\n",
    "\n",
    "def main_test():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "\n",
    "    if True:\n",
    "        model.load_state_dict(torch.load(\"example_results/mnist_cnn_5.ckpt\"))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(\"example_results/pruned_model.ckpt\"))\n",
    "    \n",
    "\n",
    "    # model = torch.load(model.state_dict(), \"example_results/mnist_cnn_30.ckpt\")\n",
    "    if False:\n",
    "        test(args, model, device, test_loader, 0)\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38eb5e24-3f20-4788-8344-66a98d6791bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1242e1a-c691-4cb6-a279-9904cedcd806",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n"
     ]
    }
   ],
   "source": [
    "model_to_prune = main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "270319e6-3827-47cd-a579-476c89217968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(model_to_prune.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {},
   "source": [
    "# DecentNet trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfa123-54e1-4053-94c3-52b45ec0859c",
   "metadata": {},
   "source": [
    "## DecentFilter\n",
    "* conv2d problem: https://stackoverflow.com/questions/61269421/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # convolution happens in here\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels=32, \n",
    "                 i_filter=[],\n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 padding_mode=\"zeros\",\n",
    "                 dilation=3, \n",
    "                 transposed=None, \n",
    "                 device=None, \n",
    "                 dtype=None):\n",
    "        \n",
    "        #print(\"device\", device)\n",
    "        \n",
    "        \n",
    "        # out_channels = 1\n",
    "        # groups = 1\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = stride # _pair(stride)\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        \n",
    "        \n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "            \n",
    "        self.in_channels = in_channels\n",
    "        self.i_filter = i_filter # index of this filter\n",
    "        # self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        #self.output_padding = output_padding\n",
    "        # self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "        # print(factory_kwargs)\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        # filters x channels x kernel x kernel\n",
    "        self.weight = Parameter(torch.empty((1, in_channels, *kernel_size), **factory_kwargs)) # .to(device)\n",
    "        \n",
    "        \n",
    "        # set each channel true\n",
    "        self.active = np.array([True] * self.in_channels)\n",
    "        \n",
    "        self.init_position()\n",
    "        \n",
    "        # for each filter, we need different true false vales for our channels\n",
    "        #active = list(np.random.choice([True, False], size=w_channels, replace=True, p=None))\n",
    "\n",
    "        # w_filters x w_channels x kernel x kernel\n",
    "        #weights = torch.autograd.Variable(torch.randn(1,w_channels,3,3))\n",
    "\n",
    "        \n",
    "        \n",
    "        #self.importance = Parameter(torch.empty(\n",
    "        #    (in_channels), **factory_kwargs))\n",
    "            \n",
    "        if False: \n",
    "            # bias:\n",
    "            # where should the bias be???\n",
    "            self.bias = Parameter(torch.empty(1, **factory_kwargs))\n",
    "        else:\n",
    "            #self.bias = False\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        \n",
    "        # reset weights and bias - in filter or in layer?\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def init_position(self):\n",
    "        # numbers fro 0 to 81\n",
    "        # 0/0 to \n",
    "        self.position = list(np.random.randint(81, size=self.in_channels))\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        # randomly initialise the positional array\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        \n",
    "        # todo - weight has to be the one in the filter\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    \n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input[:,self.active,:,:], self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, groups=1)\n",
    "        else:\n",
    "            # this is written in c++\n",
    "            # todo: cuda is ... not a variable ...\n",
    "            out = F.conv2d(input[:,self.active,:,:], weight, bias, self.stride, self.padding, self.dilation, groups=1)\n",
    "        \n",
    "            # print(out.shape, \"- batch x filters x width x height\")        \n",
    "\n",
    "            return out\n",
    "        \n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "    \n",
    "    def update(self):\n",
    "        # channel deactivation\n",
    "        # require_grad = False/True for each channel\n",
    "        \n",
    "        deactivate_ids = [1, 2, 6, 9]\n",
    "        \n",
    "        self.active[deactivate_ids] = False\n",
    "        \n",
    "        print(self.weight.shape)\n",
    "        \n",
    "        print(self.weight[:,self.active,:,:].shape)\n",
    "        \n",
    "        # this is totally wrong\n",
    "        self.weight = self.weight[:,self.active,:,:].detach()\n",
    "        self.weight = self.weight.cuda()\n",
    "        \n",
    "        print(\"weight\")\n",
    "        print(self.weight.shape)\n",
    "        print(self.active)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "99cba028-66b0-4781-8334-855d22186e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([True] * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2c4e7-d451-4246-8253-9d4591875a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "5cf5bf8e-bc64-4f06-b95d-e7cc16ea42d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 3, 0])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = torch.tensor([1, 6, 9, 40, 5, 3, 4]) # np.arange(50)\n",
    "# l = l[[1, 6, 2, 4, 3]]\n",
    "\n",
    "l[[1, 6, 2, 4, 3]] = False\n",
    "\n",
    "l \n",
    "#np.where(l == 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdab4-e0b0-4523-882f-dad3ebf06090",
   "metadata": {},
   "source": [
    "## DecentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "fddb74c7-5940-424d-aab8-2c75efd914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLayer(torch.nn.Module):\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'in_channels', #  'output_padding',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "        \n",
    "        \n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: _size_2_t,\n",
    "                 stride: _size_2_t = 1,\n",
    "                 padding: Union[str, _size_2_t] = 0,\n",
    "                 dilation: _size_2_t = 1,\n",
    "                 transposed: bool = False,\n",
    "                 #output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 #groups: int = 1,\n",
    "                 bias: bool = True,\n",
    "                 padding_mode: str = \"zeros\",\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = stride # _pair(stride)\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # torch.nn.ModuleList\n",
    "        \n",
    "        self.geometry_array = np.full(81, np.nan)\n",
    "        self.module_list = nn.ModuleList([])\n",
    "        for i_filter in range(out_channels):\n",
    "            self.module_list.append(DecentFilter(in_channels=in_channels, i_filter=i_filter))\n",
    "            self.geometry_array[i_filter]=i_filter # id of the filter\n",
    "        \n",
    "        \n",
    "        np.random.shuffle(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.reshape((9,9), order='C')\n",
    "        \n",
    "        # a = np.full(81, np.nan)\n",
    "        # geometry_array = a.reshape((9,9))\n",
    "        \n",
    "        print()\n",
    "        print(self.geometry_array)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # reset in initialisation\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \n",
    "    def sparcify(self) -> None:\n",
    "        # pruning\n",
    "        \n",
    "        # delete layer with id\n",
    "        # delete channels in each layer with id\n",
    "        \n",
    "        # change positions\n",
    "        # change\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        pass\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        \n",
    "        # todo - weight has to be the one in the filter\n",
    "        # init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        \"\"\"\n",
    "        not needed due to instance norm layer\n",
    "        https://github.com/pytorch/vision/issues/4914\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\"\"\"\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "            \n",
    "    def update(self):\n",
    "        for module in self.module_list:\n",
    "            module.update()\n",
    "        \n",
    "            \n",
    "            \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \n",
    "        output_list = []\n",
    "        for module in self.module_list:\n",
    "            output_list.append(module(input))\n",
    "         \n",
    "        out = torch.cat(output_list, dim=1)\n",
    "        # mean = torch.mean(out, 0, keepdim=True)\n",
    "        \n",
    "        return out\n",
    "        # return self._conv_forward(input, self.weight, self.bias)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c186cf1-c7db-45da-b779-f7ab88f3896f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 6.  3. 16. nan 29. nan nan nan nan]\n",
      " [nan 17. nan 25. 14. nan nan 12. 30.]\n",
      " [nan 27. nan nan nan nan nan nan  8.]\n",
      " [nan nan nan nan nan nan nan 24. 31.]\n",
      " [nan nan nan 15. nan  4. nan 28. nan]\n",
      " [22. nan  1. nan  2. nan nan nan nan]\n",
      " [nan nan 10. nan nan nan 18. 26. 19.]\n",
      " [nan nan 20.  7.  5. nan nan  9. 21.]\n",
      " [nan 11. nan nan 13. nan  0. 23. nan]]\n",
      "\n",
      "\n",
      "[[20. nan nan nan nan 35. 22. 41. 13.]\n",
      " [nan  3. nan 12. 21. 36. 32. 37. 14.]\n",
      " [28. nan 17. 27. 10. nan nan nan nan]\n",
      " [nan  1. 11. 33. 42. 47. 26. 40. 39.]\n",
      " [nan 30. nan nan  8.  4. nan 29. nan]\n",
      " [23. 38. nan nan nan 34.  2.  7. 45.]\n",
      " [ 6. 25. 19. nan 16.  9. nan 24. 18.]\n",
      " [nan nan 46. nan nan 31. 43. nan nan]\n",
      " [ 0. nan 44. nan nan  5. nan nan 15.]]\n",
      "\n",
      "\n",
      "[[56. 50. 47. nan  3. 33.  6. 32. 42.]\n",
      " [61.  0. 49. 40. nan 11. 52. 38. 54.]\n",
      " [nan 23.  9. 30. 18. 48. 29. 16. nan]\n",
      " [46.  7. 59. 17. nan 27. 58. 28. 19.]\n",
      " [57. nan 37. 60. nan 22. 51. nan 45.]\n",
      " [55.  4. 43. 21. 15. 31.  8. 26. 44.]\n",
      " [ 1. 53. 36. 20. 62. 35. 14. nan nan]\n",
      " [ 5. 24. 25.  2. nan 39. nan 13. nan]\n",
      " [12. nan 10. nan 41. nan 34. 63. nan]]\n",
      "\n",
      "\n",
      "[[nan nan nan nan nan nan nan  4. nan]\n",
      " [nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan  7. nan nan  9. nan]\n",
      " [ 6. nan nan nan nan nan nan  1. nan]\n",
      " [nan nan nan nan nan nan nan nan nan]\n",
      " [nan  8. nan  3. nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan]\n",
      " [nan  5. nan nan nan nan  0. nan  2.]]\n",
      "\n",
      "torch.Size([1, 32, 3, 3])\n",
      "torch.Size([1, 28, 3, 3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.cuda.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[462], line 199\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39msave_model:\n\u001b[0;32m    195\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist_cnn.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 199\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[462], line 185\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gamma\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     test(model, device, test_loader)\n\u001b[0;32m    187\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[462], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     73\u001b[0m target_multi_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mscatter_(\u001b[38;5;241m1\u001b[39m, target, \u001b[38;5;241m1.\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i_batch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m#print(data.shape) # torch.Size([4, 1, 28, 28])\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m#print(target)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    tensor([[8],\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    [7],\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    [2],\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    [7]])\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[462], line 56\u001b[0m, in \u001b[0;36mDecentNet.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m#self.decent1.update()\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecent2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecent3\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecent1x1\u001b[38;5;241m.\u001b[39mupdate()\n",
      "Cell \u001b[1;32mIn[461], line 118\u001b[0m, in \u001b[0;36mDecentLayer.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_list:\n\u001b[1;32m--> 118\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[458], line 141\u001b[0m, in \u001b[0;36mDecentFilter.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[:,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive,:,:]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# this is totally wrong\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[:,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive,:,:]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1635\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   1634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1635\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1636\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Parameter or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1637\u001b[0m                         \u001b[38;5;241m.\u001b[39mformat(torch\u001b[38;5;241m.\u001b[39mtypename(value), name))\n\u001b[0;32m   1638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(name, value)\n\u001b[0;32m   1639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot assign 'torch.cuda.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "class DecentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecentNet, self).__init__()\n",
    "        \n",
    "        dim = [1, 32, 48, 64, 10]\n",
    "        assert not any(i > 81 for i in dim), \"filters need to be less than 81\"\n",
    "        \n",
    "        self.decent1 = DecentLayer(dim[0], dim[1], kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent2 = DecentLayer(dim[1], dim[2], kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent3 = DecentLayer(dim[2], dim[3], kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent1x1 = DecentLayer(dim[3], dim[-1], kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "\n",
    "        self.fc3 = nn.Linear(dim[-1], dim[-1])\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        self.bias1 = torch.nn.InstanceNorm2d(dim[1])\n",
    "        self.bias2 = torch.nn.InstanceNorm2d(dim[2])\n",
    "        self.bias3 = torch.nn.InstanceNorm2d(dim[3])\n",
    "        self.bias1x1 = torch.nn.InstanceNorm2d(dim[-1])\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x = self.mish1(x)\n",
    "        x = self.bias1(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x = self.mish2(x)\n",
    "        x = self.bias2(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x = self.mish3(x)\n",
    "        x = self.bias3(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        x = self.bias1x1(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def update(self):\n",
    "        #self.decent1.update()\n",
    "        self.decent2.update()\n",
    "        self.decent3.update()\n",
    "        self.decent1x1.update()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for i_batch, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if i_batch == 0:\n",
    "            \n",
    "            model.update()\n",
    "            \n",
    "            #print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            #print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            #print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i_batch * len(data), len(train_loader.dataset),\n",
    "                100. * i_batch / len(train_loader), loss.item()))\n",
    "            \n",
    "            model.update()\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 1\n",
    "        self.save_model = False\n",
    "        \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = DecentNet().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdb954-7eba-4e84-86d9-a9684ededaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.empty((1,81))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "5f7906f9-52a5-4af9-92f5-e3ca3060b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.full(81, np.nan)\n",
    "geometry_array = a.reshape((9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d5d002d4-9e52-46c4-ab0a-e5552a978463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geometry_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe389e-2713-4a65-81d2-6bea9d7bbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d27aa-4fc3-4b43-920d-071d3cc464d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1, 0, 5, 2])\n",
    "labels = labels.unsqueeze(0)\n",
    "\n",
    "target = torch.zeros(labels.size(0), 10).scatter_(1, labels, 1.)\n",
    "print(target)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb570b41-9eec-4d5c-9a5c-f74fabb03cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "16*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b784a059-0ada-423c-a4c1-112548c099de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv2.importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558d1bb-7c6b-4ad1-a04b-e845e812c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv2.importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3c4a0-af85-4589-9fe9-f997d39a0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.conv2.weight.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95870660-6418-4b85-a673-13e266a53960",
   "metadata": {},
   "source": [
    "# conv filter test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "16303538-75ae-4064-ae26-848926552bb7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 1024, 100, 100]) - batch x groups*channels x width x height\n",
      "torch.Size([10, 1024, 3, 3]) - filters x channels x kernel x kernel\n",
      "torch.Size([27, 10, 100, 100]) - batch x filters x width x height\n",
      "\n",
      "**************************************************\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_12532\\2723409993.py:95: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  i_tmp = inputs[:,active,:,:]\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_12532\\2723409993.py:96: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  w_tmp = weights[:,active,:,:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3233394622802734\n",
      "3.112518548965454\n",
      "\n",
      "torch.Size([27, 10, 100, 100]) - batch x filters x width x height\n",
      "\n",
      "\n",
      "torch.Size([27, 1, 100, 100]) - mean accross the filters (no sense here ...)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one filter\n",
    "\n",
    "w_groups = 1 # groups of the channels\n",
    "w_channels = 1024 # input channels\n",
    "w_filters = 10\n",
    "\n",
    "assert w_filters % w_groups == 0\n",
    "assert w_channels % w_groups == 0\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "# w_filters x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "# batch size x w_filters x width x height\n",
    "out = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "print(weights.shape, \"- filters x channels x kernel x kernel\")\n",
    "print(out.shape, \"- batch x filters x width x height\")\n",
    "print()\n",
    "\n",
    "print(\"*\"*50)\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "output_list = []\n",
    "\n",
    "# for each filter, we need different true false vales for our channels\n",
    "active = list(np.random.choice([True, False], size=w_channels, replace=True, p=None))\n",
    "\n",
    "# w_filters x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(1,w_channels,3,3))\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "output_list = []\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    for _ in range(w_filters):\n",
    "        \n",
    "        pass\n",
    "\n",
    "        #print()\n",
    "        #print(active)\n",
    "        #print()\n",
    "        #print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "        #print(weights.shape, \"- filters x channels x kernel x kernel\")\n",
    "\n",
    "\n",
    "        # need to remove weight and input channels according to active list for each filter\n",
    "        #print(inputs.shape)\n",
    "        #print(inputs[:,active,:,:].shape)\n",
    "\n",
    "        #print(weights.shape)\n",
    "        #print(weights[:,active,:,:].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # batch size x w_filters x width x height\n",
    "        \n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            #output_list.append(this_output)\n",
    "        #print(this_output.shape, \"- batch x 1 filter x width x height\")\n",
    "\n",
    "    #out = torch.cat(output_list, dim=1)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    i_tmp = inputs[:,active,:,:]\n",
    "    w_tmp = weights[:,active,:,:]\n",
    "    this_output = F.conv2d(i_tmp, w_tmp, padding=1, groups=w_groups)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    this_output = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print(out.shape, \"- batch x filters x width x height\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# take the mean of all - we can remove all sorts of information from the out tensor\n",
    "mean = torch.mean(out, 1, keepdim=True)\n",
    "print(mean.shape, \"- mean accross the filters (no sense here ...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5c8e3c4e-80a1-448f-9b3c-4ae14cd7bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512048\n",
      "512048\n",
      "4124000\n",
      "18432048\n",
      "18456000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_12532\\3088527220.py:6: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(sys.getsizeof(b.storage())) # 1310776 (bytes)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_12532\\3088527220.py:10: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(sys.getsizeof(a.storage())) # 1310776 (bytes)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_12532\\3088527220.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(sys.getsizeof(weights.storage())) # 1310776 (bytes) 36912\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "#b = torch.empty(w_filters, w_channels, dtype=torch.bool)\n",
    "b = torch.ByteTensor(500, w_channels)\n",
    "print(sys.getsizeof(b.storage())) # 1310776 (bytes)\n",
    "\n",
    "#a = torch.empty(w_filters, w_channels, dtype=torch.uint8)\n",
    "a = torch.ByteTensor(500, w_channels)\n",
    "print(sys.getsizeof(a.storage())) # 1310776 (bytes)\n",
    "\n",
    "active = list(np.random.choice([True, False], size=w_channels, replace=True, p=None))\n",
    "print(sys.getsizeof(active)*500) # 1310776 (bytes)\n",
    "\n",
    "weights = torch.FloatTensor(torch.randn(500,w_channels,3,3))\n",
    "print(sys.getsizeof(weights.storage())) # 1310776 (bytes) 36912\n",
    "print(36912*500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6be2b255-ccc3-4809-948c-cd4dc5efe661",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.7220e+00,  2.4680e-01,  2.2001e-01],\n",
       "          [-6.9529e-01,  1.6975e-01,  2.1292e-01],\n",
       "          [-2.6754e-01,  1.1633e-01, -7.4321e-01]],\n",
       "\n",
       "         [[-4.5518e-02, -5.4037e-01, -1.8785e+00],\n",
       "          [-3.8001e-01,  1.4221e-01, -8.9661e-01],\n",
       "          [-3.3265e-01, -1.2735e+00,  7.6793e-01]],\n",
       "\n",
       "         [[ 1.1746e+00,  6.9472e-02, -8.5933e-01],\n",
       "          [ 1.0164e+00, -9.4645e-02, -3.7711e-01],\n",
       "          [-1.0853e+00, -2.2829e-01, -1.9036e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-7.2700e-01,  3.5489e-01,  1.6242e+00],\n",
       "          [ 1.0532e+00,  1.3561e+00, -4.8595e-01],\n",
       "          [-4.2580e-01,  2.4004e-02,  1.0566e-01]],\n",
       "\n",
       "         [[ 1.2261e+00,  8.0269e-01, -2.5002e-01],\n",
       "          [ 1.1333e+00, -1.9306e+00, -2.2312e-01],\n",
       "          [ 4.0879e-01, -4.0783e-01, -9.5857e-02]],\n",
       "\n",
       "         [[-6.7657e-01,  1.0683e+00,  1.4584e-01],\n",
       "          [ 7.4717e-01, -1.7725e+00,  1.1041e+00],\n",
       "          [-9.6799e-01, -4.8334e-01,  2.5417e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.0673e+00,  3.3373e+00,  1.1454e+00],\n",
       "          [ 6.7643e-01,  1.5134e+00,  1.9555e+00],\n",
       "          [ 1.0660e+00, -7.1219e-02,  1.7201e+00]],\n",
       "\n",
       "         [[-8.3795e-01,  1.6864e-01, -2.5395e+00],\n",
       "          [-1.0414e+00,  3.3468e-02, -5.5463e-01],\n",
       "          [-4.3015e-01, -6.3635e-01,  1.7467e+00]],\n",
       "\n",
       "         [[-7.6298e-01, -3.3919e-01,  1.2347e+00],\n",
       "          [ 3.0799e-01,  6.2220e-01,  9.4813e-01],\n",
       "          [ 6.6683e-01, -1.5959e-01, -1.4640e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.1861e-01, -9.9787e-01,  9.4744e-01],\n",
       "          [ 1.6939e+00,  5.0657e-02,  5.4973e-01],\n",
       "          [-1.6484e+00, -1.8335e+00,  2.0065e-01]],\n",
       "\n",
       "         [[ 2.1159e+00, -1.1687e+00, -1.8742e-01],\n",
       "          [-3.3960e-02,  1.0808e+00, -1.4913e-01],\n",
       "          [-9.5957e-01, -4.4972e-01, -2.0790e+00]],\n",
       "\n",
       "         [[ 3.6977e-01, -2.6145e-01, -1.7423e+00],\n",
       "          [-1.3005e+00,  1.0368e+00,  2.1107e-01],\n",
       "          [ 1.8338e-02, -3.6635e-01,  7.5164e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3342e+00,  6.2754e-01, -6.4290e-01],\n",
       "          [ 6.2619e-01,  9.3312e-02,  8.1278e-01],\n",
       "          [-1.0381e+00,  1.2094e+00,  1.9919e-02]],\n",
       "\n",
       "         [[-2.8035e-01,  2.0432e-01, -9.4282e-02],\n",
       "          [ 2.5832e-01,  1.4713e+00, -5.9925e-01],\n",
       "          [ 2.7526e-01, -2.1318e+00, -1.3991e+00]],\n",
       "\n",
       "         [[ 9.1852e-01,  2.1222e-01,  1.8222e-01],\n",
       "          [ 2.2379e-01, -2.3949e-01,  3.2264e-01],\n",
       "          [ 2.0265e+00,  7.4183e-01,  7.1837e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.2309e-01,  7.8887e-01,  9.4822e-01],\n",
       "          [ 1.8670e+00,  8.7108e-01,  3.4254e-01],\n",
       "          [-5.3573e-01, -9.7246e-02,  2.3363e-01]],\n",
       "\n",
       "         [[ 1.2708e+00,  1.2740e+00,  5.8701e-01],\n",
       "          [ 9.4788e-01, -6.8204e-02, -1.2326e+00],\n",
       "          [-7.4285e-01, -6.3727e-01,  5.7913e-01]],\n",
       "\n",
       "         [[ 5.5181e-01,  1.1307e-01,  1.6435e-01],\n",
       "          [ 2.3357e-01,  7.1940e-01,  1.8174e+00],\n",
       "          [ 1.7438e+00, -1.5644e+00,  1.3195e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 9.5626e-02,  8.2825e-01,  3.6897e-01],\n",
       "          [-5.1242e-01, -1.4587e+00, -1.2194e+00],\n",
       "          [ 1.2834e+00, -4.1432e-01, -2.2410e-03]],\n",
       "\n",
       "         [[-8.6830e-01, -1.0462e+00,  7.3659e-02],\n",
       "          [ 5.5260e-01, -1.1068e+00, -1.1426e+00],\n",
       "          [ 1.2807e+00, -5.1247e-02, -1.3900e+00]],\n",
       "\n",
       "         [[ 6.2773e-01,  7.4166e-01, -6.1299e-01],\n",
       "          [ 5.3500e-01, -4.1268e-01, -7.1774e-02],\n",
       "          [ 3.6685e-01, -8.1994e-01, -9.9742e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.1486e+00,  1.7217e+00, -1.3363e+00],\n",
       "          [ 3.3184e-01,  2.7209e-01, -5.4518e-01],\n",
       "          [-3.1084e-01, -2.0497e-02,  9.5508e-03]],\n",
       "\n",
       "         [[ 1.3306e+00,  1.6327e+00,  1.2351e+00],\n",
       "          [-2.2258e+00, -7.4649e-01,  1.9075e-01],\n",
       "          [-1.1409e+00, -1.7565e+00, -7.9040e-01]],\n",
       "\n",
       "         [[ 3.1130e-01, -5.4247e-01, -1.0884e+00],\n",
       "          [ 5.5876e-01,  1.8931e-01,  5.4586e-01],\n",
       "          [-3.8483e-01, -3.3955e-01, -1.3894e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5566e+00,  9.1028e-01,  4.8520e-01],\n",
       "          [ 7.1432e-01,  1.9480e+00, -1.0675e+00],\n",
       "          [ 5.6340e-01,  4.9427e-02,  1.1426e+00]],\n",
       "\n",
       "         [[ 2.2767e-01,  8.5497e-01,  1.0345e-01],\n",
       "          [-3.0228e-01,  8.0224e-01, -7.9375e-01],\n",
       "          [ 1.2223e+00, -3.5669e-01, -1.2960e+00]],\n",
       "\n",
       "         [[-1.1479e-02,  2.1767e+00, -6.8921e-01],\n",
       "          [-8.0538e-01, -1.1284e+00,  1.7049e+00],\n",
       "          [-3.9532e-01, -8.5415e-01,  4.6304e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.7852e-01, -1.6903e+00,  1.7430e-01],\n",
       "          [-3.0661e-01, -1.5748e+00, -5.9665e-01],\n",
       "          [ 1.6325e+00,  8.7201e-01, -6.6618e-01]],\n",
       "\n",
       "         [[-3.6712e-01,  1.1718e+00,  1.3426e+00],\n",
       "          [-6.3153e-01,  9.7062e-01, -1.2827e+00],\n",
       "          [-1.4627e+00, -1.3201e+00, -4.1026e-01]],\n",
       "\n",
       "         [[-1.4612e+00,  1.9325e+00, -2.5974e-01],\n",
       "          [ 5.0512e-01, -8.7735e-01, -5.8564e-01],\n",
       "          [ 6.1252e-01, -4.6317e-01, -7.7308e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.2432e-01,  2.0138e+00, -4.6069e-01],\n",
       "          [-8.3733e-01, -2.1720e-01,  1.1598e+00],\n",
       "          [-1.1645e+00,  5.8180e-01, -1.0890e-01]],\n",
       "\n",
       "         [[ 8.5531e-03, -2.0913e+00, -2.3541e+00],\n",
       "          [ 6.9381e-01, -4.2755e-01, -1.2571e+00],\n",
       "          [ 5.6116e-01,  6.0871e-01,  3.1661e-01]],\n",
       "\n",
       "         [[ 6.6029e-01, -1.8150e-01, -1.6708e+00],\n",
       "          [ 1.2241e+00,  2.3797e+00, -1.5333e-01],\n",
       "          [-5.9962e-01, -6.4145e-01,  6.7257e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.6842e-01,  1.0129e+00,  5.2581e-01],\n",
       "          [ 9.0343e-01,  3.2258e-01,  6.9417e-02],\n",
       "          [-3.3018e-01,  9.9368e-01, -1.4888e+00]],\n",
       "\n",
       "         [[ 4.1782e-01, -1.3226e+00, -1.4613e+00],\n",
       "          [-9.0830e-01,  1.0610e+00, -1.0586e+00],\n",
       "          [-1.1612e+00, -1.2972e-01,  5.1630e-01]],\n",
       "\n",
       "         [[-1.0839e+00,  1.3777e+00, -1.2767e+00],\n",
       "          [ 5.0376e-01, -2.7020e-01, -1.2101e+00],\n",
       "          [ 6.7514e-01,  2.3962e-01, -9.2157e-01]]]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1dadee83-7300-4844-8284-8d0ef29aaee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, True, True, True, False, False, True, True, False]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(2, size=10)\n",
    "\n",
    "list(np.random.choice([True, False], size=10, replace=True, p=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e1c72946-baf5-4c90-b84e-14f14c8d949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80, 33, 28, 27,  7,  7, 35, 33, 48, 74, 56,  7, 18,  8, 65, 31,  6,\n",
       "       25, 50, 55, 61, 29, 52, 58, 22, 25, 29, 58, 15,  9, 39, 52, 52, 79,\n",
       "       73, 30, 50, 67, 72, 71, 15, 50, 28, 26, 15,  7, 38, 33, 79,  5, 42,\n",
       "       25, 42, 68, 49, 74, 44,  5, 41, 44, 16, 25, 59, 48, 11, 68, 27, 25,\n",
       "       44, 67,  8, 13,  9, 66, 80,  5, 73, 71, 47, 76, 53, 43, 29, 36, 36,\n",
       "       43, 61, 60, 27, 27, 30,  1, 16, 58, 42, 29, 53, 31, 67, 55, 63, 38,\n",
       "       66, 28, 41, 66, 76, 36, 61, 65,  8, 69, 33, 81, 12,  2, 44, 49, 52,\n",
       "       43, 57, 77, 63, 33, 55, 44, 41, 61,  3,  7, 34, 57, 20, 48, 18, 50,\n",
       "       67, 52, 79, 25, 51, 27, 46, 39, 55, 74, 60, 37, 55, 40, 52, 49, 66,\n",
       "       10, 58, 55, 21, 80,  4,  5, 17, 56, 31, 30, 72, 16, 16, 34, 11, 77,\n",
       "       66,  9, 37, 24, 62,  1, 21, 74,  5, 16, 66, 59, 30, 39, 68, 35, 77,\n",
       "       54, 14, 74, 56, 68, 27, 38, 24, 31, 11, 43, 54,  7])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(1, 82, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "cc2754cb-b071-4267-b53c-5f2190a7baa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "95feb489-36f2-4d83-81e3-5e21a530f3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 3, 5, 0, 8, 6, 8, 2]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(a=4, size=2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "24e5d97e-7408-4c52-8891-864827810c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 189, 100, 100]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 7, 3, 3]) - filter 1 x channels x kernel x kernel\n",
      "Given groups=27, expected weight to be at least 27 at dimension 0, but got weight of size [1, 7, 3, 3] instead\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one filter\n",
    "\n",
    "w_groups = 27 # groups of the channels\n",
    "w_channels = 7 # input channels\n",
    "w_filters = 200\n",
    "batch_size = 27\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(batch_size, w_channels*w_groups, 100,100))\n",
    "#inputs = torch.autograd.Variable(torch.randn(w_channels*w_groups, 100,100))\n",
    "\n",
    "# w_groups x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(1, w_channels,3,3))\n",
    "#weights = torch.autograd.Variable(torch.randn(w_groups*w_channels,3,3))\n",
    "\n",
    "print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "print(weights.shape, \"- filter 1 x channels x kernel x kernel\")\n",
    "\n",
    "try:\n",
    "    o_list = []\n",
    "    for _ in range(w_filters):\n",
    "        # batch size x groups x width x height\n",
    "        out = F.conv2d(inputs, weights, groups=w_groups)\n",
    "        o_list.append(out)\n",
    "        # take the mean of all - we can remove all sorts of information from the out tensor\n",
    "        #mean = torch.mean(out, 1, keepdim=True)\n",
    "    \n",
    "    print(torch.cat(o_list, dim=1).shape, \"- batch x filters x width x height\")\n",
    "    #print(mean.shape, \"- mean accross the groups\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3d443-372f-4548-804e-11efbebe6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = torch.randn(8, 4, 3, 3)\n",
    "inputs = torch.randn(1, 4, 5, 5)\n",
    "F.conv2d(inputs, filters, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bf7b8-35d2-4f68-b045-0ba93bd4c5fc",
   "metadata": {},
   "source": [
    "# Visualise filters and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536067b-6098-4844-9ef4-7a5db5a23156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net() # .to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320d57b-3e22-404f-ad6c-b04515fe2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "def visChannels(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    \n",
    "    plt.figure(figsize=(nrow,rows) )\n",
    "    plt.title(f\"Channels with index {ch}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "def visFilters(tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.title(f\"Filter {filt}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "def visFilters_subplot(subplot, tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    subplot.set_title(f\"Filter {filt+1} with {c} channels\")\n",
    "    subplot.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "    subplot.axis('off')\n",
    "    \n",
    "layer = 1\n",
    "filter = model.conv2.weight.data.clone()\n",
    "\n",
    "print(model.conv2.weight.shape)\n",
    "\n",
    "# need to match the network parameters!!!!\n",
    "in_channels = 5\n",
    "out_filters = 3 # 64\n",
    "\n",
    "\n",
    "fig, subplot = plt.subplots(out_filters, figsize=(10, 10))\n",
    "fig.suptitle(f'Layer with shape {list(model.conv2.weight.shape)} [out, in, kernel, kernel]')\n",
    "\n",
    "for filt in range(0, out_filters):\n",
    "    \n",
    "    visFilters_subplot(subplot[filt], filter, filt=filt, allkernels=False)\n",
    "\n",
    "    #plt.axis('off')\n",
    "    #plt.ioff()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"example_results/filter_with_weights.png\")\n",
    "plt.show()\n",
    "    \n",
    "if False:    \n",
    "    for filt in range(0, out_filters):\n",
    "\n",
    "        visFilters(filter, filt=filt, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"examples/example_results/filter_with_weights.png\")\n",
    "        plt.show()\n",
    "\n",
    "    for ch in range(0, in_channels):\n",
    "\n",
    "        visChannels(filter, ch=ch, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791fa94-5bb8-4d3a-ad97-72cc5a59025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "res = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655d64a-7fc0-42c7-880e-69e7be6acedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.layer1[0].conv1.bias == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3322395-f2a0-40e1-a0e8-86c090586f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.layer1[0].conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d52543-cd67-4d00-963a-5d1effbfc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.extra_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77091394-6e75-4aa3-84c3-a9ebc7ce932e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
