{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DecentNet from conv layer\n",
    "\n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "     \n",
    "* pruning actually doesn\"t work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46441962-88b7-4cd6-bd4a-a71a6fbbd427",
   "metadata": {},
   "source": [
    "pip install:\n",
    "    pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../helper', './helper', '/helper', 'helper', 'C:\\\\Users\\\\Christina\\\\Documents\\\\datasceyence\\\\examples', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\python39.zip', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\DLLs', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy', '', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\pixelssl-0.1.4-py3.9.egg', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "# from torch.nn import init\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from typing import Optional, List, Tuple, Union\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "sys.path.insert(0, \"/helper\")\n",
    "sys.path.insert(0, \"./helper\")\n",
    "sys.path.insert(0, \"../helper\")\n",
    "print(sys.path)\n",
    "\n",
    "# own module\n",
    "from visualisation.feature_map import *\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7f1ab-1129-436d-aed0-5643651c84a0",
   "metadata": {},
   "source": [
    "# Conv experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38836e4-6905-4e0c-b344-bcc3b8094388",
   "metadata": {},
   "source": [
    "## conv2d layer (slightly adapted original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972ff66-723c-43a1-a9d9-80c43b450efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConvNd(torch.nn.Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    in_channels: int\n",
    "    _reversed_padding_repeated_twice: List[int]\n",
    "    out_channels: int\n",
    "    kernel_size: Tuple[int, ...]\n",
    "    stride: Tuple[int, ...]\n",
    "    padding: Union[str, Tuple[int, ...]]\n",
    "    dilation: Tuple[int, ...]\n",
    "    transposed: bool\n",
    "    output_padding: Tuple[int, ...]\n",
    "    groups: int\n",
    "    padding_mode: str\n",
    "    weight: Tensor\n",
    "    bias: Optional[Tensor]\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Tuple[int, ...],\n",
    "                 stride: Tuple[int, ...],\n",
    "                 padding: Tuple[int, ...],\n",
    "                 dilation: Tuple[int, ...],\n",
    "                 transposed: bool,\n",
    "                 output_padding: Tuple[int, ...],\n",
    "                 groups: int,\n",
    "                 bias: bool,\n",
    "                 padding_mode: str,\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        print(factory_kwargs)\n",
    "        super().__init__()\n",
    "        if groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            #self.importance = Parameter(torch.empty(\n",
    "            #    (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            #self.importance = Parameter(torch.empty(\n",
    "            #    (out_channels, in_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "class CustomConv2d(_ConvNd):\n",
    "    \n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = stride #_pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super().__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        print(self.stride)\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369256cb-1ea4-4ff2-8ce5-a820511e0779",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823e218-5eb3-44d9-a277-2cf17b772c84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CustomConv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv2 = CustomConv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv3 = CustomConv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv1x1 = CustomConv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "        self.K = 100 \n",
    "        self.L = 10 # last one\n",
    "        self.num_of_bases = 1 # 3rd dim\n",
    "        \n",
    "        if False:\n",
    "            self.conv1 = Conv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv2 = Conv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv3 = Conv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv1x1 = Conv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "        # 4x16384\n",
    "        # self.fc1 = nn.Linear(10*10*10, 10)\n",
    "        #self.fc2 = nn.Linear(10, 10)\n",
    "        \n",
    "        #self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        #self.sub_concept_pooling = nn.modules.MaxPool2d((self.K, 1), stride=(1,1))\n",
    "        #self.instance_pooling = nn.modules.MaxPool2d((opt.num_of_bases, 1), stride=(1,1))\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.mish1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.mish2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.mish3(x)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        #print(x.size())\n",
    "        #print(x.size()[2:])\n",
    "        \n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        #x = x.view(-1, self.L, self.K, 10)\n",
    "        \n",
    "        # input, kernel_size, stride, padding, dilation, ceil_mode\n",
    "        #x = self.sub_concept_pooling(x).view(-1, self.L, self.num_of_bases).permute(0,2,1).unsqueeze(1)\n",
    "        \n",
    "        # output = F.sigmoid(x)\n",
    "        # x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        #x = torch.flatten(x, 1)\n",
    "        # x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.fc2(x)\n",
    "        #output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76fa05-b47e-4264-85cf-766d8ca060d3",
   "metadata": {},
   "source": [
    "## normal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819306e-ff93-4f38-898b-4c1cd2221b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for i_batch, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if i_batch == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i_batch * len(data), len(train_loader.dataset),\n",
    "                100. * i_batch / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            \"\"\"\n",
    "            if i == 0 and epoch % args.log_interval == 0:\n",
    "            # if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot(path=f\"example_results/feature_map_{epoch}.png\")\n",
    "                \"\"\"\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.1\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 5\n",
    "        self.save_model = True\n",
    "        \n",
    "\n",
    "def main_train():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader, epoch)\n",
    "        #scheduler.step()\n",
    "        \n",
    "        \n",
    "        if args.save_model and epoch % args.log_interval == 0:\n",
    "            torch.save(model.state_dict(), f\"example_results/mnist_cnn_{epoch}.ckpt\")\n",
    "\n",
    "\n",
    "def main_test():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "\n",
    "    if True:\n",
    "        model.load_state_dict(torch.load(\"example_results/mnist_cnn_5.ckpt\"))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(\"example_results/pruned_model.ckpt\"))\n",
    "    \n",
    "\n",
    "    # model = torch.load(model.state_dict(), \"example_results/mnist_cnn_30.ckpt\")\n",
    "    if False:\n",
    "        test(args, model, device, test_loader, 0)\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb5e24-3f20-4788-8344-66a98d6791bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1242e1a-c691-4cb6-a279-9904cedcd806",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_to_prune= main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270319e6-3827-47cd-a579-476c89217968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(model_to_prune.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {},
   "source": [
    "# DecentNet trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db53f551-b504-444f-b318-762eb857195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[1, 0, 5, 2]])\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor([1, 0, 5, 2])\n",
    "labels = labels.unsqueeze(0)\n",
    "\n",
    "target = torch.zeros(labels.size(0), 10).scatter_(1, labels, 1.)\n",
    "print(target)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a387d2d9-2247-4330-9fb7-c44aa0a2322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common pair at indices [1, 0]: (2, 2), (4, 4)\n",
      "Common pair at indices [2, 5]: (4, 4), (8, 8)\n",
      "[1 2]\n",
      "[0 5]\n"
     ]
    }
   ],
   "source": [
    "# Two lists of data\n",
    "ms_in = [1, 2, 4]\n",
    "ns_in = [2, 4, 8]\n",
    "\n",
    "ms = [2, 2, 2, 5, 9, 4]\n",
    "ns = [4, 2, 2, 3, 6, 8]\n",
    "\n",
    "# Find the indices (IDs) of pairs that exist in both lists\n",
    "common_pairs = [[f, x] for f, (item1, item2) in enumerate(zip(ms_in, ns_in)) for x, (item3, item4) in enumerate(zip(ms, ns)) if (item1==item3 and item2==item4)]\n",
    "\n",
    "# Print the common pairs\n",
    "for pair in common_pairs:\n",
    "    print(f\"Common pair at indices {pair}: {ms_in[pair[0]], ms[pair[1]]}, {ns_in[pair[0]], ns[pair[1]]}\")\n",
    "    \n",
    "a = np.array(common_pairs)\n",
    "f_ids = a[:,0]\n",
    "x_ids = a[:,1]\n",
    "\n",
    "print(f_ids)\n",
    "print(x_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    def __init__(self, value, ms, ns):\n",
    "        \n",
    "        self.ms = ms # list of integers\n",
    "        self.ns = ns # list of integers\n",
    "        self.channels = value # list of tensors\n",
    "                \n",
    "    def set(self, value, ms, ns):\n",
    "        self.ms = ms\n",
    "        self.ns = ns\n",
    "        self.channels = value\n",
    "    \n",
    "    def get(self):\n",
    "        return self.channels, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        # amout of channels need to have same length as m and n lists\n",
    "        return 'X(channels: ' + str(self.channels.shape) +' at positions: ms= ' + ', '.join(str(m.item()) for m in self.ms) + ', ns= ' + ', '.join(str(n.item()) for n in self.ns) + ')'\n",
    "    \n",
    "    \n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfa123-54e1-4053-94c3-52b45ec0859c",
   "metadata": {},
   "source": [
    "## DecentFilter\n",
    "* conv2d problem: https://stackoverflow.com/questions/61269421/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # convolution happens in here\n",
    "    \n",
    "    def __init__(self, ms_in, ns_in, m_out, n_out, n_channels=1,\n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 padding_mode=\"zeros\",\n",
    "                 dilation=3, \n",
    "                 transposed=None, \n",
    "                 device=None, \n",
    "                 dtype=None):\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "            \n",
    "        # conv \n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = stride\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        \n",
    "        \n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "            \n",
    "        self.n_channels = n_channels\n",
    "        # self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.padding_mode = padding_mode\n",
    "        \n",
    "        # position\n",
    "        self.ms_in = ms_in # list\n",
    "        self.ns_in = ns_in # list\n",
    "        self.m_out = m_out # single integer\n",
    "        self.n_out = n_out # single integer\n",
    "        \n",
    "        # weight\n",
    "        # filters x channels x kernel x kernel\n",
    "        #self.weights = torch.autograd.Variable(torch.randn(1,n_channels,*kernel_size)).to(\"cuda\")\n",
    "        # self.weights = torch.nn.Parameter(torch.randn(1,n_channels,*kernel_size))\n",
    "        self.weights = torch.nn.Parameter(torch.empty((1, n_channels, *kernel_size), **factory_kwargs))\n",
    "            \n",
    "        # bias    \n",
    "        if False: \n",
    "            # bias:\n",
    "            # where should the bias be???\n",
    "            self.bias = Parameter(torch.empty(1, **factory_kwargs))\n",
    "        else:\n",
    "            #self.bias = False\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # reset weights and bias in filter\n",
    "        self.reset_parameters()\n",
    "            \n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        torch.nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))        \n",
    "        \n",
    "        \n",
    "    def forward(self, x:X) -> Tensor:\n",
    "        \n",
    "        # weights = 1 filter x channels x kernel x kernel\n",
    "        # x = batch x channels x width x height\n",
    "        \n",
    "        # filter channels and weights based on the ms and ns (input)\n",
    "        tmp_ms = x.ms\n",
    "        tmp_ns = x.ns\n",
    "        \n",
    "        tmp_ms_in = self.ms_in\n",
    "        tmp_ns_in = self.ns_in\n",
    "        \n",
    "        # Two lists of data\n",
    "        #ms_in = [1, 2, 3, 4, 5]\n",
    "        #ns_in = [2, 4, 4, 8, 3]\n",
    "\n",
    "        #ms = [2, 2, 2, 5, 4]\n",
    "        #ns = [4, 2, 2, 3, 8]\n",
    "\n",
    "        # Find the indices (IDs) of pairs that exist in both lists\n",
    "        common_pairs = [[f, x] for f, (item1, item2) in enumerate(zip(tmp_ms_in, tmp_ns_in)) for x, (item3, item4) in enumerate(zip(tmp_ms, tmp_ns)) if (item1==item3 and item2==item4)]\n",
    "        \n",
    "        if False:\n",
    "            print(common_pairs)\n",
    "            print(len(self.ms_in))\n",
    "            print(len(self.ns_in))\n",
    "            print(len(x.ms))\n",
    "            print(len(x.ns))\n",
    "\n",
    "            for pair in common_pairs:\n",
    "                print(f\"Common pair at indices {pair}: {tmp_ms_in[pair[0]], tmp_ms[pair[1]]}, {tmp_ns_in[pair[0]], tmp_ns[pair[1]]}\")\n",
    "        \n",
    "        a = np.array(common_pairs)\n",
    "        f_ids = a[:,0]\n",
    "        x_ids = a[:,1]\n",
    "        \n",
    "        tmp_x = x.channels[:, x_ids, :, :]\n",
    "        tmp_f = self.weights[:, f_ids, :, :]\n",
    "        \n",
    "        if self.padding_mode != 'zeros':\n",
    "            x_channels = torch.nn.functional.conv2d(F.pad(tmp_x, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            tmp_f, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, groups=1)\n",
    "        else:\n",
    "            # this is written in c++\n",
    "            x_channels = torch.nn.functional.conv2d(tmp_x, tmp_f, self.bias, self.stride, self.padding, self.dilation, groups=1)\n",
    "        \n",
    "        # print(x_channels.shape, \"- batch x filters x width x height\")        \n",
    "        return x_channels\n",
    "            \n",
    "    \n",
    "    def update(self):\n",
    "        # channel deactivation\n",
    "        # require_grad = False/True for each channel\n",
    "        \n",
    "        #deactivate_ids = [1, 2, 6]\n",
    "        \n",
    "        #self.active[deactivate_ids] = False\n",
    "        \n",
    "        print(\"updating here\")\n",
    "        \n",
    "        remove_ids = random.sample(range(0, 8), 5) # this is actually the keep ids rn\n",
    "        self.weights = torch.nn.Parameter(self.weights[:, remove_ids, :, :])\n",
    "        # torch.nn.Parameter(torch.empty((1, n_channels, *kernel_size), **factory_kwargs))\n",
    "        \n",
    "        self.ms_in = [self.ms_in[i] for i in remove_ids] # self.ms_in[remove_ids]\n",
    "        self.ns_in = [self.ns_in[i] for i in remove_ids] # self.ns_in[remove_ids]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"weight\")\n",
    "        #print(self.weight.shape)\n",
    "        \n",
    "        #print(self.weight[:,self.active,:,:].shape)\n",
    "        \n",
    "        # this is totally wrong - iterative will break after first iteration\n",
    "        \n",
    "        #print()\n",
    "        \n",
    "        # Good to hear it’s working, although I would think you’ll get an error at some point in your code, as the cuda() call creates a non-leaf tensor.\n",
    "        #self.weight = torch.nn.Parameter(  self.weight[:,self.active,:,:] ) # .detach().cpu().numpy()\n",
    "        #self.weight = self.weight.cuda()\n",
    "        \n",
    "        \n",
    "        #print(self.weight.shape)\n",
    "        #print(self.active)\n",
    "    \n",
    "    def set_position_and_value(self, value, m_out, n_out):\n",
    "        self.weights = value # weights in this filter\n",
    "        self.m_out = m_out # single integer\n",
    "        self.n_out = n_out # single integer\n",
    "        \n",
    "    \n",
    "    def get_position_and_value(self):\n",
    "        return self.weights, self.m_out, self.n_out\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'DecentFilter(weights: ' + str(self.weights.shape) + ' at position: m_out=' + str(self.m_out) + ', n_out=' + str(self.n_out) + ')' + \\\n",
    "    '\\n with inputs: ms_in= ' + ', '.join(str(m.item()) for m in self.ms_in) + ', ns_in= ' + ', '.join(str(n.item()) for n in self.ns_in) + ')'\n",
    "    __repr__ = __str__\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdab4-e0b0-4523-882f-dad3ebf06090",
   "metadata": {},
   "source": [
    "## DecentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddb74c7-5940-424d-aab8-2c75efd914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLayer(torch.nn.Module):\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'n_channels', #  'output_padding',\n",
    "                     'n_filters', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "        \n",
    "        \n",
    "    def __init__(self, ms_in:list, ns_in:list, n_channels:int, n_filters:int,\n",
    "                 kernel_size: _size_2_t,   # not in use\n",
    "                 stride: _size_2_t = 1,  # not in use\n",
    "                 padding: Union[str, _size_2_t] = 0,  # not in use\n",
    "                 dilation: _size_2_t = 1,  # not in use\n",
    "                 transposed: bool = False,  # not in use\n",
    "                 #output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 #groups: int = 1,\n",
    "                 bias: bool = True,  # not in use\n",
    "                 padding_mode: str = \"zeros\",  # not in use\n",
    "                 device=None,  # not in use\n",
    "                 dtype=None) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # position\n",
    "        self.ms_in = ms_in\n",
    "        self.ns_in = ns_in\n",
    "\n",
    "        # use techniques from coo matrix\n",
    "        self.geometry_array = np.full(81, np.nan)\n",
    "        # plus 1 here cause of to_sparse array\n",
    "        self.geometry_array[0:n_filters] = range(1,n_filters+1)\n",
    "        np.random.shuffle(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.reshape((9,9), order='C')\n",
    "        self.geometry_array = torch.tensor(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.to_sparse(sparse_dim=2).to(\"cuda\")\n",
    "\n",
    "        print(self.geometry_array)\n",
    "        print(self.geometry_array.values())\n",
    "\n",
    "        self.filter_list = torch.nn.ModuleList([])\n",
    "        for i_filter in range(n_filters):\n",
    "            # minus 1 here cause of to_sparse array\n",
    "            index = (self.geometry_array.values()-1 == i_filter).nonzero(as_tuple=True)[0]\n",
    "            m_out = self.geometry_array.indices()[0][index]\n",
    "            n_out = self.geometry_array.indices()[1][index]\n",
    "            f = DecentFilter(ms_in, ns_in, m_out, n_out, n_channels)\n",
    "            self.filter_list.append(f)\n",
    "            # self.register_parameter(f\"filter {i_filter}\", f.weights)\n",
    "            \n",
    "            #torch.nn.Parameter(torch.empty((1, n_channels, *kernel_size), **factory_kwargs))\n",
    "        \n",
    "    def sparcify(self) -> None:\n",
    "        # pruning based on a metric\n",
    "        \n",
    "        # delete layer with id\n",
    "        # delete channels in each layer with id\n",
    "        \n",
    "        # change positions\n",
    "        # change\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "            \n",
    "    def update(self):\n",
    "        for f in self.filter_list:\n",
    "            f.update()\n",
    "        \n",
    "    def forward(self, x: X) -> Tensor:\n",
    "        \n",
    "        # calculate output for each filter\n",
    "        output_list = []\n",
    "        m_list = []\n",
    "        n_list = []\n",
    "        for f in self.filter_list:\n",
    "            # output = filter(input)\n",
    "            output_list.append(f(x))\n",
    "            m_list.append(f.m_out)\n",
    "            n_list.append(f.n_out)\n",
    "        x.ms = m_list\n",
    "        x.ns = n_list\n",
    "        x.channels = torch.cat(output_list, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def get_filter_positions(self):\n",
    "        \n",
    "        ms_out = []\n",
    "        ns_out = []\n",
    "        for f in self.filter_list:\n",
    "            ms_out.append(f.m_out)\n",
    "            ns_out.append(f.n_out)\n",
    "        \n",
    "        return ms_out, ns_out\n",
    "             \n",
    "\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c186cf1-c7db-45da-b779-f7ab88f3896f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentNet(nn.Module):\n",
    "    def __init__(self, dim=[1, 8, 16, 24], n_classes=10) -> None:\n",
    "        super(DecentNet, self).__init__()\n",
    "        \n",
    "        dim.append(n_classes)\n",
    "        #dim = [1, 32, 48, 64, 10]        \n",
    "        assert not any(i > 81 for i in dim), \"filters need to be less than 81\"\n",
    "        \n",
    "        # backbone\n",
    "        m0 = [torch.tensor(0)]\n",
    "        n0 = [torch.tensor(0)]\n",
    "        self.decent1 = DecentLayer(m0, n0, dim[0], dim[1], kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        m1,n1 = self.decent1.get_filter_positions()\n",
    "        self.decent2 = DecentLayer(m1, n1, dim[1], dim[2], kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        m2,n2 = self.decent2.get_filter_positions()\n",
    "        self.decent3 = DecentLayer(m2, n2, dim[2], dim[3], kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        m3,n3 = self.decent3.get_filter_positions()\n",
    "        self.decent1x1 = DecentLayer(m3, n3, dim[3], dim[-1], kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "        # head\n",
    "        self.fc = torch.nn.Linear(dim[-1], dim[-1])\n",
    "    \n",
    "        # activation\n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        # bias\n",
    "        self.bias1 = torch.nn.InstanceNorm2d(dim[1])\n",
    "        self.bias2 = torch.nn.InstanceNorm2d(dim[2])\n",
    "        self.bias3 = torch.nn.InstanceNorm2d(dim[3])\n",
    "        self.bias1x1 = torch.nn.InstanceNorm2d(dim[-1])\n",
    "        \n",
    "        # activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.channels = self.mish1(x.channels)\n",
    "        x.channels = self.bias1(x.channels)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x.channels = self.mish2(x.channels)\n",
    "        x.channels = self.bias2(x.channels)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.channels = self.mish3(x.channels)\n",
    "        x.channels = self.bias3(x.channels)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.channels = self.mish1x1(x.channels)\n",
    "        x.channels = self.bias1x1(x.channels)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        # global max pooling for MIL\n",
    "        x.channels = F.max_pool2d(x.channels, kernel_size=x.channels.size()[2:])\n",
    "        \n",
    "        x.channels = x.channels.reshape(x.channels.size(0), -1)\n",
    "        x.channels = self.fc(x.channels) \n",
    "        \n",
    "        x.channels = self.sigmoid(x.channels)\n",
    "        \n",
    "        return x.channels\n",
    "    \n",
    "    def update(self):\n",
    "        #self.decent1.update()\n",
    "        self.decent2.update()\n",
    "        self.decent3.update()\n",
    "        self.decent1x1.update()\n",
    "        \n",
    "        # measurement for updating\n",
    "        \n",
    "        # update layer by layer\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f2c55a-0f86-423d-97fb-aa1502f4fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecentNet(n_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3988da38-6bbe-4665-b6a1-dd682020435f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X(channels: torch.Size([5, 2, 30, 30]) at positions: ms= 5, 7, ns= 3, 12)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.autograd.Variable(torch.randn(5, 2, 30, 30)) # batch x channel x width x height\n",
    "# dense_input.shape\n",
    "\n",
    "# todo: ms need to have same size as channel\n",
    "\n",
    "X(tmp, [torch.tensor(5), torch.tensor(7)], [torch.tensor(3), torch.tensor(12)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "### Lightning version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "import os\n",
    "import torchmetrics\n",
    "\n",
    "class DecentLightning(pl.LightningModule):\n",
    "    def __init__(self, kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.model = DecentNet(n_classes=self.n_classes).to(\"cuda\")\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.lr = model_kwargs[\"lr\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        \n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "        self.f1score = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # 1       \n",
    "        loss = self._loss_n_metrics(batch, mode=\"train\")\n",
    "        if batch_idx < 5:\n",
    "            print(\"training_step\", batch_idx)\n",
    "        #loss = torch.tensor(1)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # 2\n",
    "        self._loss_n_metrics(batch, mode=\"val\")\n",
    "        if batch_idx < 5:\n",
    "            print(\"validation_step\", batch_idx)\n",
    "        #self._loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        print(\"on_validation_epoch_end\")\n",
    "        # 3\n",
    "        \n",
    "        \"\"\"\n",
    "        for parameter in self.parameters():\n",
    "            print(\"parameter\")\n",
    "            print(parameter)\n",
    "        \"\"\" \n",
    "        #return\n",
    "        \n",
    "        print(self.model)\n",
    "        \n",
    "        self.model.update()\n",
    "        \n",
    "        print(\"*\"*50)\n",
    "        print(\"*\"*50)\n",
    "        print(\"*\"*50)\n",
    "        print(\"model is updated now\")\n",
    "        print(\"*\"*50)\n",
    "        print(\"*\"*50)\n",
    "        print(\"*\"*50)\n",
    "        \n",
    "        print(self.model)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        print(\"on_train_epoch_end\")\n",
    "        # 4\n",
    "        \n",
    "\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(\"configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "            lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
    "            return [optimizer], [lr_scheduler]\n",
    "        else:\n",
    "            return optim.SGD(self.parameters(), lr=self.lr)\n",
    "        \n",
    "    def get_cc(self):\n",
    "        # compute connection cost\n",
    "        # adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        # def get_cc(self, weight_factor=2.0, bias_penalize=True, no_penalize_last=False):\n",
    "        # https://stackoverflow.com/questions/74086766/how-to-find-total-cost-of-each-path-in-graph-using-dictionary-in-python\n",
    "        cc = 0\n",
    "        \"\"\"\n",
    "        num_linear = len(self.linears)\n",
    "        for i in range(num_linear):\n",
    "            if i == num_linear - 1 and no_penalize_last:\n",
    "                weight_factor = 0.\n",
    "            biolinear = self.linears[i]\n",
    "            dist = torch.sum(torch.abs(biolinear.out_coordinates.unsqueeze(dim=1) - biolinear.in_coordinates.unsqueeze(dim=0)),dim=2)\n",
    "            cc += torch.mean(torch.abs(biolinear.linear.weight)*(weight_factor*dist+self.l0))\n",
    "            if bias_penalize == True:\n",
    "                cc += torch.mean(torch.abs(biolinear.linear.bias)*(self.l0))\n",
    "        if self.token_embedding:\n",
    "            cc += torch.mean(torch.abs(self.embedding)*(self.l0))\n",
    "            #pass\n",
    "        \"\"\"\n",
    "        return cc\n",
    "    \n",
    "    def _loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        # make it an X object\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img) # cause of the forward function\n",
    "        \n",
    "        ground_truth = ground_truth.unsqueeze(1)\n",
    "        \n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth.size(0), self.n_classes).scatter_(1, ground_truth.to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \n",
    "        loss = self.criterion(model_output, ground_truth_multi_hot)\n",
    "        cc = self.get_cc() * self.cc_weight\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        \n",
    "        acc = self.accuracy(preds=model_output, target=ground_truth_multi_hot) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "        f1score = self.f1score(preds=model_output, target=ground_truth_multi_hot) \n",
    "        \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_f1score', f1score, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_routine(**kwargs):\n",
    "    \n",
    "    print(\"train kwargs\", kwargs['train_kwargs'])\n",
    "    print(\"model kwargs\", kwargs['model_kwargs'])\n",
    "    \n",
    "    train_kwargs = kwargs['train_kwargs']\n",
    "    \n",
    "    transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_set = datasets.MNIST('example_data', train=True, download=True, transform=transform)\n",
    "    val_set = datasets.MNIST('example_data', train=False, transform=transform)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=train_kwargs[\"batch_size\"])\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=train_kwargs[\"batch_size\"])\n",
    "    \n",
    "    logger = CSVLogger(\"example_results/lightning_logs\", name=\"tmp_exp\")\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(train_kwargs[\"ckpt_path\"], \"example_results\"),\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         logger=logger,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         max_epochs=train_kwargs[\"epochs\"],\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    \n",
    "    \n",
    "    \n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(train_kwargs[\"ckpt_path\"], \"DecentNet.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = DecentLightning.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(19) # To be reproducable\n",
    "        \n",
    "        # Initialize the LightningModule and LightningDataModule\n",
    "        model = DecentLightning(kwargs)\n",
    "        \n",
    "        #print(model)\n",
    "\n",
    "        # Train the model using a Trainer\n",
    "        trainer.fit(model, train_dataloader, val_dataloader)\n",
    "        \n",
    "        #print(model)\n",
    "        \n",
    "        model = DecentLightning.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39ad8b29-48de-4f62-88da-381f60d698f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 19\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'epochs': 1, 'batch_size': 16, 'ckpt_path': '', 'device': 'cuda'}\n",
      "model kwargs {'n_classes': 10, 'criterion': BCELoss(), 'optimizer': 'adamw', 'lr': 0.001, 'cc_weight': 0.2}\n",
      "tensor(indices=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "                        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,\n",
      "                        4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6,\n",
      "                        6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8,\n",
      "                        8, 8, 8, 8, 8],\n",
      "                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0,\n",
      "                        1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1,\n",
      "                        2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n",
      "                        3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3,\n",
      "                        4, 5, 6, 7, 8]]),\n",
      "       values=tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 7., nan,\n",
      "                      nan, nan, nan, nan, nan, 2., nan, 8., nan, nan, nan, nan, 4., nan,\n",
      "                      nan, nan, nan, nan, nan, nan, nan, 1., nan, nan, nan, nan, nan, nan,\n",
      "                      nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan, 3., nan, nan, nan, nan, nan, nan, 5., nan, nan, nan, nan, nan,\n",
      "                      nan, 6., nan, nan, nan, nan, nan, nan, nan, nan, nan]),\n",
      "       device='cuda:0', size=(9, 9), nnz=81, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 7., nan, nan, nan, nan, nan,\n",
      "        nan, 2., nan, 8., nan, nan, nan, nan, 4., nan, nan, nan, nan, nan, nan, nan, nan, 1.,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, 3., nan, nan, nan, nan, nan, nan, 5., nan, nan, nan, nan, nan, nan, 6.,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor(indices=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "                        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,\n",
      "                        4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6,\n",
      "                        6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8,\n",
      "                        8, 8, 8, 8, 8],\n",
      "                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0,\n",
      "                        1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1,\n",
      "                        2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n",
      "                        3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3,\n",
      "                        4, 5, 6, 7, 8]]),\n",
      "       values=tensor([nan, nan, 12., nan, nan, nan, 14., nan,  3.,  8., nan,\n",
      "                      nan, 11., nan, nan, nan, nan, nan, nan,  9., nan, 15.,\n",
      "                      nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan,  6., nan, nan, nan, nan,  7., nan, nan,  2., nan,\n",
      "                      nan, 16., nan, nan, nan, nan, nan, nan, nan, 13., nan,\n",
      "                      nan, nan, nan,  4., nan,  5., nan, nan, nan, nan, nan,\n",
      "                      10., nan, nan, nan]),\n",
      "       device='cuda:0', size=(9, 9), nnz=81, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo)\n",
      "tensor([nan, nan, 12., nan, nan, nan, 14., nan,  3.,  8., nan, nan, 11., nan,\n",
      "        nan, nan, nan, nan, nan,  9., nan, 15., nan,  1., nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan,  6., nan, nan, nan, nan,  7., nan, nan,  2., nan, nan,\n",
      "        16., nan, nan, nan, nan, nan, nan, nan, 13., nan, nan, nan, nan,  4.,\n",
      "        nan,  5., nan, nan, nan, nan, nan, 10., nan, nan, nan],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "tensor(indices=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "                        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,\n",
      "                        4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6,\n",
      "                        6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8,\n",
      "                        8, 8, 8, 8, 8],\n",
      "                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0,\n",
      "                        1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1,\n",
      "                        2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n",
      "                        3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3,\n",
      "                        4, 5, 6, 7, 8]]),\n",
      "       values=tensor([ 2., nan,  6., nan, 20.,  7., nan, nan, nan, 16., nan,\n",
      "                      nan, nan, nan, nan, nan, nan, 21., 18., 10., nan, nan,\n",
      "                      nan, nan,  1., 23., 22.,  9., 13., nan, nan, nan, nan,\n",
      "                      nan, 11., nan, nan, 14., nan, nan, 17., 19., nan, nan,\n",
      "                      nan, nan, nan, nan, nan, 12., nan, nan, nan, nan, nan,\n",
      "                      nan, nan, nan,  3.,  4., nan, nan, nan, nan, nan, nan,\n",
      "                      nan, nan, nan, nan, nan,  5., nan,  8., nan, nan, nan,\n",
      "                      nan, 24., 15., nan]),\n",
      "       device='cuda:0', size=(9, 9), nnz=81, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo)\n",
      "tensor([ 2., nan,  6., nan, 20.,  7., nan, nan, nan, 16., nan, nan, nan, nan,\n",
      "        nan, nan, nan, 21., 18., 10., nan, nan, nan, nan,  1., 23., 22.,  9.,\n",
      "        13., nan, nan, nan, nan, nan, 11., nan, nan, 14., nan, nan, 17., 19.,\n",
      "        nan, nan, nan, nan, nan, nan, nan, 12., nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan,  3.,  4., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan,  5., nan,  8., nan, nan, nan, nan, 24., 15., nan],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "tensor(indices=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "                        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,\n",
      "                        4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6,\n",
      "                        6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8,\n",
      "                        8, 8, 8, 8, 8],\n",
      "                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0,\n",
      "                        1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1,\n",
      "                        2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n",
      "                        3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3,\n",
      "                        4, 5, 6, 7, 8]]),\n",
      "       values=tensor([nan,  9., nan, nan, nan, nan, nan,  6., nan, nan, nan,\n",
      "                      nan, nan, nan, nan, nan,  5., nan, nan, nan, nan,  2.,\n",
      "                      nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan, nan,  3., nan,  7., nan,  1., nan, nan,  4., nan,\n",
      "                      nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan, nan, 10., nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan,  8., nan, nan]),\n",
      "       device='cuda:0', size=(9, 9), nnz=81, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo)\n",
      "tensor([nan,  9., nan, nan, nan, nan, nan,  6., nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan,  5., nan, nan, nan, nan,  2., nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan,  3., nan,  7., nan,  1., nan, nan,\n",
      "         4., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 10., nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan,  8., nan, nan],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | DecentNet          | 7.0 K \n",
      "1 | criterion | BCELoss            | 0     \n",
      "2 | accuracy  | MulticlassAccuracy | 0     \n",
      "3 | f1score   | MulticlassF1Score  | 0     \n",
      "-------------------------------------------------\n",
      "7.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configure_optimizers\n",
      "Sanity Checking DataLoader 0:   0%|                                                              | 0/2 [00:00<?, ?it/s]validation_step 0\n",
      "Sanity Checking DataLoader 0:  50%|███████████████████████████                           | 1/2 [00:01<00:01,  0.95it/s]validation_step 1\n",
      "Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████| 2/2 [00:02<00:00,  0.93it/s]on_validation_epoch_end\n",
      "DecentNet(\n",
      "  (decent1): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "    )\n",
      "  )\n",
      "  (decent2): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (8): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (9): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (10): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (11): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([2], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (12): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (13): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (14): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "      (15): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([2], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 6, 2, 7, 7, 1, 2, ns_in= 8, 1, 3, 8, 1, 8, 3, 3)\n",
      "    )\n",
      "  )\n",
      "  (decent3): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([2], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (8): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (9): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (10): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (11): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (12): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (13): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (14): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (15): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (16): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (17): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (18): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (19): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (20): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (21): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (22): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "      (23): DecentFilter(weights: torch.Size([1, 16, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 0, 7, 7, 5, 5, 1, 2, 8, 1, 0, 7, 0, 2, 6, ns_in= 5, 8, 8, 6, 8, 0, 5, 0, 1, 5, 3, 2, 1, 6, 3, 2)\n",
      "    )\n",
      "  )\n",
      "  (decent1x1): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 24, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 6, 6, 7, 0, 0, 8, 3, 2, 3, 5, 3, 4, 8, 1, 4, 2, 4, 0, 1, 2, 2, 8, ns_in= 6, 0, 4, 5, 8, 2, 5, 1, 0, 1, 7, 4, 1, 1, 7, 0, 4, 0, 5, 4, 8, 8, 7, 6)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 24, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 6, 6, 7, 0, 0, 8, 3, 2, 3, 5, 3, 4, 8, 1, 4, 2, 4, 0, 1, 2, 2, 8, ns_in= 6, 0, 4, 5, 8, 2, 5, 1, 0, 1, 7, 4, 1, 1, 7, 0, 4, 0, 5, 4, 8, 8, 7, 6)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 24, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 6, 6, 7, 0, 0, 8, 3, 2, 3, 5, 3, 4, 8, 1, 4, 2, 4, 0, 1, 2, 2, 8, ns_in= 6, 0, 4, 5, 8, 2, 5, 1, 0, 1, 7, 4, 1, 1, 7, 0, 4, 0, 5, 4, 8, 8, 7, 6)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 24, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 6, 6, 7, 0, 0, 8, 3, 2, 3, 5, 3, 4, 8, 1, 4, 2, 4, 0, 1, 2, 2, 8, ns_in= 6, 0, 4, 5, 8, 2, 5, 1, 0, 1, 7, 4, 1, 1, 7, 0, 4, 0, 5, 4, 8, 8, 7, 6)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 24, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 6, 6, 7, 0, 0, 8, 3, 2, 3, 5, 3, 4, 8, 1, 4, 2, 4, 0, 1, 2, 2, 8, ns_in= 6, 0, 4, 5, 8, 2, 5, 1, 0, 1, 7, 4, 1, 1, 7, 0, 4, 0, 5, 4, 8, 8, 7, 6)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 24, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 6, 6, 7, 0, 0, 8, 3, 2, 3, 5, 3, 4, 8, 1, 4, 2, 4, 0, 1, 2, 2, 8, ns_in= 6, 0, 4, 5, 8, 2, 5, 1, 0, 1, 7, 4, 1, 1, 7, 0, 4, 0, 5, 4, 8, 8, 7, 6)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 24, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 6, 6, 7, 0, 0, 8, 3, 2, 3, 5, 3, 4, 8, 1, 4, 2, 4, 0, 1, 2, 2, 8, ns_in= 6, 0, 4, 5, 8, 2, 5, 1, 0, 1, 7, 4, 1, 1, 7, 0, 4, 0, 5, 4, 8, 8, 7, 6)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 24, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 6, 6, 7, 0, 0, 8, 3, 2, 3, 5, 3, 4, 8, 1, 4, 2, 4, 0, 1, 2, 2, 8, ns_in= 6, 0, 4, 5, 8, 2, 5, 1, 0, 1, 7, 4, 1, 1, 7, 0, 4, 0, 5, 4, 8, 8, 7, 6)\n",
      "      (8): DecentFilter(weights: torch.Size([1, 24, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 6, 6, 7, 0, 0, 8, 3, 2, 3, 5, 3, 4, 8, 1, 4, 2, 4, 0, 1, 2, 2, 8, ns_in= 6, 0, 4, 5, 8, 2, 5, 1, 0, 1, 7, 4, 1, 1, 7, 0, 4, 0, 5, 4, 8, 8, 7, 6)\n",
      "      (9): DecentFilter(weights: torch.Size([1, 24, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 6, 6, 7, 0, 0, 8, 3, 2, 3, 5, 3, 4, 8, 1, 4, 2, 4, 0, 1, 2, 2, 8, ns_in= 6, 0, 4, 5, 8, 2, 5, 1, 0, 1, 7, 4, 1, 1, 7, 0, 4, 0, 5, 4, 8, 8, 7, 6)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (mish1): Mish()\n",
      "  (mish2): Mish()\n",
      "  (mish3): Mish()\n",
      "  (mish1x1): Mish()\n",
      "  (bias1): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias2): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias3): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias1x1): InstanceNorm2d(10, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "updating here\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "model is updated now\n",
      "**************************************************\n",
      "**************************************************\n",
      "**************************************************\n",
      "DecentNet(\n",
      "  (decent1): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, ns_in= 0)\n",
      "    )\n",
      "  )\n",
      "  (decent2): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 1, 7, 2, 2, ns_in= 8, 3, 1, 3, 1)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 6, 7, 2, 2, ns_in= 3, 3, 1, 3, 1)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 3, 6, 2, 7, ns_in= 1, 8, 3, 8, 8)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 1, 3, 6, 2, ns_in= 1, 3, 8, 3, 3)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 7, 1, 2, 2, ns_in= 8, 1, 3, 1, 3)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 3, 7, 2, 7, ns_in= 8, 8, 1, 3, 8)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 1, 3, 2, 7, ns_in= 1, 3, 8, 8, 8)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 7, 7, 2, 1, ns_in= 1, 1, 8, 8, 3)\n",
      "      (8): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 7, 6, 7, 2, ns_in= 3, 8, 3, 1, 1)\n",
      "      (9): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 2, 7, 7, 3, ns_in= 3, 1, 1, 8, 8)\n",
      "      (10): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 2, 2, 7, 7, ns_in= 8, 3, 1, 8, 1)\n",
      "      (11): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([2], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 3, 7, 2, 1, ns_in= 3, 8, 1, 8, 3)\n",
      "      (12): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 2, 7, 3, 1, ns_in= 3, 8, 1, 8, 3)\n",
      "      (13): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 6, 2, 7, 2, ns_in= 3, 3, 8, 1, 1)\n",
      "      (14): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 3, 2, 2, 7, 2, ns_in= 8, 1, 3, 8, 8)\n",
      "      (15): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([2], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 2, 3, 6, 2, ns_in= 3, 8, 8, 3, 1)\n",
      "    )\n",
      "  )\n",
      "  (decent3): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 0, 7, 5, 5, ns_in= 0, 8, 6, 0, 5)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, 7, 5, 1, 7, ns_in= 8, 6, 5, 0, 8)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 1, 5, 0, 7, ns_in= 5, 0, 8, 8, 6)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 5, 7, 2, 5, 7, ns_in= 8, 6, 5, 5, 8)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 5, 7, 0, 1, ns_in= 5, 8, 6, 8, 0)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([2], device='cuda:0'))\n",
      "       with inputs: ms_in= 5, 1, 5, 7, 0, ns_in= 8, 0, 5, 6, 8)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 5, 2, 5, 7, ns_in= 0, 5, 5, 8, 6)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 5, 7, 5, 1, 2, ns_in= 8, 6, 0, 0, 5)\n",
      "      (8): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 5, 7, 5, 0, 2, ns_in= 5, 8, 0, 8, 5)\n",
      "      (9): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 5, 0, 5, 5, 7, ns_in= 0, 8, 8, 5, 8)\n",
      "      (10): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 1, 0, 5, 5, ns_in= 5, 0, 8, 8, 5)\n",
      "      (11): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 0, 7, 1, 5, ns_in= 6, 8, 8, 0, 8)\n",
      "      (12): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 5, 5, 7, 0, 1, ns_in= 0, 5, 8, 8, 0)\n",
      "      (13): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 5, 1, 5, 7, ns_in= 8, 0, 0, 5, 6)\n",
      "      (14): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 2, 5, 5, 5, ns_in= 0, 5, 8, 0, 5)\n",
      "      (15): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 5, 2, 7, 5, ns_in= 0, 5, 5, 8, 0)\n",
      "      (16): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 5, 7, 5, 0, ns_in= 8, 0, 6, 5, 8)\n",
      "      (17): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
      "       with inputs: ms_in= 5, 0, 5, 7, 5, ns_in= 0, 8, 8, 6, 5)\n",
      "      (18): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 5, 5, 5, 7, 7, ns_in= 8, 0, 5, 8, 6)\n",
      "      (19): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
      "       with inputs: ms_in= 5, 7, 2, 0, 5, ns_in= 5, 8, 5, 8, 0)\n",
      "      (20): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 5, 7, 0, 5, 1, ns_in= 0, 6, 8, 5, 0)\n",
      "      (21): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 5, 5, 1, ns_in= 5, 8, 5, 0, 0)\n",
      "      (22): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 1, 2, 5, 7, ns_in= 6, 0, 5, 8, 8)\n",
      "      (23): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 1, 5, 5, 7, 2, ns_in= 0, 0, 5, 8, 5)\n",
      "    )\n",
      "  )\n",
      "  (decent1x1): DecentLayer(\n",
      "    (filter_list): ModuleList(\n",
      "      (0): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, 0, 2, 7, 6, ns_in= 0, 5, 6, 8, 5)\n",
      "      (1): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
      "       with inputs: ms_in= 2, 0, 0, 0, 6, ns_in= 6, 2, 5, 0, 4)\n",
      "      (2): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
      "       with inputs: ms_in= 6, 7, 0, 0, 2, ns_in= 5, 8, 2, 5, 6)\n",
      "      (3): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 7, 0, 0, 0, 6, ns_in= 8, 0, 5, 2, 4)\n",
      "      (4): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, 7, 6, 8, 2, ns_in= 0, 8, 5, 1, 6)\n",
      "      (5): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
      "       with inputs: ms_in= 8, 6, 7, 6, 0, ns_in= 1, 4, 8, 5, 0)\n",
      "      (6): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, 8, 2, 0, 6, ns_in= 2, 1, 6, 0, 4)\n",
      "      (7): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
      "       with inputs: ms_in= 6, 8, 6, 7, 0, ns_in= 4, 1, 5, 8, 0)\n",
      "      (8): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([0], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
      "       with inputs: ms_in= 8, 6, 2, 7, 6, ns_in= 1, 4, 6, 8, 5)\n",
      "      (9): DecentFilter(weights: torch.Size([1, 5, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
      "       with inputs: ms_in= 0, 8, 7, 2, 6, ns_in= 0, 1, 8, 6, 5)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (mish1): Mish()\n",
      "  (mish2): Mish()\n",
      "  (mish3): Mish()\n",
      "  (mish1x1): Mish()\n",
      "  (bias1): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias2): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias3): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (bias1x1): InstanceNorm2d(10, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch 0:   0%|                                                                                | 0/3750 [00:00<?, ?it/s]training_step 0\n",
      "Epoch 0:   0%|                                                              | 1/3750 [00:00<27:05,  2.31it/s, v_num=17]training_step 1\n",
      "Epoch 0:   0%|                                                              | 2/3750 [00:00<26:22,  2.37it/s, v_num=17]training_step 2\n",
      "Epoch 0:   0%|                                                              | 3/3750 [00:01<27:14,  2.29it/s, v_num=17]training_step 3\n",
      "Epoch 0:   0%|                                                              | 4/3750 [00:01<27:27,  2.27it/s, v_num=17]training_step 4\n",
      "Epoch 0:   1%|▋                                                            | 43/3750 [00:16<23:42,  2.61it/s, v_num=17]"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:/Users/Christina/Documents/datasceyence/examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, results \u001b[38;5;241m=\u001b[39m \u001b[43mdev_routine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcriterion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcc_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtrain_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;66;43;03m# 'test_batch_size': 1,\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mckpt_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 52\u001b[0m, in \u001b[0;36mdev_routine\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloader, val_dataloader)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m#print(model)\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mDecentLightning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Load best checkpoint after training\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Test best model on validation and test set\u001b[39;00m\n\u001b[0;32m     55\u001b[0m val_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(model, val_loader, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1552\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1478\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1479\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1550\u001b[0m \n\u001b[0;32m   1551\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1552\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m _load_from_checkpoint(\n\u001b[0;32m   1553\u001b[0m         \u001b[38;5;28mcls\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1554\u001b[0m         checkpoint_path,\n\u001b[0;32m   1555\u001b[0m         map_location,\n\u001b[0;32m   1556\u001b[0m         hparams_file,\n\u001b[0;32m   1557\u001b[0m         strict,\n\u001b[0;32m   1558\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1559\u001b[0m     )\n\u001b[0;32m   1560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:61\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m map_location \u001b[38;5;241m=\u001b[39m map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[1;32m---> 61\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[0;32m     64\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[0;32m     65\u001b[0m     checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:54\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(path_or_url, map_location)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[0;32m     51\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     53\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\fsspec\\spec.py:1199\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1198\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[1;32m-> 1199\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(\n\u001b[0;32m   1200\u001b[0m         path,\n\u001b[0;32m   1201\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   1202\u001b[0m         block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m   1203\u001b[0m         autocommit\u001b[38;5;241m=\u001b[39mac,\n\u001b[0;32m   1204\u001b[0m         cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[0;32m   1205\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1206\u001b[0m     )\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1208\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\fsspec\\implementations\\local.py:183\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\fsspec\\implementations\\local.py:314\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\fsspec\\implementations\\local.py:319\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m--> 319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[0;32m    321\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/Christina/Documents/datasceyence/examples'"
     ]
    }
   ],
   "source": [
    "model, results = dev_routine(model_kwargs={\n",
    "                                'n_classes': 10,\n",
    "                                'criterion': torch.nn.BCELoss(),\n",
    "                                'optimizer': \"adamw\",\n",
    "                                'lr': 0.001,\n",
    "                                'cc_weight': 0.2,\n",
    "                            },\n",
    "                            train_kwargs={\n",
    "                                'epochs': 1,\n",
    "                                'batch_size': 16,\n",
    "                                # 'test_batch_size': 1,\n",
    "                                'ckpt_path': \"\",\n",
    "                                'device': \"cuda\"\n",
    "                            }\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f86b61-75b7-4a80-b39d-50a0959badbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'kwargs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDecentLightning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'kwargs'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb9d4c30-9266-4c29-862b-fffef138cebe",
   "metadata": {},
   "source": [
    "### normal train without lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d89cfe-7561-438c-a0b4-bb3c0c951716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for i_batch, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if i_batch == 5:\n",
    "            \n",
    "            \n",
    "            #model.update()\n",
    "            \n",
    "            #print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            #print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            #print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i_batch * len(data), len(train_loader.dataset),\n",
    "                100. * i_batch / len(train_loader), loss.item()))\n",
    "            \n",
    "            # model.update()\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 1\n",
    "        self.save_model = False\n",
    "        \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = DecentNet().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        model.update() \n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.ckpt\")\n",
    "\n",
    "    return model\n",
    "\n",
    "if False:\n",
    "    model = main()\n",
    "\n",
    "    for i in model.parameters():\n",
    "        print(i.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95870660-6418-4b85-a673-13e266a53960",
   "metadata": {},
   "source": [
    "# conv filter test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7bceb-331c-4f16-be5f-3c5c5d91ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16303538-75ae-4064-ae26-848926552bb7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one filter\n",
    "\n",
    "w_groups = 1 # groups of the channels\n",
    "w_channels = 1024 # input channels\n",
    "w_filters = 10\n",
    "\n",
    "assert w_filters % w_groups == 0\n",
    "assert w_channels % w_groups == 0\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "# w_filters x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "# batch size x w_filters x width x height\n",
    "out = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "print(weights.shape, \"- filters x channels x kernel x kernel\")\n",
    "print(out.shape, \"- batch x filters x width x height\")\n",
    "print()\n",
    "\n",
    "print(\"*\"*50)\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "output_list = []\n",
    "\n",
    "# for each filter, we need different true false vales for our channels\n",
    "active = list(np.random.choice([True, False], size=w_channels, replace=True, p=None))\n",
    "\n",
    "# w_filters x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(1,w_channels,3,3))\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "output_list = []\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    for _ in range(w_filters):\n",
    "        \n",
    "        pass\n",
    "\n",
    "        #print()\n",
    "        #print(active)\n",
    "        #print()\n",
    "        #print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "        #print(weights.shape, \"- filters x channels x kernel x kernel\")\n",
    "\n",
    "\n",
    "        # need to remove weight and input channels according to active list for each filter\n",
    "        #print(inputs.shape)\n",
    "        #print(inputs[:,active,:,:].shape)\n",
    "\n",
    "        #print(weights.shape)\n",
    "        #print(weights[:,active,:,:].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # batch size x w_filters x width x height\n",
    "        \n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            #output_list.append(this_output)\n",
    "        #print(this_output.shape, \"- batch x 1 filter x width x height\")\n",
    "\n",
    "    #out = torch.cat(output_list, dim=1)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    i_tmp = inputs[:,active,:,:]\n",
    "    w_tmp = weights[:,active,:,:]\n",
    "    this_output = F.conv2d(i_tmp, w_tmp, padding=1, groups=w_groups)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    this_output = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print(out.shape, \"- batch x filters x width x height\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# take the mean of all - we can remove all sorts of information from the out tensor\n",
    "mean = torch.mean(out, 1, keepdim=True)\n",
    "print(mean.shape, \"- mean accross the filters (no sense here ...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e3c4e-80a1-448f-9b3c-4ae14cd7bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "#b = torch.empty(w_filters, w_channels, dtype=torch.bool)\n",
    "b = torch.ByteTensor(500, w_channels)\n",
    "print(sys.getsizeof(b.storage())) # 1310776 (bytes)\n",
    "\n",
    "#a = torch.empty(w_filters, w_channels, dtype=torch.uint8)\n",
    "a = torch.ByteTensor(500, w_channels)\n",
    "print(sys.getsizeof(a.storage())) # 1310776 (bytes)\n",
    "\n",
    "active = list(np.random.choice([True, False], size=w_channels, replace=True, p=None))\n",
    "print(sys.getsizeof(active)*500) # 1310776 (bytes)\n",
    "\n",
    "weights = torch.FloatTensor(torch.randn(500,w_channels,3,3))\n",
    "print(sys.getsizeof(weights.storage())) # 1310776 (bytes) 36912\n",
    "print(36912*500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2b255-ccc3-4809-948c-cd4dc5efe661",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dadee83-7300-4844-8284-8d0ef29aaee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(2, size=10)\n",
    "\n",
    "list(np.random.choice([True, False], size=10, replace=True, p=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c72946-baf5-4c90-b84e-14f14c8d949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(1, 82, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2754cb-b071-4267-b53c-5f2190a7baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "9*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95feb489-36f2-4d83-81e3-5e21a530f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(a=4, size=2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5d97e-7408-4c52-8891-864827810c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one filter\n",
    "\n",
    "w_groups = 27 # groups of the channels\n",
    "w_channels = 7 # input channels\n",
    "w_filters = 200\n",
    "batch_size = 27\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(batch_size, w_channels*w_groups, 100,100))\n",
    "#inputs = torch.autograd.Variable(torch.randn(w_channels*w_groups, 100,100))\n",
    "\n",
    "# w_groups x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(1, w_channels,3,3))\n",
    "#weights = torch.autograd.Variable(torch.randn(w_groups*w_channels,3,3))\n",
    "\n",
    "print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "print(weights.shape, \"- filter 1 x channels x kernel x kernel\")\n",
    "\n",
    "try:\n",
    "    o_list = []\n",
    "    for _ in range(w_filters):\n",
    "        # batch size x groups x width x height\n",
    "        out = F.conv2d(inputs, weights, groups=w_groups)\n",
    "        o_list.append(out)\n",
    "        # take the mean of all - we can remove all sorts of information from the out tensor\n",
    "        #mean = torch.mean(out, 1, keepdim=True)\n",
    "    \n",
    "    print(torch.cat(o_list, dim=1).shape, \"- batch x filters x width x height\")\n",
    "    #print(mean.shape, \"- mean accross the groups\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3d443-372f-4548-804e-11efbebe6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = torch.randn(8, 4, 3, 3)\n",
    "inputs = torch.randn(1, 4, 5, 5)\n",
    "F.conv2d(inputs, filters, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bf7b8-35d2-4f68-b045-0ba93bd4c5fc",
   "metadata": {},
   "source": [
    "# Visualise filters and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536067b-6098-4844-9ef4-7a5db5a23156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net() # .to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320d57b-3e22-404f-ad6c-b04515fe2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "def visChannels(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    \n",
    "    plt.figure(figsize=(nrow,rows) )\n",
    "    plt.title(f\"Channels with index {ch}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "def visFilters(tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.title(f\"Filter {filt}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "def visFilters_subplot(subplot, tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    subplot.set_title(f\"Filter {filt+1} with {c} channels\")\n",
    "    subplot.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "    subplot.axis('off')\n",
    "    \n",
    "layer = 1\n",
    "filter = model.conv2.weight.data.clone()\n",
    "\n",
    "print(model.conv2.weight.shape)\n",
    "\n",
    "# need to match the network parameters!!!!\n",
    "in_channels = 5\n",
    "out_filters = 3 # 64\n",
    "\n",
    "\n",
    "fig, subplot = plt.subplots(out_filters, figsize=(10, 10))\n",
    "fig.suptitle(f'Layer with shape {list(model.conv2.weight.shape)} [out, in, kernel, kernel]')\n",
    "\n",
    "for filt in range(0, out_filters):\n",
    "    \n",
    "    visFilters_subplot(subplot[filt], filter, filt=filt, allkernels=False)\n",
    "\n",
    "    #plt.axis('off')\n",
    "    #plt.ioff()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"example_results/filter_with_weights.png\")\n",
    "plt.show()\n",
    "    \n",
    "if False:    \n",
    "    for filt in range(0, out_filters):\n",
    "\n",
    "        visFilters(filter, filt=filt, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"examples/example_results/filter_with_weights.png\")\n",
    "        plt.show()\n",
    "\n",
    "    for ch in range(0, in_channels):\n",
    "\n",
    "        visChannels(filter, ch=ch, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791fa94-5bb8-4d3a-ad97-72cc5a59025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "res = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655d64a-7fc0-42c7-880e-69e7be6acedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.layer1[0].conv1.bias == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3322395-f2a0-40e1-a0e8-86c090586f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.layer1[0].conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d52543-cd67-4d00-963a-5d1effbfc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.extra_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77091394-6e75-4aa3-84c3-a9ebc7ce932e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
