{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {},
   "source": [
    "* https://github.com/MSKCC-Computational-Pathology/MIL-nature-medicine-2019/blob/master/MIL_train.py\n",
    "\n",
    "* https://github.com/binli123/dsmil-wsi/blob/master/attention_map.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b972ff66-723c-43a1-a9d9-80c43b450efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from typing import Optional, List, Tuple, Union\n",
    "\n",
    "class _ConvNd(torch.nn.Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    in_channels: int\n",
    "    _reversed_padding_repeated_twice: List[int]\n",
    "    out_channels: int\n",
    "    kernel_size: Tuple[int, ...]\n",
    "    stride: Tuple[int, ...]\n",
    "    padding: Union[str, Tuple[int, ...]]\n",
    "    dilation: Tuple[int, ...]\n",
    "    transposed: bool\n",
    "    output_padding: Tuple[int, ...]\n",
    "    groups: int\n",
    "    padding_mode: str\n",
    "    weight: Tensor\n",
    "    bias: Optional[Tensor]\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Tuple[int, ...],\n",
    "                 stride: Tuple[int, ...],\n",
    "                 padding: Tuple[int, ...],\n",
    "                 dilation: Tuple[int, ...],\n",
    "                 transposed: bool,\n",
    "                 output_padding: Tuple[int, ...],\n",
    "                 groups: int,\n",
    "                 bias: bool,\n",
    "                 padding_mode: str,\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "class CustomConv2d(_ConvNd):\n",
    "    \n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = _pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super().__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2823e218-5eb3-44d9-a277-2cf17b772c84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', 'C:\\\\Users\\\\Prinzessin\\\\projects\\\\decentnet\\\\datasceyence\\\\examples', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\python39.zip', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\DLLs', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta', '', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Prinzessin\\\\.ipython']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28752\\3345652139.py\u001b[0m in \u001b[0;36m<cell line: 228>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28752\\3345652139.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtest_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    662\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    985\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    986\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 987\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "sys.path.insert(0, \"/helper\")\n",
    "sys.path.insert(0, \"./helper\")\n",
    "sys.path.insert(0, \"../helper\")\n",
    "print(sys.path)\n",
    "\n",
    "# own module\n",
    "from visualisation.feature_map import *\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CustomConv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv2 = CustomConv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv3 = CustomConv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv1x1 = CustomConv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "        # 4x16384\n",
    "        # self.fc1 = nn.Linear(10*10*10, 10)\n",
    "        #self.fc2 = nn.Linear(10, 10)\n",
    "        \n",
    "        #self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.mish1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.mish2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.mish3(x)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        #print(x.size())\n",
    "        #print(x.size()[2:])\n",
    "        \n",
    "        x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        # x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        #x = torch.flatten(x, 1)\n",
    "        # x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.fc2(x)\n",
    "        #output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if batch_idx == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 1\n",
    "        self.save_model = False\n",
    "        \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74c529-433c-476d-843d-4fe79cc4c847",
   "metadata": {},
   "source": [
    "# DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0769c61b-e163-43aa-921a-aed95bf9ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from typing import Optional, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # convolution happens in here\n",
    "    \n",
    "    def __init__(self, transposed):\n",
    "        \n",
    "        out_channels = 1\n",
    "    \n",
    "        # weight + importance + activat\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (in_channels, out_channels), **factory_kwargs))\n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (out_channels, in_channels), **factory_kwargs))\n",
    "            \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        \n",
    "        # reset weights and bias - in filter or in layer?\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        # randomly initialise the positional array\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "    \n",
    "    def update(self):\n",
    "        # channel deactivation\n",
    "        # require_grad = False/True for each channel\n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class DecentLayer(torch.nn.Module):\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "        \n",
    "        \n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: _size_2_t,\n",
    "                 stride: _size_2_t = 1,\n",
    "                 padding: Union[str, _size_2_t] = 0,\n",
    "                 dilation: _size_2_t = 1,\n",
    "                 transposed: bool = False,\n",
    "                 output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 groups: int = 1,\n",
    "                 bias: bool = True,\n",
    "                 padding_mode: str = \"zeros\",\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "            \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        \n",
    "        module_list = []\n",
    "        for i_filter in out_channels:\n",
    "            module_list.append(DecentFilter())\n",
    "            \n",
    "        \n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # reset in initialisation\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "            \n",
    "            \n",
    "    def update(self):\n",
    "        # prunging\n",
    "        pass\n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b694c13-beba-4d0c-a055-a91eb7d9c77b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3413300792.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Prinzessin\\AppData\\Local\\Temp\\ipykernel_15656\\3413300792.py\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    layer_id = 0\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class _ConvNd(torch.nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    in_channels: int\n",
    "    _reversed_padding_repeated_twice: List[int]\n",
    "    out_channels: int\n",
    "    kernel_size: Tuple[int, ...]\n",
    "    stride: Tuple[int, ...]\n",
    "    padding: Union[str, Tuple[int, ...]]\n",
    "    dilation: Tuple[int, ...]\n",
    "    transposed: bool\n",
    "    output_padding: Tuple[int, ...]\n",
    "    groups: int\n",
    "    padding_mode: str\n",
    "    weight: Tensor\n",
    "    bias: Optional[Tensor]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class CustomConv2d(_ConvNd):\n",
    "    \n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', 'C:\\\\Users\\\\Prinzessin\\\\projects\\\\decentnet\\\\datasceyence\\\\examples', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\python39.zip', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\DLLs', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta', '', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Prinzessin\\\\.ipython']\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.705156\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.692756\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.689414\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.679317\n",
      "\n",
      "Test set: Average loss: 0.6711, Accuracy: 1144/10000 (11%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.667882\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.652637\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.626749\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.587804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15656\\3617462962.py\u001b[0m in \u001b[0;36m<cell line: 228>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15656\\3617462962.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15656\\3617462962.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(model, device, test_loader)\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# .to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0mtarget_multi_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15656\\3617462962.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmish3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecent1x1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmish1x1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15656\\4113608024.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15656\\4113608024.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;31m# this is written in c++ - try not to change ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    150\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "sys.path.insert(0, \"/helper\")\n",
    "sys.path.insert(0, \"./helper\")\n",
    "sys.path.insert(0, \"../helper\")\n",
    "print(sys.path)\n",
    "\n",
    "# own module\n",
    "from visualisation.feature_map import *\n",
    "\n",
    "\n",
    "\n",
    "class DecentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecentNet, self).__init__()\n",
    "        self.decent1 = DecentLayer(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent2 = DecentLayer(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent3 = DecentLayer(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent1x1 = DecentLayer(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x = self.mish1(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x = self.mish2(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x = self.mish3(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if batch_idx == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 1\n",
    "        self.save_model = False\n",
    "        \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = DecentNet().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0dfe389e-2713-4a65-81d2-6bea9d7bbd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128])\n",
      "torch.Size([10, 128, 1, 1])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i in model.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da6083-0fe4-4c47-8171-7f010c18b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 3\n",
    "num_classes = 10\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "outputs_before_sigmoid = torch.randn(batch_size, num_classes)\n",
    "sigmoid_outputs = torch.sigmoid(outputs_before_sigmoid)\n",
    "\n",
    "# classes = [[2, 4, 7], [3, 6, 9]]\n",
    "labels = torch.tensor([[1], [9], [4]])\n",
    "# labels = labels.unsqueeze(0)\n",
    "target_classes = torch.zeros(labels.size(0), 10).scatter_(1, labels, 1.)\n",
    "\n",
    "\n",
    "# target_classes = torch.randint(0, 2, (batch_size, num_classes)).to(torch.float32)  # randints in [0, 2).\n",
    "\n",
    "loss = loss_fn(sigmoid_outputs, target_classes)\n",
    "\n",
    "# alternatively, use BCE with logits, on outputs before sigmoid.\n",
    "loss_fn_2 = torch.nn.BCEWithLogitsLoss()\n",
    "loss2 = loss_fn_2(outputs_before_sigmoid, target_classes)\n",
    "\n",
    "print(sigmoid_outputs)\n",
    "\n",
    "print(target_classes)\n",
    "\n",
    "print(loss)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d27aa-4fc3-4b43-920d-071d3cc464d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1, 0, 5, 2])\n",
    "labels = labels.unsqueeze(0)\n",
    "\n",
    "target = torch.zeros(labels.size(0), 10).scatter_(1, labels, 1.)\n",
    "print(target)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784a059-0ada-423c-a4c1-112548c099de",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.empty(3, 4, 5)\n",
    "t.size()\n",
    "\n",
    "t.size(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae197ae-7a60-4445-9c11-91614ed75c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c674872-1f7b-4772-9bff-cf6b896c45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "64*16*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2736a566-a08f-4fa1-b1b0-b29884bc9af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "12800/128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b52d2-f69b-4699-8bbf-165a70fc1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b00b39-433d-4ad6-9cf9-c61d805b5511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16303538-75ae-4064-ae26-848926552bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 16, 100, 100])\n",
      "torch.Size([8, 2, 3, 3])\n",
      "torch.Size([27, 8, 100, 100])\n",
      "torch.Size([27, 1, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "w_groups = 8\n",
    "w_channels = 2\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "# w_groups x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(w_groups,w_channels,3,3))\n",
    "\n",
    "# batch size x groups x width x height\n",
    "out = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "\n",
    "# take the mean of all - we can remove all sorts of information from the out tensor\n",
    "mean = torch.mean(out, 1, keepdim=True)\n",
    "\n",
    "print(inputs.shape)\n",
    "print(weights.shape)\n",
    "print(out.shape)\n",
    "print(mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bf7b8-35d2-4f68-b045-0ba93bd4c5fc",
   "metadata": {},
   "source": [
    "# Visualise filters and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c536067b-6098-4844-9ef4-7a5db5a23156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net() # .to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1320d57b-3e22-404f-ad6c-b04515fe2fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 3, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "def visChannels(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    \n",
    "    plt.figure(figsize=(nrow,rows) )\n",
    "    plt.title(f\"Channels with index {ch}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "def visFilters(tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.title(f\"Filter {filt}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "def visFilters_subplot(subplot, tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    subplot.set_title(f\"Filter {filt+1} with {c} channels\")\n",
    "    subplot.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "    subplot.axis('off')\n",
    "    \n",
    "layer = 1\n",
    "filter = model.conv2.weight.data.clone()\n",
    "\n",
    "print(model.conv2.weight.shape)\n",
    "\n",
    "# need to match the network parameters!!!!\n",
    "in_channels = 5\n",
    "out_filters = 3 # 64\n",
    "\n",
    "\n",
    "fig, subplot = plt.subplots(out_filters, figsize=(10, 10))\n",
    "fig.suptitle(f'Layer with shape {list(model.conv2.weight.shape)} [out, in, kernel, kernel]')\n",
    "\n",
    "for filt in range(0, out_filters):\n",
    "    \n",
    "    visFilters_subplot(subplot[filt], filter, filt=filt, allkernels=False)\n",
    "\n",
    "    #plt.axis('off')\n",
    "    #plt.ioff()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"example_results/filter_with_weights.png\")\n",
    "plt.show()\n",
    "    \n",
    "if False:    \n",
    "    for filt in range(0, out_filters):\n",
    "\n",
    "        visFilters(filter, filt=filt, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"examples/example_results/filter_with_weights.png\")\n",
    "        plt.show()\n",
    "\n",
    "    for ch in range(0, in_channels):\n",
    "\n",
    "        visChannels(filter, ch=ch, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "75f94075-d7c8-4192-be6b-3f4d3ceb3f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 32, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.conv2.weight.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ae149e2-a884-4906-b687-d08f10ec78c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv2.importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b31db158-ef7d-47ab-a091-6a11468a0571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.3631e-44,  4.8011e+30,  1.5726e+02,  ...,  0.0000e+00,\n",
       "          1.4013e-45,  7.0065e-45],\n",
       "        [ 3.3631e-44,  1.8037e+28,  1.5726e+02,  ...,  7.1449e+31,\n",
       "         -4.6441e-03,  7.9173e-43],\n",
       "        [ 1.5849e-42,  7.9050e+31, -4.6442e-03,  ...,  7.0672e-10,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        ...,\n",
       "        [-4.6476e-03,  7.9173e-43,  3.6434e-44,  ...,  1.1210e-44,\n",
       "          2.2561e-43,  3.5032e-44],\n",
       "        [ 1.8217e-44,  7.9173e-43, -4.6446e-03,  ...,  1.8217e-44,\n",
       "          3.3631e-44,  4.4842e-44],\n",
       "        [ 5.1064e+01,  7.9173e-43,  1.4013e-45,  ...,  1.1096e+27,\n",
       "          7.7067e+31,  6.7415e+22]], requires_grad=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv2.importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c511c76-c1fc-42a8-85d7-bfecd2e34159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
