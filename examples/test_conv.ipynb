{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DecentNet from conv layer\n",
    "\n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "     \n",
    "* pruning actually doesn\"t work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df9f06-ef7e-4c73-887b-5b2d129ff9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../helper', './helper', '/helper', 'helper', 'C:\\\\Users\\\\Prinzessin\\\\projects\\\\decentnet\\\\datasceyence\\\\examples', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\python39.zip', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\DLLs', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta', '', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Prinzessin\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from typing import Optional, List, Tuple, Union\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "sys.path.insert(0, \"/helper\")\n",
    "sys.path.insert(0, \"./helper\")\n",
    "sys.path.insert(0, \"../helper\")\n",
    "print(sys.path)\n",
    "\n",
    "# own module\n",
    "from visualisation.feature_map import *\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38836e4-6905-4e0c-b344-bcc3b8094388",
   "metadata": {},
   "source": [
    "# conv2d layer (slightly adapted original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b972ff66-723c-43a1-a9d9-80c43b450efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConvNd(torch.nn.Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    in_channels: int\n",
    "    _reversed_padding_repeated_twice: List[int]\n",
    "    out_channels: int\n",
    "    kernel_size: Tuple[int, ...]\n",
    "    stride: Tuple[int, ...]\n",
    "    padding: Union[str, Tuple[int, ...]]\n",
    "    dilation: Tuple[int, ...]\n",
    "    transposed: bool\n",
    "    output_padding: Tuple[int, ...]\n",
    "    groups: int\n",
    "    padding_mode: str\n",
    "    weight: Tensor\n",
    "    bias: Optional[Tensor]\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Tuple[int, ...],\n",
    "                 stride: Tuple[int, ...],\n",
    "                 padding: Tuple[int, ...],\n",
    "                 dilation: Tuple[int, ...],\n",
    "                 transposed: bool,\n",
    "                 output_padding: Tuple[int, ...],\n",
    "                 groups: int,\n",
    "                 bias: bool,\n",
    "                 padding_mode: str,\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        print(factory_kwargs)\n",
    "        super().__init__()\n",
    "        if groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "class CustomConv2d(_ConvNd):\n",
    "    \n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = stride #_pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super().__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        print(self.stride)\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369256cb-1ea4-4ff2-8ce5-a820511e0779",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2823e218-5eb3-44d9-a277-2cf17b772c84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CustomConv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv2 = CustomConv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv3 = CustomConv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv1x1 = CustomConv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "        self.K = 100 \n",
    "        self.L = 10 # last one\n",
    "        self.num_of_bases = 1 # 3rd dim\n",
    "        \n",
    "        if False:\n",
    "            self.conv1 = Conv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv2 = Conv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv3 = Conv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv1x1 = Conv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "        # 4x16384\n",
    "        # self.fc1 = nn.Linear(10*10*10, 10)\n",
    "        #self.fc2 = nn.Linear(10, 10)\n",
    "        \n",
    "        #self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        #self.sub_concept_pooling = nn.modules.MaxPool2d((self.K, 1), stride=(1,1))\n",
    "        #self.instance_pooling = nn.modules.MaxPool2d((opt.num_of_bases, 1), stride=(1,1))\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.mish1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.mish2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.mish3(x)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        #print(x.size())\n",
    "        #print(x.size()[2:])\n",
    "        \n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        #x = x.view(-1, self.L, self.K, 10)\n",
    "        \n",
    "        # input, kernel_size, stride, padding, dilation, ceil_mode\n",
    "        #x = self.sub_concept_pooling(x).view(-1, self.L, self.num_of_bases).permute(0,2,1).unsqueeze(1)\n",
    "        \n",
    "        # output = F.sigmoid(x)\n",
    "        # x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        #x = torch.flatten(x, 1)\n",
    "        # x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.fc2(x)\n",
    "        #output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76fa05-b47e-4264-85cf-766d8ca060d3",
   "metadata": {},
   "source": [
    "# normal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0819306e-ff93-4f38-898b-4c1cd2221b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if batch_idx == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            \"\"\"\n",
    "            if i == 0 and epoch % args.log_interval == 0:\n",
    "            # if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot(path=f\"example_results/feature_map_{epoch}.png\")\n",
    "                \"\"\"\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.1\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 5\n",
    "        self.save_model = True\n",
    "        \n",
    "\n",
    "def main_train():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader, epoch)\n",
    "        #scheduler.step()\n",
    "        \n",
    "        \n",
    "        if args.save_model and epoch % args.log_interval == 0:\n",
    "            torch.save(model.state_dict(), f\"example_results/mnist_cnn_{epoch}.ckpt\")\n",
    "\n",
    "\n",
    "def main_test():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "\n",
    "    if True:\n",
    "        model.load_state_dict(torch.load(\"example_results/mnist_cnn_5.ckpt\"))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(\"example_results/pruned_model.ckpt\"))\n",
    "    \n",
    "\n",
    "    # model = torch.load(model.state_dict(), \"example_results/mnist_cnn_30.ckpt\")\n",
    "    if False:\n",
    "        test(args, model, device, test_loader, 0)\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38eb5e24-3f20-4788-8344-66a98d6791bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1242e1a-c691-4cb6-a279-9904cedcd806",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n"
     ]
    }
   ],
   "source": [
    "model_to_prune = main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "270319e6-3827-47cd-a579-476c89217968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(model_to_prune.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {},
   "source": [
    "# DecentNet trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfa123-54e1-4053-94c3-52b45ec0859c",
   "metadata": {},
   "source": [
    "## DecentFilter\n",
    "* conv2d problem: https://stackoverflow.com/questions/61269421/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # convolution happens in here\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels=32, \n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 padding_mode=\"zeros\",\n",
    "                 dilation=3, \n",
    "                 transposed=None, \n",
    "                 device=None, \n",
    "                 dtype=None):\n",
    "        \n",
    "        #print(\"device\", device)\n",
    "        \n",
    "        \n",
    "        out_channels = 1\n",
    "        groups = 1\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = stride # _pair(stride)\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        \n",
    "        \n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "            \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        #self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "                \n",
    "        \n",
    "        \n",
    "        # print(factory_kwargs)\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        # weight + importance + activat\n",
    "\n",
    "        self.weight = Parameter(torch.empty(\n",
    "            (in_channels, *kernel_size), **factory_kwargs)).to(device)\n",
    "        self.importance = Parameter(torch.empty(\n",
    "            (in_channels), **factory_kwargs))\n",
    "            \n",
    "        if False: \n",
    "            # bias:\n",
    "            # where should the bias be???\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            #self.bias = False\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        \n",
    "        # reset weights and bias - in filter or in layer?\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        # randomly initialise the positional array\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        \n",
    "        # todo - weight has to be the one in the filter\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    \n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        \n",
    "        print(\"stride\", self.stride)\n",
    "        \n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        \n",
    "        # input = input.squeeze(1)\n",
    "        weight = weight.unsqueeze(1)\n",
    "        \n",
    "        print(input.shape, \"- batch x groups*channels x width x height\")\n",
    "        print(weight.shape, \"- groups x channels x kernel x kernel\")\n",
    "        \n",
    "        # todo: cuda is ... not a variable ...\n",
    "        \n",
    "        out = F.conv2d(input, weight.cuda(), bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "        \n",
    "        print(out.shape, \"- batch x groups x width x height\")        \n",
    "        \n",
    "        return out\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "    \n",
    "    def update(self):\n",
    "        # channel deactivation\n",
    "        # require_grad = False/True for each channel\n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdab4-e0b0-4523-882f-dad3ebf06090",
   "metadata": {},
   "source": [
    "## DecentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fddb74c7-5940-424d-aab8-2c75efd914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLayer(torch.nn.Module):\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'in_channels', #  'output_padding',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "        \n",
    "        \n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: _size_2_t,\n",
    "                 stride: _size_2_t = 1,\n",
    "                 padding: Union[str, _size_2_t] = 0,\n",
    "                 dilation: _size_2_t = 1,\n",
    "                 transposed: bool = False,\n",
    "                 #output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 #groups: int = 1,\n",
    "                 bias: bool = True,\n",
    "                 padding_mode: str = \"zeros\",\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = stride # _pair(stride)\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # torch.nn.ModuleList\n",
    "        \n",
    "        self.module_list = nn.ModuleList([])\n",
    "        for i_filter in range(out_channels):\n",
    "            self.module_list.append(DecentFilter(in_channels=in_channels))\n",
    "            \n",
    "        \"\"\"\n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" \n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups), **factory_kwargs))\n",
    "        \"\"\" \n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # reset in initialisation\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        pass\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        \n",
    "        # todo - weight has to be the one in the filter\n",
    "        # init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        \"\"\"\n",
    "        not needed due to instance norm layer\n",
    "        https://github.com/pytorch/vision/issues/4914\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\"\"\"\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "            \n",
    "            \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \n",
    "        output_list = []\n",
    "        for module in self.module_list:\n",
    "            output_list.append(module(input))\n",
    "         \n",
    "        out = torch.cat(output_list)\n",
    "        mean = torch.mean(out, 0, keepdim=True)\n",
    "        \n",
    "        return mean\n",
    "        # return self._conv_forward(input, self.weight, self.bias)\n",
    "            \n",
    "    def update(self):\n",
    "        # prunging\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c186cf1-c7db-45da-b779-f7ab88f3896f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([16, 1, 28, 28]) - batch x groups*channels x width x height\n",
      "torch.Size([1, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([16, 1, 22, 22]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 1, 22, 22]) - batch x groups*channels x width x height\n",
      "torch.Size([32, 1, 3, 3]) - groups x channels x kernel x kernel\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups x width x height\n",
      "stride 1\n",
      "torch.Size([1, 32, 16, 16]) - batch x groups*channels x width x height\n",
      "torch.Size([64, 1, 3, 3]) - groups x channels x kernel x kernel\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 1, 3, 3], expected input[1, 32, 16, 16] to have 1 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21652\\3366150521.py\u001b[0m in \u001b[0;36m<cell line: 182>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21652\\3366150521.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21652\\3366150521.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# .to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21652\\3366150521.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecent3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmish3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21652\\164341698.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0moutput_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0moutput_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21652\\1252314627.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21652\\1252314627.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m# todo: cuda is ... not a variable ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         out = F.conv2d(input, weight.cuda(), bias, self.stride,\n\u001b[0m\u001b[0;32m    106\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 1, 3, 3], expected input[1, 32, 16, 16] to have 1 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "class DecentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecentNet, self).__init__()\n",
    "        self.decent1 = DecentLayer(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent2 = DecentLayer(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent3 = DecentLayer(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent1x1 = DecentLayer(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        self.bias1 = torch.nn.InstanceNorm2d(32)\n",
    "        self.bias2 = torch.nn.InstanceNorm2d(64)\n",
    "        self.bias3 = torch.nn.InstanceNorm2d(128)\n",
    "        self.bias1x1 = torch.nn.InstanceNorm2d(10)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x = self.mish1(x)\n",
    "        x = self.bias1(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x = self.mish2(x)\n",
    "        x = self.bias2(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x = self.mish3(x)\n",
    "        x = self.bias3(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        x = self.bias1x1(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if batch_idx == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 1\n",
    "        self.save_model = False\n",
    "        \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = DecentNet().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe389e-2713-4a65-81d2-6bea9d7bbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d27aa-4fc3-4b43-920d-071d3cc464d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1, 0, 5, 2])\n",
    "labels = labels.unsqueeze(0)\n",
    "\n",
    "target = torch.zeros(labels.size(0), 10).scatter_(1, labels, 1.)\n",
    "print(target)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb570b41-9eec-4d5c-9a5c-f74fabb03cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "16*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784a059-0ada-423c-a4c1-112548c099de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv2.importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558d1bb-7c6b-4ad1-a04b-e845e812c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv2.importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3c4a0-af85-4589-9fe9-f997d39a0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.conv2.weight.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95870660-6418-4b85-a673-13e266a53960",
   "metadata": {},
   "source": [
    "# conv filter test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16303538-75ae-4064-ae26-848926552bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one filter\n",
    "\n",
    "w_groups = 8 # groups of the channels\n",
    "w_channels = 2 # input channels\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "# w_groups x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(w_groups,w_channels,3,3))\n",
    "\n",
    "# batch size x groups x width x height\n",
    "out = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "\n",
    "# take the mean of all - we can remove all sorts of information from the out tensor\n",
    "mean = torch.mean(out, 1, keepdim=True)\n",
    "\n",
    "print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "print(weights.shape, \"- groups x channels x kernel x kernel\")\n",
    "print(out.shape, \"- batch x groups x width x height\")\n",
    "print(mean.shape, \"- mean accross the groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5d97e-7408-4c52-8891-864827810c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one filter\n",
    "\n",
    "w_groups = 8 # groups of the channels\n",
    "w_channels = 2 # input channels\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,100,100))\n",
    "\n",
    "# w_groups x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(w_channels,3,3))\n",
    "\n",
    "print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "print(weights.shape, \"- groups x channels x kernel x kernel\")\n",
    "\n",
    "try:\n",
    "    # batch size x groups x width x height\n",
    "    out = F.conv2d(inputs, weights, groups=w_channels)\n",
    "    # take the mean of all - we can remove all sorts of information from the out tensor\n",
    "    mean = torch.mean(out, 1, keepdim=True)\n",
    "    \n",
    "    print(out.shape, \"- batch x groups x width x height\")\n",
    "    print(mean.shape, \"- mean accross the groups\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3d443-372f-4548-804e-11efbebe6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = torch.randn(8, 4, 3, 3)\n",
    "inputs = torch.randn(1, 4, 5, 5)\n",
    "F.conv2d(inputs, filters, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bf7b8-35d2-4f68-b045-0ba93bd4c5fc",
   "metadata": {},
   "source": [
    "# Visualise filters and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536067b-6098-4844-9ef4-7a5db5a23156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net() # .to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320d57b-3e22-404f-ad6c-b04515fe2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "def visChannels(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    \n",
    "    plt.figure(figsize=(nrow,rows) )\n",
    "    plt.title(f\"Channels with index {ch}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "def visFilters(tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.title(f\"Filter {filt}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "def visFilters_subplot(subplot, tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    subplot.set_title(f\"Filter {filt+1} with {c} channels\")\n",
    "    subplot.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "    subplot.axis('off')\n",
    "    \n",
    "layer = 1\n",
    "filter = model.conv2.weight.data.clone()\n",
    "\n",
    "print(model.conv2.weight.shape)\n",
    "\n",
    "# need to match the network parameters!!!!\n",
    "in_channels = 5\n",
    "out_filters = 3 # 64\n",
    "\n",
    "\n",
    "fig, subplot = plt.subplots(out_filters, figsize=(10, 10))\n",
    "fig.suptitle(f'Layer with shape {list(model.conv2.weight.shape)} [out, in, kernel, kernel]')\n",
    "\n",
    "for filt in range(0, out_filters):\n",
    "    \n",
    "    visFilters_subplot(subplot[filt], filter, filt=filt, allkernels=False)\n",
    "\n",
    "    #plt.axis('off')\n",
    "    #plt.ioff()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"example_results/filter_with_weights.png\")\n",
    "plt.show()\n",
    "    \n",
    "if False:    \n",
    "    for filt in range(0, out_filters):\n",
    "\n",
    "        visFilters(filter, filt=filt, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"examples/example_results/filter_with_weights.png\")\n",
    "        plt.show()\n",
    "\n",
    "    for ch in range(0, in_channels):\n",
    "\n",
    "        visChannels(filter, ch=ch, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791fa94-5bb8-4d3a-ad97-72cc5a59025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "res = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655d64a-7fc0-42c7-880e-69e7be6acedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.layer1[0].conv1.bias == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3322395-f2a0-40e1-a0e8-86c090586f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.layer1[0].conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d52543-cd67-4d00-963a-5d1effbfc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.extra_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77091394-6e75-4aa3-84c3-a9ebc7ce932e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
