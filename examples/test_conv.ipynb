{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DecentNet from conv layer\n",
    "\n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "     \n",
    "* pruning actually doesn\"t work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46441962-88b7-4cd6-bd4a-a71a6fbbd427",
   "metadata": {},
   "source": [
    "pip install:\n",
    "    pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../helper', './helper', '/helper', 'helper', 'C:\\\\Users\\\\Christina\\\\Documents\\\\datasceyence\\\\examples', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\python39.zip', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\DLLs', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy', '', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\pixelssl-0.1.4-py3.9.egg', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\Pythonwin']\n",
      "\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "# from torch.nn import init\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from typing import Optional, List, Tuple, Union\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "sys.path.insert(0, \"/helper\")\n",
    "sys.path.insert(0, \"./helper\")\n",
    "sys.path.insert(0, \"../helper\")\n",
    "print(sys.path)\n",
    "\n",
    "# own module\n",
    "from visualisation.feature_map import *\n",
    "\n",
    "import random\n",
    "\n",
    "print()\n",
    "print(\"cuda available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7f1ab-1129-436d-aed0-5643651c84a0",
   "metadata": {},
   "source": [
    "# Conv experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38836e4-6905-4e0c-b344-bcc3b8094388",
   "metadata": {},
   "source": [
    "## conv2d layer (slightly adapted original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b972ff66-723c-43a1-a9d9-80c43b450efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConvNd(torch.nn.Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    in_channels: int\n",
    "    _reversed_padding_repeated_twice: List[int]\n",
    "    out_channels: int\n",
    "    kernel_size: Tuple[int, ...]\n",
    "    stride: Tuple[int, ...]\n",
    "    padding: Union[str, Tuple[int, ...]]\n",
    "    dilation: Tuple[int, ...]\n",
    "    transposed: bool\n",
    "    output_padding: Tuple[int, ...]\n",
    "    groups: int\n",
    "    padding_mode: str\n",
    "    weight: Tensor\n",
    "    bias: Optional[Tensor]\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Tuple[int, ...],\n",
    "                 stride: Tuple[int, ...],\n",
    "                 padding: Tuple[int, ...],\n",
    "                 dilation: Tuple[int, ...],\n",
    "                 transposed: bool,\n",
    "                 output_padding: Tuple[int, ...],\n",
    "                 groups: int,\n",
    "                 bias: bool,\n",
    "                 padding_mode: str,\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        print(factory_kwargs)\n",
    "        super().__init__()\n",
    "        if groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            #self.importance = Parameter(torch.empty(\n",
    "            #    (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            #self.importance = Parameter(torch.empty(\n",
    "            #    (out_channels, in_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "class CustomConv2d(_ConvNd):\n",
    "    \n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = stride #_pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super().__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        print(self.stride)\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369256cb-1ea4-4ff2-8ce5-a820511e0779",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2823e218-5eb3-44d9-a277-2cf17b772c84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CustomConv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv2 = CustomConv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv3 = CustomConv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv1x1 = CustomConv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "        self.K = 100 \n",
    "        self.L = 10 # last one\n",
    "        self.num_of_bases = 1 # 3rd dim\n",
    "        \n",
    "        if False:\n",
    "            self.conv1 = Conv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv2 = Conv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv3 = Conv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv1x1 = Conv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "        # 4x16384\n",
    "        # self.fc1 = nn.Linear(10*10*10, 10)\n",
    "        #self.fc2 = nn.Linear(10, 10)\n",
    "        \n",
    "        #self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        #self.sub_concept_pooling = nn.modules.MaxPool2d((self.K, 1), stride=(1,1))\n",
    "        #self.instance_pooling = nn.modules.MaxPool2d((opt.num_of_bases, 1), stride=(1,1))\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.mish1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.mish2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.mish3(x)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        #print(x.size())\n",
    "        #print(x.size()[2:])\n",
    "        \n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        #x = x.view(-1, self.L, self.K, 10)\n",
    "        \n",
    "        # input, kernel_size, stride, padding, dilation, ceil_mode\n",
    "        #x = self.sub_concept_pooling(x).view(-1, self.L, self.num_of_bases).permute(0,2,1).unsqueeze(1)\n",
    "        \n",
    "        # output = F.sigmoid(x)\n",
    "        # x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        #x = torch.flatten(x, 1)\n",
    "        # x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.fc2(x)\n",
    "        #output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76fa05-b47e-4264-85cf-766d8ca060d3",
   "metadata": {},
   "source": [
    "## normal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0819306e-ff93-4f38-898b-4c1cd2221b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for i_batch, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if i_batch == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i_batch * len(data), len(train_loader.dataset),\n",
    "                100. * i_batch / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            \"\"\"\n",
    "            if i == 0 and epoch % args.log_interval == 0:\n",
    "            # if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot(path=f\"example_results/feature_map_{epoch}.png\")\n",
    "                \"\"\"\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.1\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 5\n",
    "        self.save_model = True\n",
    "        \n",
    "\n",
    "def main_train():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader, epoch)\n",
    "        #scheduler.step()\n",
    "        \n",
    "        \n",
    "        if args.save_model and epoch % args.log_interval == 0:\n",
    "            torch.save(model.state_dict(), f\"example_results/mnist_cnn_{epoch}.ckpt\")\n",
    "\n",
    "\n",
    "def main_test():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "\n",
    "    if True:\n",
    "        model.load_state_dict(torch.load(\"example_results/mnist_cnn_5.ckpt\"))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(\"example_results/pruned_model.ckpt\"))\n",
    "    \n",
    "\n",
    "    # model = torch.load(model.state_dict(), \"example_results/mnist_cnn_30.ckpt\")\n",
    "    if False:\n",
    "        test(args, model, device, test_loader, 0)\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38eb5e24-3f20-4788-8344-66a98d6791bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1242e1a-c691-4cb6-a279-9904cedcd806",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_to_prune= main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270319e6-3827-47cd-a579-476c89217968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(model_to_prune.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {},
   "source": [
    "# DecentNet trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db53f551-b504-444f-b318-762eb857195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[1, 0, 5, 2]])\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor([1, 0, 5, 2])\n",
    "labels = labels.unsqueeze(0)\n",
    "\n",
    "target = torch.zeros(labels.size(0), 10).scatter_(1, labels, 1.)\n",
    "print(target)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a387d2d9-2247-4330-9fb7-c44aa0a2322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common pair at indices [1, 0]: (2, 2), (4, 4)\n",
      "Common pair at indices [2, 5]: (4, 4), (8, 8)\n",
      "[1 2]\n",
      "[0 5]\n"
     ]
    }
   ],
   "source": [
    "# Two lists of data\n",
    "ms_in = [1, 2, 4]\n",
    "ns_in = [2, 4, 8]\n",
    "\n",
    "ms_x = [2, 2, 2, 5, 9, 4]\n",
    "ns = [4, 2, 2, 3, 6, 8]\n",
    "\n",
    "# Find the indices (IDs) of pairs that exist in both lists\n",
    "common_pairs = [[f, x] for f, (item1, item2) in enumerate(zip(ms_in, ns_in)) for x, (item3, item4) in enumerate(zip(ms_x, ns)) if (item1==item3 and item2==item4)]\n",
    "\n",
    "# Print the common pairs\n",
    "for pair in common_pairs:\n",
    "    print(f\"Common pair at indices {pair}: {ms_in[pair[0]], ms_x[pair[1]]}, {ns_in[pair[0]], ns[pair[1]]}\")\n",
    "    \n",
    "a = np.array(common_pairs)\n",
    "f_ids = a[:,0]\n",
    "x_ids = a[:,1]\n",
    "\n",
    "print(f_ids)\n",
    "print(x_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        \n",
    "        self.ms_x = ms_x # list of integers\n",
    "        self.ns_x = ns_x # list of integers\n",
    "        self.data = data # list of tensors\n",
    "                \n",
    "    def set(self, data, ms_x, ns_x):\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        self.data = data\n",
    "    \n",
    "    def get(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        # amout of channels need to have same length as m and n lists\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    \n",
    "    \n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff8f8ab-976a-4199-974f-9b1cb93c1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 5 == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfa123-54e1-4053-94c3-52b45ec0859c",
   "metadata": {},
   "source": [
    "## DecentFilter\n",
    "* conv2d problem: https://stackoverflow.com/questions/61269421/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # convolution happens in here\n",
    "    \n",
    "    def __init__(self, ms_in, ns_in, m_this, n_this,\n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 padding_mode=\"zeros\",\n",
    "                 dilation=3, \n",
    "                 # transposed=None, \n",
    "                 device=None, \n",
    "                 dtype=None):\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        \n",
    "        # padding\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        \n",
    "         \n",
    "        # convolution\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding_mode = padding_mode\n",
    "        self.padding = padding\n",
    "        self.dilation = _pair(dilation)\n",
    "        #self.transposed = transposed\n",
    "        \n",
    "        \n",
    "        # weights\n",
    "        assert len(ms_in) == len(ns_in), \"ms_in and ns_in are not of same length\"\n",
    "        self.n_weights = len(ms_in)\n",
    "        \n",
    "        # position\n",
    "        self.ms_in = ms_in # list\n",
    "        self.ns_in = ns_in # list\n",
    "        self.m_this = m_this # single integer\n",
    "        self.n_this = n_this # single integer\n",
    "        \n",
    "        # weight\n",
    "        # filters x channels x kernel x kernel\n",
    "        # self.weights = torch.autograd.Variable(torch.randn(1,n_weights,*self.kernel_size)).to(\"cuda\")\n",
    "        # self.weights = torch.nn.Parameter(torch.randn(1,n_weights,*self.kernel_size))\n",
    "        self.weights = torch.nn.Parameter(torch.empty((1, self.n_weights, *self.kernel_size), **factory_kwargs))\n",
    "            \n",
    "        # bias    \n",
    "        if False: \n",
    "            # bias:\n",
    "            # where should the bias be???\n",
    "            self.bias = Parameter(torch.empty(1, **factory_kwargs))\n",
    "        else:\n",
    "            #self.bias = False\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # reset weights and bias in filter\n",
    "        self.reset_parameters()\n",
    "            \n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*self.kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        torch.nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))        \n",
    "        \n",
    "    def forward(self, x:X) -> Tensor:\n",
    "        \n",
    "        # weights = 1 filter x channels x kernel x kernel\n",
    "        # x = batch x channels x width x height\n",
    "\n",
    "        # Find the indices (IDs) of pairs that exist in both lists\n",
    "        common_pairs = [[i_in, i_x] for i_in, (m_in, n_in) in enumerate(zip(self.ms_in, self.ns_in)) for i_x, (m_x, n_x) in enumerate(zip(x.ms_x, x.ns_x)) if (m_in==m_x and n_in==n_x)]\n",
    "        \n",
    "        if False:\n",
    "            print(common_pairs)\n",
    "            print(len(self.ms_in))\n",
    "            print(len(self.ns_in))\n",
    "            print(len(x.ms_x))\n",
    "            print(len(x.ns_x))\n",
    "\n",
    "            for pair in common_pairs:\n",
    "                print(f\"Common pair at indices {pair}: {self.ms_in[pair[0]], tmp_ms[pair[1]]}, {self.ns_in[pair[0]], tmp_ns[pair[1]]}\")\n",
    "        \n",
    "        common_pairs_a = np.array(common_pairs)\n",
    "        try:\n",
    "            f_ids = common_pairs_a[:,0]\n",
    "            x_ids = common_pairs_a[:,1]\n",
    "        except Exception as e:\n",
    "            print(common_pairs_a)\n",
    "            print(common_pairs_a.shape)\n",
    "            print(len(self.ms_in))\n",
    "            print(len(self.ns_in))\n",
    "            print(len(x.ms_x))\n",
    "            print(len(x.ns_x))\n",
    "            print(e)\n",
    "        \n",
    "        # filter data and weights based on common pairs of data and weights\n",
    "        tmp_x = x.data[:, x_ids, :, :]\n",
    "        tmp_w = self.weights[:, f_ids, :, :]\n",
    "        \n",
    "        if self.padding_mode != 'zeros':\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(F.pad(tmp_x, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            tmp_w, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, groups=1)\n",
    "        else:\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(tmp_x, tmp_w, self.bias, self.stride, self.padding, self.dilation, groups=1)\n",
    "        \n",
    "        # print(x_data.shape, \"- batch x filters x width x height\")        \n",
    "        return x_data\n",
    "    \n",
    "    \"\"\"\n",
    "    def set_position_and_value(self, value, m_this, n_this):\n",
    "        self.weights = value # weights in this filter\n",
    "        self.m_this = m_this # single integer\n",
    "        self.n_this = n_this # single integer\n",
    "    \n",
    "    def get_position_and_value(self):\n",
    "        return self.weights, self.m_this, self.n_this\n",
    "    \"\"\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'DecentFilter(weights: ' + str(self.weights.shape) + ' at position: m_this=' + str(self.m_this) + ', n_this=' + str(self.n_this) + ')' + \\\n",
    "    '\\n with inputs: ms_in= ' + ', '.join(str(m.item()) for m in self.ms_in) + ', ns_in= ' + ', '.join(str(n.item()) for n in self.ns_in) + ')'\n",
    "    __repr__ = __str__\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdab4-e0b0-4523-882f-dad3ebf06090",
   "metadata": {},
   "source": [
    "## DecentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fddb74c7-5940-424d-aab8-2c75efd914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLayer(torch.nn.Module):\n",
    "    __constants__ = ['stride', 'padding', 'dilation', # 'groups',\n",
    "                     'padding_mode', # 'n_channels', #  'output_padding', # 'n_filters',\n",
    "                     'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "                \n",
    "    def __init__(self, ms_in:list, ns_in:list, n_filters:int,\n",
    "                 kernel_size: _size_2_t,  \n",
    "                 stride: _size_2_t = 1,  \n",
    "                 padding: Union[str, _size_2_t] = 0,  \n",
    "                 dilation: _size_2_t = 1,\n",
    "                 #transposed: bool = False, \n",
    "                 grid_size:int=81,\n",
    "                 #output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 #groups: int = 1,\n",
    "                 bias: bool = True,  # not in use\n",
    "                 padding_mode: str = \"zeros\",  # not in use\n",
    "                 device=None,  # not in use\n",
    "                 dtype=None) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # position\n",
    "        self.ms_in = ms_in\n",
    "        self.ns_in = ns_in\n",
    "        \n",
    "        grid_sqrt = math.sqrt(grid_size)\n",
    "        assert grid_sqrt == int(grid_sqrt), f\"square root ({grid_sqrt}) from grid size {grid_size} not possible; possible exampes: 81 (9*9), 144 (12*12)\"\n",
    "        grid_sqrt = int(grid_sqrt)\n",
    "        \n",
    "        # use techniques from coo matrix\n",
    "        self.geometry_array = np.full(grid_size, np.nan)\n",
    "        # plus 1 here cause of to_sparse array\n",
    "        self.geometry_array[0:n_filters] = range(1,n_filters+1)\n",
    "        np.random.shuffle(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.reshape((grid_sqrt,grid_sqrt), order='C')\n",
    "        self.geometry_array = torch.tensor(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.to_sparse(sparse_dim=2).to(\"cuda\")\n",
    "\n",
    "        #print(self.geometry_array)\n",
    "        #print(self.geometry_array.values())\n",
    "\n",
    "        self.filter_list = torch.nn.ModuleList([])\n",
    "        for i_filter in range(n_filters):\n",
    "            # minus 1 here cause of to_sparse array\n",
    "            index = (self.geometry_array.values()-1 == i_filter).nonzero(as_tuple=True)[0]\n",
    "            m_this = self.geometry_array.indices()[0][index]\n",
    "            n_this = self.geometry_array.indices()[1][index]\n",
    "            f = DecentFilter(ms_in, ns_in, m_this, n_this, \n",
    "                             kernel_size=kernel_size, \n",
    "                             stride=stride, padding=padding, dilation=dilation)\n",
    "            self.filter_list.append(f)\n",
    "            # self.register_parameter(f\"filter {i_filter}\", f.weights)\n",
    "            \n",
    "            #torch.nn.Parameter(torch.empty((1, n_channels, *kernel_size), **factory_kwargs))\n",
    "    \n",
    "    def compute_connection_cost(self) -> list:\n",
    "        # compute connection cost for loss and swapping\n",
    "        # adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        # def get_cc(self, weight_factor=2.0, bias_penalize=True, no_penalize_last=False):\n",
    "        # https://stackoverflow.com/questions/74086766/how-to-find-total-cost-of-each-path-in-graph-using-dictionary-in-python\n",
    "        cc = [0.0]\n",
    "        \n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        num_linear = len(self.linears)\n",
    "        for i in range(num_linear):\n",
    "            if i == num_linear - 1 and no_penalize_last:\n",
    "                weight_factor = 0.\n",
    "            biolinear = self.linears[i]\n",
    "            dist = torch.sum(torch.abs(biolinear.out_coordinates.unsqueeze(dim=1) - biolinear.in_coordinates.unsqueeze(dim=0)),dim=2)\n",
    "            cc += torch.mean(torch.abs(biolinear.linear.weight)*(weight_factor*dist+self.l0))\n",
    "            if bias_penalize == True:\n",
    "                cc += torch.mean(torch.abs(biolinear.linear.bias)*(self.l0))\n",
    "        if self.token_embedding:\n",
    "            cc += torch.mean(torch.abs(self.embedding)*(self.l0))\n",
    "            #pass\n",
    "        \"\"\"\n",
    "        return cc\n",
    "    \n",
    "    def compute_importance_metric(self):\n",
    "        \n",
    "        im = 0\n",
    "        \n",
    "        return im\n",
    "    \n",
    "    def prune(self) -> None:\n",
    "        \n",
    "        print(\"prune here\")\n",
    "        \n",
    "        for f in self.filter_list:\n",
    "            # f.update()\n",
    "            \n",
    "            # only keep \"the best\" weights\n",
    "            keep_ids = random.sample(range(0, 8), 5) # this is actually the keep ids rn\n",
    "            f.weights = torch.nn.Parameter(f.weights[:, keep_ids, :, :])\n",
    "            f.ms_in = [f.ms_in[i] for i in keep_ids] # self.ms_in[remove_ids]\n",
    "            f.ns_in = [f.ns_in[i] for i in keep_ids] # self.ns_in[remove_ids]\n",
    "        \n",
    "        \n",
    "        # pruning based on a metric\n",
    "        \n",
    "        # delete layer with id\n",
    "        # delete channels in each layer with id\n",
    "        \n",
    "        # channel deactivation\n",
    "        # require_grad = False/True for each channel\n",
    "        #deactivate_ids = [1, 2, 6]\n",
    "        #self.active[deactivate_ids] = False\n",
    "        #print(\"weight\")\n",
    "        #print(self.weight.shape)\n",
    "        #print(self.weight[:,self.active,:,:].shape)\n",
    "        # this is totally wrong - iterative will break after first iteration\n",
    "        #print()\n",
    "        # Good to hear it’s working, although I would think you’ll get an error at some point in your code, as the cuda() call creates a non-leaf tensor.\n",
    "        #self.weight = torch.nn.Parameter(  self.weight[:,self.active,:,:] ) # .detach().cpu().numpy()\n",
    "        #self.weight = self.weight.cuda()\n",
    "        #print(self.weight.shape)\n",
    "        #print(self.active)\n",
    "        \n",
    "    def swap(self):\n",
    "    \n",
    "        # change positions\n",
    "        # change\n",
    "        \n",
    "        print(\"swap here\")\n",
    "        \n",
    "        self.m_this = self.m_this # single integer\n",
    "        self.n_this = self.n_this # single integer\n",
    "    \n",
    "    \"\"\"\n",
    "    def reset_parameters(self) -> None:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def extra_repr(self):\n",
    "        \n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        # return s.format(**self.__dict__)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "    \"\"\"       \n",
    "\n",
    "        \n",
    "    def forward(self, x: X) -> Tensor:\n",
    "        \n",
    "        # calculate output for each filter\n",
    "        output_list = []\n",
    "        m_list = []\n",
    "        n_list = []\n",
    "        for f in self.filter_list:\n",
    "            # output = filter(input)\n",
    "            output_list.append(f(x))\n",
    "            m_list.append(f.m_this)\n",
    "            n_list.append(f.n_this)\n",
    "        x.ms_x = m_list\n",
    "        x.ns_x = n_list\n",
    "        x.data = torch.cat(output_list, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \"\"\"\n",
    "    def get_filter_positions(self):\n",
    "        \n",
    "        ms_this = []\n",
    "        ns_this = []\n",
    "        for f in self.filter_list:\n",
    "            ms_this.append(f.m_this)\n",
    "            ns_this.append(f.n_this)\n",
    "        \n",
    "        return ms_this, ns_this\n",
    "    \"\"\"\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d986320e-2189-4edf-a7c8-1de1af933a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import torchvision\n",
    "    tmp = torchvision.models.squeezenet1_0(weights=torchvision.models.SqueezeNet1_0_Weights.IMAGENET1K_V1)\n",
    "    tmp.classifier[1] = torch.nn.Conv2d(512, 10, (3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c186cf1-c7db-45da-b779-f7ab88f3896f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentNet(nn.Module):\n",
    "    def __init__(self, out_dim:list=[1, 16, 32, 64], n_classes:int=10, grid_size:int=81) -> None:\n",
    "        super(DecentNet, self).__init__()\n",
    "        \n",
    "        out_dim.append(n_classes)\n",
    "        #out_dim = [1, 32, 48, 64, 10]       \n",
    "        assert not any(i > grid_size for i in out_dim), f\"filters need to be less than {grid_size}\"\n",
    "        \n",
    "        # backbone\n",
    "        \n",
    "        ms_in_1 = [torch.tensor(0)]\n",
    "        ns_in_1 = [torch.tensor(0)]\n",
    "        assert out_dim[0] == len(ms_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ms_in_1={len(ms_in_1)})\"\n",
    "        assert out_dim[0] == len(ns_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ns_in_1={len(ns_in_1)})\"\n",
    "        self.decent1 = DecentLayer(ms_in=ms_in_1, ns_in=ns_in_1, n_filters=out_dim[1], kernel_size=3, stride=1, padding=0, dilation=1, grid_size=grid_size)\n",
    "        \n",
    "        ms_in_2,ns_in_2 = self.decent1.get_filter_positions()\n",
    "        assert out_dim[1] == len(ms_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ms_in_2={len(ms_in_2)})\"\n",
    "        assert out_dim[1] == len(ns_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ns_in_2={len(ns_in_2)})\"\n",
    "        self.decent2 = DecentLayer(ms_in=ms_in_2, ns_in=ns_in_2, n_filters=out_dim[2], kernel_size=3, stride=1, padding=0, dilation=1, grid_size=grid_size)\n",
    "        \n",
    "        ms_in_3,ns_in_3 = self.decent2.get_filter_positions()\n",
    "        assert out_dim[2] == len(ms_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ms_in_3={len(ms_in_3)})\"\n",
    "        assert out_dim[2] == len(ns_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ns_in_3={len(ns_in_3)})\"\n",
    "        self.decent3 = DecentLayer(ms_in=ms_in_3, ns_in=ns_in_3, n_filters=out_dim[3], kernel_size=3, stride=1, padding=0, dilation=1, grid_size=grid_size)\n",
    "        \n",
    "        ms_in_1x1,ns_in_1x1 = self.decent3.get_filter_positions()\n",
    "        assert out_dim[3] == len(ms_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ms_in_1x1={len(ms_in_1x1)})\"\n",
    "        assert out_dim[3] == len(ns_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ns_in_1x1={len(ns_in_1x1)})\"\n",
    "        self.decent1x1 = DecentLayer(ms_in=ms_in_1x1, ns_in=ns_in_1x1, n_filters=out_dim[-1], kernel_size=1, stride=1, padding=0, dilation=1, grid_size=grid_size)\n",
    "        \n",
    "        #self.tmp = torchvision.models.squeezenet1_0(torchvision.models.SqueezeNet1_0_Weights.IMAGENET1K_V1)\n",
    "        #self.tmp.classifier[1] = torch.nn.Conv2d(512, 10, kernel_size=(3,3))\n",
    "        \n",
    "        # head\n",
    "        self.fc = torch.nn.Linear(out_dim[-1], out_dim[-1])\n",
    "    \n",
    "        # activation\n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        # bias\n",
    "        self.bias1 = torch.nn.InstanceNorm2d(out_dim[1])\n",
    "        self.bias2 = torch.nn.InstanceNorm2d(out_dim[2])\n",
    "        self.bias3 = torch.nn.InstanceNorm2d(out_dim[3])\n",
    "        self.bias1x1 = torch.nn.InstanceNorm2d(out_dim[-1])\n",
    "        \n",
    "        # activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        # global max pooling for MIL\n",
    "        x.data = F.max_pool2d(x.data, kernel_size=x.data.size()[2:])\n",
    "        \n",
    "        x.data = x.data.reshape(x.data.size(0), -1)\n",
    "        x.data = self.fc(x.data) \n",
    "        \n",
    "        # x.data = self.sigmoid(x.data)\n",
    "        \n",
    "        # x.data = self.tmp(x.data)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    def update(self):\n",
    "        #self.decent1.update()\n",
    "        self.decent2.update()\n",
    "        self.decent3.update()\n",
    "        self.decent1x1.update()\n",
    "        \n",
    "        # measurement for updating\n",
    "        \n",
    "        # update layer by layer\n",
    "        \n",
    "    def swapping(self):\n",
    "        # adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        pass\n",
    "        \n",
    "    def compute_connection_cost(self):\n",
    "        # mean of connection cost\n",
    "        cc = []\n",
    "        \n",
    "        cc.append(self.decent2.compute_connection_cost())\n",
    "        cc.append(self.decent3.compute_connection_cost())\n",
    "        cc.append(self.decent1x1.compute_connection_cost())\n",
    "        \n",
    "        return np.mean(cc)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f2c55a-0f86-423d-97fb-aa1502f4fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecentNet(n_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3988da38-6bbe-4665-b6a1-dd682020435f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X(data: torch.Size([5, 2, 30, 30]) at positions: ms_x= 5, 7, ns_x= 3, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.autograd.Variable(torch.randn(5, 2, 30, 30)) # batch x channel x width x height\n",
    "# dense_input.shape\n",
    "\n",
    "# todo: ms need to have same size as channel\n",
    "\n",
    "X(tmp, [torch.tensor(5), torch.tensor(7)], [torch.tensor(3), torch.tensor(12)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "### Lightning version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "import os\n",
    "import torchmetrics\n",
    "\n",
    "class DecentLightning(pl.LightningModule):\n",
    "    def __init__(self, kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.grid_size = model_kwargs[\"grid_size\"]\n",
    "        self.out_dim = model_kwargs[\"out_dim\"]\n",
    "        \n",
    "        self.model = DecentNet(n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim).to(\"cuda\")\n",
    "        \n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "        self.f1score = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        print(\"configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimiser = optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimiser, milestones=[100,150], gamma=0.1)\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        else:\n",
    "            optimiser = optim.SGD(self.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        \n",
    "        print(\"* EPOCH START * \" * 50)\n",
    "        \n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            \n",
    "            print(self.model)\n",
    "            self.model.update()\n",
    "\n",
    "            print(\"*\"*50)\n",
    "            print(\"*\"*50)\n",
    "            print(\"*\"*50)\n",
    "            print(\"model is updated now\")\n",
    "            print(\"*\"*50)\n",
    "            print(\"*\"*50)\n",
    "            print(\"*\"*50)\n",
    "        \n",
    "            print(self.model)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # 1       \n",
    "        loss = self._loss_n_metrics(batch, mode=\"train\")\n",
    "        if batch_idx < 5:\n",
    "            print(\"training_step\", batch_idx)\n",
    "        #loss = torch.tensor(1)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # 2\n",
    "        self._loss_n_metrics(batch, mode=\"val\")\n",
    "        if batch_idx < 5:\n",
    "            print(\"validation_step\", batch_idx)\n",
    "        #self._loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        print(\"on_validation_epoch_end\")\n",
    "        # 3\n",
    "        \n",
    "        \"\"\"\n",
    "        for parameter in self.parameters():\n",
    "            print(\"parameter\")\n",
    "            print(parameter)\n",
    "        \"\"\" \n",
    "        #return\n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        print(\"on_train_epoch_end\")\n",
    "        # 4\n",
    "        pass\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._loss_n_metrics(batch, mode=\"test\")\n",
    "        if batch_idx < 5:\n",
    "            print(\"test_step\", batch_idx)\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        print(\"on_test_epoch_end\")\n",
    "        # 3\n",
    "        \n",
    "        \"\"\"\n",
    "        for parameter in self.parameters():\n",
    "            print(\"parameter\")\n",
    "            print(parameter)\n",
    "        \"\"\" \n",
    "        #return\n",
    "        \n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def _loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img) # cause of the forward function\n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth.unsqueeze(1).size(0), self.n_classes).scatter_(1, ground_truth.unsqueeze(1).to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \n",
    "        loss = self.criterion(model_output, ground_truth) # ground_truth_multi_hot)\n",
    "        cc = torch.mean(self.model.compute_connection_cost()) * self.cc_weight\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "        \n",
    "        #print(model_output)\n",
    "        #print(pred_i)\n",
    "        #print(ground_truth)\n",
    "        \n",
    "        acc = self.accuracy(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "        f1score = self.f1score(preds=pred_i, target=ground_truth) \n",
    "        \n",
    "        if random.randint(1, 50) == 5:\n",
    "            print()\n",
    "            print(\"p\", pred_i)\n",
    "            print(\"g\", ground_truth)\n",
    "            print(\"a\", acc)\n",
    "            print(\"f\", f1score)\n",
    "            print(\"l\", loss)\n",
    "        \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_f1score', f1score, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa8cae21-08fe-4e44-a1ea-a7ae8d12ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset = datasets.MNIST('example_data', train=True, download=True, transform=transform)\n",
    "val_set = datasets.MNIST('example_data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(len(val_set))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def dev_routine(**kwargs):\n",
    "    \n",
    "    print(\"train kwargs\", kwargs['train_kwargs'])\n",
    "    print(\"model kwargs\", kwargs['model_kwargs'])\n",
    "    \n",
    "    train_kwargs = kwargs['train_kwargs']\n",
    "    \n",
    "    transform=transforms.Compose([\n",
    "        #transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(size=train_kwargs[\"img_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.MNIST('example_data', train=False, download=True, transform=transform)\n",
    "    # val_set = datasets.MNIST('example_data', train=False, transform=transform)\n",
    "    \n",
    "    print(len(dataset))\n",
    "    \n",
    "    # Split the indices in a stratified way\n",
    "    indices = np.arange(len(dataset))\n",
    "    train_indices, val_indices = train_test_split(indices, train_size=0.8, test_size=0.2, stratify=dataset.targets)\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_subset, shuffle=True, batch_size=train_kwargs[\"batch_size\"], num_workers=16)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_subset, shuffle=False, batch_size=train_kwargs[\"batch_size\"], num_workers=16)\n",
    "    \n",
    "    logger = CSVLogger(\"example_results/lightning_logs\", name=\"tmp_exp\")\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(train_kwargs[\"ckpt_path\"], \"example_results\"),\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         log_every_n_steps=5,\n",
    "                         logger=logger,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         max_epochs=train_kwargs[\"epochs\"],\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1score\"),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    \n",
    "    \n",
    "    \n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(train_kwargs[\"ckpt_path\"], \"DecentNet.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = DecentLightning.load_from_checkpoint(pretrained_filename, kwargs=kwargs) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(19) # To be reproducable\n",
    "                \n",
    "        # Initialize the LightningModule and LightningDataModule\n",
    "        model = DecentLightning(kwargs)\n",
    "        \n",
    "\n",
    "        # Train the model using a Trainer\n",
    "        trainer.fit(model, train_dataloader, val_dataloader)\n",
    "        \n",
    "        # we don't save the positions here ...\n",
    "        # model = DecentLightning.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, kwargs=kwargs) # Load best checkpoint after training\n",
    "\n",
    "        \n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_dataloader, verbose=False)\n",
    "    # test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test on valset\": val_result[0][\"test_acc\"]}\n",
    "    \n",
    "    print(result)\n",
    "\n",
    "    return model, result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39ad8b29-48de-4f62-88da-381f60d698f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'epochs': 20, 'img_size': 96, 'batch_size': 16, 'ckpt_path': '', 'device': 'cuda'}\n",
      "model kwargs {'n_classes': 10, 'out_dim': [1, 32, 64, 96], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 0.2, 'update_every_nth_epoch': 5}\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the kwargs:  {'model_kwargs': {'n_classes': 10, 'out_dim': [1, 32, 64, 96], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 0.2, 'update_every_nth_epoch': 5}, 'train_kwargs': {'epochs': 20, 'img_size': 96, 'batch_size': 16, 'ckpt_path': '', 'device': 'cuda'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | DecentNet          | 75.1 K\n",
      "1 | criterion | CrossEntropyLoss   | 0     \n",
      "2 | accuracy  | MulticlassAccuracy | 0     \n",
      "3 | f1score   | MulticlassF1Score  | 0     \n",
      "-------------------------------------------------\n",
      "75.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.1 K    Total params\n",
      "0.300     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configure_optimizers\n",
      "Sanity Checking: |                                                                               | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                              | 0/2 [00:00<?, ?it/s]validation_step 0\n",
      "Sanity Checking DataLoader 0:  50%|███████████████████████████                           | 1/2 [00:39<00:39,  0.03it/s]validation_step 1\n",
      "Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████████████| 2/2 [01:18<00:00,  0.03it/s]on_validation_epoch_end\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                 | 0/500 [00:00<?, ?it/s]* EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * * EPOCH START * \n",
      "training_step 0\n",
      "Epoch 0:   0%|                                                             | 1/500 [00:42<5:55:24,  0.02it/s, v_num=39]training_step 1\n",
      "Epoch 0:   0%|▏                                                            | 2/500 [01:14<5:10:50,  0.03it/s, v_num=39]training_step 2\n",
      "Epoch 0:   1%|▎                                                            | 3/500 [01:51<5:06:59,  0.03it/s, v_num=39]training_step 3\n",
      "Epoch 0:   1%|▍                                                            | 4/500 [02:23<4:56:09,  0.03it/s, v_num=39]training_step 4\n",
      "Epoch 0:   5%|███▏                                                        | 27/500 [15:27<4:30:41,  0.03it/s, v_num=39]\n",
      "p tensor([9, 2, 1, 9, 9, 2, 9, 2, 2, 2, 2, 2, 2, 2, 9, 9], device='cuda:0')\n",
      "g tensor([2, 9, 1, 3, 3, 9, 6, 4, 0, 3, 0, 8, 4, 6, 2, 0], device='cuda:0')\n",
      "a tensor(0.0625, device='cuda:0')\n",
      "f tensor(0.0625, device='cuda:0')\n",
      "l tensor(2.1527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:   7%|████                                                        | 34/500 [19:32<4:27:51,  0.03it/s, v_num=39]\n",
      "p tensor([2, 4, 9, 2, 1, 2, 9, 3, 9, 9, 3, 2, 4, 9, 9, 1], device='cuda:0')\n",
      "g tensor([9, 4, 3, 9, 4, 9, 8, 2, 2, 2, 3, 6, 0, 2, 2, 4], device='cuda:0')\n",
      "a tensor(0.1250, device='cuda:0')\n",
      "f tensor(0.1250, device='cuda:0')\n",
      "l tensor(2.1767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  22%|████████████▍                                            | 109/500 [1:02:58<3:45:53,  0.03it/s, v_num=39]\n",
      "p tensor([8, 8, 3, 1, 3, 8, 8, 3, 8, 3, 3, 3, 8, 3, 1, 3], device='cuda:0')\n",
      "g tensor([6, 6, 8, 1, 5, 6, 8, 3, 0, 8, 6, 2, 7, 5, 4, 5], device='cuda:0')\n",
      "a tensor(0.1875, device='cuda:0')\n",
      "f tensor(0.1875, device='cuda:0')\n",
      "l tensor(2.0222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  23%|█████████████▎                                           | 117/500 [1:07:37<3:41:22,  0.03it/s, v_num=39]\n",
      "p tensor([6, 3, 0, 3, 3, 1, 3, 3, 2, 3, 6, 3, 6, 3, 3, 6], device='cuda:0')\n",
      "g tensor([6, 3, 4, 5, 7, 1, 9, 5, 2, 4, 7, 3, 6, 5, 0, 0], device='cuda:0')\n",
      "a tensor(0.3750, device='cuda:0')\n",
      "f tensor(0.3750, device='cuda:0')\n",
      "l tensor(1.9870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  25%|██████████████▍                                          | 127/500 [1:13:27<3:35:46,  0.03it/s, v_num=39]\n",
      "p tensor([6, 9, 6, 6, 9, 1, 4, 4, 1, 9, 9, 1, 9, 6, 1, 1], device='cuda:0')\n",
      "g tensor([4, 6, 4, 3, 2, 1, 6, 4, 4, 6, 2, 1, 9, 0, 1, 1], device='cuda:0')\n",
      "a tensor(0.3750, device='cuda:0')\n",
      "f tensor(0.3750, device='cuda:0')\n",
      "l tensor(1.5634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  28%|███████████████▊                                         | 139/500 [1:20:31<3:29:08,  0.03it/s, v_num=39]\n",
      "p tensor([8, 8, 3, 3, 3, 1, 3, 3, 3, 8, 3, 4, 3, 8, 8, 8], device='cuda:0')\n",
      "g tensor([2, 0, 5, 5, 2, 1, 2, 9, 2, 6, 5, 4, 7, 0, 6, 9], device='cuda:0')\n",
      "a tensor(0.1250, device='cuda:0')\n",
      "f tensor(0.1250, device='cuda:0')\n",
      "l tensor(2.0972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  33%|██████████████████▉                                      | 166/500 [1:36:33<3:14:16,  0.03it/s, v_num=39]\n",
      "p tensor([2, 0, 2, 7, 2, 4, 9, 3, 9, 4, 2, 4, 8, 4, 8, 8], device='cuda:0')\n",
      "g tensor([6, 0, 5, 7, 0, 4, 7, 3, 7, 7, 5, 7, 4, 4, 0, 0], device='cuda:0')\n",
      "a tensor(0.3125, device='cuda:0')\n",
      "f tensor(0.3125, device='cuda:0')\n",
      "l tensor(1.9405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  55%|███████████████████████████████▏                         | 274/500 [2:41:39<2:13:20,  0.03it/s, v_num=39]\n",
      "p tensor([7, 7, 2, 9, 7, 2, 1, 9, 9, 2, 2, 7, 2, 6, 7, 9], device='cuda:0')\n",
      "g tensor([5, 9, 7, 9, 5, 6, 4, 2, 3, 3, 2, 9, 3, 2, 9, 4], device='cuda:0')\n",
      "a tensor(0.1250, device='cuda:0')\n",
      "f tensor(0.1250, device='cuda:0')\n",
      "l tensor(1.9209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  60%|██████████████████████████████████▍                      | 302/500 [2:58:04<1:56:45,  0.03it/s, v_num=39]\n",
      "p tensor([4, 4, 0, 0, 1, 0, 4, 1, 0, 2, 4, 0, 4, 0, 2, 3], device='cuda:0')\n",
      "g tensor([9, 6, 0, 0, 1, 2, 4, 1, 8, 2, 7, 0, 7, 0, 0, 2], device='cuda:0')\n",
      "a tensor(0.5000, device='cuda:0')\n",
      "f tensor(0.5000, device='cuda:0')\n",
      "l tensor(1.4838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  68%|██████████████████████████████████████▊                  | 341/500 [3:21:17<1:33:51,  0.03it/s, v_num=39]\n",
      "p tensor([9, 9, 8, 6, 5, 8, 4, 6, 4, 6, 3, 5, 6, 4, 1, 6], device='cuda:0')\n",
      "g tensor([7, 9, 8, 6, 5, 8, 9, 8, 9, 3, 2, 2, 0, 4, 1, 8], device='cuda:0')\n",
      "a tensor(0.4375, device='cuda:0')\n",
      "f tensor(0.4375, device='cuda:0')\n",
<<<<<<< HEAD
      "l tensor(1.6676, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd35ffb848814a999168c94e22f066c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_step 0\n",
      "test_step 1\n",
      "test_step 2\n",
      "test_step 3\n",
      "test_step 4\n",
      "tensor([7, 1, 6, 2, 7, 7, 6, 2, 7, 6, 7, 2, 7, 2, 2, 1], device='cuda:0')\n",
      "a tensor(0.3750, device='cuda:0')\n",
      "f tensor(0.3750, device='cuda:0')\n",
      "l tensor(1.8961, device='cuda:0')\n",
      "tensor([7, 2, 2, 2, 7, 7, 7, 2, 7, 7, 2, 2, 7, 1, 1, 6], device='cuda:0')\n",
      "a tensor(0.2500, device='cuda:0')\n",
      "f tensor(0.2500, device='cuda:0')\n",
      "l tensor(1.9321, device='cuda:0')\n",
      "tensor([1, 7, 6, 2, 7, 2, 6, 7, 7, 2, 7, 2, 7, 7, 2, 7], device='cuda:0')\n",
      "a tensor(0.2500, device='cuda:0')\n",
      "f tensor(0.2500, device='cuda:0')\n",
      "l tensor(2.0330, device='cuda:0')\n",
      "tensor([7, 7, 7, 7, 6, 2, 2, 2, 7, 7, 7, 2, 2, 7, 2, 7], device='cuda:0')\n",
      "a tensor(0.1250, device='cuda:0')\n",
      "f tensor(0.1250, device='cuda:0')\n",
      "l tensor(2.2159, device='cuda:0')\n",
      "tensor([7, 7, 7, 6, 2, 7, 2, 6, 6, 1, 7, 7, 2, 7, 7, 7], device='cuda:0')\n",
      "a tensor(0.1875, device='cuda:0')\n",
      "f tensor(0.1875, device='cuda:0')\n",
      "l tensor(1.9914, device='cuda:0')\n",
      "tensor([7, 7, 1, 2, 7, 1, 6, 7, 7, 2, 1, 1, 7, 2, 7, 7], device='cuda:0')\n",
      "a tensor(0.3125, device='cuda:0')\n",
      "f tensor(0.3125, device='cuda:0')\n",
      "l tensor(2.0284, device='cuda:0')\n",
      "tensor([7, 7, 2, 7, 7, 2, 7, 7, 7, 1, 1, 7, 2, 7, 7, 2], device='cuda:0')\n",
      "a tensor(0.1250, device='cuda:0')\n",
      "f tensor(0.1250, device='cuda:0')\n",
      "l tensor(2.2315, device='cuda:0')\n",
      "tensor([2, 2, 7, 7, 6, 1, 6, 6, 7, 2, 1, 7, 7, 7, 7, 2], device='cuda:0')\n",
      "a tensor(0.3125, device='cuda:0')\n",
      "f tensor(0.3125, device='cuda:0')\n",
      "l tensor(1.8556, device='cuda:0')\n",
      "tensor([7, 7, 6, 6, 1, 7, 7, 2, 2, 1, 7, 7, 1, 6, 2, 7], device='cuda:0')\n",
      "a tensor(0.3125, device='cuda:0')\n",
      "f tensor(0.3125, device='cuda:0')\n",
      "l tensor(1.7857, device='cuda:0')\n",
      "tensor([2, 1, 7, 2, 7, 2, 2, 6, 7, 7, 7, 1, 2, 6, 7, 1], device='cuda:0')\n",
      "a tensor(0.1875, device='cuda:0')\n",
      "f tensor(0.1875, device='cuda:0')\n",
      "l tensor(2.0410, device='cuda:0')\n",
      "tensor([7, 1, 7, 6, 7, 6, 1, 2, 7, 7, 1, 7, 7, 2, 2, 2], device='cuda:0')\n",
      "a tensor(0.1250, device='cuda:0')\n",
      "f tensor(0.1250, device='cuda:0')\n",
      "l tensor(2.1620, device='cuda:0')\n",
      "tensor([2, 2, 1, 6, 7, 2, 2, 2, 6, 7, 7, 7, 7, 7, 2, 6], device='cuda:0')\n",
      "a tensor(0.1250, device='cuda:0')\n",
      "f tensor(0.1250, device='cuda:0')\n",
      "l tensor(2.2020, device='cuda:0')\n",
      "tensor([1, 7, 8, 6, 7, 7, 2, 1, 1, 7, 6, 7, 2, 7, 7, 2], device='cuda:0')\n",
      "a tensor(0.1875, device='cuda:0')\n",
      "f tensor(0.1875, device='cuda:0')\n",
      "l tensor(2.0784, device='cuda:0')\n",
      "tensor([7, 1, 7, 7, 7, 8, 7, 1, 2, 2, 2, 2, 2, 7, 6, 7], device='cuda:0')\n",
      "a tensor(0.3125, device='cuda:0')\n",
      "f tensor(0.3125, device='cuda:0')\n",
      "l tensor(1.9631, device='cuda:0')\n",
      "tensor([7, 6, 7, 1, 7, 1, 1, 2, 2, 1, 6, 7, 7, 1, 7, 1], device='cuda:0')\n",
      "a tensor(0.2500, device='cuda:0')\n",
      "f tensor(0.2500, device='cuda:0')\n",
      "l tensor(1.7685, device='cuda:0')\n",
      "tensor([2, 1, 1, 2, 2, 2, 2, 2, 6, 2, 7, 2, 2, 7, 7, 2], device='cuda:0')\n",
      "a tensor(0.1250, device='cuda:0')\n",
      "f tensor(0.1250, device='cuda:0')\n",
      "l tensor(2.0982, device='cuda:0')\n",
      "on_test_epoch_end\n",
      "{'test on valset': 0.26350000500679016}\n"
=======
      "l tensor(1.5649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  75%|██████████████████████████████████████████▌              | 373/500 [3:39:58<1:14:53,  0.03it/s, v_num=39]\n",
      "p tensor([3, 0, 0, 0, 4, 0, 0, 0, 0, 5, 0, 2, 3, 4, 2, 1], device='cuda:0')\n",
      "g tensor([2, 2, 6, 6, 4, 8, 8, 4, 4, 3, 6, 7, 3, 7, 3, 1], device='cuda:0')\n",
      "a tensor(0.1875, device='cuda:0')\n",
      "f tensor(0.1875, device='cuda:0')\n",
      "l tensor(2.0094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  76%|███████████████████████████████████████████              | 378/500 [3:42:58<1:11:57,  0.03it/s, v_num=39]\n",
      "p tensor([3, 0, 9, 9, 9, 9, 2, 9, 9, 4, 0, 9, 1, 0, 9, 0], device='cuda:0')\n",
      "g tensor([3, 8, 8, 4, 7, 7, 0, 0, 0, 4, 8, 0, 1, 5, 4, 4], device='cuda:0')\n",
      "a tensor(0.1875, device='cuda:0')\n",
      "f tensor(0.1875, device='cuda:0')\n",
      "l tensor(1.8344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  77%|███████████████████████████████████████████▉             | 385/500 [3:47:02<1:07:48,  0.03it/s, v_num=39]\n",
      "p tensor([7, 9, 9, 9, 1, 9, 9, 9, 0, 3, 3, 9, 7, 9, 9, 3], device='cuda:0')\n",
      "g tensor([7, 2, 8, 8, 1, 9, 6, 3, 0, 2, 3, 8, 8, 0, 6, 2], device='cuda:0')\n",
      "a tensor(0.3125, device='cuda:0')\n",
      "f tensor(0.3125, device='cuda:0')\n",
      "l tensor(2.0484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 0:  81%|███████████████████████████████████████████████▌           | 403/500 [3:57:48<57:14,  0.03it/s, v_num=39]"
>>>>>>> ddadd58f6d8ca7d62d6fe8215d45e133557733d1
     ]
    }
   ],
   "source": [
    "model, results = dev_routine(model_kwargs={\n",
    "                                'n_classes': 10,\n",
    "                                'out_dim' : [1, 32, 64, 96],\n",
    "                                'grid_size' : 18*18,\n",
    "                                'criterion': torch.nn.CrossEntropyLoss(),# torch.nn.BCEWithLogitsLoss(),\n",
    "                                'optimizer': \"sgd\",\n",
    "                                'base_lr': 0.001,\n",
    "                                'min_lr' : 0.00001,\n",
    "                                'momentum' : 0.9,\n",
    "                                'lr_update' : 100,\n",
    "                                'cc_weight': 0.2,\n",
    "                                'update_every_nth_epoch' : 5,\n",
    "\n",
    "                            },\n",
    "                            train_kwargs={\n",
    "                                'epochs': 20,\n",
    "                                'img_size' : 96,\n",
    "                                'batch_size': 16,\n",
    "                                # 'test_batch_size': 1,\n",
    "                                'ckpt_path': \"\", # not in use??\n",
    "                                'device': \"cuda\"\n",
    "                            }\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa588988-6be6-4f0c-8069-143b87d37bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f4513-419a-433b-97ff-6f54c5a6d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b09e4-644a-4cca-9da3-a47986de1a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfeca4-659f-46e6-bd25-270aa25ae08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca471c2c-819f-44aa-a8f9-e318aacf3f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a04f0-8c5d-44f1-b0b9-616c64341cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79954f6-112e-4036-aa78-1930828cf4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19929df-c8e1-4548-95e4-cd7c807f1b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f4c7f-d650-41fb-b1ef-953fba2e3e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97faa8-afbb-4869-b630-ec3c18936e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd46dc-dcde-48c4-bc34-f9bf5bcad6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb9d4c30-9266-4c29-862b-fffef138cebe",
   "metadata": {},
   "source": [
    "### normal train without lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d89cfe-7561-438c-a0b4-bb3c0c951716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for i_batch, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if i_batch == 5:\n",
    "            \n",
    "            \n",
    "            #model.update()\n",
    "            \n",
    "            #print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            #print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            #print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i_batch * len(data), len(train_loader.dataset),\n",
    "                100. * i_batch / len(train_loader), loss.item()))\n",
    "            \n",
    "            # model.update()\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 1\n",
    "        self.save_model = False\n",
    "        \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = DecentNet().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        model.update() \n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.ckpt\")\n",
    "\n",
    "    return model\n",
    "\n",
    "if False:\n",
    "    model = main()\n",
    "\n",
    "    for i in model.parameters():\n",
    "        print(i.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95870660-6418-4b85-a673-13e266a53960",
   "metadata": {},
   "source": [
    "# conv filter test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7bceb-331c-4f16-be5f-3c5c5d91ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16303538-75ae-4064-ae26-848926552bb7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one filter\n",
    "\n",
    "w_groups = 1 # groups of the channels\n",
    "w_channels = 1024 # input channels\n",
    "w_filters = 10\n",
    "\n",
    "assert w_filters % w_groups == 0\n",
    "assert w_channels % w_groups == 0\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "# w_filters x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "# batch size x w_filters x width x height\n",
    "out = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "print(weights.shape, \"- filters x channels x kernel x kernel\")\n",
    "print(out.shape, \"- batch x filters x width x height\")\n",
    "print()\n",
    "\n",
    "print(\"*\"*50)\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "output_list = []\n",
    "\n",
    "# for each filter, we need different true false vales for our channels\n",
    "active = list(np.random.choice([True, False], size=w_channels, replace=True, p=None))\n",
    "\n",
    "# w_filters x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(1,w_channels,3,3))\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "output_list = []\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    for _ in range(w_filters):\n",
    "        \n",
    "        pass\n",
    "\n",
    "        #print()\n",
    "        #print(active)\n",
    "        #print()\n",
    "        #print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "        #print(weights.shape, \"- filters x channels x kernel x kernel\")\n",
    "\n",
    "\n",
    "        # need to remove weight and input channels according to active list for each filter\n",
    "        #print(inputs.shape)\n",
    "        #print(inputs[:,active,:,:].shape)\n",
    "\n",
    "        #print(weights.shape)\n",
    "        #print(weights[:,active,:,:].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # batch size x w_filters x width x height\n",
    "        \n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            #output_list.append(this_output)\n",
    "        #print(this_output.shape, \"- batch x 1 filter x width x height\")\n",
    "\n",
    "    #out = torch.cat(output_list, dim=1)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    i_tmp = inputs[:,active,:,:]\n",
    "    w_tmp = weights[:,active,:,:]\n",
    "    this_output = F.conv2d(i_tmp, w_tmp, padding=1, groups=w_groups)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weights = torch.autograd.Variable(torch.randn(w_filters,w_channels,3,3))\n",
    "\n",
    "start = time.time()\n",
    "for i in range (10):\n",
    "    this_output = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print(out.shape, \"- batch x filters x width x height\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# take the mean of all - we can remove all sorts of information from the out tensor\n",
    "mean = torch.mean(out, 1, keepdim=True)\n",
    "print(mean.shape, \"- mean accross the filters (no sense here ...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e3c4e-80a1-448f-9b3c-4ae14cd7bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "#b = torch.empty(w_filters, w_channels, dtype=torch.bool)\n",
    "b = torch.ByteTensor(500, w_channels)\n",
    "print(sys.getsizeof(b.storage())) # 1310776 (bytes)\n",
    "\n",
    "#a = torch.empty(w_filters, w_channels, dtype=torch.uint8)\n",
    "a = torch.ByteTensor(500, w_channels)\n",
    "print(sys.getsizeof(a.storage())) # 1310776 (bytes)\n",
    "\n",
    "active = list(np.random.choice([True, False], size=w_channels, replace=True, p=None))\n",
    "print(sys.getsizeof(active)*500) # 1310776 (bytes)\n",
    "\n",
    "weights = torch.FloatTensor(torch.randn(500,w_channels,3,3))\n",
    "print(sys.getsizeof(weights.storage())) # 1310776 (bytes) 36912\n",
    "print(36912*500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2b255-ccc3-4809-948c-cd4dc5efe661",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dadee83-7300-4844-8284-8d0ef29aaee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(2, size=10)\n",
    "\n",
    "list(np.random.choice([True, False], size=10, replace=True, p=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c72946-baf5-4c90-b84e-14f14c8d949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(1, 82, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2754cb-b071-4267-b53c-5f2190a7baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "9*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95feb489-36f2-4d83-81e3-5e21a530f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(a=4, size=2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5d97e-7408-4c52-8891-864827810c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# this is one filter\n",
    "\n",
    "w_groups = 27 # groups of the channels\n",
    "w_channels = 7 # input channels\n",
    "w_filters = 200\n",
    "batch_size = 27\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(batch_size, w_channels*w_groups, 100,100))\n",
    "#inputs = torch.autograd.Variable(torch.randn(w_channels*w_groups, 100,100))\n",
    "\n",
    "# w_groups x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(1, w_channels,3,3))\n",
    "#weights = torch.autograd.Variable(torch.randn(w_groups*w_channels,3,3))\n",
    "\n",
    "print(inputs.shape, \"- batch x groups*channels x width x height\")\n",
    "print(weights.shape, \"- filter 1 x channels x kernel x kernel\")\n",
    "\n",
    "try:\n",
    "    o_list = []\n",
    "    for _ in range(w_filters):\n",
    "        # batch size x groups x width x height\n",
    "        out = F.conv2d(inputs, weights, groups=w_groups)\n",
    "        o_list.append(out)\n",
    "        # take the mean of all - we can remove all sorts of information from the out tensor\n",
    "        #mean = torch.mean(out, 1, keepdim=True)\n",
    "    \n",
    "    print(torch.cat(o_list, dim=1).shape, \"- batch x filters x width x height\")\n",
    "    #print(mean.shape, \"- mean accross the groups\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3d443-372f-4548-804e-11efbebe6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = torch.randn(8, 4, 3, 3)\n",
    "inputs = torch.randn(1, 4, 5, 5)\n",
    "F.conv2d(inputs, filters, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bf7b8-35d2-4f68-b045-0ba93bd4c5fc",
   "metadata": {},
   "source": [
    "# Visualise filters and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536067b-6098-4844-9ef4-7a5db5a23156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net() # .to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320d57b-3e22-404f-ad6c-b04515fe2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "def visChannels(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    \n",
    "    plt.figure(figsize=(nrow,rows) )\n",
    "    plt.title(f\"Channels with index {ch}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "def visFilters(tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.title(f\"Filter {filt}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "def visFilters_subplot(subplot, tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    subplot.set_title(f\"Filter {filt+1} with {c} channels\")\n",
    "    subplot.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "    subplot.axis('off')\n",
    "    \n",
    "layer = 1\n",
    "filter = model.conv2.weight.data.clone()\n",
    "\n",
    "print(model.conv2.weight.shape)\n",
    "\n",
    "# need to match the network parameters!!!!\n",
    "in_channels = 5\n",
    "out_filters = 3 # 64\n",
    "\n",
    "\n",
    "fig, subplot = plt.subplots(out_filters, figsize=(10, 10))\n",
    "fig.suptitle(f'Layer with shape {list(model.conv2.weight.shape)} [out, in, kernel, kernel]')\n",
    "\n",
    "for filt in range(0, out_filters):\n",
    "    \n",
    "    visFilters_subplot(subplot[filt], filter, filt=filt, allkernels=False)\n",
    "\n",
    "    #plt.axis('off')\n",
    "    #plt.ioff()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"example_results/filter_with_weights.png\")\n",
    "plt.show()\n",
    "    \n",
    "if False:    \n",
    "    for filt in range(0, out_filters):\n",
    "\n",
    "        visFilters(filter, filt=filt, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"examples/example_results/filter_with_weights.png\")\n",
    "        plt.show()\n",
    "\n",
    "    for ch in range(0, in_channels):\n",
    "\n",
    "        visChannels(filter, ch=ch, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791fa94-5bb8-4d3a-ad97-72cc5a59025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "res = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655d64a-7fc0-42c7-880e-69e7be6acedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.layer1[0].conv1.bias == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3322395-f2a0-40e1-a0e8-86c090586f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.layer1[0].conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d52543-cd67-4d00-963a-5d1effbfc8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.extra_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77091394-6e75-4aa3-84c3-a9ebc7ce932e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
