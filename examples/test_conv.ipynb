{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DecentNet from conv layer\n",
    "\n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "     \n",
    "* pruning actually doesn\"t work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df9f06-ef7e-4c73-887b-5b2d129ff9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', '../helper', './helper', '/helper', 'helper', 'C:\\\\Users\\\\Christina\\\\Documents\\\\datasceyence\\\\examples', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\python39.zip', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\DLLs', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy', '', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\pixelssl-0.1.4-py3.9.egg', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Christina\\\\anaconda3\\\\envs\\\\chrisy\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from typing import Optional, List, Tuple, Union\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "sys.path.insert(0, \"/helper\")\n",
    "sys.path.insert(0, \"./helper\")\n",
    "sys.path.insert(0, \"../helper\")\n",
    "print(sys.path)\n",
    "\n",
    "# own module\n",
    "from visualisation.feature_map import *\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38836e4-6905-4e0c-b344-bcc3b8094388",
   "metadata": {},
   "source": [
    "# slightly adapted original conv2d layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b972ff66-723c-43a1-a9d9-80c43b450efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConvNd(torch.nn.Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    in_channels: int\n",
    "    _reversed_padding_repeated_twice: List[int]\n",
    "    out_channels: int\n",
    "    kernel_size: Tuple[int, ...]\n",
    "    stride: Tuple[int, ...]\n",
    "    padding: Union[str, Tuple[int, ...]]\n",
    "    dilation: Tuple[int, ...]\n",
    "    transposed: bool\n",
    "    output_padding: Tuple[int, ...]\n",
    "    groups: int\n",
    "    padding_mode: str\n",
    "    weight: Tensor\n",
    "    bias: Optional[Tensor]\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Tuple[int, ...],\n",
    "                 stride: Tuple[int, ...],\n",
    "                 padding: Tuple[int, ...],\n",
    "                 dilation: Tuple[int, ...],\n",
    "                 transposed: bool,\n",
    "                 output_padding: Tuple[int, ...],\n",
    "                 groups: int,\n",
    "                 bias: bool,\n",
    "                 padding_mode: str,\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        print(factory_kwargs)\n",
    "        super().__init__()\n",
    "        if groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "class CustomConv2d(_ConvNd):\n",
    "    \n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = _pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super().__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369256cb-1ea4-4ff2-8ce5-a820511e0779",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2823e218-5eb3-44d9-a277-2cf17b772c84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CustomConv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv2 = CustomConv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv3 = CustomConv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv1x1 = CustomConv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "        self.K = 100 \n",
    "        self.L = 10 # last one\n",
    "        self.num_of_bases = 1 # 3rd dim\n",
    "        \n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv3 = Conv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv1x1 = Conv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "        # 4x16384\n",
    "        # self.fc1 = nn.Linear(10*10*10, 10)\n",
    "        #self.fc2 = nn.Linear(10, 10)\n",
    "        \n",
    "        #self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        #self.sub_concept_pooling = nn.modules.MaxPool2d((self.K, 1), stride=(1,1))\n",
    "        #self.instance_pooling = nn.modules.MaxPool2d((opt.num_of_bases, 1), stride=(1,1))\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.mish1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.mish2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.mish3(x)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        #print(x.size())\n",
    "        #print(x.size()[2:])\n",
    "        \n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        #x = x.view(-1, self.L, self.K, 10)\n",
    "        \n",
    "        # input, kernel_size, stride, padding, dilation, ceil_mode\n",
    "        #x = self.sub_concept_pooling(x).view(-1, self.L, self.num_of_bases).permute(0,2,1).unsqueeze(1)\n",
    "        \n",
    "        # output = F.sigmoid(x)\n",
    "        # x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        #x = torch.flatten(x, 1)\n",
    "        # x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.fc2(x)\n",
    "        #output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76fa05-b47e-4264-85cf-766d8ca060d3",
   "metadata": {},
   "source": [
    "# normal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "0819306e-ff93-4f38-898b-4c1cd2221b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if batch_idx == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            \"\"\"\n",
    "            if i == 0 and epoch % args.log_interval == 0:\n",
    "            # if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot(path=f\"example_results/feature_map_{epoch}.png\")\n",
    "                \"\"\"\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.1\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 5\n",
    "        self.save_model = True\n",
    "        \n",
    "\n",
    "def main_train():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader, epoch)\n",
    "        #scheduler.step()\n",
    "        \n",
    "        \n",
    "        if args.save_model and epoch % args.log_interval == 0:\n",
    "            torch.save(model.state_dict(), f\"example_results/mnist_cnn_{epoch}.ckpt\")\n",
    "\n",
    "\n",
    "def main_test():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "\n",
    "    if False:\n",
    "        model.load_state_dict(torch.load(\"example_results/mnist_cnn_30.ckpt\"))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(\"example_results/model_to_prune.ckpt\"))\n",
    "    \n",
    "\n",
    "    # model = torch.load(model.state_dict(), \"example_results/mnist_cnn_30.ckpt\")\n",
    "    test(args, model, device, test_loader, 0)\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "38eb5e24-3f20-4788-8344-66a98d6791bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "b1242e1a-c691-4cb6-a279-9904cedcd806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n",
      "\n",
      "Test set: Average loss: 0.0432, Accuracy: 9504/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_to_prune = main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "270319e6-3827-47cd-a579-476c89217968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(model_to_prune.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1600afd4-2fe5-4ec4-9897-48b4d41e17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "1af5d987-2114-4f96-9e0a-dc4b729a7378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prune.random_unstructured(model_to_prune.conv1, name=\"weight\", amount=0.7)\n",
    "prune.l1_unstructured(model_to_prune.conv1, name=\"weight\", amount=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d925a154-07ff-41e9-a8cd-c8e0aafe1e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(model_to_prune.conv1.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "bc04cfa0-3a7d-4643-860f-1777cb50248b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(203, <torch.nn.utils.prune.PruningContainer object at 0x000001C429F19D00>)])\n"
     ]
    }
   ],
   "source": [
    "print(model_to_prune.conv1._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4ab825b1-26d9-4c29-87ca-156935131a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.remove(model_to_prune.conv1, \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "a13da622-c987-48f6-b786-95dd1ca7601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_prune.state_dict(), f\"example_results/model_to_prune.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0c477705-3082-4f88-803f-86d9b41273d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0000,  0.7362,  0.4725],\n",
       "          [-0.5204, -0.7766, -0.7060],\n",
       "          [ 0.0000,  0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000,  0.0000],\n",
       "          [ 0.7078,  0.6269,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.4803, -0.0000, -0.4314],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.4378,  0.0000,  0.5423]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.4291, -0.7454,  0.0000],\n",
       "          [-0.0000, -0.4654,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.6190, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.4790]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000, -0.7152],\n",
       "          [ 0.0000, -0.8952, -0.5247],\n",
       "          [-0.0000, -0.7241, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.5130,  0.6512],\n",
       "          [-0.5300,  0.0000,  0.0000],\n",
       "          [-0.5174, -0.0000,  0.4737]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.4948],\n",
       "          [-0.4502, -0.4473, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.4564]]],\n",
       "\n",
       "\n",
       "        [[[-0.5282,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.4642],\n",
       "          [ 0.0000,  0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.5566, -0.5031, -0.0000],\n",
       "          [ 0.4453, -0.5906, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6441,  0.0000, -0.0000],\n",
       "          [-0.4744,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.4610],\n",
       "          [ 0.0000,  0.6561,  0.0000],\n",
       "          [-0.5219,  0.4962, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.4707,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.4946],\n",
       "          [ 0.5903,  0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.8117, -0.5877],\n",
       "          [ 0.0000,  0.7187, -0.6652],\n",
       "          [ 0.6927, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.4457, -0.4480, -0.4891],\n",
       "          [ 0.4493,  0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.5310,  0.6294],\n",
       "          [ 0.0000,  0.0000,  0.5025],\n",
       "          [-0.7678, -1.0894, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.7120, -0.0000, -0.0000],\n",
       "          [ 0.6616, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.5943,  0.0000],\n",
       "          [-0.0000, -0.7105, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4872,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000, -0.4606,  0.0000],\n",
       "          [-0.0000, -0.4997,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.6116,  0.6629,  0.0000],\n",
       "          [-0.0000,  0.8441, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.5722]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.4309,  0.0000],\n",
       "          [ 0.0000, -0.5728,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000,  0.4351, -0.5576],\n",
       "          [ 0.0000,  0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.5899,  0.7277],\n",
       "          [-0.7111,  0.0000,  0.5006],\n",
       "          [-0.4759,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5860,  0.5844,  0.0000],\n",
       "          [-0.7281, -0.5939, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.4445, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.4980,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.4543, -0.4890, -0.0000]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_prune.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f6449771-a589-4743-a26c-07f47511c9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 2.4186e-01,  7.3618e-01,  4.7251e-01],\n",
       "          [-5.2043e-01, -7.7660e-01, -7.0596e-01],\n",
       "          [ 3.7320e-01,  1.3328e-01, -3.3667e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.6025e-02, -2.0193e-01,  2.0153e-01],\n",
       "          [ 7.0779e-01,  6.2687e-01,  3.9265e-01],\n",
       "          [-9.1555e-02, -3.1360e-01,  2.3301e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.8032e-01, -2.4113e-01, -4.3138e-01],\n",
       "          [ 1.5370e-01, -4.7714e-02, -2.6493e-01],\n",
       "          [ 4.3778e-01,  3.2077e-01,  5.4227e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.0896e-01, -3.4886e-01, -2.0324e-01],\n",
       "          [-4.2905e-01, -7.4539e-01,  3.5054e-01],\n",
       "          [-3.2022e-01, -4.6544e-01,  1.6349e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.0838e-01, -1.6769e-01,  3.3082e-01],\n",
       "          [ 4.0189e-01, -6.1903e-01, -3.5406e-01],\n",
       "          [ 2.1773e-01,  3.2768e-01,  4.7896e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5843e-01, -1.7253e-01, -7.1522e-01],\n",
       "          [ 3.7553e-01, -8.9519e-01, -5.2467e-01],\n",
       "          [-4.0743e-02, -7.2409e-01, -1.0951e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.4149e-02,  5.1302e-01,  6.5120e-01],\n",
       "          [-5.2999e-01,  1.8953e-01,  1.5452e-01],\n",
       "          [-5.1736e-01, -2.3589e-01,  4.7370e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.4441e-01, -5.3504e-02, -4.9480e-01],\n",
       "          [-4.5016e-01, -4.4731e-01, -6.7081e-02],\n",
       "          [-2.7409e-01, -1.3875e-03,  4.5638e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.2816e-01,  8.8850e-02,  2.6822e-01],\n",
       "          [ 1.5490e-01,  2.1510e-01,  4.6420e-01],\n",
       "          [ 2.1217e-01,  2.7170e-01, -6.9141e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0833e-03,  1.2667e-01, -4.1300e-01],\n",
       "          [ 5.5658e-01, -5.0305e-01, -2.6241e-01],\n",
       "          [ 4.4528e-01, -5.9062e-01, -2.7448e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.9483e-01,  2.1209e-01, -5.3132e-02],\n",
       "          [ 8.4825e-02, -1.5591e-03, -3.1838e-01],\n",
       "          [-2.3624e-01, -2.7763e-01, -2.9481e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 6.4410e-01,  1.4123e-01, -1.0535e-01],\n",
       "          [-4.7441e-01,  7.2677e-02,  9.6528e-02],\n",
       "          [-7.3577e-02, -3.5120e-01,  9.2869e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.6607e-01, -2.7512e-01, -4.6100e-01],\n",
       "          [ 2.8602e-01,  6.5608e-01,  3.2578e-01],\n",
       "          [-5.2191e-01,  4.9623e-01, -1.5431e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0809e-01, -3.6785e-01,  1.2889e-01],\n",
       "          [-7.1608e-04,  2.4447e-01,  1.9967e-01],\n",
       "          [ 4.0826e-01,  4.7066e-01,  1.5429e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.9358e-02, -1.3554e-01,  3.7611e-01],\n",
       "          [-2.6056e-01, -1.1479e-01,  1.9362e-01],\n",
       "          [-3.4330e-01, -1.6546e-01, -1.5632e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 7.4617e-02,  2.4223e-02,  3.3013e-01],\n",
       "          [ 2.7997e-01,  4.0894e-01,  4.9465e-01],\n",
       "          [ 5.9026e-01,  2.1132e-01, -4.2598e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7125e-01,  8.1175e-01, -5.8771e-01],\n",
       "          [ 2.9393e-01,  7.1868e-01, -6.6522e-01],\n",
       "          [ 6.9268e-01, -2.0069e-01, -1.4122e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.4573e-01, -4.4801e-01, -4.8911e-01],\n",
       "          [ 4.4934e-01,  4.2021e-01, -3.1942e-02],\n",
       "          [-2.1181e-01,  5.7009e-03,  1.9500e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.9085e-01,  5.3097e-01,  6.2941e-01],\n",
       "          [ 2.7362e-01,  2.2022e-01,  5.0245e-01],\n",
       "          [-7.6776e-01, -1.0894e+00, -1.2273e-01]]],\n",
       "\n",
       "\n",
       "        [[[-6.2148e-02,  5.8382e-03,  2.8691e-01],\n",
       "          [-7.7284e-02, -3.3317e-01, -1.0102e-01],\n",
       "          [-4.1971e-01, -2.0836e-01,  2.5599e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.0718e-01, -2.7770e-01, -2.4905e-01],\n",
       "          [ 7.1195e-01, -2.7908e-01, -1.6287e-01],\n",
       "          [ 6.6157e-01, -4.7127e-02, -3.3134e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.0560e-01,  5.9434e-01,  3.3114e-01],\n",
       "          [-1.0609e-01, -7.1048e-01, -3.3513e-01],\n",
       "          [ 3.8790e-02,  3.7191e-01, -1.8021e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.8716e-01,  4.2340e-02,  3.7675e-02],\n",
       "          [ 1.1893e-01,  2.6500e-01,  1.0819e-01],\n",
       "          [-4.0266e-01, -3.0247e-01, -3.7209e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0319e-01, -1.3283e-01, -8.7973e-03],\n",
       "          [ 1.6694e-01, -4.6056e-01,  5.0662e-02],\n",
       "          [-8.1928e-02, -4.9969e-01,  2.1884e-01]]],\n",
       "\n",
       "\n",
       "        [[[-6.1156e-01,  6.6289e-01,  3.9887e-01],\n",
       "          [-3.3019e-01,  8.4411e-01, -3.0553e-01],\n",
       "          [-3.8626e-02,  2.8691e-01, -5.7223e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.9281e-02,  1.8275e-01,  1.0650e-01],\n",
       "          [-1.9630e-01, -4.3093e-01,  8.3429e-02],\n",
       "          [ 3.3456e-01, -5.7284e-01,  4.1064e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.7943e-02,  8.3732e-02, -3.2617e-01],\n",
       "          [-2.5700e-01,  4.3513e-01, -5.5759e-01],\n",
       "          [ 6.4217e-02,  1.1302e-01, -4.2022e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.6192e-02,  5.8986e-01,  7.2772e-01],\n",
       "          [-7.1106e-01,  2.8531e-01,  5.0056e-01],\n",
       "          [-4.7587e-01,  7.3999e-02,  2.1478e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.8600e-01,  5.8436e-01,  1.2607e-01],\n",
       "          [-7.2806e-01, -5.9390e-01, -5.7094e-02],\n",
       "          [ 4.1218e-01,  2.1422e-01, -1.8028e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.6545e-01,  1.4350e-01, -1.4049e-02],\n",
       "          [ 6.6276e-02, -3.3666e-01,  7.5568e-02],\n",
       "          [ 4.0217e-01,  7.6532e-02, -1.2675e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.8253e-01, -2.1297e-01, -3.5533e-01],\n",
       "          [-2.6988e-01,  4.4453e-01, -2.5769e-01],\n",
       "          [-3.7181e-01,  1.2004e-01,  2.3297e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.4160e-01,  4.9801e-01,  2.5325e-01],\n",
       "          [ 2.6238e-01,  2.6906e-01, -3.4264e-01],\n",
       "          [ 4.5434e-01, -4.8897e-01, -7.1114e-02]]]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_prune.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e7bfb-1bd1-415d-b2ee-7c7a2fadcab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49bfa123-54e1-4053-94c3-52b45ec0859c",
   "metadata": {},
   "source": [
    "# DecentFilter\n",
    "* conv2d problem: https://stackoverflow.com/questions/61269421/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7b29f-7f27-41db-a0eb-b8ee699149ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # convolution happens in here\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels=32, \n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 padding_mode=\"zeros\",\n",
    "                 dilation=3, \n",
    "                 transposed=None, \n",
    "                 device=None, \n",
    "                 dtype=None):\n",
    "        \n",
    "        \n",
    "        out_channels = 1\n",
    "        groups = 1\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = stride # _pair(stride)\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        \n",
    "        \n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "            \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        #self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "                \n",
    "        \n",
    "        \n",
    "        # print(factory_kwargs)\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        # weight + importance + activat\n",
    "\n",
    "        self.weight = Parameter(torch.empty(\n",
    "            (in_channels, *kernel_size), **factory_kwargs))\n",
    "        self.importance = Parameter(torch.empty(\n",
    "            (in_channels), **factory_kwargs))\n",
    "            \n",
    "        if False: \n",
    "            # bias:\n",
    "            # where should the bias be???\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            #self.bias = False\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        \n",
    "        # reset weights and bias - in filter or in layer?\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        # randomly initialise the positional array\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        \n",
    "        # todo - weight has to be the one in the filter\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    \n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        \n",
    "        print(\"stride\", self.stride)\n",
    "        \n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "    \n",
    "    def update(self):\n",
    "        # channel deactivation\n",
    "        # require_grad = False/True for each channel\n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdab4-e0b0-4523-882f-dad3ebf06090",
   "metadata": {},
   "source": [
    "# DecentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fddb74c7-5940-424d-aab8-2c75efd914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLayer(torch.nn.Module):\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'in_channels', #  'output_padding',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "        \n",
    "        \n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: _size_2_t,\n",
    "                 stride: _size_2_t = 1,\n",
    "                 padding: Union[str, _size_2_t] = 0,\n",
    "                 dilation: _size_2_t = 1,\n",
    "                 transposed: bool = False,\n",
    "                 #output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 #groups: int = 1,\n",
    "                 bias: bool = True,\n",
    "                 padding_mode: str = \"zeros\",\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = stride # _pair(stride)\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.module_list = []\n",
    "        for i_filter in range(out_channels):\n",
    "            self.module_list.append(DecentFilter(in_channels=in_channels))\n",
    "            \n",
    "        \"\"\"\n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" \n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups), **factory_kwargs))\n",
    "        \"\"\" \n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # reset in initialisation\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        pass\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        \n",
    "        # todo - weight has to be the one in the filter\n",
    "        # init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        \"\"\"\n",
    "        not needed due to instance norm layer\n",
    "        https://github.com/pytorch/vision/issues/4914\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\"\"\"\n",
    "\n",
    "    def extra_repr(self):\n",
    "        \"\"\"\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "            \n",
    "            \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \n",
    "        output_list = []\n",
    "        for module in self.module_list:\n",
    "            output_list.append(module(input))\n",
    "            \n",
    "        return output_list\n",
    "        # return self._conv_forward(input, self.weight, self.bias)\n",
    "            \n",
    "    def update(self):\n",
    "        # prunging\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c186cf1-c7db-45da-b779-f7ab88f3896f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected stride to be a single integer value or a list of 1 values to match the convolution dimensions, but got stride=[1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[191], line 182\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39msave_model:\n\u001b[0;32m    178\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist_cnn.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 182\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[191], line 168\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, gamma\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 168\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     test(model, device, test_loader)\n\u001b[0;32m    170\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[191], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     57\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# .to(device)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 59\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m target_multi_hot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mscatter_(\u001b[38;5;241m1\u001b[39m, target, \u001b[38;5;241m1.\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[191], line 25\u001b[0m, in \u001b[0;36mDecentNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecent1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmish1(x)\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias1(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[190], line 128\u001b[0m, in \u001b[0;36mDecentLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    126\u001b[0m output_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_list:\n\u001b[1;32m--> 128\u001b[0m     output_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_list\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[189], line 98\u001b[0m, in \u001b[0;36mDecentFilter.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[189], line 94\u001b[0m, in \u001b[0;36mDecentFilter._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m     90\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m     91\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# this is written in c++ - try not to change ...\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected stride to be a single integer value or a list of 1 values to match the convolution dimensions, but got stride=[1, 1]"
     ]
    }
   ],
   "source": [
    "class DecentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecentNet, self).__init__()\n",
    "        self.decent1 = DecentLayer(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent2 = DecentLayer(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent3 = DecentLayer(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.decent1x1 = DecentLayer(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        self.bias1 = torch.nn.InstanceNorm2d(32)\n",
    "        self.bias2 = torch.nn.InstanceNorm2d(64)\n",
    "        self.bias3 = torch.nn.InstanceNorm2d(128)\n",
    "        self.bias1x1 = torch.nn.InstanceNorm2d(10)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x = self.mish1(x)\n",
    "        x = self.bias1(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x = self.mish2(x)\n",
    "        x = self.bias2(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x = self.mish3(x)\n",
    "        x = self.bias3(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        x = self.bias1x1(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if batch_idx == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 1\n",
    "        self.save_model = False\n",
    "        \n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = DecentNet().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe389e-2713-4a65-81d2-6bea9d7bbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d27aa-4fc3-4b43-920d-071d3cc464d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1, 0, 5, 2])\n",
    "labels = labels.unsqueeze(0)\n",
    "\n",
    "target = torch.zeros(labels.size(0), 10).scatter_(1, labels, 1.)\n",
    "print(target)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b784a059-0ada-423c-a4c1-112548c099de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv2.importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558d1bb-7c6b-4ad1-a04b-e845e812c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv2.importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3c4a0-af85-4589-9fe9-f997d39a0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.conv2.weight.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95870660-6418-4b85-a673-13e266a53960",
   "metadata": {},
   "source": [
    "# conv filter test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16303538-75ae-4064-ae26-848926552bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 16, 100, 100])\n",
      "torch.Size([8, 2, 3, 3])\n",
      "torch.Size([27, 8, 100, 100])\n",
      "torch.Size([27, 1, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "w_groups = 8\n",
    "w_channels = 2\n",
    "\n",
    "# batch size x channels (= w_groups*w_channels) x width x height\n",
    "inputs = torch.autograd.Variable(torch.randn(27,w_groups*w_channels,100,100))\n",
    "\n",
    "# w_groups x w_channels x kernel x kernel\n",
    "weights = torch.autograd.Variable(torch.randn(w_groups,w_channels,3,3))\n",
    "\n",
    "# batch size x groups x width x height\n",
    "out = F.conv2d(inputs, weights, padding=1, groups=w_groups)\n",
    "\n",
    "# take the mean of all - we can remove all sorts of information from the out tensor\n",
    "mean = torch.mean(out, 1, keepdim=True)\n",
    "\n",
    "print(inputs.shape)\n",
    "print(weights.shape)\n",
    "print(out.shape)\n",
    "print(mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801bf7b8-35d2-4f68-b045-0ba93bd4c5fc",
   "metadata": {},
   "source": [
    "# Visualise filters and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c536067b-6098-4844-9ef4-7a5db5a23156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net() # .to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1320d57b-3e22-404f-ad6c-b04515fe2fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 3, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "def visChannels(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    \n",
    "    plt.figure(figsize=(nrow,rows) )\n",
    "    plt.title(f\"Channels with index {ch}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "def visFilters(tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.title(f\"Filter {filt}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "def visFilters_subplot(subplot, tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    subplot.set_title(f\"Filter {filt+1} with {c} channels\")\n",
    "    subplot.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "    subplot.axis('off')\n",
    "    \n",
    "layer = 1\n",
    "filter = model.conv2.weight.data.clone()\n",
    "\n",
    "print(model.conv2.weight.shape)\n",
    "\n",
    "# need to match the network parameters!!!!\n",
    "in_channels = 5\n",
    "out_filters = 3 # 64\n",
    "\n",
    "\n",
    "fig, subplot = plt.subplots(out_filters, figsize=(10, 10))\n",
    "fig.suptitle(f'Layer with shape {list(model.conv2.weight.shape)} [out, in, kernel, kernel]')\n",
    "\n",
    "for filt in range(0, out_filters):\n",
    "    \n",
    "    visFilters_subplot(subplot[filt], filter, filt=filt, allkernels=False)\n",
    "\n",
    "    #plt.axis('off')\n",
    "    #plt.ioff()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"example_results/filter_with_weights.png\")\n",
    "plt.show()\n",
    "    \n",
    "if False:    \n",
    "    for filt in range(0, out_filters):\n",
    "\n",
    "        visFilters(filter, filt=filt, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"examples/example_results/filter_with_weights.png\")\n",
    "        plt.show()\n",
    "\n",
    "    for ch in range(0, in_channels):\n",
    "\n",
    "        visChannels(filter, ch=ch, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e791fa94-5bb8-4d3a-ad97-72cc5a59025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Christina/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|| 44.7M/44.7M [00:00<00:00, 105MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "res = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c655d64a-7fc0-42c7-880e-69e7be6acedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.layer1[0].conv1.bias == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e3322395-f2a0-40e1-a0e8-86c090586f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 3, 3])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.layer1[0].conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "97d52543-cd67-4d00-963a-5d1effbfc8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.extra_repr of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.extra_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77091394-6e75-4aa3-84c3-a9ebc7ce932e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
