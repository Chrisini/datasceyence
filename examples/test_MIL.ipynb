{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dde33fa-7d10-40ac-b268-1a8a02155ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multiple-Instance Learning\n",
    "* https://github.com/rhgao/Deep-MIML-Network/blob/master/models/MIML.py\n",
    "* https://github.com/MSKCC-Computational-Pathology/MIL-nature-medicine-2019/blob/master/MIL_train.py\n",
    "* https://github.com/binli123/dsmil-wsi/blob/master/attention_map.py\n",
    "\n",
    "* https://github.com/jusiro/mil_histology/tree/main/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794d3fba-e7ba-4c78-ac88-aa34ae1a0761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6611, 0.7882, 0.5612, 0.4733, 0.2705, 0.1679, 0.6662, 0.8779, 0.8617,\n",
      "         0.4137],\n",
      "        [0.7446, 0.7820, 0.3772, 0.8418, 0.1998, 0.0950, 0.5275, 0.4706, 0.6483,\n",
      "         0.1633],\n",
      "        [0.8233, 0.6053, 0.9176, 0.7274, 0.6315, 0.1594, 0.8142, 0.4319, 0.4438,\n",
      "         0.1718]])\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "tensor(0.9628)\n",
      "tensor(0.9628)\n"
     ]
    }
   ],
   "source": [
    "# MIL Loss\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "batch_size = 3\n",
    "num_classes = 10\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "outputs_before_sigmoid = torch.randn(batch_size, num_classes)\n",
    "sigmoid_outputs = torch.sigmoid(outputs_before_sigmoid)\n",
    "\n",
    "# classes = [[2, 4, 7], [3, 6, 9]]\n",
    "labels = torch.tensor([[1], [9], [4]])\n",
    "# labels = labels.unsqueeze(0)\n",
    "target_classes = torch.zeros(labels.size(0), 10).scatter_(1, labels, 1.)\n",
    "\n",
    "\n",
    "# target_classes = torch.randint(0, 2, (batch_size, num_classes)).to(torch.float32)  # randints in [0, 2).\n",
    "\n",
    "loss = loss_fn(sigmoid_outputs, target_classes)\n",
    "\n",
    "# alternatively, use BCE with logits, on outputs before sigmoid.\n",
    "loss_fn_2 = torch.nn.BCEWithLogitsLoss()\n",
    "loss2 = loss_fn_2(outputs_before_sigmoid, target_classes)\n",
    "\n",
    "print(sigmoid_outputs)\n",
    "\n",
    "print(target_classes)\n",
    "\n",
    "print(loss)\n",
    "print(loss2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22dc30aa-4821-491b-aa6f-637ee1cb66f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.MultiLabelMarginLoss()\n",
    "x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])\n",
    "# for target y, only consider labels 3 and 0, not after label -1\n",
    "y = torch.LongTensor([[3, 0, -1, 1]])\n",
    "# 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))\n",
    "loss(x, y)\n",
    "#tensor(0.85...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829aeff-9343-406d-8c46-f3a114eb48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not my code, never tested it\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from .base_model import BaseModel\n",
    "from . import networks\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "class MIMLModel(BaseModel):\n",
    "    def name(self):\n",
    "        return 'MIMLModel'\n",
    "\n",
    "    def initialize(self, opt):\n",
    "        BaseModel.initialize(self, opt)\n",
    "        self.isTrain = opt.isTrain\n",
    "\n",
    "        if opt.using_multi_labels:\n",
    "            self.label = self.Tensor(opt.batchSize, opt.L)\n",
    "        else:\n",
    "            self.label = self.Tensor(opt.batchSize)\n",
    "        self.bases = self.Tensor(opt.batchSize, opt.F, opt.num_of_bases)\n",
    "\n",
    "        self.BasesNet = networks.BasesNet(opt)\n",
    "        self.sub_concept_pooling = nn.modules.MaxPool2d((opt.K, 1), stride=(1,1))\n",
    "        self.instance_pooling = nn.modules.MaxPool2d((opt.num_of_bases,1), stride=(1,1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        if opt.using_multi_labels:\n",
    "            self.loss = nn.MultiLabelMarginLoss()\n",
    "        else:\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        if(len(opt.gpu_ids)>0):\n",
    "            self.BasesNet.cuda(opt.gpu_ids[0])\n",
    "            self.sub_concept_pooling.cuda(opt.gpu_ids[0])\n",
    "            self.instance_pooling.cuda(opt.gpu_ids[0])\n",
    "            self.softmax.cuda(opt.gpu_ids[0])\n",
    "            self.sigmoid.cuda(opt.gpu_ids[0])\n",
    "            self.loss.cuda(opt.gpu_ids[0])\n",
    "\n",
    "        networks.init_weights(self.BasesNet, self.opt.init_type)\n",
    "\n",
    "        if(self.isTrain):\n",
    "            if opt.using_multi_labels:\n",
    "                self.optimizer = optim.Adam(list(self.BasesNet.parameters()), lr=opt.learning_rate, weight_decay=0.00001)\n",
    "            else:\n",
    "                self.optimizer = optim.Adam(list(self.BasesNet.parameters()), lr=opt.learning_rate, weight_decay=0.00001)\n",
    "        else:\n",
    "            self.BasesNet.eval()\n",
    "\n",
    "        self.batch_loss = []\n",
    "        self.batch_accuracy = []\n",
    "        self.batch_ap = []\n",
    "\n",
    "    def forward(self, input, volatile=False):\n",
    "        bases = input['bases'].unsqueeze(3) #add another dimension for 2D convolution, a trick to replace fc with 1x1conv\n",
    "        label = input['label']\n",
    "        self.bases.resize_(bases.size()).copy_(bases)\n",
    "        self.label.resize_(label.size()).copy_(label)\n",
    "\n",
    "        #print(self.bases.size())\n",
    "        # shape: (batchSize, L*K, num_of_bases)\n",
    "        basesnet_output = self.BasesNet(Variable(self.bases, requires_grad=False, volatile=volatile)).view(-1, self.opt.L, self.opt.K, self.opt.num_of_bases)\n",
    "        #print(\"sub_concept_layer_output:\",basesnet_output.size())\n",
    "        # shape: (batchSize, L, K, num_of_bases)\n",
    "        sub_concept_pooling_output = self.sub_concept_pooling(basesnet_output).view(-1, self.opt.L, self.opt.num_of_bases).permute(0,2,1).unsqueeze(1)\n",
    "        #print(\"sub_concept_pooling_output:\",sub_concept_pooling_output.size())\n",
    "        #softmax\n",
    "        if self.opt.with_softmax:\n",
    "            softmax_normalization_output = self.softmax(sub_concept_pooling_output)\n",
    "            self.output = self.instance_pooling(softmax_normalization_output).view(-1, self.opt.L)\n",
    "        else:\n",
    "            self.output = self.instance_pooling(sub_concept_pooling_output).view(-1, self.opt.L)\n",
    "\n",
    "    def getInstanceLabelRelation(self, input, volatile=True):\n",
    "        bases = input['bases'].unsqueeze(3) #add another dimension for 2D convolution, a trick to replace fc with 1x1conv\n",
    "        label = input['label']\n",
    "        self.bases.resize_(bases.size()).copy_(bases)\n",
    "        self.label.resize_(label.size()).copy_(label)\n",
    "\n",
    "        basesnet_output = self.BasesNet(Variable(self.bases, requires_grad=False, volatile=volatile)).view(-1, self.opt.L, self.opt.K, self.opt.num_of_bases)\n",
    "        instanceLabelRelation = self.sub_concept_pooling(basesnet_output).view(-1, self.opt.L, self.opt.num_of_bases).permute(0,2,1)\n",
    "        if self.opt.using_multi_labels:\n",
    "            self.output = self.instance_pooling(instanceLabelRelation.unsqueeze(1)).view(-1, self.opt.L)\n",
    "            prediction = np.zeros(self.output.size())\n",
    "            gt_label = np.zeros(self.output.size())\n",
    "            max_label = self.output.max(dim=1)[1].data\n",
    "            for i in range(gt_label.shape[0]):\n",
    "                prediction[i,max_label[i]] = 1\n",
    "            prediction[self.softmax(self.output).data.cpu().numpy() >= 0.3] = 1\n",
    "            for index, x in np.ndenumerate(self.label.cpu().numpy()):\n",
    "                if x == -1:\n",
    "                    continue\n",
    "                else:\n",
    "                    gt_label[index[0],int(x)] = 1\n",
    "            return self.softmax(instanceLabelRelation).data.cpu().numpy(), gt_label, prediction\n",
    "        else:\n",
    "            if self.opt.with_softmax:\n",
    "                self.output = self.instance_pooling(self.softmax(instanceLabelRelation).unsqueeze(1)).view(-1, self.opt.L)\n",
    "            else:\n",
    "                self.output = self.instance_pooling(instanceLabelRelation.unsqueeze(1)).view(-1, self.opt.L)\n",
    "            prediction = self.output.max(dim=1)[1].data.float()\n",
    "            return self.softmax(instanceLabelRelation).data.cpu().numpy(), label, prediction\n",
    "\n",
    "    def decrease_learning_rate(self, times, factor):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.opt.learning_rate * pow(factor, times)\n",
    "        print(\"current learning rate:\",self.opt.learning_rate * pow(factor, times))\n",
    "\n",
    "    def backward(self):\n",
    "        if self.opt.using_multi_labels:\n",
    "            label = Variable(self.label, requires_grad=False).long()\n",
    "            prediction = np.zeros(self.output.size())\n",
    "            #construt ground-truth label to compute mAP\n",
    "            gt_label = np.zeros(self.output.size())\n",
    "            max_label = self.output.max(dim=1)[1].data\n",
    "            for i in range(gt_label.shape[0]):\n",
    "                prediction[i,max_label[i]] = 1\n",
    "            prediction[self.softmax(self.output).data.cpu().numpy() >= 0.3] = 1\n",
    "            for index, x in np.ndenumerate(self.label.cpu().numpy()):\n",
    "                if x == -1:\n",
    "                    continue\n",
    "                else:\n",
    "                    gt_label[index[0],int(x)] = 1\n",
    "            ap = average_precision_score(gt_label.T, prediction.T)\n",
    "            self.batch_ap.append(ap)\n",
    "        else:\n",
    "            label = Variable(self.label, requires_grad=False).long()\n",
    "            prediction = self.output.max(dim=1)[1].data.float()\n",
    "            correct = (self.label.eq(prediction)).sum()\n",
    "            accuracy = correct*1.0/self.label.size()[0]\n",
    "            self.batch_accuracy.append(accuracy)\n",
    "        loss = self.loss(self.output, label)\n",
    "        self.batch_loss.append(loss.data[0]) \n",
    "        loss.backward()\n",
    "\n",
    "    def display_train(self, writer, index):\n",
    "        loss = sum(self.batch_loss)/len(self.batch_loss)\n",
    "        writer.add_scalar('data/loss', loss, index)\n",
    "        print('loss: ' + str(loss))\n",
    "        self.batch_loss = []\n",
    "        if self.opt.using_multi_labels:\n",
    "            ap = sum(self.batch_ap)/len(self.batch_ap)\n",
    "            writer.add_scalar('data/mAP', ap, index)\n",
    "            print('mAP: ' + str(ap))\n",
    "            self.batch_ap = []\n",
    "        else:\n",
    "            accuracy = sum(self.batch_accuracy)/len(self.batch_accuracy)\n",
    "            writer.add_scalar('data/accuracy', accuracy, index)\n",
    "            print('accuracy: ' + str(accuracy))\n",
    "            self.batch_accuracy = []\n",
    "\n",
    "    def display_val(self, writer, index, dataset_val):\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        aps = []\n",
    "        for i, val_data in enumerate(dataset_val):\n",
    "            if i >= self.opt.validation_batches:\n",
    "                break\n",
    "            if self.opt.using_multi_labels:\n",
    "                ap, loss = self.test_multi_label(val_data)\n",
    "                aps.append(ap)\n",
    "            else:\n",
    "                accuracy,loss = self.test(val_data)\n",
    "                accuracies.append(accuracy)\n",
    "            losses.append(loss)\n",
    "        if self.opt.using_multi_labels:\n",
    "            ap = sum(aps) / len(aps)\n",
    "            writer.add_scalar('data/val_mAP', ap, index)\n",
    "            print('validation mAP is: ' + str(ap))\n",
    "        else:\n",
    "            accuracy = sum(accuracies)/len(accuracies)\n",
    "            writer.add_scalar('data/val_accuracy', accuracy, index)\n",
    "            print('validation accuracy is: ' + str(accuracy))\n",
    "        loss = sum(losses)/len(losses)\n",
    "        writer.add_scalar('data/val_loss', loss, index)\n",
    "        print('validation loss is: ' + str(loss))\n",
    "\n",
    "    def test(self, input):\n",
    "        self.forward(input, volatile=True)\n",
    "        prediction = self.output.max(dim=1)[1].data.float()\n",
    "        correct = (self.label.eq(prediction)).sum()\n",
    "        accuracy = correct*1.0/self.label.size()[0]\n",
    "        label = Variable(self.label.long(), requires_grad=False)\n",
    "        loss = self.loss(self.output, label).data.cpu().numpy()[0]\n",
    "        return accuracy, loss\n",
    "\n",
    "    def test_multi_label(self, input):\n",
    "        self.forward(input, volatile=True)\n",
    "        prediction = np.zeros(self.output.size())\n",
    "        gt_label = np.zeros(self.output.size())\n",
    "        max_label = self.output.max(dim=1)[1].data\n",
    "        for i in range(gt_label.shape[0]):\n",
    "            prediction[i,max_label[i]] = 1\n",
    "        prediction[self.softmax(self.output).data.cpu().numpy() >= 0.3] = 1\n",
    "        for index, x in np.ndenumerate(self.label.cpu().numpy()):\n",
    "            if x == -1:\n",
    "                continue\n",
    "            else:\n",
    "                gt_label[index[0],int(x)] = 1\n",
    "        ap = average_precision_score(gt_label.T, prediction.T)\n",
    "        label = Variable(self.label, requires_grad=False).long()\n",
    "        loss = self.loss(self.output, label).data.cpu().numpy()[0]\n",
    "        return ap, loss\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb30e3e-1063-4afd-af65-537adbdd3260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab623074-2dfc-4fff-9478-886743427d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f16866-b5fe-4ae6-892c-a3b1b40914e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd706985-d7b3-432a-8619-13f23f0e4e47",
   "metadata": {},
   "source": [
    "# mil_histology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cbecfe-4905-4553-b269-65a6b6a5328e",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a924089-4e7a-4f56-9e53-7a40f095871f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e14b11f670>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# general\n",
    "import os\n",
    "import random\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# image\n",
    "import skimage.transform\n",
    "import skimage.util\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# torch \n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(19)\n",
    "random.seed(19)\n",
    "torch.manual_seed(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61ffeb-4337-467b-85de-9fa8a781c74e",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334a27ff-6de8-46bb-b382-ae256a6af98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MILDataset(object):\n",
    "\n",
    "    def __init__(self, dir_images, data_frame, classes, bag_id='bag_name', input_shape=(3, 224, 224),\n",
    "                 data_augmentation=False, images_on_ram=False, channel_first=True,\n",
    "                 pMIL=False, proportions=None, only_primary=False, dataframe_instances=False):\n",
    "\n",
    "        \"\"\"Dataset object for MIL.\n",
    "            Dataset object which aims to organize images and labels from a dataset in the form of bags.\n",
    "        Args:\n",
    "          dir_images: (h, w, channels)\n",
    "          data_frame: pandas dataframe with ground truth information.\n",
    "                      Each bag is one raw, with 'bag_name' as identifier.\n",
    "          classes: list of classes of interest in data_fame (i.e. ['G3', 'G4', 'G5'])\n",
    "          input_shape: image input shape (channels first).\n",
    "          data_augmentation: whether to perform data augmentation (True) or not (False).\n",
    "          images_on_ram: whether to load images on ram (True) or not (False). Recommended for accelerated training.\n",
    "\n",
    "        Returns:\n",
    "          MILDataset object\n",
    "        Last Updates: Julio Silva (19/03/21)\n",
    "        \"\"\"\n",
    "\n",
    "        'Internal states initialization'\n",
    "        self.dir_images = dir_images\n",
    "        self.data_frame = data_frame\n",
    "        self.classes = classes\n",
    "        self.bag_id = bag_id\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.input_shape = input_shape\n",
    "        self.images_on_ram = images_on_ram\n",
    "        self.channel_first = channel_first\n",
    "        self.pMIL = pMIL\n",
    "        self.proportions = proportions\n",
    "        self.images = os.listdir(dir_images)\n",
    "        self.only_primary = only_primary\n",
    "        self.dataframe_instances = dataframe_instances\n",
    "\n",
    "        # Filter patches whose slide is not in the dataframe\n",
    "        idx = np.in1d([ID.split('_')[0] for ID in self.images], self.data_frame[self.bag_id])\n",
    "        images = [self.images[i] for i in range(self.images.__len__()) if idx[i]]\n",
    "        self.images = images\n",
    "\n",
    "        # Filter slides in the dataframe whose patches are not in the images folder\n",
    "        self.data_frame = self.data_frame[\n",
    "            np.in1d(self.data_frame[self.bag_id], [ID.split('_')[0] for ID in images])]\n",
    "\n",
    "        # Organize bags in the form of dictionary: one key clusters indexes from all instances\n",
    "        self.D = dict()\n",
    "        for i, item in enumerate([ID.split('_')[0] for ID in self.images]):\n",
    "            if item not in self.D:\n",
    "                self.D[item] = [i]\n",
    "            else:\n",
    "                self.D[item].append(i)\n",
    "\n",
    "        self.y = self.data_frame[self.classes].values\n",
    "        self.indexes = np.arange(len(self.images))\n",
    "\n",
    "        if self.pMIL:\n",
    "            self.proportions = self.data_frame[self.proportions].values\n",
    "\n",
    "            self.O = []\n",
    "            for i in np.arange(self.y.shape[0]):\n",
    "                proportions = self.proportions[i, :]\n",
    "                o = np.zeros((1, 3))\n",
    "\n",
    "                if proportions[0] != 0 and proportions[1] != 0:\n",
    "                    if proportions[0] == proportions[1]:\n",
    "                        o[0, proportions[0] - 3] = 1\n",
    "                    else:\n",
    "                        o[0, proportions[0] - 3] = 0.8\n",
    "                        o[0, proportions[1] - 3] = 0.2\n",
    "\n",
    "                o = self.ordering_matrix(o.tolist()[0])\n",
    "                self.O.append(o)\n",
    "\n",
    "        if self.images_on_ram:\n",
    "\n",
    "            # Pre-allocate images\n",
    "            self.X = np.zeros((len(self.indexes), input_shape[0], input_shape[1], input_shape[2]), dtype=np.float32)\n",
    "            self.Yglobal = np.ones((len(self.indexes), len(self.classes) + 1), dtype=np.float32)\n",
    "\n",
    "            if self.dataframe_instances is not False:\n",
    "                self.y_instances = -1 * np.ones((len(self.indexes), 4))\n",
    "\n",
    "            # Load, and normalize images\n",
    "            print('[INFO]: Training on ram: Loading images')\n",
    "            for i in np.arange(len(self.indexes)):\n",
    "                print(str(i) + '/' + str(len(self.indexes)), end='\\r')\n",
    "\n",
    "                ID = self.images[self.indexes[i]]\n",
    "                # Load image\n",
    "                x = Image.open(os.path.join(self.dir_images, ID))\n",
    "                x = np.asarray(x)\n",
    "                # Normalization\n",
    "                x = self.image_normalization(x)\n",
    "                self.X[self.indexes[i], :, :, :] = x\n",
    "                self.Yglobal[self.indexes[i], 1:] = self.data_frame[classes][self.data_frame[self.bag_id] == self.images[i].split('_')[0]]\n",
    "\n",
    "                if self.dataframe_instances is not False:\n",
    "                    idx = np.argwhere(list(self.dataframe_instances['image_name'] == self.images[i]))\n",
    "                    if idx.shape[0] > 0:\n",
    "                        self.y_instances[i, :] = self.dataframe_instances[['NC', 'G3', 'G4', 'G5']].values[idx[0], :]\n",
    "\n",
    "            print('[INFO]: Images loaded')\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.indexes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.images[self.indexes[index]]\n",
    "\n",
    "        if self.images_on_ram:\n",
    "            x = np.squeeze(self.X[self.indexes[index], :, :, :])\n",
    "        else:\n",
    "            # Load image\n",
    "            x = Image.open(os.path.join(self.dir_images, ID))\n",
    "            x = np.asarray(x)\n",
    "            # Normalization\n",
    "            x = self.image_normalization(x)\n",
    "\n",
    "        # data augmentation\n",
    "        if self.data_augmentation:\n",
    "            x_augm = self.image_transformation(x.copy())\n",
    "        else:\n",
    "            x_augm = None\n",
    "\n",
    "        return x, x_augm\n",
    "\n",
    "    def image_transformation(self, img):\n",
    "\n",
    "        if self.channel_first:\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            img = np.fliplr(img)\n",
    "        if random.random() > 0.5:\n",
    "            img = np.flipud(img)\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.random() * 60 - 30\n",
    "            img = skimage.transform.rotate(img, angle)\n",
    "        #if random.random() > 0.5:\n",
    "        #    img = skimage.util.random_noise(img, var=random.random() ** 2)\n",
    "        #if random.random() > 0.5:\n",
    "        #    img = img + random.random() - 0.5\n",
    "        #    img = np.clip(img, 0, 1)\n",
    "\n",
    "        if self.channel_first:\n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def image_normalization(self, x):\n",
    "        # image resize\n",
    "        x = cv2.resize(x, (self.input_shape[1], self.input_shape[2]))\n",
    "        # intensity normalization\n",
    "        x = x / 255.0\n",
    "        # channel first\n",
    "        if self.channel_first:\n",
    "            x = np.transpose(x, (2, 0, 1))\n",
    "        # numeric type\n",
    "        x.astype('float32')\n",
    "        return x\n",
    "\n",
    "    def plot_image(self, x, norm_intensity=False):\n",
    "        # channel first\n",
    "        if self.channel_first:\n",
    "            x = np.transpose(x, (1, 2, 0))\n",
    "        if norm_intensity:\n",
    "            x = x / 255.0\n",
    "\n",
    "        plt.imshow(x)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def cifar10_test_dataset(self, dir_dataset):\n",
    "        files = os.listdir(dir_dataset)\n",
    "        files = [iFile for iFile in files if iFile != 'Thumbs.db']\n",
    "\n",
    "        Y = []\n",
    "        X = []\n",
    "        for iFile in files:\n",
    "            if 'Other' in iFile:\n",
    "                y = 0\n",
    "            else:\n",
    "                y = int(iFile.split('_')[-2][-1])\n",
    "\n",
    "            # Load image\n",
    "            x = Image.open(os.path.join(dir_dataset, iFile))\n",
    "            x = np.asarray(x)\n",
    "            # Normalization\n",
    "            x = self.image_normalization(x)\n",
    "\n",
    "            Y.append(y)\n",
    "            X.append(x)\n",
    "\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "    def ordering_matrix(self, p):\n",
    "\n",
    "        if not self.only_primary:\n",
    "            nRestrictions = len(np.where(np.array(p) > 0)[0])\n",
    "        else:\n",
    "            nRestrictions = len(np.where(np.array(p) > 0)[0]) - 1\n",
    "\n",
    "        if nRestrictions <= 0:\n",
    "            return [np.zeros((1, len(p))), np.zeros((1, len(p)))]\n",
    "\n",
    "        # p: numpy array with proportion of used classes\n",
    "        O = np.zeros((nRestrictions, len(p)))\n",
    "\n",
    "        # Sort proportion values\n",
    "        indexes = np.flip(np.argsort(p))\n",
    "\n",
    "        for i in np.arange(0, nRestrictions):\n",
    "            O[i, indexes[i]] = -1\n",
    "\n",
    "        # p: numpy array with proportion of used classes\n",
    "        if nRestrictions > 1:\n",
    "            O2 = np.zeros((nRestrictions-1, len(p)))\n",
    "            for i in np.arange(0, nRestrictions-1):\n",
    "                O2[i, indexes[i]] = -1\n",
    "                O2[i, indexes[i + 1]] = 1\n",
    "        else:\n",
    "            O2 = np.zeros((1, len(p)))\n",
    "\n",
    "        return [O, O2]\n",
    "\n",
    "\n",
    "class MILDataGenerator(object):\n",
    "\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, max_instances=512):\n",
    "\n",
    "        \"\"\"Data Generator object for MIL.\n",
    "            Process a MIL dataset object to output batches of instances and its respective labels.\n",
    "        Args:\n",
    "          dataset: MIL datasetdataset object.\n",
    "          batch_size: batch size (number of bags). It will be usually set to 1.\n",
    "          shuffle: whether to shuffle the bags (True) or not (False).\n",
    "          max_instances: maximum amount of instances allowed due to computational limitations.\n",
    "\n",
    "        Returns:\n",
    "          MILDataGenerator object\n",
    "        Last Updates: Julio Silva (19/03/21)\n",
    "        \"\"\"\n",
    "\n",
    "        'Internal states initialization'\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.dataset.data_frame))\n",
    "        self.max_instances = max_instances\n",
    "\n",
    "        self._idx = 0\n",
    "        self._reset()\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        N = len(self.indexes)\n",
    "        b = self.batch_size\n",
    "        return N // b + bool(N % b)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "\n",
    "        # If dataset is completed, stop iterator\n",
    "        if self._idx >= len(self.dataset.data_frame):\n",
    "            self._reset()\n",
    "            raise StopIteration()\n",
    "\n",
    "        # Get samples of data frame to use in the batch\n",
    "        df_row = self.dataset.data_frame.iloc[self.indexes[self._idx]]\n",
    "\n",
    "        # Get bag-level label\n",
    "        Y = df_row[self.dataset.classes].to_list()\n",
    "        Y = np.expand_dims(np.array(Y), 0)\n",
    "\n",
    "        # Get ordering matrix\n",
    "        if self.dataset.pMIL:\n",
    "            O = self.dataset.O[self.indexes[self._idx]]\n",
    "\n",
    "        # Select instances from bag\n",
    "        ID = list(df_row[[self.dataset.bag_id]].values)[0]\n",
    "        images_id = self.dataset.D[ID]\n",
    "\n",
    "        # Memory limitation of patches in one slide\n",
    "        if len(images_id) > self.max_instances:\n",
    "            images_id = random.sample(images_id, self.N)\n",
    "        # Minimum number os patches in a slide (by precaution).\n",
    "        if len(images_id) < 4:\n",
    "            images_id.extend(images_id)\n",
    "\n",
    "        self.instances_indexes = images_id\n",
    "\n",
    "        # Load images and include into the batch\n",
    "        X = []\n",
    "        X_augm = []\n",
    "        for i in images_id:\n",
    "            x, x_augm = self.dataset.__getitem__(i)\n",
    "            X.append(x)\n",
    "            X_augm.append(x_augm)\n",
    "\n",
    "        # Update bag index iterator\n",
    "        self._idx += self.batch_size\n",
    "\n",
    "        if self.dataset.pMIL:\n",
    "            if self.dataset.data_augmentation:\n",
    "                return np.array(X).astype('float32'), np.array(Y).astype('float32'), O, np.array(X_augm).astype('float32')\n",
    "            else:\n",
    "                return np.array(X).astype('float32'), np.array(Y).astype('float32'), O, None\n",
    "        else:\n",
    "            if self.dataset.data_augmentation:\n",
    "                return np.array(X).astype('float32'), np.array(Y).astype('float32'), None, np.array(X_augm).astype('float32')\n",
    "            else:\n",
    "                return np.array(X).astype('float32'), np.array(Y).astype('float32'), None, None\n",
    "\n",
    "    def _reset(self):\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indexes)\n",
    "        self._idx = 0\n",
    "\n",
    "\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y, transform=True):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "        self.indexes = np.arange(0, X.shape[0])\n",
    "        self.channel_first = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx, :, :, :]\n",
    "        label = self.Y[idx]\n",
    "        if self.transform:\n",
    "            image = self.image_transformation(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def image_transformation(self, img):\n",
    "\n",
    "        if self.channel_first:\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            img = np.fliplr(img)\n",
    "        if random.random() > 0.5:\n",
    "            img = np.flipud(img)\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.random() * 60 - 30\n",
    "            img = skimage.transform.rotate(img, angle)\n",
    "\n",
    "        if self.channel_first:\n",
    "            img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "class CustomGenerator(object):\n",
    "    def __init__(self, train_dataset, bs, shuffle=True):\n",
    "        self.dataset = train_dataset\n",
    "        self.bs = bs\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = train_dataset.indexes.copy()\n",
    "        self._idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return round(len(self.indexes) / self.bs)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "\n",
    "        if self._idx + self.bs >= len(self.indexes):\n",
    "            self._reset()\n",
    "            raise StopIteration()\n",
    "\n",
    "        # Load images and include into the batch\n",
    "        X, Y = [], []\n",
    "        for i in np.arange(self._idx, self._idx + self.bs):\n",
    "\n",
    "            x, y = self.dataset.__getitem__(self.indexes[i])\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "\n",
    "        self._idx += self.bs\n",
    "\n",
    "        return torch.tensor(np.array(X).astype('float32')), torch.tensor(np.array(Y).astype('float32'))\n",
    "\n",
    "    def _reset(self):\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indexes)\n",
    "        self._idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4367c08-19ce-40a6-be0c-73077657ba1a",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f188323a-6dda-47d7-8a19-3b800bce952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MILArchitecture(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, classes, mode='embedding', aggregation='mean', backbone='VGG19', include_background=False):\n",
    "        super(MILArchitecture, self).__init__()\n",
    "\n",
    "        \"\"\"Data Generator object for MIL.\n",
    "            CNN based architecture for MIL classification.\n",
    "        Args:\n",
    "          classes: \n",
    "          mode:\n",
    "          aggregation: max, mean, attentionMIL, mcAttentionMIL\n",
    "          backbone:\n",
    "          include_background:\n",
    "\n",
    "        Returns:\n",
    "          MILDataGenerator object\n",
    "        Last Updates: Julio Silva (19/03/21)\n",
    "        \"\"\"\n",
    "\n",
    "        'Internal states initialization'\n",
    "\n",
    "        self.classes = classes\n",
    "        self.mode = mode\n",
    "        self.aggregation = aggregation\n",
    "        self.backbone = backbone\n",
    "        self.include_background = include_background\n",
    "        self.C = []\n",
    "        self.prototypical = False\n",
    "\n",
    "        if self.include_background:\n",
    "            self.nClasses = len(classes) + 1\n",
    "        else:\n",
    "            self.nClasses = len(classes)\n",
    "        self.eps = 1e-6\n",
    "\n",
    "        # Backbone\n",
    "        self.bb = Encoder(pretrained=True, backbone=backbone, aggregation=True)\n",
    "        # Classifiers\n",
    "        if self.aggregation == 'mcAttentionMIL':\n",
    "            self.classifiers = torch.nn.ModuleList()\n",
    "            for i in np.arange(0, self.nClasses):\n",
    "                self.classifiers.append(torch.nn.Linear(512, 1))\n",
    "        else:\n",
    "            self.classifier = torch.nn.Linear(512, self.nClasses)\n",
    "        # MIL aggregation\n",
    "        self.milAggregation = MILAggregation(aggregation=aggregation, nClasses=self.nClasses, mode=self.mode)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Patch-Level feature extraction\n",
    "        features = self.bb(images)\n",
    "\n",
    "        if self.mode == 'instance':\n",
    "            # Classification\n",
    "            patch_classification = torch.softmax(self.classifier(torch.squeeze(features)), 1)\n",
    "\n",
    "            # MIL aggregation\n",
    "            global_classification = self.milAggregation(patch_classification)\n",
    "\n",
    "        if self.mode == 'embedding' or self.mode == 'mixed':  # Activation on BCE loss\n",
    "            # Embedding aggregation\n",
    "            if self.aggregation == 'mcAttentionMIL':\n",
    "                embedding, patch_classification = self.milAggregation(torch.squeeze(features))\n",
    "                global_classifications = []\n",
    "                for i in np.arange(0, self.nClasses):\n",
    "                    global_classifications.append(self.classifiers[i](embedding[:, i]))\n",
    "                global_classification = torch.cat(global_classifications, dim=0)\n",
    "            elif self.aggregation == 'attentionMIL':\n",
    "                embedding, w = self.milAggregation(torch.squeeze(features))\n",
    "                global_classification = self.classifier(embedding)\n",
    "                patch_classification = w\n",
    "            else:\n",
    "                embedding = self.milAggregation(torch.squeeze(features))\n",
    "                global_classification = self.classifier(embedding)\n",
    "                patch_classification = self.classifier(torch.squeeze(features))\n",
    "\n",
    "        if self.include_background:\n",
    "            global_classification = global_classification[1:]\n",
    "\n",
    "        return global_classification, patch_classification, features\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained=True, backbone='resnet18', aggregation=False):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.aggregation = aggregation\n",
    "        self.pretrained = pretrained\n",
    "        self.backbone = backbone\n",
    "\n",
    "        if backbone == 'resnet18':\n",
    "            resnet = torchvision.models.resnet18(pretrained=pretrained)\n",
    "            self.F = torch.nn.Sequential(resnet.conv1,\n",
    "                                         resnet.bn1,\n",
    "                                         resnet.relu,\n",
    "                                         resnet.maxpool,\n",
    "                                         resnet.layer1,\n",
    "                                         resnet.layer2,\n",
    "                                         resnet.layer3,\n",
    "                                         resnet.layer4)\n",
    "        elif backbone == 'vgg19':\n",
    "            vgg19 = torchvision.models.vgg16(pretrained=pretrained)\n",
    "            self.F = vgg19.features\n",
    "\n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.F(x)\n",
    "\n",
    "        # register the hook\n",
    "        h = out.register_hook(self.activations_hook)\n",
    "\n",
    "        if self.aggregation:\n",
    "            out = torch.nn.AdaptiveAvgPool2d((1, 1))(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)\n",
    "\n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "\n",
    "class MILAggregation(torch.nn.Module):\n",
    "    def __init__(self, aggregation='mean', nClasses=2, mode='embedding'):\n",
    "        super(MILAggregation, self).__init__()\n",
    "\n",
    "        \"\"\"Aggregation module for MIL.\n",
    "        Args:\n",
    "          aggregation:\n",
    "\n",
    "        Returns:\n",
    "          MILAggregation module for CNN MIL Architecture\n",
    "        Last Updates: Julio Silva (19/03/21)\n",
    "        \"\"\"\n",
    "\n",
    "        self.mode = mode\n",
    "        self.aggregation = aggregation\n",
    "        self.nClasses = nClasses\n",
    "\n",
    "        if self.aggregation == 'attentionMIL':\n",
    "            self.attentionModule = attentionMIL()\n",
    "\n",
    "        if self.aggregation == 'mcAttentionMIL':\n",
    "            self.attentionModules = torch.nn.ModuleList()\n",
    "            for i in np.arange(0, self.nClasses):\n",
    "                self.attentionModules.append(attentionMIL())\n",
    "\n",
    "    def forward(self, feats):\n",
    "\n",
    "        if self.aggregation == 'max':\n",
    "            embedding = torch.max(feats, dim=0)[0]\n",
    "            return embedding\n",
    "        elif self.aggregation == 'mean':\n",
    "            embedding = torch.mean(feats, dim=0)\n",
    "            return embedding\n",
    "        elif self.aggregation == 'attentionMIL':\n",
    "            # Attention embedding from Ilse et al. (2018) for MIL. It only works at the binary scenario at instance-level\n",
    "            embedding, w_logits = self.attentionModule(feats)\n",
    "            return embedding, torch.softmax(w_logits, dim=0)\n",
    "\n",
    "        elif self.aggregation == 'mcAttentionMIL':\n",
    "            attention_weights = []\n",
    "            embeddings = []\n",
    "            for i in np.arange(0, self.nClasses):\n",
    "                embeddings.append(self.attentionModules[i](feats)[0].unsqueeze(1))\n",
    "                attention_weights.append(self.attentionModules[i](feats)[1])\n",
    "            #patch_classification = torch.softmax(torch.cat(attention_weights, 1), 0)\n",
    "            if self.mode == 'embedding':\n",
    "                embedding = torch.cat(embeddings, 1)\n",
    "                patch_classification = torch.softmax(torch.cat(attention_weights, 1), 1)\n",
    "\n",
    "                #w = patch_classification\n",
    "\n",
    "            elif self.mode == 'mixed':\n",
    "\n",
    "                patch_classification = torch.softmax(torch.cat(attention_weights, 1), 0)\n",
    "                w = patch_classification * (1/torch.sum(patch_classification, 0) + 1e-6)\n",
    "\n",
    "                feats = torch.transpose(feats, 1, 0)\n",
    "                embedding = torch.squeeze(torch.mm(feats, w))\n",
    "\n",
    "            return embedding, patch_classification\n",
    "\n",
    "\n",
    "class attentionMIL(torch.nn.Module):\n",
    "    def __init__(self, L=512, D=128, K=1):\n",
    "        super(attentionMIL, self).__init__()\n",
    "\n",
    "        # Attention embedding from Ilse et al. (2018) for MIL. It only works at the binary scenario.\n",
    "\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "        self.attention_V = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.L, self.D),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "        self.attention_U = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.L, self.D),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_weights = torch.nn.Linear(self.D, self.K)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # Attention weights computation\n",
    "        A_V = self.attention_V(feats)  # Attention\n",
    "        A_U = self.attention_U(feats)  # Gate\n",
    "        w_logits = self.attention_weights(A_V * A_U)  # Probabilities - softmax over instances\n",
    "\n",
    "        # Weighted average computation per class\n",
    "        feats = torch.transpose(feats, 1, 0)\n",
    "        embedding = torch.squeeze(torch.mm(feats, torch.softmax(w_logits, dim=0)))  # KxL\n",
    "\n",
    "        return embedding, w_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d922d1c-5b73-4edb-9d8c-649a3433b77f",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe4f5b5-d20e-4205-9599-9363741203da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MILTrainer():\n",
    "    def __init__(self, dir_out, network, lr=1*1e-4, pMIL=False, margin=0, t_ic=10,\n",
    "                 t_pc=10, alpha_ic=1, alpha_pc=1, alpha_ce=1, id='', early_stopping=False,\n",
    "                 scheduler=False, virtual_batch_size=1, criterion='auc', alpha_H=0.01):\n",
    "\n",
    "        self.dir_results = dir_out\n",
    "        if not os.path.isdir(self.dir_results):\n",
    "            os.mkdir(self.dir_results)\n",
    "\n",
    "        # Other\n",
    "        self.best_auc = 0.\n",
    "        self.init_time = 0\n",
    "        self.lr = lr\n",
    "        self.L_epoch = 0\n",
    "        self.L_lc = []\n",
    "        self.Lce_lc_val = []\n",
    "        self.macro_auc_lc_val = []\n",
    "        self.macro_auc_lc_train = []\n",
    "        self.i_epoch = 0\n",
    "        self.epochs = 0\n",
    "        self.i_iteration = 0\n",
    "        self.iterations = 0\n",
    "        self.network = network\n",
    "        self.test_generator = []\n",
    "        self.train_generator = []\n",
    "        self.preds_train = []\n",
    "        self.refs_train = []\n",
    "        self.pMIL = pMIL\n",
    "        self.alpha_ce = alpha_ce\n",
    "        self.best_criterion = 0\n",
    "        self.best_epoch = 0\n",
    "        self.metrics = {}\n",
    "        self.id = id\n",
    "        self.early_stopping = early_stopping\n",
    "        self.scheduler = scheduler\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.constrain_cumpliment_lc = []\n",
    "        self.constrain_proportion_lc = []\n",
    "        self.criterion = criterion\n",
    "        self.alpha_H = alpha_H\n",
    "        self.H_iteration = 0.\n",
    "        self.H_epoch = 0.\n",
    "\n",
    "        # Set optimizers\n",
    "        self.params = list(self.network.parameters())\n",
    "\n",
    "        if self.pMIL:\n",
    "            self.Lp_iteration = 0\n",
    "            self.Lp_epoch = 0\n",
    "            self.Lp_lc = []\n",
    "            self.m = margin\n",
    "            self.t_ic = t_ic\n",
    "            self.t_pc = t_pc\n",
    "            self.alpha_ic = alpha_ic\n",
    "            self.alpha_pc = alpha_pc\n",
    "            self.constrain_cumpliment = 0.\n",
    "            self.constraint_proportion = 0.\n",
    "\n",
    "\n",
    "        self.opt = torch.optim.SGD(self.params, lr=self.lr)\n",
    "        #self.opt = torch.optim.Adam(self.params, lr=self.lr)\n",
    "\n",
    "        # Set losses\n",
    "        if network.mode == 'embedding' or network.mode == 'mixed':\n",
    "            self.L = torch.nn.BCEWithLogitsLoss().cuda()\n",
    "        elif network.mode == 'instance':\n",
    "            self.L = torch.nn.BCELoss().cuda()\n",
    "\n",
    "    def train(self, train_generator, val_generator, test_generator, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.iterations = len(train_generator)\n",
    "        self.train_generator = train_generator\n",
    "        self.val_generator = val_generator\n",
    "        self.test_generator = test_generator\n",
    "        self.preds_train = []\n",
    "        self.refs_train = []\n",
    "\n",
    "        # Move network to gpu\n",
    "        self.network.cuda()\n",
    "\n",
    "        self.init_time = timer()\n",
    "        for i_epoch in range(epochs):\n",
    "            self.i_epoch = i_epoch\n",
    "            # init epoch losses\n",
    "            self.L_epoch = 0\n",
    "            self.Lpc_iteration = 0\n",
    "            self.Lic_iteration = 0\n",
    "            self.Lic_epoch = 0\n",
    "            self.Lpc_epoch = 0\n",
    "            self.H_iteration = 0.\n",
    "            self.H_epoch = 0.\n",
    "            self.constrain_cumpliment_iteration = 0.\n",
    "            self.constrain_cumpliment_epoch = 0.\n",
    "            self.constrain_proportion_epoch = 0.\n",
    "            self.constrain_ic_proportion_epoch = 0.\n",
    "            self.j = 0.\n",
    "            self.jj = 0.\n",
    "            n = 0\n",
    "            nn = 0\n",
    "\n",
    "            if self.scheduler:\n",
    "                if (self.i_epoch + 1) % 50 == 0:\n",
    "                    for g in self.opt.param_groups:\n",
    "                        g['lr'] = self.lr / 2\n",
    "\n",
    "            # Loop over training dataset\n",
    "            print('[Training]: at bag level...')\n",
    "            for self.i_iteration, (X, Y, O, X_augm) in enumerate(self.train_generator):\n",
    "\n",
    "                X = torch.tensor(X).cuda().float()\n",
    "                if X_augm is None:\n",
    "                    X_augm = X\n",
    "                else:\n",
    "                    X_augm = torch.tensor(X_augm).cuda().float()\n",
    "                Y = torch.tensor(Y).cuda().float()\n",
    "\n",
    "                # Set model to training mode and clear gradients\n",
    "                self.network.train()\n",
    "\n",
    "                # Forward network\n",
    "                Yhat, yhat, features = self.network(X_augm)\n",
    "\n",
    "                if self.network.mode == 'instance':\n",
    "                    Yhat = torch.clip(Yhat, min=0.01, max=0.98)\n",
    "\n",
    "                # Estimate losses\n",
    "                Lce = self.L(Yhat, torch.squeeze(Y))\n",
    "\n",
    "                # Update overall losses\n",
    "                L = Lce * self.alpha_ce\n",
    "\n",
    "                if self.alpha_H > 0:\n",
    "                    H = torch.mean(-torch.sum(yhat * torch.log(yhat + 1e-12), dim=(-1)))\n",
    "                    self.H_iteration = H\n",
    "\n",
    "                    L += - self.alpha_H * self.H_iteration\n",
    "\n",
    "                if self.pMIL:\n",
    "\n",
    "                    O_ic = np.array(O[0]).astype('float32')\n",
    "                    O_pc = np.array(O[1]).astype('float32')\n",
    "\n",
    "                    if self.network.include_background:\n",
    "                        yhat = yhat[:, 1:]\n",
    "\n",
    "                    if self.alpha_ic > 0:\n",
    "                        if np.max(np.abs(O_ic)) == 1:\n",
    "                            # Move O matrix to tensor\n",
    "                            Ot = torch.tensor(O_ic).cuda().float()\n",
    "                            # Obtain proportion vector\n",
    "                            P = torch.mean(yhat, 0)\n",
    "                            # Obtain z\n",
    "                            z = torch.matmul(Ot, P.unsqueeze(-1))\n",
    "                            self.Lic_iteration = log_barrier(z, t=self.t_ic).squeeze()\n",
    "                            # Update overall losses\n",
    "                            L += self.alpha_ic * self.Lp_iteration\n",
    "                            n += 1\n",
    "\n",
    "                            self.constrain_ic_proportion_epoch += np.sum(z.cpu().detach().numpy())\n",
    "                            self.jj += 1\n",
    "                        else:\n",
    "                            self.Lic_iteration = torch.tensor(0).cuda()\n",
    "\n",
    "                    if self.alpha_pc > 0:\n",
    "                        if np.max(np.abs(O_pc)) == 1:\n",
    "                            # Move O matrix to tensor\n",
    "                            Ot2 = torch.tensor(O_pc).cuda().float()\n",
    "                            # Obtain proportion vector\n",
    "                            P = torch.mean(yhat, 0)\n",
    "                            # Obtain z\n",
    "                            z = torch.matmul(Ot2, P.unsqueeze(-1)) + self.m\n",
    "                            self.Lpc_iteration = log_barrier(z, t=self.t_pc).squeeze()\n",
    "                            # Update overall losses\n",
    "                            L += self.alpha_pc * self.Lpc_iteration\n",
    "                            nn += 1\n",
    "\n",
    "                            if (z-self.m) < 0:\n",
    "                                self.constrain_cumpliment_iteration = 1\n",
    "                            else:\n",
    "                                self.constrain_cumpliment_iteration = 0\n",
    "                            self.constrain_proportion_epoch += z.cpu().detach().numpy() - self.m\n",
    "                            self.j += 1\n",
    "                        else:\n",
    "                            self.Lpc_iteration = torch.tensor(0).cuda()\n",
    "                            self.constrain_cumpliment_iteration = 0\n",
    "\n",
    "                # Backward gradients\n",
    "                L = L / self.virtual_batch_size\n",
    "                L.backward()\n",
    "\n",
    "                # Update weights and clear gradients\n",
    "                if ((self.i_epoch + 1) % self.virtual_batch_size) == 0:\n",
    "                    self.opt.step()\n",
    "                    self.opt.zero_grad()\n",
    "\n",
    "                ######################################\n",
    "                ## --- Iteration/Epoch end\n",
    "\n",
    "                # Save predictions\n",
    "                self.preds_train.append(Yhat.detach().cpu().numpy())\n",
    "                self.refs_train.append(Y.detach().cpu().numpy())\n",
    "\n",
    "                # Display losses per iteration\n",
    "                self.display_losses(self.i_epoch + 1, self.epochs, self.i_iteration + 1, self.iterations,\n",
    "                                    Lce.cpu().detach().numpy(),\n",
    "                                    end_line='\\r')\n",
    "\n",
    "                # Update epoch's losses\n",
    "                self.L_epoch += Lce.cpu().detach().numpy() / len(self.train_generator)\n",
    "                if self.pMIL and np.max(np.abs(O_ic)) == 1 and self.alpha_ic > 0:\n",
    "                    self.Lic_epoch += self.Lic_iteration.cpu().detach().numpy()\n",
    "                if self.pMIL and np.max(np.abs(O_pc)) == 1 and self.alpha_pc > 0:\n",
    "                    self.Lpc_epoch += self.Lpc_iteration.cpu().detach().numpy()\n",
    "                    self.constrain_cumpliment_epoch += self.constrain_cumpliment_iteration\n",
    "                if self.alpha_H > 0.:\n",
    "                    self.H_epoch += self.H_iteration.cpu().detach().numpy() / len(self.train_generator)\n",
    "\n",
    "            # Epoch-end processes\n",
    "            if self.pMIL and self.alpha_ic > 0:\n",
    "                self.Lic_epoch = self.Lic_epoch / n\n",
    "                self.constrain_ic_proportion_epoch = np.squeeze(self.constrain_ic_proportion_epoch) / self.jj\n",
    "            if self.pMIL and self.alpha_pc > 0:\n",
    "                self.Lpc_epoch = self.Lpc_epoch / nn\n",
    "                self.constrain_cumpliment_epoch = self.constrain_cumpliment_epoch / nn\n",
    "                self.constrain_proportion_epoch = np.squeeze(self.constrain_proportion_epoch) / self.j\n",
    "\n",
    "                self.constrain_cumpliment_lc.append(self.constrain_cumpliment_epoch)\n",
    "                self.constrain_proportion_lc.append(self.constrain_proportion_epoch)\n",
    "\n",
    "            self.on_epoch_end()\n",
    "\n",
    "            if self.early_stopping:\n",
    "                if self.i_epoch + 1 == (self.best_epoch + 20):\n",
    "                    break\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        # Obtain epoch-level metrics\n",
    "        macro_auc = roc_auc_score(np.squeeze(np.array(self.refs_train)), np.array(self.preds_train), multi_class='ovr')\n",
    "        self.macro_auc_lc_train.append(macro_auc)\n",
    "\n",
    "        # Display losses\n",
    "        self.display_losses(self.i_epoch + 1, self.epochs, self.iterations, self.iterations, self.L_epoch, macro_auc,\n",
    "                            end_line='\\n')\n",
    "        # Update learning curves\n",
    "        self.L_lc.append(self.L_epoch)\n",
    "\n",
    "        # Obtain results on validation set\n",
    "        Lce_val, macro_auc_val = self.test_bag_level_classification(self.val_generator)\n",
    "\n",
    "        # Save loss value into learning curve\n",
    "        self.Lce_lc_val.append(Lce_val)\n",
    "        self.macro_auc_lc_val.append(macro_auc_val)\n",
    "\n",
    "        metrics = {'epoch': self.i_epoch + 1, 'AUCtrain': np.round(self.macro_auc_lc_train[-1], 4),\n",
    "                   'AUCval': np.round(self.macro_auc_lc_val[-1], 4)}\n",
    "        with open(self.dir_results + self.id + 'metrics.json', 'w') as fp:\n",
    "            json.dump(metrics, fp)\n",
    "        print(metrics)\n",
    "\n",
    "        if (self.i_epoch + 1) > 10:\n",
    "            if self.criterion == 'auc':\n",
    "                if self.best_criterion < self.macro_auc_lc_val[-1]:\n",
    "                    self.best_criterion = self.macro_auc_lc_val[-1]\n",
    "                    self.best_epoch = (self.i_epoch + 1)\n",
    "\n",
    "                    torch.save(self.network, self.dir_results + self.id + 'network_weights_best.pth')\n",
    "\n",
    "            elif self.criterion == 'z':\n",
    "                if self.best_criterion < (-self.constrain_proportion_epoch):\n",
    "                    self.best_criterion = -self.constrain_proportion_epoch\n",
    "                    self.best_epoch = (self.i_epoch + 1)\n",
    "\n",
    "                    torch.save(self.network, self.dir_results + self.id + 'network_weights_best.pth')\n",
    "\n",
    "        # Each xx epochs, test models and plot learning curves\n",
    "        if (self.i_epoch + 1) % 5 == 0:\n",
    "            # Save weights\n",
    "            torch.save(self.network, self.dir_results + self.id + 'network_weights.pth')\n",
    "\n",
    "            # Plot learning curve\n",
    "            self.plot_learning_curves()\n",
    "\n",
    "            # Test at instance level\n",
    "            X = self.test_generator.dataset.X[self.test_generator.dataset.y_instances[:, 0] != -1, :, :, :]\n",
    "            Y = self.test_generator.dataset.y_instances[self.test_generator.dataset.y_instances[:, 0] != -1, :]\n",
    "            acc, f1, k2 = self.test_instance_level_classification(X, Y, self.test_generator.dataset.classes)\n",
    "\n",
    "        if (self.epochs == (self.i_epoch + 1)) or (self.early_stopping and (self.i_epoch + 1 == (self.best_epoch + 20))):\n",
    "            print('-' * 20)\n",
    "            print('-' * 20)\n",
    "\n",
    "            self.network = torch.load(self.dir_results + self.id + 'network_weights_best.pth')\n",
    "\n",
    "            # Obtain results on validation set\n",
    "            Lce_val, macro_auc_val = self.test_bag_level_classification(self.val_generator)\n",
    "\n",
    "            # Obtain results on validation set\n",
    "            Lce_test, macro_auc_test = self.test_bag_level_classification(self.test_generator)\n",
    "\n",
    "            # Test at instance level\n",
    "            X = self.test_generator.dataset.X[self.test_generator.dataset.y_instances[:, 0] != -1, :, :, :]\n",
    "            Y = self.test_generator.dataset.y_instances[self.test_generator.dataset.y_instances[:, 0] != -1, :]\n",
    "            acc, f1, k2 = self.test_instance_level_classification(X, Y, self.test_generator.dataset.classes)\n",
    "\n",
    "            metrics = {'epoch': self.best_epoch, 'AUCtest': np.round(macro_auc_test, 4),\n",
    "                       'AUCval': np.round(macro_auc_val, 4), 'acc': np.round(acc, 4),\n",
    "                       'f1': np.round(f1, 4), 'k2': np.round(k2, 4),\n",
    "                       }\n",
    "\n",
    "            if self.alpha_pc:\n",
    "                metrics['constrain_cumpliment'] = np.round(self.constrain_cumpliment_lc[self.best_epoch-1], 4)\n",
    "                metrics['constrain_proportion'] = np.round(self.constrain_proportion_lc[self.best_epoch-1], 4)\n",
    "\n",
    "            with open(self.dir_results + self.id + 'best_metrics.json', 'w') as fp:\n",
    "                json.dump(metrics, fp)\n",
    "            print(metrics)\n",
    "\n",
    "            self.metrics = metrics\n",
    "            print('-' * 20)\n",
    "            print('-' * 20)\n",
    "\n",
    "    def plot_learning_curves(self):\n",
    "        def plot_subplot(axes, x, y, y_axis):\n",
    "            axes.grid()\n",
    "            for i in range(x.shape[0]):\n",
    "                axes.plot(x[i, :], y[i, :], 'o-')\n",
    "            axes.set_ylabel(y_axis)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(20, 15))\n",
    "        plot_subplot(axes[0], np.tile(np.arange(self.i_epoch + 1), (2, 1)) + 1, np.array([self.L_lc, self.Lce_lc_val]), \"Lce\")\n",
    "        plot_subplot(axes[1], np.tile(np.arange(self.i_epoch + 1), (2, 1)) + 1, np.array([self.macro_auc_lc_train, self.macro_auc_lc_val]), \"mAUC\")\n",
    "\n",
    "        plt.savefig(self.dir_results + self.id + 'learning_curve.png')\n",
    "\n",
    "    def display_losses(self, i_epoch, epochs, iteration, total_iterations, Lce, macro_auc=0, end_line=''):\n",
    "\n",
    "        info = \"[INFO] Epoch {}/{}  -- Step {}/{}: Lce={:.4f} ; AUC={:.4f}\".format(\n",
    "                i_epoch, epochs, iteration, total_iterations, Lce, macro_auc)\n",
    "\n",
    "        if self.alpha_H > 0:\n",
    "            if end_line == '\\n':\n",
    "                info += ' ; H=' + str(np.round(self.H_epoch, 4))\n",
    "            else:\n",
    "                info += ' ; H=' + str(np.round(self.H_iteration.cpu().detach().numpy(), 4))\n",
    "\n",
    "        if self.pMIL and end_line == '\\n':\n",
    "            if self.alpha_pc > 0:\n",
    "                info += ' ; IC=' + str(np.round(self.Lic_epoch, 4))\n",
    "                info += '{' + str(np.round(self.constrain_ic_proportion_epoch, 4)) + '}'\n",
    "            if self.alpha_ic > 0:\n",
    "                info += ' ; PC=' + str(np.round(self.Lpc_epoch, 4))\n",
    "                info += '{' + str(np.round(self.constrain_cumpliment_epoch, 4)) + '}'\n",
    "                info += '{' + str(np.round(self.constrain_proportion_epoch, 4)) + '}  '\n",
    "        if self.pMIL and end_line == '\\r':\n",
    "            if self.alpha_pc > 0:\n",
    "                info += ' ; IC=' + str(np.round(self.Lic_iteration.cpu().detach().numpy(), 4))\n",
    "            if self.alpha_ic > 0:\n",
    "                info += ' ; PC=' + str(np.round(self.Lpc_iteration.cpu().detach().numpy(), 4))\n",
    "                info += '{' + str(np.round(self.constrain_cumpliment_iteration, 4)) + '}  '\n",
    "\n",
    "        # Print losses\n",
    "        et = str(datetime.timedelta(seconds=timer() - self.init_time))\n",
    "        print(info + ',ET=' + et, end=end_line)\n",
    "\n",
    "    def test_instance_level_classification(self, X, Y, classes):\n",
    "        classes = ['NC'] + classes\n",
    "\n",
    "        self.network.eval()\n",
    "        print(['INFO: Testing at instance level...'])\n",
    "\n",
    "        Yhat = []\n",
    "        for iInstance in np.arange(0, X.shape[0]):\n",
    "            print(str(iInstance+1) + '/' + str(X.shape[0]), end='\\r')\n",
    "\n",
    "            # Tensorize input\n",
    "            x = torch.tensor(X[iInstance, :, :, :]).cuda().float()\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "            if self.network.aggregation == 'mcAttentionMIL':\n",
    "                yhat = self.network.milAggregation(torch.squeeze(self.network.bb(x)).unsqueeze(0))[1]\n",
    "                yhat = torch.argmax(yhat).detach().cpu().numpy()\n",
    "            else:\n",
    "                # Make prediction\n",
    "                if not self.network.prototypical:\n",
    "                    yhat = torch.softmax(self.network.classifier(torch.squeeze(self.network.bb(x))), 0)\n",
    "                else:\n",
    "                    yhat = torch.softmax(- torch.cdist(torch.squeeze(self.network.bb(x)).unsqueeze(0), self.network.C, p=2.0), 1)\n",
    "                yhat = torch.argmax(yhat).detach().cpu().numpy()\n",
    "\n",
    "            Yhat.append(yhat)\n",
    "        Yhat = np.array(Yhat)\n",
    "        Y = np.argmax(Y, 1)\n",
    "\n",
    "        cr = classification_report(Y, Yhat, target_names=classes, digits=4)\n",
    "        acc = accuracy_score(Y, Yhat)\n",
    "        f1 = f1_score(Y, Yhat, average='macro')\n",
    "        cm = confusion_matrix(Y, Yhat)\n",
    "        k2 = cohen_kappa_score(Y, Yhat, weights='quadratic')\n",
    "\n",
    "        print('Instance Level kappa: ' + str(np.round(k2, 4)), end='\\n')\n",
    "\n",
    "        f = open(self.dir_results + self.id + 'report.txt', 'w')\n",
    "        f.write('Title\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n\\nKappa\\n\\n{}\\n'.format(cr, cm, k2))\n",
    "        f.close()\n",
    "\n",
    "        return acc, f1, k2\n",
    "\n",
    "    def test_bag_level_classification(self, test_generator, binary=False):\n",
    "        self.network.eval()\n",
    "        print('[VALIDATION]: at bag level...')\n",
    "\n",
    "        # Loop over training dataset\n",
    "        Y_all = []\n",
    "        Yhat_all = []\n",
    "        Lce_e = 0\n",
    "        for self.i_iteration, (X, Y, O, _) in enumerate(test_generator):\n",
    "            X = torch.tensor(X).cuda().float()\n",
    "            Y = torch.tensor(Y).cuda().float()\n",
    "\n",
    "            # Set model to training mode and clear gradients\n",
    "\n",
    "            # Forward network\n",
    "            Yhat, _, _ = self.network(X)\n",
    "            # Estimate losses\n",
    "            Lce = self.L(Yhat, torch.squeeze(Y))\n",
    "            Lce_e += Lce.cpu().detach().numpy() / len(test_generator)\n",
    "\n",
    "            Y_all.append(Y.detach().cpu().numpy())\n",
    "            Yhat_all.append(Yhat.detach().cpu().numpy())\n",
    "\n",
    "            # Display losses per iteration\n",
    "            self.display_losses(self.i_epoch + 1, self.epochs, self.i_iteration + 1, len(test_generator),\n",
    "                                Lce.cpu().detach().numpy(),\n",
    "                                end_line='\\r')\n",
    "        # Obtain overall metrics\n",
    "        Yhat_all = np.array(Yhat_all)\n",
    "        Y_all = np.squeeze(np.array(Y_all))\n",
    "\n",
    "        if binary:\n",
    "            Yhat_all = np.max(Yhat_all, 1)\n",
    "            Y_all = np.max(Y_all, 1)\n",
    "\n",
    "        macro_auc = roc_auc_score(Y_all, Yhat_all, multi_class='ovr')\n",
    "\n",
    "        # Display losses per epoch\n",
    "        self.display_losses(self.i_epoch + 1, self.epochs, self.i_iteration + 1, len(test_generator),\n",
    "                            Lce_e, macro_auc,\n",
    "                            end_line='\\n')\n",
    "\n",
    "        return Lce_e, macro_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c33488-4b58-46bf-8095-e9afe9128a26",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "669fbc9d-e4b2-4acd-8762-a5304d806eb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7184\\1518965115.py\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7184\\1518965115.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi_iteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_iteration\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdir_data_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Set data generators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1189\u001b[0m                 \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xls\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m                 ext = inspect_excel_format(\n\u001b[0m\u001b[0;32m   1192\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m   1071\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m     ) as handle:\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.csv'"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "\n",
    "    metrics = []\n",
    "    for i_iteration in np.arange(0, args.iterations):\n",
    "        id = str(i_iteration) + '_'\n",
    "        df = pd.read_excel(args.dir_data_frame)\n",
    "\n",
    "        # Set data generators\n",
    "        dataset_train = MILDataset(args.dir_images, df[df['Partition'] == 'train'], args.classes,\n",
    "                                   bag_id='slide_name', input_shape=args.input_shape,\n",
    "                                   data_augmentation=args.data_augmentation, images_on_ram=args.images_on_ram,\n",
    "                                   pMIL=args.pMIL, proportions=args.proportions)\n",
    "        data_generator_train = MILDataGenerator(dataset_train, batch_size=1, shuffle=True, max_instances=512)\n",
    "\n",
    "        dataset_val = MILDataset(args.dir_images, df[df['Partition'] == 'val'], args.classes,\n",
    "                                 bag_id='slide_name', input_shape=args.input_shape,\n",
    "                                 data_augmentation=args.data_augmentation, images_on_ram=args.images_on_ram,\n",
    "                                 pMIL=args.pMIL, proportions=args.proportions)\n",
    "        data_generator_val = MILDataGenerator(dataset_val, batch_size=1, shuffle=False, max_instances=512)\n",
    "\n",
    "        dataset_test = MILDataset(args.dir_images, df[df['Partition'] == 'test'], args.classes,\n",
    "                                  bag_id='slide_name', input_shape=args.input_shape,\n",
    "                                  data_augmentation=args.data_augmentation, images_on_ram=args.images_on_ram,\n",
    "                                  pMIL=args.pMIL, proportions=args.proportions,\n",
    "                                  dataframe_instances=pd.read_excel(args.dir_data_frame_test))\n",
    "        data_generator_test = MILDataGenerator(dataset_test, batch_size=1, shuffle=False, max_instances=512)\n",
    "\n",
    "        # Set network architecture\n",
    "        network = MILArchitecture(args.classes, mode=args.mode, aggregation=args.aggregation,\n",
    "                                  backbone='vgg19', include_background=args.include_background)\n",
    "\n",
    "        # Perform training\n",
    "        trainer = MILTrainer(args.dir_results + args.experiment_name + '/', network,\n",
    "                             lr=args.lr, pMIL=args.pMIL, margin=args.margin,\n",
    "                             alpha_ic=args.alpha_ic, alpha_pc=args.alpha_pc, t_ic=args.t_ic,\n",
    "                             t_pc=args.t_pc, alpha_ce=args.alpha_ce, id=id,\n",
    "                             early_stopping=args.early_stopping, scheduler=args.scheduler,\n",
    "                             virtual_batch_size=args.virtual_batch_size,\n",
    "                             criterion=args.criterion,\n",
    "                             alpha_H=args.alpha_H)\n",
    "        trainer.train(train_generator=data_generator_train, val_generator=data_generator_val,\n",
    "                      test_generator=data_generator_test, epochs=args.epochs)\n",
    "\n",
    "        metrics.append([list(trainer.metrics.values())[1:]])\n",
    "\n",
    "    # Get overall metrics\n",
    "    metrics = np.squeeze(np.array(metrics))\n",
    "\n",
    "    mu = np.mean(metrics, axis=0)\n",
    "    std = np.std(metrics, axis=0)\n",
    "\n",
    "    info = \"AUCtest={:.4f}({:.4f}) ; AUCval={:.4f}({:.4f})  ; acc={:.4f}({:.4f}) ; f1-score={:.4f}({:.4f}) ; k2={:.4f}({:.4f})\".format(\n",
    "          mu[0], std[0], mu[1], std[1], mu[2], std[2], mu[3], std[3], mu[4], std[4])\n",
    "    if args.alpha_pc > 0:\n",
    "        info += \" ; constrain_cumpliment={:.4f}({:.4f}) ; constrain_proportion={:.4f}({:.4f})\".format(\n",
    "          mu[5], std[5], mu[6], std[6])\n",
    "\n",
    "    f = open(args.dir_results + args.experiment_name + '/' + 'method_metrics.txt', 'w')\n",
    "    f.write(info)\n",
    "    f.close()\n",
    "\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.dir_images = \"\"\n",
    "        self.dir_data_frame = \".csv\"\n",
    "        self.dir_data_frame_test = \".csv\"\n",
    "        self.dir_results = \"results\"\n",
    "        self.criterion = \"z\"\n",
    "        self.experiment_name = \"tmp1\"\n",
    "        self.classes = [\"algae\"]\n",
    "        self.proportions = [\"Primary\", \"Secondary\"]\n",
    "        self.input_shape = [3, 224, 224]\n",
    "        self.images_on_ram = False\n",
    "        self.epochs = 100\n",
    "        self.aggregation = \"max\"\n",
    "        self.mode = \"instance\"\n",
    "        self.include_background = True\n",
    "        self.lr = 1*1e-2\n",
    "        self.pMIL = False\n",
    "        \n",
    "        self.alpha_ce = 1\n",
    "        self.margin = 0.\n",
    "        self.alpha_ic = 1\n",
    "        self.alpha_pc = 1\n",
    "        self.alpha_H = 0\n",
    "        self.t_ic = 15\n",
    "        self.t_pc = 5\n",
    "        self.data_augmentation = True\n",
    "        self.iterations = 3\n",
    "        \n",
    "        self.early_stopping = True\n",
    "        self.scheduler = True\n",
    "        self.virtual_batch_size = 1\n",
    "        \n",
    "\n",
    "args = Arguments()\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a36486-e959-468c-8a80-ffb2f89c1056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
