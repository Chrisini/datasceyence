{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78aa12ab-b8f3-429d-a10c-4e8c088952c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../helper', './helper', '/helper', 'helper', 'C:\\\\Users\\\\Prinzessin\\\\projects\\\\decentnet\\\\datasceyence\\\\examples', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\python39.zip', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\DLLs', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta', '', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\Prinzessin\\\\anaconda3\\\\envs\\\\feta\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Prinzessin\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import torch\n",
    "import torchvision.transforms\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from typing import Optional, List, Tuple, Union\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from typing import Optional, List, Tuple, Union\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "sys.path.insert(0, \"/helper\")\n",
    "sys.path.insert(0, \"./helper\")\n",
    "sys.path.insert(0, \"../helper\")\n",
    "print(sys.path)\n",
    "\n",
    "# own module\n",
    "from visualisation.feature_map import *\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "mode = \"l1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9e16b51-652b-4fd5-acf4-8100fabead90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConvNd(torch.nn.Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        ...\n",
    "\n",
    "    in_channels: int\n",
    "    _reversed_padding_repeated_twice: List[int]\n",
    "    out_channels: int\n",
    "    kernel_size: Tuple[int, ...]\n",
    "    stride: Tuple[int, ...]\n",
    "    padding: Union[str, Tuple[int, ...]]\n",
    "    dilation: Tuple[int, ...]\n",
    "    transposed: bool\n",
    "    output_padding: Tuple[int, ...]\n",
    "    groups: int\n",
    "    padding_mode: str\n",
    "    weight: Tensor\n",
    "    bias: Optional[Tensor]\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Tuple[int, ...],\n",
    "                 stride: Tuple[int, ...],\n",
    "                 padding: Tuple[int, ...],\n",
    "                 dilation: Tuple[int, ...],\n",
    "                 transposed: bool,\n",
    "                 output_padding: Tuple[int, ...],\n",
    "                 groups: int,\n",
    "                 bias: bool,\n",
    "                 padding_mode: str,\n",
    "                 device=None,\n",
    "                 dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        print(factory_kwargs)\n",
    "        super().__init__()\n",
    "        if groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        # `_reversed_padding_repeated_twice` is the padding to be passed to\n",
    "        # `F.pad` if needed (e.g., for non-zero padding types that are\n",
    "        # implemented as two ops: padding + conv). `F.pad` accepts paddings in\n",
    "        # reverse order than the dimension.\n",
    "        if isinstance(self.padding, str):\n",
    "            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)\n",
    "            if padding == 'same':\n",
    "                for d, k, i in zip(dilation, kernel_size,\n",
    "                                   range(len(kernel_size) - 1, -1, -1)):\n",
    "                    total_padding = d * (k - 1)\n",
    "                    left_pad = total_padding // 2\n",
    "                    self._reversed_padding_repeated_twice[2 * i] = left_pad\n",
    "                    self._reversed_padding_repeated_twice[2 * i + 1] = (\n",
    "                        total_padding - left_pad)\n",
    "        else:\n",
    "            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)\n",
    "\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (in_channels, out_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        else:\n",
    "            self.weight = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n",
    "            self.importance = Parameter(torch.empty(\n",
    "                (out_channels, in_channels // groups), **factory_kwargs))\n",
    "            \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "class CustomConv2d(_ConvNd):\n",
    "    \n",
    "    # additionally needed\n",
    "    \"\"\"\n",
    "    \n",
    "    position\n",
    "    activated channels\n",
    "    connection between channels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: _size_2_t,\n",
    "        stride: _size_2_t = 1,\n",
    "        padding: Union[str, _size_2_t] = 0,\n",
    "        dilation: _size_2_t = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        kernel_size_ = _pair(kernel_size)\n",
    "        stride_ = _pair(stride)\n",
    "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
    "        dilation_ = _pair(dilation)\n",
    "        super().__init__(\n",
    "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
    "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
    "        \n",
    "        # this layer id\n",
    "        layer_id = 0\n",
    "        \n",
    "        # within this layer, a whole filter can be deactivated\n",
    "        # within a filter, single channels can be deactivated\n",
    "        # within this layer, filters can be swapped\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            if fan_in != 0:\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        \n",
    "        # this is written in c++ - try not to change ...\n",
    "        return F.conv2d(input, weight, bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return self._conv_forward(input, self.weight, self.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a96c3fb-8653-47ae-a1bd-b7ff856ff5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = CustomConv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv2 = CustomConv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv3 = CustomConv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "        self.conv1x1 = CustomConv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        \n",
    "        self.K = 100 \n",
    "        self.L = 10 # last one\n",
    "        self.num_of_bases = 1 # 3rd dim\n",
    "        \n",
    "        if False:\n",
    "            self.conv1 = Conv2d(1, 32, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv2 = Conv2d(32, 64, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv3 = Conv2d(64, 128, kernel_size=3, stride=1, padding=0, dilation=3)\n",
    "            self.conv1x1 = Conv2d(128, 10, kernel_size=1, stride=1, padding=0, dilation=1)\n",
    "        #self.dropout1 = nn.Dropout(0.25)\n",
    "        #self.dropout2 = nn.Dropout(0.5)\n",
    "        # 4x16384\n",
    "        # self.fc1 = nn.Linear(10*10*10, 10)\n",
    "        #self.fc2 = nn.Linear(10, 10)\n",
    "        \n",
    "        #self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        \n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        #self.sub_concept_pooling = nn.modules.MaxPool2d((self.K, 1), stride=(1,1))\n",
    "        #self.instance_pooling = nn.modules.MaxPool2d((opt.num_of_bases, 1), stride=(1,1))\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.mish1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.mish2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.mish3(x)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        x = self.mish1x1(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        #x = F.max_pool2d(x, 2)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        #print(x.size())\n",
    "        #print(x.size()[2:])\n",
    "        \n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # x = self.flat(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        #x = x.view(-1, self.L, self.K, 10)\n",
    "        \n",
    "        # input, kernel_size, stride, padding, dilation, ceil_mode\n",
    "        #x = self.sub_concept_pooling(x).view(-1, self.L, self.num_of_bases).permute(0,2,1).unsqueeze(1)\n",
    "        \n",
    "        # output = F.sigmoid(x)\n",
    "        # x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        #x = torch.flatten(x, 1)\n",
    "        # x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.fc2(x)\n",
    "        #output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e28a308-6510-412a-9df9-bdad33b211f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "        \n",
    "        if batch_idx == -1:\n",
    "            print(data.shape) # torch.Size([4, 1, 28, 28])\n",
    "            print(target)\n",
    "            \"\"\"\n",
    "            tensor([[8],\n",
    "            [7],\n",
    "            [2],\n",
    "            [7]])\n",
    "            \"\"\"\n",
    "            print(target_multi_hot)\n",
    "            \"\"\"\n",
    "            tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "            [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "            [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
    "            \"\"\"\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, target_multi_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % (args.log_interval*1000) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.unsqueeze(1) # .to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            target_multi_hot = torch.zeros(target.size(0), 10).scatter_(1, target, 1.).to(device)\n",
    "            test_loss += F.binary_cross_entropy(output, target_multi_hot, reduction='mean').item()\n",
    "        \n",
    "            # test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.to(device).view_as(pred)).sum().item()\n",
    "            \n",
    "            \"\"\"\n",
    "            if i == 0 and epoch % args.log_interval == 0:\n",
    "            # if False: # i == 0:\n",
    "                print(data.shape)\n",
    "                layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "                # run feature map\n",
    "                dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "                dd.run(data)\n",
    "                dd.plot(path=f\"example_results/feature_map_{epoch}.png\")\n",
    "                \"\"\"\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1\n",
    "        self.epochs = 100\n",
    "        self.lr = 0.1\n",
    "        self.gamma = 0.7\n",
    "        self.log_interval = 5\n",
    "        self.save_model = True\n",
    "        \n",
    "\n",
    "def main_train():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('example_data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    #scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader, epoch)\n",
    "        #scheduler.step()\n",
    "        \n",
    "        \n",
    "        if args.save_model and epoch % args.log_interval == 0:\n",
    "            torch.save(model.state_dict(), f\"example_results/mnist_cnn_{epoch}.ckpt\")\n",
    "\n",
    "\n",
    "def main_test():\n",
    "    # Training settings\n",
    "    args = Parser()\n",
    "    \n",
    "    if True:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset2 = datasets.MNIST('example_data', train=False,\n",
    "                       transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "\n",
    "    if True:\n",
    "        model.load_state_dict(torch.load(\"example_results/mnist_cnn_5.ckpt\"))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(\"example_results/pruned_model.ckpt\"))\n",
    "    \n",
    "\n",
    "    # model = torch.load(model.state_dict(), \"example_results/mnist_cnn_30.ckpt\")\n",
    "    test(args, model, device, test_loader, 0)\n",
    "    \n",
    "    return model\n",
    "        \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "225c9223-c100-4fe1-aad9-fcd64b422ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n",
      "{'device': None, 'dtype': None}\n",
      "\n",
      "Test set: Average loss: 0.2134, Accuracy: 5488/10000 (55%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_to_prune = main_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "84ed9226-1b19-49eb-b784-e951eed0c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"l1\":\n",
    "    prune.l1_unstructured(model_to_prune.conv2, name=\"weight\", amount=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a683125-a4ab-418b-b121-ca64683d61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"random\":\n",
    "    prune.random_unstructured(model_to_prune.conv2, name=\"weight\", amount=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c12e9df-a2f0-4d7c-a458-9a991e0a7275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(2, <torch.nn.utils.prune.L1Unstructured object at 0x0000018586A03820>)])\n"
     ]
    }
   ],
   "source": [
    "print(model_to_prune.conv2._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad1a4592-f6d1-4144-a8dc-799b40d52aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), dilation=(3, 3))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.remove(model_to_prune.conv2, \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15583249-6be0-4c28-8221-6c559e2a4cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_prune.state_dict(), f\"example_results/pruned_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13b7b0fc-0514-4b28-85cd-ff27beab9c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.0587,  0.0000,  0.0640],\n",
      "          [ 0.0000,  0.0000, -0.0502],\n",
      "          [-0.0000, -0.0550,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0487, -0.0000],\n",
      "          [ 0.0626, -0.0000,  0.0448],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0469,  0.0000],\n",
      "          [ 0.0000,  0.0675, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0516,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0551,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0468, -0.0581, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0436]],\n",
      "\n",
      "         [[ 0.0442,  0.0792,  0.0000],\n",
      "          [ 0.0447, -0.0000,  0.0000],\n",
      "          [ 0.0441, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0685,  0.0542,  0.0449],\n",
      "          [-0.0000, -0.0457,  0.0537],\n",
      "          [ 0.0000,  0.0000,  0.0488]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0544,  0.0000],\n",
      "          [-0.0000,  0.0542,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0521]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0733,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0578],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.0582],\n",
      "          [ 0.0000,  0.0747,  0.0000],\n",
      "          [-0.0000,  0.0540, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0493, -0.0441, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0552],\n",
      "          [ 0.0000,  0.0000, -0.0517],\n",
      "          [ 0.0477, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0456, -0.0511, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0520, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0524,  0.0000, -0.0534],\n",
      "          [ 0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0652,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0546]],\n",
      "\n",
      "         [[-0.0600, -0.0689,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0674, -0.0516, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0681, -0.0590, -0.0470],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0677,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0515, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0623],\n",
      "          [ 0.0000,  0.0000,  0.0464]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0525],\n",
      "          [-0.0000,  0.0464,  0.0496],\n",
      "          [ 0.0624, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0776,  0.0515, -0.0558],\n",
      "          [ 0.0435,  0.0568,  0.0775],\n",
      "          [ 0.0863,  0.0714,  0.0532]],\n",
      "\n",
      "         [[ 0.0475,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0696,  0.0481, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0739,  0.0524],\n",
      "          [ 0.0000, -0.0664,  0.0675],\n",
      "          [-0.0000, -0.0568,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0517],\n",
      "          [ 0.0804,  0.0000,  0.1165],\n",
      "          [ 0.0473,  0.1239,  0.0919]],\n",
      "\n",
      "         [[-0.0000,  0.0811,  0.0000],\n",
      "          [-0.0000,  0.0554, -0.0638],\n",
      "          [ 0.0000, -0.0792, -0.0632]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0633,  0.0000],\n",
      "          [ 0.0000,  0.0525,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0433, -0.0000],\n",
      "          [ 0.0497,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0447, -0.0000],\n",
      "          [-0.0000,  0.0477,  0.0491],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0491,  0.0000],\n",
      "          [-0.0000,  0.0618, -0.0000],\n",
      "          [ 0.0000,  0.0622,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0544,  0.0654,  0.0503],\n",
      "          [ 0.0000,  0.0000,  0.0828],\n",
      "          [-0.0000,  0.0457,  0.0000]],\n",
      "\n",
      "         [[ 0.0465,  0.0532,  0.0000],\n",
      "          [ 0.0000,  0.0439,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0546]],\n",
      "\n",
      "         [[ 0.0559,  0.0000,  0.0000],\n",
      "          [ 0.0709,  0.0547,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]]]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model_to_prune.conv2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "804ac3f1-caec-4d8b-a446-98551c8720c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03fbc036-d6c6-4409-a7aa-2b91b37e0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_to_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf354b65-6328-4ca7-9bcb-758816985ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 3, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "def visChannels(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    \n",
    "    plt.figure(figsize=(nrow,rows) )\n",
    "    plt.title(f\"Channels with index {ch}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "def visFilters(tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.title(f\"Filter {filt}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "def visFilters_subplot(subplot, tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    subplot.set_title(f\"Filter {filt+1} with {c} channels\")\n",
    "    subplot.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "    subplot.axis('off')\n",
    "    \n",
    "layer = 1\n",
    "filter = model.conv2.weight.data.clone()\n",
    "\n",
    "print(model.conv2.weight.shape)\n",
    "\n",
    "# need to match the network parameters!!!!\n",
    "in_channels = 5\n",
    "out_filters = 3 # 64\n",
    "\n",
    "\n",
    "fig, subplot = plt.subplots(out_filters, figsize=(10, 10))\n",
    "fig.suptitle(f'Layer with shape {list(model.conv2.weight.shape)} [out, in, kernel, kernel]')\n",
    "\n",
    "for filt in range(0, out_filters):\n",
    "    \n",
    "    visFilters_subplot(subplot[filt], filter, filt=filt, allkernels=False)\n",
    "\n",
    "    #plt.axis('off')\n",
    "    #plt.ioff()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"example_results/filter_with_weights.png\")\n",
    "plt.show()\n",
    "    \n",
    "if False:    \n",
    "    for filt in range(0, out_filters):\n",
    "\n",
    "        visFilters(filter, filt=filt, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"examples/example_results/filter_with_weights.png\")\n",
    "        plt.show()\n",
    "\n",
    "    for ch in range(0, in_channels):\n",
    "\n",
    "        visChannels(filter, ch=ch, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985633f7-4625-4c2c-9f3a-014b2df159eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
