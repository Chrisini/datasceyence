{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f61201-fe2f-43c2-bc80-2728ee7ee3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "d4aa4ed7-878e-4cec-85e4-baa9c4e88663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class X:\n",
    "    def __init__(self, value, m, n):\n",
    "        \n",
    "        self.m = m # list of integers\n",
    "        self.n = n # list of integers\n",
    "        self.channels = value # list of tensors\n",
    "                \n",
    "    def set(self, value, m, n):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.channels = value\n",
    "    \n",
    "    def get(self):\n",
    "        return self.channels, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(channels: ' + str(self.channels.shape) +' at position: m=' + str(self.m) + ', n=' + str(self.n) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class Filter(torch.nn.Module):\n",
    "    def __init__(self, ms_in, ns_in, m_out, n_out, n_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ms_in = ms_in # list\n",
    "        self.ns_in = ns_in # list\n",
    "        self.m_out = m_out # single integer\n",
    "        self.n_out = n_out # single integer\n",
    "        self.weights = torch.autograd.Variable(torch.randn(1,n_channels,3,3))\n",
    "        \n",
    "    def forward(self, x_channels):\n",
    "        \n",
    "        #print(x.values())\n",
    "    \n",
    "        x_channels = torch.nn.functional.conv2d(x_channels, self.weights)\n",
    "        \n",
    "        #print(x.value)\n",
    "        \n",
    "        return x_channels\n",
    "    \n",
    "    def set(self, value, m_out, n_out):\n",
    "        self.ms_in = None # list\n",
    "        self.ns_in = None # list\n",
    "        self.m_out = m_out # single integer\n",
    "        self.n_out = n_out # single integer\n",
    "        self.weights = value # weights in this filter\n",
    "    \n",
    "    def get(self):\n",
    "        return self.m_out, self.n_out, self.weights\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'DecentFilter(weights: ' + str(self.weights.shape) + ' at position: m_out=' + str(self.m_out) + ', n_out=' + str(self.n_out) + ')' + \\\n",
    "    '\\n with inputs: ms_in= ' + ', '.join(str(m.item()) for m in self.ms_in) + ', ns_in= ' + ', '.join(str(n.item()) for n in self.ns_in) + ')'\n",
    "    __repr__ = __str__\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Layer(torch.nn.Module):  \n",
    "    def __init__(self, ms_in, ns_in, n_channels, n_filters):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ms_in = ms_in\n",
    "        self.ns_in = ns_in\n",
    "                \n",
    "        self.geometry_array = np.full(81, np.nan)\n",
    "        # plus 1 here cause of to_sparse array\n",
    "        self.geometry_array[0:n_filters] = range(1,n_filters+1)\n",
    "        np.random.shuffle(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.reshape((9,9), order='C')\n",
    "        self.geometry_array = torch.tensor(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.to_sparse(sparse_dim=2).to(\"cuda\")\n",
    "\n",
    "        print(self.geometry_array)\n",
    "        print(self.geometry_array.values())\n",
    "        \n",
    "        self.filter_list = torch.nn.ModuleList([])\n",
    "        for i_filter in range(n_filters):\n",
    "            # minus 1 here cause of to_sparse array\n",
    "            index = (self.geometry_array.values()-1 == i_filter).nonzero(as_tuple=True)[0]\n",
    "            m_out = self.geometry_array.indices()[0][index]\n",
    "            n_out = self.geometry_array.indices()[1][index]\n",
    "            self.filter_list.append(Filter(ms_in, ns_in, m_out, n_out, n_channels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        print(\"the 1:\", x)\n",
    "        \n",
    "        output_list = []\n",
    "        for f in self.filter_list:\n",
    "            output_list.append(f(x.channels))\n",
    "        \n",
    "        # needs to be for each channel??\n",
    "        x.channels = torch.cat(output_list, dim=1)\n",
    "        # mean = torch.mean(out, 0, keepdim=True)\n",
    "        \n",
    "        print(\"the 2:\", x)\n",
    "        \n",
    "        return x #, pos\n",
    "    \n",
    "    def get_filter_positions(self):\n",
    "        \n",
    "        ms_out = []\n",
    "        ns_out = []\n",
    "        for f in self.filter_list:\n",
    "            ms_out.append(f.m_out)\n",
    "            ns_out.append(f.n_out)\n",
    "        \n",
    "        return ms_out, ns_out\n",
    "    \n",
    "    #def __str__(self):\n",
    "    #    return 'Layer(filters: )' #  + str(self.weights.shape) + ' at position: m_out=' + str(self.m_out) + ', n_out=' + str(self.n_out) + ')'\n",
    "    #__repr__ = __str__\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        dim = [1, 32, 48, 64, 10]\n",
    "        dim = [1, 8, 16, 24, 10]\n",
    "        assert not any(i > 81 for i in dim), \"filters need to be less than 81\"\n",
    "        \n",
    "        m0 = [torch.tensor(0)]\n",
    "        n0 = [torch.tensor(0)]\n",
    "        #self.layer0 = Layer(m,n)\n",
    "        #m,n = layer0.get_positions()\n",
    "        self.layer1 = Layer(m0, n0, dim[0], dim[1])\n",
    "        m1,n1 = self.layer1.get_filter_positions()\n",
    "        self.layer2 = Layer(m1, n1, dim[1], dim[2])\n",
    "        m2,n2 = self.layer2.get_filter_positions()\n",
    "    def forward(self, x):\n",
    "        # x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32009cf-3a70-4d3f-b0a8-c393b08f3670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "9fd4f63e-2451-4a74-9612-e1157e60b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "                        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,\n",
      "                        4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6,\n",
      "                        6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8,\n",
      "                        8, 8, 8, 8, 8],\n",
      "                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0,\n",
      "                        1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1,\n",
      "                        2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n",
      "                        3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3,\n",
      "                        4, 5, 6, 7, 8]]),\n",
      "       values=tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan, nan, nan, nan, nan, nan, nan, nan, nan, 6., nan, nan, nan, nan,\n",
      "                      nan, nan, nan, 2., nan, nan, nan, nan, 3., 8., nan, nan, 1., nan,\n",
      "                      nan, nan, nan, 4., nan, nan, nan, nan, nan, nan, nan, 7., nan, nan,\n",
      "                      nan, nan, 5., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]),\n",
      "       device='cuda:0', size=(9, 9), nnz=81, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, 6., nan, nan, nan, nan, nan, nan, nan, 2., nan, nan, nan, nan,\n",
      "        3., 8., nan, nan, 1., nan, nan, nan, nan, 4., nan, nan, nan, nan, nan, nan, nan, 7.,\n",
      "        nan, nan, nan, nan, 5., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "tensor(indices=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "                        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,\n",
      "                        4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6,\n",
      "                        6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8,\n",
      "                        8, 8, 8, 8, 8],\n",
      "                       [0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0,\n",
      "                        1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1,\n",
      "                        2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n",
      "                        3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3,\n",
      "                        4, 5, 6, 7, 8]]),\n",
      "       values=tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan, nan, nan, nan, nan, nan, 14., nan, nan, nan, nan,\n",
      "                      nan,  1.,  9., nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "                      nan, nan, nan, nan, nan,  3., 11., nan, nan,  6., 16.,\n",
      "                      nan, nan, nan, nan, nan,  5., nan, nan, nan, nan, nan,\n",
      "                      nan, nan, nan, nan,  4., 15., nan, nan, 13., nan, nan,\n",
      "                      nan, nan,  8.,  2., nan, nan, 12., nan, 10., nan,  7.,\n",
      "                      nan, nan, nan, nan]),\n",
      "       device='cuda:0', size=(9, 9), nnz=81, dtype=torch.float64,\n",
      "       layout=torch.sparse_coo)\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, 14., nan, nan, nan, nan, nan,  1.,  9., nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  3., 11., nan, nan,\n",
      "         6., 16., nan, nan, nan, nan, nan,  5., nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan,  4., 15., nan, nan, 13., nan, nan, nan, nan,  8.,  2.,\n",
      "        nan, nan, 12., nan, 10., nan,  7., nan, nan, nan, nan],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "a412da17-0807-4dd8-b6a4-e49286737eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer1): Layer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([3], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (4): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (5): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (6): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "      (7): DecentFilter(weights: torch.Size([1, 1, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([1], device='cuda:0'))\n",
       "       with inputs: ms_in= 0, ns_in= 0)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Layer(\n",
       "    (filter_list): ModuleList(\n",
       "      (0): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (1): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (2): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([2], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (3): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (4): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([5], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (5): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (6): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([4], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (7): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([5], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (8): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([2], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (9): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([2], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (10): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([3], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (11): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([8], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (12): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([7], device='cuda:0'), n_out=tensor([0], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (13): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([1], device='cuda:0'), n_out=tensor([8], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (14): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([6], device='cuda:0'), n_out=tensor([6], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "      (15): DecentFilter(weights: torch.Size([1, 8, 3, 3]) at position: m_out=tensor([4], device='cuda:0'), n_out=tensor([7], device='cuda:0'))\n",
       "       with inputs: ms_in= 4, 3, 4, 5, 6, 2, 5, 4, ns_in= 4, 4, 0, 0, 4, 5, 8, 1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "07fd8d57-0a5e-4a66-8810-9a4515a7b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.autograd.Variable(torch.randn(1, 1, 30, 30)) # one image in batch\n",
    "# dense_input.shape\n",
    "\n",
    "dense_input = X(tmp, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "67d857ef-1b6d-4c25-8ce2-f7a49b2a2b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1: X(channels: torch.Size([1, 1, 30, 30]) at position: m=tensor(0), n=tensor(0))\n",
      "the 2: X(channels: torch.Size([1, 8, 28, 28]) at position: m=tensor(0), n=tensor(0))\n",
      "the 1: X(channels: torch.Size([1, 8, 28, 28]) at position: m=tensor(0), n=tensor(0))\n",
      "the 2: X(channels: torch.Size([1, 16, 26, 26]) at position: m=tensor(0), n=tensor(0))\n"
     ]
    }
   ],
   "source": [
    "a = n(dense_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "9370d7f5-d92f-44a9-8174-9435c6698fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X(channels: torch.Size([1, 16, 26, 26]) at position: m=0, n=0)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a4f45aed-424e-446a-8d87-f6338fa70ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter(weights: torch.Size([1, 1, 3, 3]) at position: m=4, n=1)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filter([2, 3, 5], [8, 2, 4], 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8501f1-0f46-4c05-a3a1-d11a0efee297",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(a.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "910d0457-5dbd-47c2-8d67-47cf4c32c2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "[0 1 2 3 4 5 6 7 8 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "[[0 0 0 0 5 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0]\n",
      " [8 0 0 0 0 0 9 0 0]\n",
      " [0 0 0 0 4 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 7]\n",
      " [0 0 0 0 0 6 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.full(81, 0)\n",
    "print(arr)\n",
    "arr[0:10] = range(0,10)\n",
    "print(arr)\n",
    "np.random.shuffle(arr)\n",
    "arr = arr.reshape((9,9), order='C')\n",
    "print(arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "41f614ed-f334-4234-80a8-96f38d526b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1, 2, 3, 4, 4, 5, 7, 8],\n",
       "                       [4, 7, 1, 6, 0, 6, 4, 8, 5]]),\n",
       "       values=tensor([5, 2, 3, 1, 8, 9, 4, 7, 6]),\n",
       "       device='cuda:0', size=(9, 9), nnz=9, dtype=torch.int32,\n",
       "       layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dense_input = torch.tensor(arr)\n",
    "\n",
    "co = dense_input.to_sparse(sparse_dim=2).to(\"cuda\")\n",
    "co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7959f7f5-54c9-4f1f-9823-29e8ad03f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (co.values() == 3).nonzero(as_tuple=True)[0]\n",
    "co.indices()[0][index]\n",
    "co.indices()[1][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "bde21ee1-9bb2-4557-b883-bc83732af036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2], device='cuda:0')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.indices()[0][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4f0cd802-d731-4471-9a89-65344792499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.indices()[1][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e925c7f-f016-4930-98ac-ab23bb65b176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066d9dd-be21-4813-8ec5-1c05b197d22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a16072-f3ff-4b12-8eb2-7e86487747ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1adaa3-1e17-40a4-9f0c-56c5cfd05554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15a0b8-b533-4f23-9052-74e089a4b0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103496fb-958d-4dda-91ab-5122c16dea5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba62592-3595-4141-8a2e-0538d7c7e9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a266e-89b2-48ad-ae3a-e52df4ac93ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5601cf41-d288-4eca-a9bc-b9bb01ce98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying with sparse ... doesn't work cause it gets applied on all dimensions or smthg ... I want a 9x9 matrix, not a 9x9x30x30 (grid, grid, img size, img size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "883fc1c3-e617-4b57-a1df-5e5b0f80e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Filter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        a = 0\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(x.values())\n",
    "        \n",
    "        values = x.values()\n",
    "        w_channels = 1\n",
    "        weight = torch.autograd.Variable(torch.randn(1,w_channels,3,3))\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        values_conv = torch.nn.functional.conv2d(values, weight)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = torch.sparse_coo_tensor(x.indices(), values_conv, x.size())\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer0 = Layer()\n",
    "        self.layer1 = Layer()\n",
    "        self.layer2 = Layer()\n",
    "    def forward(self, x):\n",
    "        # x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d2a65d21-1174-40ca-8216-f7e0c7339f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "46b55620-8a1e-4f96-8c85-80266bbba81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 30])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_input = torch.autograd.Variable(torch.randn(1,30,30)) # one image in batch\n",
    "dense_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "42519041-8f62-48c3-8bce-f7d4f9b77362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "                         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "                         0,  0],\n",
       "                       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                        14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                        28, 29]]),\n",
       "       values=tensor([[ 6.2641e-01,  1.3117e+00, -3.7951e-01,  4.3884e-01,\n",
       "                        2.7492e-01,  1.4680e+00,  1.7618e-01, -5.8837e-02,\n",
       "                        3.7669e-01,  6.6832e-02, -8.3039e-01, -9.2211e-01,\n",
       "                        3.6151e-01, -5.6165e-01, -6.5651e-01, -2.5526e-01,\n",
       "                       -1.2958e+00, -3.1554e-01,  4.9020e-01, -4.6814e-01,\n",
       "                        1.4990e-01, -1.6607e+00, -1.0564e+00,  4.8079e-01,\n",
       "                       -1.7671e+00, -2.0465e+00,  2.9619e-01, -4.3248e-01,\n",
       "                        2.1706e+00,  6.6934e-02],\n",
       "                      [ 6.3521e-02,  7.6555e-01,  6.5613e-02,  9.4959e-02,\n",
       "                       -8.9209e-02, -1.6276e+00,  1.3398e-01,  3.6787e-01,\n",
       "                        1.1010e+00, -1.2036e+00, -1.4293e+00,  7.2220e-01,\n",
       "                       -3.4823e-01,  7.1494e-01,  1.9683e-01,  2.5753e-01,\n",
       "                        3.7044e-01,  9.6639e-02, -1.6507e+00, -1.3663e-01,\n",
       "                        1.6663e-01, -2.0215e+00, -9.1247e-01, -1.1768e+00,\n",
       "                       -4.5349e-01,  4.1883e-01, -1.7338e+00,  1.8319e+00,\n",
       "                       -1.1823e+00, -6.7039e-01],\n",
       "                      [-8.0561e-01,  8.0303e-01, -8.1707e-01,  1.4705e+00,\n",
       "                       -2.0464e-01,  4.6572e-01, -4.1262e-01, -3.2693e-01,\n",
       "                        4.2915e-01, -7.5811e-01, -3.4276e-01, -5.6261e-01,\n",
       "                       -1.5707e-01, -8.8996e-01, -1.4781e+00,  9.7746e-01,\n",
       "                       -5.0540e-01, -1.2265e+00, -8.2153e-01, -7.2862e-02,\n",
       "                       -6.4831e-01, -4.2377e-01,  1.7786e+00, -1.6033e+00,\n",
       "                       -9.2501e-01, -8.5826e-01,  1.1970e+00, -7.5851e-01,\n",
       "                        3.4778e-01, -3.0588e-01],\n",
       "                      [ 2.3556e-01,  2.9601e-01,  3.7417e-01, -9.4512e-01,\n",
       "                        3.2215e-01,  1.1674e+00,  1.0223e-01, -1.6040e+00,\n",
       "                        7.7326e-01,  4.7703e-01, -1.3047e-01,  1.8948e-01,\n",
       "                       -6.9025e-02, -4.2693e-01,  7.5690e-01,  9.0268e-01,\n",
       "                        1.6359e+00, -4.6935e-02,  5.1736e-01, -5.1400e-01,\n",
       "                        8.9796e-01, -7.4594e-01, -8.7589e-01, -2.7885e-01,\n",
       "                        2.2794e+00,  9.8695e-01, -1.2615e+00,  7.2717e-01,\n",
       "                       -1.2794e+00, -2.1632e-01],\n",
       "                      [ 3.3668e-02,  4.3781e-01,  3.1794e-01,  1.4067e+00,\n",
       "                       -2.0585e-01,  1.4600e+00,  1.3842e-02, -3.1035e-01,\n",
       "                       -1.1662e+00, -2.0495e+00, -3.6830e-01, -1.4657e+00,\n",
       "                       -9.1967e-01,  8.2024e-01, -1.5180e+00,  1.1740e+00,\n",
       "                       -9.5200e-01,  7.4224e-01,  8.2562e-02,  1.4088e-01,\n",
       "                        5.0931e-01, -6.8548e-01,  1.5491e+00, -1.5398e+00,\n",
       "                        3.3653e-01,  7.6012e-01,  4.9221e-01,  1.5322e+00,\n",
       "                       -5.8358e-01,  4.7472e-01],\n",
       "                      [ 1.9632e-01, -1.6766e+00, -8.8600e-01,  1.4531e+00,\n",
       "                       -8.2567e-01,  1.3051e+00,  1.3474e+00, -8.6281e-01,\n",
       "                        5.6851e-01,  1.7755e-01,  4.2010e-01, -2.9787e-01,\n",
       "                        1.3972e+00, -4.8430e-01,  1.0708e+00, -1.2180e+00,\n",
       "                       -9.8627e-01,  1.4450e+00,  1.6082e+00,  9.4502e-01,\n",
       "                       -1.9492e-01, -7.0006e-01,  1.9772e-01,  1.0709e+00,\n",
       "                        6.3853e-01,  8.8514e-01,  3.8384e-01, -1.5598e+00,\n",
       "                       -1.2913e+00, -3.1360e-01],\n",
       "                      [-7.9307e-01,  2.1403e+00,  2.7309e-01, -1.5343e+00,\n",
       "                       -1.3136e+00,  3.5188e-01,  6.2537e-01, -1.8200e+00,\n",
       "                        7.3111e-01, -8.0354e-01, -5.3162e-01,  1.8161e+00,\n",
       "                        4.7624e-01,  3.5772e-01,  1.0868e+00,  1.4373e+00,\n",
       "                       -1.5956e-01, -7.3390e-01, -7.8615e-01, -1.3420e+00,\n",
       "                        1.2920e+00,  1.5786e+00,  1.1228e+00, -4.1919e-01,\n",
       "                       -2.0432e+00, -1.0425e+00, -3.4060e+00, -5.7184e-01,\n",
       "                       -5.6215e-01, -1.2688e+00],\n",
       "                      [ 4.6948e-01,  9.9459e-01, -1.3927e+00,  1.1453e-01,\n",
       "                       -1.2993e+00,  1.6900e+00, -2.0498e-01,  8.2212e-01,\n",
       "                       -1.1929e+00,  9.2705e-01,  9.4034e-02,  1.5290e+00,\n",
       "                       -7.7280e-01, -5.2013e-01,  3.1151e-01, -1.0816e+00,\n",
       "                        1.9028e-01, -5.3678e-01, -6.6646e-01,  6.6537e-01,\n",
       "                        2.2213e+00,  1.9879e-01,  1.7057e+00, -3.5242e-01,\n",
       "                        9.1719e-01, -4.9998e-01, -1.1017e+00,  6.7387e-01,\n",
       "                       -4.8530e-01,  8.6789e-01],\n",
       "                      [ 4.6581e-01, -1.3942e+00, -1.8294e+00,  4.7161e-01,\n",
       "                       -5.3725e-01,  9.7424e-01, -3.1877e-01,  1.3930e+00,\n",
       "                        1.8839e-01,  1.3802e+00, -2.3562e-01, -7.4340e-02,\n",
       "                        2.5637e-02, -9.9786e-01,  4.2189e-01,  1.4957e-01,\n",
       "                       -5.7960e-01,  5.1066e-01,  3.9910e-01, -1.9785e+00,\n",
       "                        3.3770e-01,  6.5083e-01,  7.6515e-01, -6.6735e-01,\n",
       "                       -1.1206e+00, -4.8265e-01,  3.7946e-01,  1.1627e-01,\n",
       "                        2.4242e-01,  1.0414e+00],\n",
       "                      [-1.8468e+00, -1.2577e+00,  2.7778e+00, -1.0085e+00,\n",
       "                        4.5064e-01,  1.0537e+00, -1.1748e+00,  1.6705e-01,\n",
       "                       -2.8280e-01,  3.9743e-01,  4.2209e-01, -8.6521e-01,\n",
       "                        4.4468e-01,  8.1347e-01, -5.1320e-01,  2.4546e-01,\n",
       "                       -2.2214e-01, -2.4401e+00, -1.0727e-01,  9.8342e-01,\n",
       "                        6.9108e-01,  8.0518e-01,  3.8572e-01, -1.3617e-01,\n",
       "                        2.0846e+00, -8.9368e-01,  3.7320e-02,  7.9508e-01,\n",
       "                       -1.6021e+00, -8.8946e-01],\n",
       "                      [ 7.2232e-01,  1.2409e-01,  1.3076e+00, -2.7352e+00,\n",
       "                        7.6809e-01,  1.2294e+00,  4.5438e-01, -8.3398e-01,\n",
       "                        7.0464e-01,  8.6086e-01,  1.4108e+00, -4.5888e-01,\n",
       "                        4.9640e-02,  8.7952e-01,  1.5991e-01, -1.1378e+00,\n",
       "                       -5.0624e-01, -2.8820e+00,  3.6547e-02,  7.5138e-01,\n",
       "                       -1.2175e+00, -5.4052e-01,  1.8689e+00,  2.9862e-01,\n",
       "                       -7.2144e-02,  7.8528e-01, -4.6348e-01,  1.0914e+00,\n",
       "                        2.0915e-01,  1.3957e+00],\n",
       "                      [-7.1312e-01,  8.9642e-01, -8.3372e-02, -8.9254e-01,\n",
       "                       -6.8064e-01, -1.9124e-01, -1.0535e-01,  4.5875e-01,\n",
       "                        2.3225e+00,  2.2075e+00,  2.3832e+00,  5.8123e-01,\n",
       "                        2.1150e-02,  3.9189e-01, -1.3333e-01,  1.1215e+00,\n",
       "                        6.7875e-01, -2.2392e-01, -2.3903e-01, -2.0919e+00,\n",
       "                        8.9177e-01, -1.9694e-01,  2.8607e-01, -5.4669e-01,\n",
       "                       -1.8374e-02, -1.3238e+00,  4.5099e-01,  7.5729e-02,\n",
       "                       -5.8929e-02, -1.0194e-01],\n",
       "                      [ 1.1952e-01, -1.4175e-01,  1.1651e+00, -4.0891e-01,\n",
       "                        2.5018e-01, -1.0057e+00, -2.1050e+00,  3.7063e-01,\n",
       "                       -2.6952e-01,  4.8192e-01, -5.4249e-02, -1.7780e+00,\n",
       "                        7.0887e-01, -1.0050e+00,  4.8574e-01, -5.3057e-01,\n",
       "                       -8.1674e-01, -8.2350e-01,  7.8539e-01, -1.7240e+00,\n",
       "                       -2.0383e+00,  7.2501e-02,  8.5498e-01, -1.0880e+00,\n",
       "                       -1.6364e+00, -4.7531e-01,  4.3257e-01, -7.0368e-01,\n",
       "                       -4.6416e-01,  1.4414e+00],\n",
       "                      [ 4.2739e-01, -2.2155e+00,  1.1827e+00, -7.4579e-01,\n",
       "                        1.6456e+00, -7.5601e-01,  1.3506e+00, -1.2479e+00,\n",
       "                        3.3734e-01,  5.8792e-01, -1.5972e+00,  4.2162e-01,\n",
       "                       -9.8443e-01, -7.5867e-02,  1.5410e+00, -9.6063e-01,\n",
       "                       -1.5042e+00,  7.7303e-01, -4.0663e-01,  6.3539e-01,\n",
       "                       -1.7852e-01, -4.2873e-01, -2.5889e-01,  1.3764e+00,\n",
       "                       -1.3483e-01,  8.3294e-01,  1.9003e+00, -2.5987e-01,\n",
       "                        2.1685e+00, -5.0577e-01],\n",
       "                      [-1.7681e-01, -6.7290e-01,  4.2690e-01,  2.7593e-01,\n",
       "                       -1.1873e+00,  1.7996e+00,  8.8568e-01, -8.1787e-01,\n",
       "                        1.5207e+00, -8.5528e-01, -1.4241e-01, -6.2069e-01,\n",
       "                       -3.8294e-01,  1.5871e+00,  2.7323e-01,  1.5020e+00,\n",
       "                       -3.5415e-01, -1.6630e+00,  1.6686e+00, -1.1424e+00,\n",
       "                       -1.6828e+00, -2.5566e+00,  6.5232e-01,  7.6287e-01,\n",
       "                       -1.0594e+00, -8.0839e-01,  4.0583e-01, -1.0245e+00,\n",
       "                        1.9105e-01, -1.0560e-01],\n",
       "                      [-1.3439e+00,  8.7290e-01,  1.0053e+00,  1.0151e+00,\n",
       "                        5.3741e-01, -4.0930e-01,  6.5675e-01,  1.7155e+00,\n",
       "                        5.8664e-01,  3.2792e-01,  8.8027e-01, -2.7453e-01,\n",
       "                        1.2684e+00, -6.5694e-01, -2.3675e-01,  4.9352e-01,\n",
       "                       -6.6430e-01, -1.2562e+00,  9.7579e-02,  6.3820e-01,\n",
       "                        2.4195e-01, -2.6207e-01, -2.1047e-01,  7.6863e-01,\n",
       "                       -3.0264e-01, -1.2038e+00, -2.5919e-01,  1.3227e+00,\n",
       "                        9.3581e-01,  7.6099e-01],\n",
       "                      [-3.6137e-01,  7.5437e-01,  9.6315e-01,  1.6479e+00,\n",
       "                       -8.9119e-01, -2.4327e-01,  2.5658e-01,  5.0116e-01,\n",
       "                       -1.2294e+00, -1.2071e-01,  2.3053e+00,  2.1699e-01,\n",
       "                       -3.7146e-01, -4.4467e-01, -7.4782e-01, -1.8842e+00,\n",
       "                       -2.8685e-01, -3.2259e-01, -1.6112e+00,  1.1276e-01,\n",
       "                       -2.2780e-01, -7.4566e-01, -9.3296e-01,  1.3696e+00,\n",
       "                       -1.4042e+00,  4.5480e-01, -1.8837e-01, -1.4144e+00,\n",
       "                        7.8806e-01,  8.0669e-01],\n",
       "                      [-3.7905e-01, -2.7655e-01, -6.1570e-01, -8.6222e-01,\n",
       "                       -1.3423e+00, -2.0516e+00,  4.0056e-01, -8.5259e-01,\n",
       "                       -6.8098e-01,  1.9580e-01, -4.7441e-01,  2.6285e-01,\n",
       "                       -4.3578e-01, -1.0722e+00,  4.5377e-02, -1.5589e+00,\n",
       "                       -2.4257e-01,  7.6968e-02, -3.0948e-01,  9.5087e-01,\n",
       "                       -7.4548e-01,  2.1883e+00, -2.9503e-01, -1.4196e+00,\n",
       "                       -1.7185e+00, -6.5038e-02, -1.6063e+00, -3.5758e-01,\n",
       "                       -1.8645e+00, -7.6629e-01],\n",
       "                      [-1.6920e+00, -2.2966e+00,  1.1338e+00,  3.5519e-01,\n",
       "                       -1.3445e+00, -1.1733e+00,  8.9884e-01,  1.5170e+00,\n",
       "                       -1.8121e+00, -4.4743e-01, -6.3577e-01,  4.5981e-01,\n",
       "                       -4.8807e-01, -2.9388e-01,  4.9896e-01, -9.8901e-01,\n",
       "                       -3.6274e-01, -1.8791e-01, -3.0883e-01, -2.0809e-01,\n",
       "                       -8.1843e-01,  9.7500e-01, -4.6434e-01, -4.2619e-01,\n",
       "                        1.2185e-01, -5.9218e-01,  8.2519e-03,  1.1436e-01,\n",
       "                        1.2684e+00,  2.9392e+00],\n",
       "                      [-1.1165e+00,  3.5605e-01,  2.8804e-01,  7.4259e-01,\n",
       "                       -2.2225e-01,  1.0930e+00, -8.8332e-01,  7.5520e-01,\n",
       "                        6.1128e-01,  1.3695e+00, -5.9588e-01, -1.0154e+00,\n",
       "                       -7.8183e-01,  1.0284e+00,  2.7618e-02, -2.7326e-01,\n",
       "                       -1.0838e+00,  3.1211e-01,  4.0790e-01, -1.6288e+00,\n",
       "                        1.2720e-01, -5.7321e-01,  1.0150e+00,  3.2059e+00,\n",
       "                        3.9837e-01,  1.9778e+00, -1.0693e+00,  1.4552e-01,\n",
       "                       -7.8389e-01,  4.6254e-01],\n",
       "                      [-3.7210e-02,  6.6388e-01, -2.7007e-01,  2.0742e+00,\n",
       "                        1.5049e+00, -1.3948e+00, -1.5896e+00, -7.9898e-01,\n",
       "                        7.4213e-01, -1.3790e+00, -3.7655e-01, -2.5628e+00,\n",
       "                        1.0548e-01,  3.4575e-01,  1.6853e-01, -1.1090e+00,\n",
       "                       -6.8395e-01,  1.9079e-01, -5.9131e-01, -2.2961e+00,\n",
       "                       -1.8968e+00, -6.7108e-01,  1.1966e+00, -1.1891e+00,\n",
       "                       -7.0966e-01, -8.6183e-01, -1.0501e+00, -1.7432e+00,\n",
       "                       -1.0159e-01, -1.4032e+00],\n",
       "                      [ 1.8983e+00, -3.3512e-01, -1.2182e+00, -1.4369e+00,\n",
       "                        4.5190e-01,  1.4017e+00, -2.3010e-01,  5.9748e-01,\n",
       "                        8.6525e-01, -1.3526e+00, -7.5474e-01, -2.8434e-01,\n",
       "                       -1.5687e+00, -7.2435e-01,  8.8646e-01, -8.4987e-01,\n",
       "                       -1.4983e+00,  2.0029e+00,  7.5881e-03, -9.1480e-01,\n",
       "                        6.5444e-01,  1.8584e+00, -6.1210e-02,  5.2720e-01,\n",
       "                        9.8321e-01,  1.0630e+00, -8.8008e-01, -1.5404e+00,\n",
       "                       -1.9037e-01, -8.9062e-01],\n",
       "                      [ 1.0593e-01,  6.1390e-01,  1.7594e+00,  6.5611e-01,\n",
       "                        1.5472e+00, -9.9148e-01,  1.9440e-01, -4.1497e-01,\n",
       "                       -1.1473e+00,  3.8352e-02, -1.0725e+00, -4.3938e-02,\n",
       "                       -1.2720e+00,  3.0727e-03,  2.1897e-01,  1.3534e+00,\n",
       "                       -9.4882e-02, -1.3223e-02, -4.7816e-01,  6.6608e-02,\n",
       "                       -4.5999e-01,  1.4015e+00,  8.7238e-02, -8.1943e-02,\n",
       "                       -1.7615e+00, -9.7198e-01, -1.9081e+00,  1.3831e+00,\n",
       "                       -8.4385e-02,  1.5317e+00],\n",
       "                      [-6.9815e-01, -3.5274e-01, -1.2230e+00, -3.4255e-01,\n",
       "                       -4.5905e-01, -9.3588e-01, -9.2046e-01, -7.5217e-01,\n",
       "                       -9.5950e-01,  2.8434e-02,  3.9093e-01,  8.0384e-01,\n",
       "                       -8.0104e-01,  6.1826e-01,  6.2920e-01, -3.6431e-02,\n",
       "                        1.9150e-02, -6.1790e-01, -5.0474e-01,  1.3565e+00,\n",
       "                       -2.3541e+00,  6.2837e-01,  1.0367e+00, -6.3918e-01,\n",
       "                        8.0171e-01,  1.3327e+00, -2.1027e-01,  4.2665e-01,\n",
       "                       -2.3993e-01, -5.0279e-01],\n",
       "                      [-2.6543e-01, -6.0700e-01, -7.7366e-01, -1.5673e+00,\n",
       "                        1.0510e-01, -2.0599e-02,  4.5879e-02, -1.1752e+00,\n",
       "                        2.7666e-01, -2.5160e-01,  7.6553e-01, -1.0039e+00,\n",
       "                       -1.4313e+00,  8.4112e-01,  4.8629e-01,  5.6600e-01,\n",
       "                       -2.1089e+00, -1.1701e-01,  7.1374e-01,  1.4149e+00,\n",
       "                       -5.5448e-01,  8.7508e-01,  8.6729e-01,  1.0263e+00,\n",
       "                       -6.2143e-01, -1.2045e+00, -1.6497e+00, -9.4165e-01,\n",
       "                       -4.1367e-01, -7.5849e-02],\n",
       "                      [ 4.0238e-01, -8.2578e-01, -6.4689e-01, -5.8547e-01,\n",
       "                        2.2766e+00,  5.0372e-01,  3.2622e-01, -8.8413e-01,\n",
       "                        1.0841e+00, -2.3174e-01,  1.9586e+00, -1.0066e+00,\n",
       "                        1.5383e-01, -4.8513e-01, -1.6094e+00, -3.1619e-01,\n",
       "                        1.5536e+00,  2.6475e+00,  5.3106e-01, -5.4205e-01,\n",
       "                       -5.3535e-01,  2.4081e-02, -9.8604e-01,  5.1803e-01,\n",
       "                        3.5994e-02, -1.2901e+00,  9.1203e-01,  1.2357e+00,\n",
       "                        3.5009e-01, -3.5302e-01],\n",
       "                      [ 2.1373e+00, -3.7025e-01, -1.6927e-01, -3.6318e-01,\n",
       "                       -4.6705e-01,  1.7404e+00, -7.8091e-01,  6.6991e-01,\n",
       "                        1.7135e-02,  1.2461e-01, -4.3094e-01,  8.9277e-01,\n",
       "                        2.9660e+00,  2.1578e+00, -4.8976e-01, -1.9988e-01,\n",
       "                       -9.8972e-01, -1.8091e-02, -2.9953e+00,  4.8836e-01,\n",
       "                       -5.0478e-01, -4.9633e-01, -1.0299e+00,  1.6732e-01,\n",
       "                       -2.2285e-01, -1.7987e-01, -2.3394e-01, -9.4300e-01,\n",
       "                       -2.8196e-01, -1.8591e+00],\n",
       "                      [-1.7535e-02, -1.1010e-01,  5.9450e-01,  3.4626e-02,\n",
       "                       -1.9440e+00,  4.6840e-01, -3.4904e-01,  2.1836e-02,\n",
       "                       -1.0947e+00, -5.8735e-01,  8.3585e-01,  3.1850e-01,\n",
       "                       -2.2874e-01,  1.4465e+00,  1.6286e+00,  6.0227e-01,\n",
       "                       -2.5581e+00, -1.2916e+00,  7.8959e-01,  4.9779e-01,\n",
       "                        4.4120e-01, -1.4363e+00, -9.2549e-01, -1.0902e+00,\n",
       "                        3.2007e-01, -1.8926e-01, -1.9713e+00,  3.5111e-01,\n",
       "                       -9.3452e-01, -1.3349e+00],\n",
       "                      [-8.2098e-01,  2.4028e+00, -1.4159e+00,  3.1505e-01,\n",
       "                       -3.4258e-01,  5.1718e-01,  1.3275e+00, -9.9211e-01,\n",
       "                        3.4434e-01, -8.9755e-01, -5.2786e-01,  5.2100e-01,\n",
       "                       -8.8240e-01, -2.1131e+00, -1.1213e+00,  2.0137e+00,\n",
       "                       -3.7540e-01,  2.5734e+00, -2.2368e+00, -3.4344e-02,\n",
       "                        8.3999e-02,  1.1871e+00,  3.0721e-01,  2.9443e-01,\n",
       "                       -1.5923e-01,  5.2127e-01, -7.5221e-01, -1.6588e+00,\n",
       "                        1.6585e+00,  9.1000e-01],\n",
       "                      [ 9.7714e-01,  1.3109e+00,  7.1032e-01,  5.6431e-01,\n",
       "                        5.9946e-01, -5.8972e-02,  1.0599e+00, -8.4772e-01,\n",
       "                       -1.1823e+00, -9.0152e-01, -9.4558e-01, -5.1928e-01,\n",
       "                       -1.1995e+00,  1.2872e+00,  4.4945e-01,  1.8285e-01,\n",
       "                       -1.3024e+00, -6.6669e-01, -1.7271e+00,  1.8417e+00,\n",
       "                       -1.6406e+00,  4.8882e-01,  1.7890e+00, -8.7401e-01,\n",
       "                        5.9313e-01, -4.1464e-01, -8.0380e-01,  4.7892e-01,\n",
       "                       -5.6946e-03,  9.9665e-02]]),\n",
       "       size=(1, 30, 30), nnz=30, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_input = dense_input.to_sparse(sparse_dim=2)\n",
    "coo_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1bcfdb0c-a456-4a03-ba3b-f86f6f6ed882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coo_input.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3473e85b-5bc6-4174-b496-23d9d07f3bbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "values has incorrect size, expected [1, 30, 30], got [1, 28, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[220], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoo_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[215], line 37\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# x = self.layer0(x)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[215], line 25\u001b[0m, in \u001b[0;36mLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m values_conv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mconv2d(values, weight)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_coo_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mRuntimeError\u001b[0m: values has incorrect size, expected [1, 30, 30], got [1, 28, 28]"
     ]
    }
   ],
   "source": [
    "a = n(coo_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f640f-f533-4f63-a698-dd1a141f846e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173d121-a7e6-4022-a245-28fd895b6d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
