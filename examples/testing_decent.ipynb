{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Aaq4vTX6lxUS",
   "metadata": {
    "id": "Aaq4vTX6lxUS"
   },
   "source": [
    "# 𝔻𝕖𝕔𝕖𝕟𝕥ℕ𝕖𝕥: 𝕕𝕚𝕤-𝕖𝕟𝕥-𝕒𝕟𝕘𝕝𝕖𝕕 𝕟𝕖𝕥\n",
    "Goal: create a neural network with disentangled early hidden layers. We want to analyse which textures contribute to the predictions.\n",
    "\n",
    "In this notebook you can\n",
    "\n",
    "1) train one or multiple DecentBlocks\n",
    "  * we use a supervised contrastive loss function\n",
    "\n",
    "2) train a DecentNet\n",
    "  * we use the cross entropy loss\n",
    "  * we freeze and use the DecentBlocks as part of the DecentNet\n",
    "  * a fusion layer is the bridge between the DecentBlocks and the combined layers\n",
    "\n",
    "3) Baseline\n",
    "\n",
    "4) NOT HERE RIGHT NOW visualise the DecentBlocks and DecentNet (work in progress)\n",
    "  * DeepDreams\n",
    "  * Feature maps\n",
    "  * Filters\n",
    "\n",
    "\n",
    "8) BIMT with CONV\n",
    "* use freeze, dropout and set channel to all zeros combination\n",
    "* https://discuss.pytorch.org/t/freezing-part-of-the-layer-weights/9457/7\n",
    "\n",
    "Todos:\n",
    "* [ ] metrics / tensorboard\n",
    "\n",
    "\n",
    "MIL: https://www.sciencedirect.com/science/article/pii/S0010482522004930"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pYryiJ8XKkm4",
   "metadata": {
    "id": "pYryiJ8XKkm4"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae53756",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eae53756",
    "outputId": "1392d54c-c059-4187-80fb-2bbb803edb84"
   },
   "outputs": [],
   "source": [
    "# basics\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import shufflenet_v2_x1_0, resnet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb816ef-1359-474a-a4d2-f5806e886b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "from helper.dataset.transform.transform import ToTensor, ResizeCrop, RandomAugmentations, Normalise\n",
    "from helper.dataset.transform.two_crop import *\n",
    "from helper.compute.loss.supcon import SupConLoss\n",
    "from helper.sampler.mixed_batch import MixedBatchSampler\n",
    "#from helper.dataset.concept import ClusterConceptDataset, PosNegConceptDataset\n",
    "from helper.model.decentblock import *\n",
    "\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "try:\n",
    "    from torchvision.models import ShuffleNet_V2_X1_0_Weights\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244d212",
   "metadata": {
    "id": "4244d212"
   },
   "source": [
    "# Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "TMN4hZi_yFAD",
   "metadata": {
    "id": "TMN4hZi_yFAD"
   },
   "outputs": [],
   "source": [
    "class Configs():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # general (half of these not in use)\n",
    "        self.num_workers = 0\n",
    "        self.epochs = 50\n",
    "        self.n_samples_per_class_per_batch = 10\n",
    "\n",
    "        # optimisation\n",
    "        self.learning_rate = 0.01\n",
    "        self.weight_decay = 1e-4\n",
    "        self.momentum = 0.9\n",
    "\n",
    "        # data - make sure it is the same size for quilted images\n",
    "        self.image_size = 500\n",
    "\n",
    "        self.prefix = \"tmp\"\n",
    "        \n",
    "        self.device = \"cpu\" # \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Paths\n",
    "        # =============================================================================\n",
    "        \n",
    "        # we read and write to an external directory!!\n",
    "        self.base_path = r\"C:/Users/Prinzessin/projects/decentnet\"\n",
    "        if not os.path.exists(self.base_path):\n",
    "            os.makedirs(self.base_path)\n",
    "        os.chdir(self.base_path) # this is now the main directory !!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        # input for decentblock\n",
    "        self.csv_filenames =     [f\"results/{self.prefix}/masks_info_label.csv\"]\n",
    "        self.concepts_path =     f\"data/{self.prefix}/concepts\"\n",
    "        \n",
    "        # input for decentnet\n",
    "        self.train_path =        r\"data/images/train\"\n",
    "        self.val_path =          r\"data/images/val\"\n",
    "        self.test_path =         r\"data/images/test\"\n",
    "\n",
    "        # output\n",
    "        self.ckpt_net_path =     f\"results/{self.prefix}/ckpts/decentnet\"\n",
    "        self.ckpt_blocks_path =  f\"results/{self.prefix}/ckpts/decentblock\"\n",
    "        self.results_path =      f\"results/{self.prefix}\"\n",
    "        \n",
    "        if not os.path.exists(self.ckpt_net_path):\n",
    "            os.makedirs(self.ckpt_net_path)\n",
    "        if not os.path.exists(self.ckpt_blocks_path):\n",
    "            os.makedirs(self.ckpt_blocks_path)\n",
    "            \n",
    "        # =============================================================================\n",
    "        # activate function calls\n",
    "        # =============================================================================    \n",
    "            \n",
    "        self.run_decentblocks = True\n",
    "        self.run_decentnet = False\n",
    "        self.run_baseline = False\n",
    "        self.run_visualisation = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZKOheDtVvdYn",
   "metadata": {
    "id": "ZKOheDtVvdYn"
   },
   "source": [
    "# 𝔻𝕖𝕔𝕖𝕟𝕥𝕌𝕟𝕚𝕥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ATBINCQDj5nY",
   "metadata": {
    "id": "ATBINCQDj5nY"
   },
   "source": [
    "## DecentBlock Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18c174",
   "metadata": {
    "id": "0f18c174"
   },
   "outputs": [],
   "source": [
    "class DecentBlock_MLP_routine():\n",
    "    \n",
    "    def __init__(self, configs):\n",
    "        \n",
    "        self.configs = configs\n",
    "        \n",
    "    def set_loader(self, mode=\"train\", ci_concept=0):\n",
    "        # =============================================================================\n",
    "        # construct data loader\n",
    "        # =============================================================================\n",
    "        \n",
    "        p_aug = 0.5\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            dataset = PosNegConceptDataset(mode=\"train\", channels=3, index_col=0, image_size=self.configs.image_size, csv_filenames=self.configs.csv_filenames, ci_concept=ci_concept, concepts_path=self.configs.concepts_path, p_aug=p_aug)\n",
    "            # dataset = EyeDataset(mode=\"train\", ci_concept=ci_concept, image_size=self.configs.image_size)\n",
    "            mbs = MixedBatchSampler(dataset.get_class_labels(), n_samples_per_class_per_batch=self.configs.n_samples_per_class_per_batch)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                dataset, \n",
    "                batch_sampler = mbs,\n",
    "                num_workers=self.configs.num_workers)\n",
    "        elif mode == \"val\":\n",
    "            dataset = PosNegConceptDataset(mode=\"val\", channels=3, index_col=0, image_size=self.configs.image_size, csv_filenames=self.configs.csv_filenames, ci_concept=ci_concept, concepts_path=self.configs.concepts_path, p_aug=p_aug)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                dataset, \n",
    "                batch_size=1, # should be 1\n",
    "                shuffle=False,\n",
    "                num_workers=self.configs.num_workers)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def set_optimizer(self, model):\n",
    "        # =============================================================================\n",
    "        # =============================================================================\n",
    "\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                                lr=self.configs.learning_rate,\n",
    "                                momentum=self.configs.momentum,\n",
    "                                weight_decay=self.configs.weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, model, optimizer, epoch, save_file):\n",
    "        # =============================================================================\n",
    "        # =============================================================================\n",
    "\n",
    "        state = {\n",
    "            'model': model.encoder.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, save_file)\n",
    "        del state\n",
    "\n",
    "    def train(self, loader, model, criterion, optimizer, epoch):\n",
    "        # =============================================================================\n",
    "        # DecentBlock\n",
    "        # one epoch training\n",
    "        # trainings pair (2) - 2 augmented versions\n",
    "        # batch size (8)\n",
    "        # image size (256 x 256)\n",
    "        # (8 x (2 x (256 x 256) ) )\n",
    "        \n",
    "        model.train()\n",
    "        loss_epoch = []    \n",
    "        for idx, batch in enumerate(loader):\n",
    "\n",
    "            #print(\"train \"*10)\n",
    "            \n",
    "            images, labels = batch[\"img\"], batch[\"lbl\"]\n",
    "            \n",
    "            images = torch.cat([images[0], images[1]], dim=0)\n",
    "\n",
    "            batch_size = labels.shape[0]\n",
    "            \n",
    "            features = model(images.to(self.configs.device))\n",
    "            f1, f2 = torch.split(features, [batch_size, batch_size], dim=0)\n",
    "            features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)        \n",
    "            loss = criterion(features, labels)\n",
    "\n",
    "            # SGD\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "            #print(\"+\"*50)\n",
    "            \n",
    "        return np.mean(loss_epoch)\n",
    "\n",
    "\n",
    "    def val(self, loader, model, criterion, epoch):\n",
    "        # =============================================================================\n",
    "        # validation decentblock\n",
    "        # trainings pair (2) - 2 augmented versions\n",
    "        # batch size (8)\n",
    "        # image size (256 x 256)\n",
    "        # (8 x (2 x (256 x 256) ) )\n",
    "        # =============================================================================\n",
    "        \n",
    "        model.eval()\n",
    "        loss_epoch = []    \n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(loader):\n",
    "            \n",
    "                images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                \n",
    "                images = torch.cat([images[0], images[1]], dim=0)\n",
    "\n",
    "                batch_size = labels.shape[0]\n",
    "\n",
    "                features = model(images.to(self.configs.device))\n",
    "                f1, f2 = torch.split(features, [batch_size, batch_size], dim=0)\n",
    "                features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)        \n",
    "                loss = criterion(features, labels)\n",
    "\n",
    "                loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "        return np.mean(loss_epoch)\n",
    "\n",
    "def decentblock_routine(configs):\n",
    "    \n",
    "    \n",
    "    temp = 0.07\n",
    "    criterion = SupConLoss(temperature=temp)\n",
    "\n",
    "    run = DecentBlock_MLP_routine(configs=configs)\n",
    "\n",
    "    # todo\n",
    "    concept_list = list(range(7))\n",
    "\n",
    "    for ci_concept in concept_list:\n",
    "        print(configs.concepts_path)\n",
    "        # for ci_concept in concept_list:\n",
    "\n",
    "        #print(\"concept\", ci_concept)\n",
    "\n",
    "        decent_block_mlp = DecentBlock(None, None, 128, device=configs.device, mode=\"train_mlp\")\n",
    "        decent_block_mlp = decent_block_mlp.to(configs.device)\n",
    "        criterion = criterion.to(configs.device)\n",
    "\n",
    "        # build data loader\n",
    "        train_loader = run.set_loader(mode=\"train\", ci_concept=ci_concept)\n",
    "        # val_loader = run.set_loader(mode=\"val\", batch_size=1, ci_concept=ci_concept)\n",
    "\n",
    "        # print(\"train_loader:\", train_loader.__len__())\n",
    "        # print(\"val_loader:\", val_loader.__len__())\n",
    "        \n",
    "        # build optimizer\n",
    "        optimizer = run.set_optimizer(decent_block_mlp)\n",
    "\n",
    "        best_loss = 0\n",
    "        # training routine\n",
    "        for epoch in range(1, configs.epochs + 1):\n",
    "\n",
    "            # train for one epoch\n",
    "            loss_train_epoch = run.train(train_loader, decent_block_mlp, criterion, optimizer, epoch)        \n",
    "            #loss_val_epoch = run.val(val_loader, decent_block_mlp, criterion, iterations)\n",
    "\n",
    "            #print(\"iter: \", iter)\n",
    "            #print(\"loss_train_epoch\", loss_train_epoch)\n",
    "            #print(\"loss_val_epoch\", loss_val_epoch)\n",
    "            \n",
    "            #if epoch < 5: # for the first 5 epochs\n",
    "            #    best_loss = loss_val_epoch\n",
    "            #elif best_loss > loss_val_epoch: # then if loss is lower than best loss (aka better)\n",
    "            #    best_loss = loss_val_epoch\n",
    "\n",
    "            best_loss = loss_train_epoch\n",
    "            save_file = os.path.join(ckpt_blocks_path, f'{prefix}_mlp_{ci_concept}_ep{iter}_{round(best_loss, 4)}.ckpt')\n",
    "            run.save_model(decent_block_mlp, optimizer, iter, save_file)\n",
    "\n",
    "        # save the last model\n",
    "        #save_file = os.path.join(ckpt_net_path, f'mlp_{ci_concept}_last_{round(loss_val_epoch, 4)}.ckpt')\n",
    "        #run.save_model(decent_block_mlp, optimizer, iter, save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ktmPY-WevwxD",
   "metadata": {
    "id": "ktmPY-WevwxD"
   },
   "source": [
    "# 𝔻𝕖𝕔𝕖𝕟𝕥ℕ𝕖𝕥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11567602",
   "metadata": {
    "id": "11567602"
   },
   "source": [
    "**How to freeze layers**\n",
    "\n",
    "The layers of DecentBlocks need to be frozen while training the DecentNet\n",
    "\n",
    "* Just adding this here for completeness. You can also freeze parameters in place without iterating over them with requires_grad_ (API).\n",
    "\n",
    "* For example say you have a RetinaNet and want to just fine-tune on the heads\n",
    "\n",
    "```\n",
    "class RetinaNet(torch.nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        self.backbone = ResNet(...)\n",
    "        self.fpn = FPN(...)\n",
    "        self.box_head = torch.nn.Sequential(...)\n",
    "        self.cls_head = torch.nn.Sequential(...)\n",
    "```\n",
    "\n",
    "Then you could freeze the backbone and FPN like this:\n",
    "\n",
    "* Getting the model\n",
    "```\n",
    "retinanet = RetinaNet(...)\n",
    "```\n",
    "\n",
    "* Freezing backbone and FPN\n",
    "```\n",
    "retinanet.backbone.requires_grad_(False)\n",
    "retinanet.fpn.requires_grad_(False)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a7359",
   "metadata": {
    "id": "908a7359"
   },
   "source": [
    "**Fusing** \n",
    "\n",
    "Fusing methods: mostly element-wise sum or maximum operations have been studied for fusing CNN feature maps from multiple views for the purpose of classification. Correspondence between multiple views is thereby lost, while fusion by concatenation or convolution were found to efficiently model correspondences between different views for other learning tasks. Comparative evaluations of different strategies for image classification are either missing or yield contradicting results.\n",
    "\n",
    "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0245230\n",
    "\n",
    "In early fusion, convolutional feature maps from the different CNN branches are stacked and subsequently processed together. \n",
    "\n",
    "\n",
    "We consider two different approaches for depth reduction: \n",
    "* (1) early fusion (max): max-pooling of the stacked feature map across the nV views\n",
    "* (2) early fusion (conv): 1 × 1 convolution across the depth of the stacked feature maps.\n",
    "\n",
    "stacking, 1x1 conv || max\n",
    "\n",
    "\n",
    "\n",
    "* In order to further achieve effective fusion of local and global features of images, this paper uses gated fusion sub-networks to adaptively fuse multiple feature maps obtained based on branch networks\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/sheryl-ai/MVGCN/blob/master/models.py\n",
    "```\n",
    "    def _view_pool(self, view_features, name, method='max'):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
    "\n",
    "        vp = tf.expand_dims(view_features[0], 0) # eg. [100] -> [1, 100]\n",
    "        for v in view_features[1:]:\n",
    "            v = tf.expand_dims(v, 0)\n",
    "            vp = tf.concat([vp, v], axis=0)\n",
    "        print ('vp before reducing:', vp.get_shape().as_list())\n",
    "        if method == 'max':\n",
    "            vp = tf.reduce_max(vp, [0], name=name)\n",
    "        elif method == 'mean':\n",
    "            vp = tf.reduce_mean(vp, [0], name=name)\n",
    "        return vp\n",
    "```    \n",
    "    \n",
    "    \n",
    "https://github.com/VChristlein/dgmp/blob/master/clamm/pooling.py\n",
    "\n",
    "\n",
    "Fusion of features after layer three of a CNN. The features from two streams are passed through max pooling, convolution, batch normalization and ReLU layers. The two outputs are then concatenated and form the input for the fourth layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bR00xLAQw0Am",
   "metadata": {
    "id": "bR00xLAQw0Am"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a65599",
   "metadata": {
    "id": "f3a65599"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet50, shufflenet_v2_x1_0\n",
    "import os\n",
    "\n",
    "class DecentNet_v1(nn.Module):\n",
    "    \n",
    "    # for freezing in train code: retinanet.backbone.requires_grad_(False)\n",
    "        \n",
    "    def __init__(self, num_classes=3, plot=False):\n",
    "        super(DecentNet_v1, self).__init__()\n",
    "        \n",
    "        print(\"init decentnet start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # get ckpt files\n",
    "        block_ckpts = os.listdir(ckpt_blocks_path)\n",
    "\n",
    "        # loads blocks\n",
    "        self.decent_blocks = nn.ModuleList([])\n",
    "\n",
    "        decent_block_116 = \"\"\n",
    "        \n",
    "        amount_of_blocks = 0\n",
    "        for block_ckpt in block_ckpts:\n",
    "            \n",
    "            # we use a shufflenet pretrained on data from a previous step\n",
    "            # these layer should be frozen during training of this model\n",
    "            decent_block = shufflenet_v2_x1_0()\n",
    "            decent_block.fc = nn.Identity()\n",
    "\n",
    "            # load shuffle net weights here\n",
    "            checkpoint = torch.load(os.path.join(ckpt_blocks_path, block_ckpt))\n",
    "            decent_block.load_state_dict(checkpoint['model'])\n",
    "\n",
    "            if True:\n",
    "              # remove layers - this line might be wrong too\n",
    "              decent_block_116 = nn.Sequential(*(list(decent_block.children())[:-4]), \n",
    "                                              nn.Conv2d(116, 5, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1)),\n",
    "                                              nn.BatchNorm2d(5),\n",
    "                                              nn.ReLU()\n",
    "                                              ).to(device)\n",
    "            else:\n",
    "              # not working for some reason ... needs to be in nn.Seq\n",
    "              decent_block_116 = nn.Sequential(*(list(decent_block.children())[:-4])).to(device)\n",
    "              self.decent_block_reduction = nn.Conv2d(116, 5, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1))\n",
    "            \n",
    "            self.decent_blocks.append(decent_block_116) # 116 output filters\n",
    "            amount_of_blocks += 1\n",
    "\n",
    "\n",
    "        if plot:\n",
    "            print(\"original\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block)\n",
    "            print(\"116\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block_116)\n",
    "                \n",
    "        # layer between decent blocks and combined layers\n",
    "        if False:\n",
    "          # single conv 1x1 ??\n",
    "          self.fusion_layer = nn.Conv2d(5*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1))\n",
    "        else:\n",
    "          # conv, batchnorm and relu\n",
    "          self.fusion_layer = nn.Sequential(\n",
    "                nn.Conv2d(5*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1)),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU()\n",
    "          )\n",
    "        \n",
    "        # combined layers\n",
    "        r50 = resnet50(pretrained=True)\n",
    "        \n",
    "        # remove early layers\n",
    "        r50.conv1 = nn.Identity()\n",
    "        r50.bn1 = nn.Identity()\n",
    "        r50.relu = nn.Identity()\n",
    "        r50.maxpool = nn.Identity()\n",
    "        r50.layer1 = nn.Identity()\n",
    "        r50.layer2 = nn.Identity()  \n",
    "                \n",
    "        # change classification head\n",
    "        in_features = r50.fc.in_features\n",
    "        r50.fc = nn.Linear(in_features, self.num_classes)\n",
    "\n",
    "        self.combined_layers = r50 \n",
    "        \n",
    "        print(\"init decentnet done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        \n",
    "        # idea: use combined very early layers (pretrained on any dataset): edges\n",
    "        # (maybe for later, needs to be taken into account for the decent block training)\n",
    "        \n",
    "        \n",
    "        # get output for each decent block\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.decent_blocks):\n",
    "            block_output = block(image)\n",
    "            # block_output = self.decent_block_reduction(block_output)\n",
    "            block_outputs.append(block_output)\n",
    "            \n",
    "            # print(\"block output shape:\", block_output.shape)\n",
    "            \n",
    "        # concat features\n",
    "        concat = torch.cat(block_outputs, dim=1)\n",
    "        # print(\"concat shape:\", concat.shape)\n",
    "\n",
    "        # fusion layer\n",
    "        fusion = self.fusion_layer(concat)\n",
    "\n",
    "        # print(\"fusion output:\", fusion.shape)\n",
    "        \n",
    "        # combined layers\n",
    "        feature_vector = self.combined_layers(fusion)\n",
    "            \n",
    "        # print(\"combined layers output shape\", feature_vector.shape) \n",
    "        # print(feature_vector.shape) \n",
    "                        \n",
    "        return feature_vector\n",
    "    \n",
    "\n",
    "class DecentNet_v2(nn.Module):\n",
    "    \n",
    "    # for freezing in train code: retinanet.backbone.requires_grad_(False)\n",
    "        \n",
    "    def __init__(self, num_classes=3, plot=False):\n",
    "        super(DecentNet_v2, self).__init__()\n",
    "        \n",
    "        print(\"init decentnet start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # get ckpt files\n",
    "        block_ckpts = os.listdir(ckpt_blocks_path)\n",
    "\n",
    "        # loads blocks\n",
    "        self.decent_blocks = nn.ModuleList([])\n",
    "\n",
    "        decent_block_116 = \"\"\n",
    "        \n",
    "        amount_of_blocks = 0\n",
    "        for block_ckpt in block_ckpts:\n",
    "            \n",
    "            # we use a shufflenet pretrained on data from a previous step\n",
    "            # these layer should be frozen during training of this model\n",
    "            decent_block = shufflenet_v2_x1_0()\n",
    "            decent_block.fc = nn.Identity()\n",
    "\n",
    "            # load shuffle net weights here\n",
    "            checkpoint = torch.load(os.path.join(ckpt_blocks_path, block_ckpt))\n",
    "            decent_block.load_state_dict(checkpoint['model'])\n",
    "\n",
    "            # remove layers - this line might be wrong too\n",
    "            decent_block_116 = nn.Sequential(*(list(decent_block.children())[:-4])).to(device)\n",
    "\n",
    "            \n",
    "            \n",
    "            self.decent_blocks.append(decent_block_116) # 116 output filters\n",
    "            amount_of_blocks += 1\n",
    "\n",
    "        if plot:\n",
    "            print(\"original\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block)\n",
    "            print(\"116\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block_116)\n",
    "                \n",
    "        # layer between decent blocks and combined layers\n",
    "        # todo, figure out how to get the 116 automatically - 58 * 2\n",
    "        # fusion_conv\n",
    "        if True:\n",
    "            self.fusion_layer = nn.Conv2d(116*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1))\n",
    "        else:\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Conv2d(116*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1)),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # combined layers\n",
    "        r50 = resnet50(pretrained=True)\n",
    "        \n",
    "        # remove early layers\n",
    "        r50.conv1 = nn.Identity()\n",
    "        r50.bn1 = nn.Identity()\n",
    "        r50.relu = nn.Identity()\n",
    "        r50.maxpool = nn.Identity()\n",
    "        r50.layer1 = nn.Identity()\n",
    "        r50.layer2 = nn.Identity()  \n",
    "        \n",
    "        # torch.nn.Sequential(*list(r50.children())[3:]) - this is not working\n",
    "        \n",
    "        # change classification head\n",
    "        in_features = r50.fc.in_features\n",
    "        r50.fc = nn.Linear(in_features, self.num_classes)\n",
    "      \n",
    "        self.combined_layers = r50 \n",
    "        \n",
    "        print(\"init decentnet done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        \n",
    "        # idea: use combined very early layers (pretrained on any dataset): edges\n",
    "        # (maybe for later, needs to be taken into account for the decent block training)\n",
    "        \n",
    "        \n",
    "        # get output for each decent block\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.decent_blocks):\n",
    "            block_output = block(image)\n",
    "            block_outputs.append(block_output)\n",
    "            # print(\"block output shape:\", block_output.shape)\n",
    "            \n",
    "        # concat features\n",
    "        concat = torch.cat(block_outputs, dim=1)\n",
    "        # print(\"concat shape:\", concat.shape)\n",
    "\n",
    "        # fusion layer\n",
    "        fusion = self.fusion_layer(concat)\n",
    "        \n",
    "        # combined layers\n",
    "        feature_vector = self.combined_layers(fusion)\n",
    "            \n",
    "        # print(\"combined layers output shape\", feature_vector.shape) \n",
    "        # print(feature_vector.shape) \n",
    "                        \n",
    "        return feature_vector\n",
    "    \n",
    "    \n",
    "\n",
    "class DecentNet_v3(nn.Module):\n",
    "    \n",
    "    # for freezing in train code: retinanet.backbone.requires_grad_(False)\n",
    "        \n",
    "    def __init__(self, num_classes=3):\n",
    "        super(DecentNet_v1, self).__init__()\n",
    "        \n",
    "        print(\"init decentnet start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # prepare early blocks list\n",
    "        ckpt_early_blocks = os.listdir(ckpt_early_blocks_path)\n",
    "        self.early_blocks = nn.ModuleList([])\n",
    "        early_block_116 = None\n",
    "        for i, early_block in enumerate(ckpt_early_blocks):            \n",
    "            early_block_116 = DecentBlock_Shuffle_EarlyBlock(ckpt_early_blocks_path, early_block, out_channels=5)\n",
    "            self.early_blocks.append(early_block_116) # 116 output filters\n",
    "                \n",
    "        # prepare early fusion (optional)\n",
    "        self.early_fusion_module = None\n",
    "        \n",
    "        # prepare late blocks list (optional)\n",
    "        self.late_blocks = nn.ModuleList([])\n",
    "        self.late_blocks.append(DecentBlock_ResNet_LateBlock())\n",
    "        \n",
    "        # prepare late fusion (optional)\n",
    "        self.late_fusion_module = None\n",
    "        \n",
    "        # prepare head blocks list\n",
    "        self.head_blocks = nn.ModuleList([])\n",
    "        self.late_blocks.append(DecentBlock_ResNet_LateBlock())\n",
    "\n",
    "        print(\"init decentnet done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # early block(s)\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.early_blocks):\n",
    "            block_output = block(x)\n",
    "            block_outputs.append(block_output)\n",
    "            \n",
    "        # concat + fusion\n",
    "        if block_outputs:\n",
    "            x = torch.cat(block_outputs, dim=1)\n",
    "            x = self.early_fusion_module(x)\n",
    "        \n",
    "        # late block(s)\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.late_blocks):\n",
    "            block_output = block(x)\n",
    "            block_outputs.append(block_output)\n",
    "            \n",
    "        # concat + fusion\n",
    "        if block_outputs:\n",
    "            x = torch.cat(block_outputs, dim=1)\n",
    "            x = self.late_fusion_module(x)\n",
    "        \n",
    "        # head block(s) (multi-task)            \n",
    "        model_outputs = {}\n",
    "        for i, block in enumerate(self.head_blocks):\n",
    "            # model output of head = pass feature vector through head\n",
    "            model_outputs[type(head).__name__] = fc_layer(x)\n",
    "        \n",
    "        return model_outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EZ6CElCnj-nL",
   "metadata": {
    "id": "EZ6CElCnj-nL"
   },
   "source": [
    "## DecentNet Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oCM5QGTfyd-d",
   "metadata": {
    "id": "oCM5QGTfyd-d"
   },
   "outputs": [],
   "source": [
    "class DecentNet_routine():\n",
    "\n",
    "    def set_loader(self, mode=\"train\", batch_size=2, num_workers=0, image_size=500):\n",
    "        # construct data loader\n",
    "                    \n",
    "        if mode == \"train\":\n",
    "                train_transform = transforms.Compose([\n",
    "                    ResizeCrop(image_size=image_size),\n",
    "                    RandomAugmentations(),\n",
    "                    ToTensor(),\n",
    "                    Normalise()\n",
    "                ])\n",
    "\n",
    "                train_dataset = datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "\n",
    "                print(set(train_dataset.targets))\n",
    "\n",
    "                loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True,\n",
    "                    num_workers=num_workers)\n",
    "            \n",
    "        elif mode == \"val\":\n",
    "\n",
    "            val_transform = transforms.Compose([\n",
    "                    ResizeCrop(image_size=image_size),\n",
    "                    ToTensor(),\n",
    "                    Normalise()\n",
    "            ])\n",
    "\n",
    "            val_dataset = datasets.ImageFolder(root=val_path, transform=val_transform)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                    val_dataset, \n",
    "                    batch_size=batch_size, # should be 1\n",
    "                    shuffle=False,\n",
    "                    num_workers=num_workers)\n",
    "\n",
    "        elif mode == \"test\":\n",
    "\n",
    "            test_transforms = transforms.Compose([\n",
    "                    ResizeCrop(image_size=image_size),\n",
    "                    ToTensor(),\n",
    "                    Normalise()\n",
    "            ])\n",
    "\n",
    "            test_dataset = datasets.ImageFolder(root=test_path, transform=test_transforms)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                    test_dataset, \n",
    "                    batch_size=batch_size, # should be 1 \n",
    "                    shuffle=False,\n",
    "                    num_workers=num_workers)\n",
    "            \n",
    "\n",
    "        return loader\n",
    "\n",
    "    def set_optimizer(self, model):\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                                lr=learning_rate,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "    def save_model(self, model, optimizer, epoch, save_file):\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, save_file)\n",
    "        del state\n",
    "\n",
    "    def train(self, loader, model, criterion, optimizer, epoch):\n",
    "        \"\"\"one epoch training\"\"\"\n",
    "        \n",
    "        model.train()\n",
    "        loss_epoch = []\n",
    "        ground_truth_all = []\n",
    "        model_output_all = []  \n",
    "        for idx, (images, labels) in enumerate(loader):\n",
    "            \n",
    "\n",
    "            # images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                        \n",
    "            model_output = model(images.to(device))\n",
    "            loss = criterion(model_output, labels.to(device))\n",
    "\n",
    "            # SGD\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "            _, highest_class = torch.max(model_output, 1)    \n",
    "            highest_class = highest_class.detach().cpu().numpy()    \n",
    "\n",
    "            ground_truth_all.extend(labels.detach().cpu().numpy())\n",
    "            model_output_all.extend(highest_class)\n",
    "\n",
    "        print(ground_truth_all)\n",
    "        print(model_output_all)\n",
    "            \n",
    "        f_score_epoch = f1_score(y_true = ground_truth_all, y_pred = model_output_all, average=\"weighted\", labels=[0,1,2])\n",
    "\n",
    "        return np.mean(loss_epoch), f_score_epoch\n",
    "\n",
    "\n",
    "    def val(self, loader, model, criterion, epoch):\n",
    "        # decentnet\n",
    "        # with labels\n",
    "        \"\"\"validation\"\"\"\n",
    "        \n",
    "        model.eval()\n",
    "        loss_epoch = []   \n",
    "        ground_truth_all = []\n",
    "        model_output_all = [] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, labels) in enumerate(loader):\n",
    "            \n",
    "                # images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                        \n",
    "                model_output = model(images.to(device))\n",
    "                loss = criterion(model_output, labels.to(device))\n",
    "                        \n",
    "                loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "                _, highest_class = torch.max(model_output, 1)    \n",
    "                highest_class = highest_class.detach().cpu().numpy()    \n",
    "\n",
    "                ground_truth_all.extend(labels.detach().cpu().numpy())\n",
    "                model_output_all.extend(highest_class)\n",
    "\n",
    "        print(ground_truth_all)\n",
    "        print(model_output_all)\n",
    "\n",
    "        f_score_epoch = f1_score(y_true = ground_truth_all, y_pred = model_output_all, average=\"weighted\", labels=[0,1,2])\n",
    "\n",
    "        return np.mean(loss_epoch), f_score_epoch\n",
    "\n",
    "\n",
    "    def test(self, loader, model):\n",
    "        # decentnet - results for non existant labels\n",
    "        # without labels\n",
    "        \n",
    "        model.eval()\n",
    "        highest_classes = [] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, labels) in enumerate(loader):\n",
    "            \n",
    "                # images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                        \n",
    "                model_output = model(images.to(device))\n",
    "                _, highest_class = torch.max(model_output, 1)    \n",
    "                highest_class = highest_class.detach().cpu().numpy()\n",
    "\n",
    "                highest_classes.extend(highest_class)    \n",
    "\n",
    "\n",
    "        print(highest_classes)\n",
    "\n",
    "        df = pd.DataFrame({'Prediction': highest_classes})\n",
    "        df.to_csv(\"test_decentnet.csv\")\n",
    "\n",
    "\n",
    "    def visualise_with_labels(self, loader, model):\n",
    "        pass\n",
    "\n",
    "\n",
    "def decentnet_routine():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    run = DecentNet_routine()\n",
    "\n",
    "    decentnet = DecentNet_v2(num_classes=3).to(device)\n",
    "\n",
    "    # freeze decent blocks\n",
    "    if True:\n",
    "        for i, child in enumerate(decentnet.decent_blocks.children()):\n",
    "            # exclude the conv layer (or sequential???) - needs to be trainable\n",
    "            for param in child[:-1].parameters():\n",
    "                param.requires_grad = False\n",
    "    else:\n",
    "        # v1, without conv layer (that is trainable)\n",
    "        decentnet.decent_blocks.requires_grad_(False)\n",
    "\n",
    "    \n",
    "    decentnet = decentnet.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # set data loader\n",
    "    train_loader = run.set_loader(mode=\"train\", batch_size=8, num_workers=num_workers, image_size=image_size)\n",
    "    val_loader = run.set_loader(mode=\"val\", batch_size=1, num_workers=num_workers, image_size=image_size)\n",
    "\n",
    "    # set optimizer\n",
    "    optimizer = run.set_optimizer(decentnet)\n",
    "\n",
    "    best_loss = 0\n",
    "    iter = 0\n",
    "    # training routine\n",
    "    for iter in range(1, iterations + 1):\n",
    "\n",
    "            # train for one epoch\n",
    "            loss_train_epoch, fscore_train_epoch = run.train(loader=train_loader, model=decentnet, criterion=criterion, optimizer=optimizer, epoch=epoch)        \n",
    "            loss_val_epoch, fscore_val_epoch = run.val(loader=val_loader, model=decentnet, criterion=criterion, epoch=epoch)\n",
    "\n",
    "            print(\"iter: \", iter)\n",
    "            print(loss_train_epoch)\n",
    "            print(fscore_train_epoch)\n",
    "            print(loss_val_epoch)\n",
    "            print(fscore_val_epoch)\n",
    "            \n",
    "            if iter < 3: # for the first 3 epochs\n",
    "                best_loss = loss_val_epoch\n",
    "            elif best_loss > loss_val_epoch: # then if loss is lower than best loss (aka better)\n",
    "                best_loss = loss_val_epoch\n",
    "                save_file = os.path.join(ckpt_net_path, f'decentnet_epoch_{iter}_{round(best_loss, 4)}.ckpt')\n",
    "                run.save_model(decentnet, optimizer, iter, save_file)\n",
    "\n",
    "    # save the last model\n",
    "    save_file = os.path.join(ckpt_net_path, f'decentnet_last_{round(loss_val_epoch, 4)}.ckpt')\n",
    "    run.save_model(decentnet, optimizer, iter, save_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vg8CPRaSNdfv",
   "metadata": {
    "id": "Vg8CPRaSNdfv"
   },
   "source": [
    "# 𝔹𝕒𝕤𝕖𝕝𝕚𝕟𝕖 ℝ𝕖𝕤ℕ𝕖𝕥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aCIOrVGINkK-",
   "metadata": {
    "id": "aCIOrVGINkK-"
   },
   "source": [
    "## Model\n",
    "\n",
    "* early stopping: https://pythonguides.com/pytorch-early-stopping/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "APta2MnINdA7",
   "metadata": {
    "id": "APta2MnINdA7"
   },
   "outputs": [],
   "source": [
    "class DecentBaseline(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=3):\n",
    "        super(DecentBaseline, self).__init__()\n",
    "        \n",
    "        print(\"init baseline start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # resnet 50\n",
    "        r50 = resnet50(pretrained=True)\n",
    "\n",
    "        # change classification head\n",
    "        in_features = r50.fc.in_features\n",
    "        r50.fc = nn.Linear(in_features, self.num_classes)\n",
    "        \n",
    "        self.r50 = r50 \n",
    "        \n",
    "        print(\"init baseline done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        # resnet 50\n",
    "        feature_vector = self.r50(image)\n",
    "                        \n",
    "        return feature_vector\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rAN0jFnCQC0y",
   "metadata": {
    "id": "rAN0jFnCQC0y"
   },
   "source": [
    "## Baseline Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRVZHWPpQFFt",
   "metadata": {
    "id": "TRVZHWPpQFFt"
   },
   "outputs": [],
   "source": [
    "def baseline_routine():\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    run = DecentNet_routine()\n",
    "\n",
    "    ########## \n",
    "    # TRAIN and VAL Decent Baseline\n",
    "    ##########\n",
    "\n",
    "    baseline = DecentBaseline(num_classes=3).to(device)\n",
    "\n",
    "\n",
    "    baseline = baseline.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # build data loader\n",
    "    train_loader = run.set_loader(mode=\"train\", batch_size=8, num_workers=num_workers, image_size=image_size)\n",
    "    val_loader = run.set_loader(mode=\"val\", batch_size=1, num_workers=num_workers, image_size=image_size)\n",
    "\n",
    "    # build optimizer\n",
    "    optimizer = run.set_optimizer(baseline)\n",
    "\n",
    "    best_loss = 0\n",
    "    iter = 0\n",
    "    # training routine\n",
    "    for iter in range(1, iterations + 1):\n",
    "\n",
    "        # train for one epoch\n",
    "        loss_train_epoch, fscore_train_epoch = run.train(loader=train_loader, model=baseline, criterion=criterion, optimizer=optimizer, epoch=iter)        \n",
    "        loss_val_epoch, fscore_val_epoch = run.val(loader=val_loader, model=baseline, criterion=criterion, epoch=epoch)\n",
    "\n",
    "        print(\"iter: \", iter)\n",
    "        print(loss_train_epoch)\n",
    "        print(fscore_train_epoch)\n",
    "        print(loss_val_epoch)\n",
    "        print(fscore_val_epoch)\n",
    "        \n",
    "        if iter < 3: # for the first 3 epochs\n",
    "            best_loss = loss_val_epoch\n",
    "        elif best_loss > loss_val_epoch: # then if loss is lower than best loss (aka better)\n",
    "            best_loss = loss_val_epoch\n",
    "            save_file = os.path.join(ckpt_net_path, f'baseline_epoch_{iter}_{round(best_loss, 4)}.ckpt')\n",
    "            run.save_model(baseline, optimizer, iter, save_file)\n",
    "\n",
    "    # save the last model\n",
    "    save_file = os.path.join(ckpt_net_path, f'baseline_last_{round(loss_val_epoch, 4)}.ckpt')\n",
    "    run.save_model(baseline, optimizer, iter, save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09300e-4f70-42b5-bc34-3b4ce3cf2c9e",
   "metadata": {},
   "source": [
    "# BIMT with convolution (this obviously didn't work either)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93b3b05c-caa3-4795-86c2-b9a1ae9bded3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nloss_fn = nn.CrossEntropyLoss()\\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\\n \\nn_epochs = 20\\nfor epoch in range(n_epochs):\\n    for inputs, labels in trainloader:\\n        # forward, backward, and then weight update\\n        y_pred = model(inputs)\\n        loss = loss_fn(y_pred, labels)\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n \\n    acc = 0\\n    count = 0\\n    for inputs, labels in testloader:\\n        y_pred = model(inputs)\\n        acc += (torch.argmax(y_pred, 1) == labels).float().sum()\\n        count += len(labels)\\n    acc /= count\\n    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\\n \\ntorch.save(model.state_dict(), \"cifar10model.pth\")\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    " \n",
    "    \n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    torchvision.transforms.ToTensor()\n",
    "     ])\n",
    "# transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    " \n",
    "#trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    " \n",
    "#batch_size = 32\n",
    "#trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "#testloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    " \n",
    "class CIFAR10Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        #self.drop1 = nn.Dropout(0.3)\n",
    " \n",
    "        self.conv2 = nn.Conv2d(5, 4, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.conv1x1 = nn.Conv2d(4, 5, kernel_size=(1,1))       \n",
    "        \n",
    "        # need global max pooling here??\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        #import torch.nn.functional as F\n",
    "        #output = F.max_pool2d(input, kernel_size=input.size()[2:])\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    " \n",
    "        #self.fc3 = nn.Linear(8192, 512)\n",
    "        #self.act3 = nn.ReLU()\n",
    "        #self.drop3 = nn.Dropout(0.5)\n",
    " \n",
    "        #self.fc4 = nn.Linear(512, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        print(\"shapes\")\n",
    "        print(\"***************\")\n",
    "        # input 3x32x32, output 32x32x32\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.act1(self.conv1(x))\n",
    "        #x = self.drop1(x)\n",
    "        # input 32x32x32, output 32x32x32\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.act2(self.conv2(x))\n",
    "        # input 32x32x32, output 32x16x16\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.conv1x1(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        # global max pooling\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size=x.size()[2:]).squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        # x = self.pool2(x)\n",
    "        # input 32x16x16, output 8192\n",
    "        x = self.flat(x)\n",
    "        # input 8192, output 512\n",
    "        #x = self.act3(self.fc3(x))\n",
    "        #x = self.drop3(x)\n",
    "        # input 512, output 10\n",
    "        #x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = CIFAR10Model()\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "\"\"\"\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    " \n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        # forward, backward, and then weight update\n",
    "        y_pred = model(inputs)\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "    acc = 0\n",
    "    count = 0\n",
    "    for inputs, labels in testloader:\n",
    "        y_pred = model(inputs)\n",
    "        acc += (torch.argmax(y_pred, 1) == labels).float().sum()\n",
    "        count += len(labels)\n",
    "    acc /= count\n",
    "    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))\n",
    " \n",
    "torch.save(model.state_dict(), \"cifar10model.pth\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f85dbafe-9e92-4858-b04f-6642de36aa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR10Model(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act1): ReLU()\n",
       "  (conv2): Conv2d(6, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act2): ReLU()\n",
       "  (conv1x1): Conv2d(4, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7872dcc-56ec-42da-b745-97ca8eb9ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes\n",
      "***************\n",
      "torch.Size([2, 1, 100, 100])\n",
      "torch.Size([2, 6, 100, 100])\n",
      "torch.Size([2, 6, 50, 50])\n",
      "torch.Size([2, 4, 50, 50])\n",
      "torch.Size([2, 4, 25, 25])\n",
      "torch.Size([2, 5, 25, 25])\n",
      "torch.Size([2, 5])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 100, 100]              60\n",
      "              ReLU-2          [-1, 6, 100, 100]               0\n",
      "         MaxPool2d-3            [-1, 6, 50, 50]               0\n",
      "            Conv2d-4            [-1, 4, 50, 50]             220\n",
      "              ReLU-5            [-1, 4, 50, 50]               0\n",
      "         MaxPool2d-6            [-1, 4, 25, 25]               0\n",
      "            Conv2d-7            [-1, 5, 25, 25]              25\n",
      "           Flatten-8                    [-1, 5]               0\n",
      "================================================================\n",
      "Total params: 305\n",
      "Trainable params: 305\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 1.23\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 1.26\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "torchsummary.summary(model, (1, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3401b5a0-14d8-4c0d-8675-2aa54eb525af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([6])\n",
      "torch.Size([4, 6, 3, 3])\n",
      "torch.Size([4])\n",
      "torch.Size([5, 4, 1, 1])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "# print(params[0])\n",
    "for param in params:\n",
    "    print(param.size())\n",
    "#print(params[0].size())\n",
    "#print(params[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9931dbd3-3d17-4285-9a2d-136a819f2188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "shapes\n",
      "***************\n",
      "torch.Size([1, 128, 128])\n",
      "torch.Size([5, 128, 128])\n",
      "torch.Size([5, 64, 64])\n",
      "torch.Size([4, 64, 64])\n",
      "torch.Size([4, 32, 32])\n",
      "torch.Size([5, 32, 32])\n",
      "torch.Size([5])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3396\\2892875891.py\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# run feature map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\decentnet\\datasceyence\\helper\\visualisation\\feature_map.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, img_tensor)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_maps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3396\\1146418522.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# x = self.pool2(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# input 32x16x16, output 8192\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;31m# input 8192, output 512\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m#x = self.act3(self.fc3(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\flatten.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from helper.visualisation.feature_map import *\n",
    "\n",
    "device=\"cuda\"\n",
    "\n",
    "# get one example image\n",
    "ichallenge_data = torchvision.datasets.ImageFolder('examples/example_data/iChallenge')\n",
    "img, label = ichallenge_data.__getitem__(1)\n",
    "\n",
    "# tensor preparation\n",
    "#to_tensor = transforms.ToTensor()\n",
    "#img = to_tensor(img).to(device)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    torchvision.transforms.Resize(350),\n",
    "    torchvision.transforms.CenterCrop((128,128)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "     ])\n",
    "\n",
    "img = transform(img).to(device)\n",
    "\n",
    "\n",
    "# layer to focus on\n",
    "#print(\"*\"*50)\n",
    "#print(\"example graph nodes:\", get_graph_node_names(model)[0][0:20])\n",
    "#print(\"*\"*50)\n",
    "layer = model.conv2 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "\n",
    "# run feature map\n",
    "dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "dd.run(img)\n",
    "dd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f079b3da-eb5c-48e7-817d-7d0aec486644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116.66666666666667"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "350/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50c9cba8-b5f9-41d9-acce-28eac21fb440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*2*2*2*2*2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f4836f70-2a9c-4b65-9a10-cbbc1d38365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv2d(4, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "shapes\n",
      "***************\n",
      "torch.Size([1, 128, 128])\n",
      "torch.Size([5, 128, 128])\n",
      "torch.Size([5, 64, 64])\n",
      "torch.Size([4, 64, 64])\n",
      "torch.Size([4, 32, 32])\n",
      "torch.Size([5, 32, 32])\n",
      "torch.Size([5])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3396\\3808234733.py\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# run feature map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\decentnet\\datasceyence\\helper\\visualisation\\feature_map.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, img_tensor)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_maps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3396\\1146418522.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# x = self.pool2(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# input 32x16x16, output 8192\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;31m# input 8192, output 512\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m#x = self.act3(self.fc3(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\flatten.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from helper.visualisation.feature_map import *\n",
    "\n",
    "device=\"cuda\"\n",
    "\n",
    "# get one example image\n",
    "ichallenge_data = torchvision.datasets.ImageFolder('examples/example_data/iChallenge')\n",
    "img, label = ichallenge_data.__getitem__(1)\n",
    "\n",
    "# tensor preparation\n",
    "#to_tensor = transforms.ToTensor()\n",
    "#img = to_tensor(img).to(device)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    torchvision.transforms.Resize(350),\n",
    "    torchvision.transforms.CenterCrop((128,128)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "     ])\n",
    "img = transform(img).to(device)\n",
    "\n",
    "\n",
    "# layer to focus on\n",
    "#print(\"*\"*50)\n",
    "#print(\"example graph nodes:\", get_graph_node_names(model)[0][0:20])\n",
    "#print(\"*\"*50)\n",
    "layer = model.conv1x1 # model.conv1[0] # model.stage2[0].branch1[2] # model.fusion_layer # conv\n",
    "\n",
    "# run feature map\n",
    "dd = FeatureMap(model=model, layer=layer, device=device, iterations=None, lr=None)\n",
    "dd.run(img)\n",
    "dd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2a4e70af-5483-40c8-9932-9255bd50ae50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFAR10Model(\n",
       "  (conv1): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act1): ReLU()\n",
       "  (conv2): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act2): ReLU()\n",
       "  (conv1x1): Conv2d(4, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bb329-bd5a-4b9d-ba02-5022924b2bd9",
   "metadata": {},
   "source": [
    "## visualise channels in filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "748cd6e2-0c0f-4330-86f8-b6900d56aecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 3, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "def visChannels(tensor, ch=0, allkernels=False, nrow=8, padding=1): \n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    \n",
    "    plt.figure(figsize=(nrow,rows) )\n",
    "    plt.title(f\"Channels with index {ch}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "def visFilters(tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.title(f\"Filter {filt}\")\n",
    "    plt.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "\n",
    "def visFilters_subplot(subplot, tensor, filt=0, allkernels=False, nrow=8, padding=1): \n",
    "    f,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(f*c, -1, w, h)\n",
    "    elif f != 3: tensor = tensor[filt,:,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    # plt.figure( figsize=(nrow,rows) )\n",
    "    subplot.set_title(f\"Filter {filt+1} with {c} channels\")\n",
    "    subplot.imshow(grid.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "    subplot.axis('off')\n",
    "    \n",
    "layer = 1\n",
    "filter = model.conv2.weight.data.clone()\n",
    "\n",
    "print(model.conv2.weight.shape)\n",
    "\n",
    "# need to match the network parameters!!!!\n",
    "in_channels = 5\n",
    "out_filters = 4\n",
    "\n",
    "\n",
    "fig, subplot = plt.subplots(out_filters)\n",
    "fig.suptitle('Layer')\n",
    "\n",
    "for filt in range(0, out_filters):\n",
    "    \n",
    "    visFilters_subplot(subplot[filt], filter, filt=filt, allkernels=False)\n",
    "\n",
    "    #plt.axis('off')\n",
    "    #plt.ioff()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"examples/example_results/filter_with_weights.png\")\n",
    "plt.show()\n",
    "    \n",
    "if False:    \n",
    "    for filt in range(0, out_filters):\n",
    "\n",
    "        visFilters(filter, filt=filt, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.tight_layout()\n",
    "        #plt.savefig(f\"examples/example_results/filter_with_weights.png\")\n",
    "        plt.show()\n",
    "\n",
    "    for ch in range(0, in_channels):\n",
    "\n",
    "        visChannels(filter, ch=ch, allkernels=False)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb129e4-2cfc-47ae-bb34-81703f498913",
   "metadata": {},
   "source": [
    "## understand channel importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b1340-35d9-432e-8514-3db96ee3c2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99583483-005c-47a4-b9ce-df576be728d6",
   "metadata": {},
   "source": [
    "## understand filter importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5366b-56fd-4627-8950-4e924c6dfb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "h5Tv2km0K77I",
   "metadata": {
    "id": "h5Tv2km0K77I"
   },
   "source": [
    "# 𝔽𝕦𝕟𝕔𝕥𝕚𝕠𝕟 𝕔𝕒𝕝𝕝𝕤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca581f7-22aa-4923-a1b9-22c23033aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750b8b5-bf7e-47e2-b215-39b5f4d46df9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3ZIQeL3VK-BI",
    "outputId": "61172eba-eea9-4ed1-95f0-9d02836c332c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if configs.run_decentblocks:\n",
    "    decentblock_routine(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6vh627W0zPLo",
   "metadata": {
    "id": "6vh627W0zPLo"
   },
   "outputs": [],
   "source": [
    "if configs.run_decentnet:\n",
    "    decentnet_routine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pRGZLABtzQdE",
   "metadata": {
    "id": "pRGZLABtzQdE"
   },
   "outputs": [],
   "source": [
    "if configs.run_baseline:\n",
    "    baseline_routine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O-MGutwczQ7L",
   "metadata": {
    "id": "O-MGutwczQ7L"
   },
   "outputs": [],
   "source": [
    "if configs.run_visualisation:\n",
    "    visualisation_routine()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nPKap00_wfgL"
   ],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
