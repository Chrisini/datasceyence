{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Aaq4vTX6lxUS",
   "metadata": {
    "id": "Aaq4vTX6lxUS"
   },
   "source": [
    "# ùîªùïñùïîùïñùïüùï•‚Ñïùïñùï•: ùïïùïöùï§-ùïñùïüùï•-ùïíùïüùïòùïùùïñùïï ùïüùïñùï•\n",
    "Goal: create a neural network with disentangled early hidden layers. We want to analyse which textures contribute to the predictions.\n",
    "\n",
    "In this notebook you can\n",
    "\n",
    "1) train one or multiple DecentBlocks\n",
    "  * we use a supervised contrastive loss function\n",
    "\n",
    "2) train a DecentNet\n",
    "  * we use the cross entropy loss\n",
    "  * we freeze and use the DecentBlocks as part of the DecentNet\n",
    "  * a fusion layer is the bridge between the DecentBlocks and the combined layers\n",
    "\n",
    "3) Baseline\n",
    "\n",
    "4) NOT HERE RIGHT NOW visualise the DecentBlocks and DecentNet (work in progress)\n",
    "  * DeepDreams\n",
    "  * Feature maps\n",
    "  * Filters\n",
    "\n",
    "Todos:\n",
    "* [ ] metrics / tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pYryiJ8XKkm4",
   "metadata": {
    "id": "pYryiJ8XKkm4"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae53756",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eae53756",
    "outputId": "1392d54c-c059-4187-80fb-2bbb803edb84"
   },
   "outputs": [],
   "source": [
    "# basics\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import shufflenet_v2_x1_0, resnet50\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "from helper.dataset.transform.transform import ToTensor, ResizeCrop, RandomAugmentations, Normalise\n",
    "from helper.dataset.transform.two_crop import *\n",
    "from helper.compute.loss.supcon import SupConLoss\n",
    "from helper.sampler.mixed_batch import MixedBatchSampler\n",
    "from helper.dataset.concept import ClusterConceptDataset, PosNegConceptDataset\n",
    "from helper.model.decentblock import *\n",
    "\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "try:\n",
    "    from torchvision.models import ShuffleNet_V2_X1_0_Weights\n",
    "except:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244d212",
   "metadata": {
    "id": "4244d212"
   },
   "source": [
    "# Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "TMN4hZi_yFAD",
   "metadata": {
    "id": "TMN4hZi_yFAD"
   },
   "outputs": [],
   "source": [
    "class Configs():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # general (half of these not in use)\n",
    "        self.num_workers = 0\n",
    "        self.epochs = 50\n",
    "        self.n_samples_per_class_per_batch = 10\n",
    "\n",
    "        # optimisation\n",
    "        self.learning_rate = 0.01\n",
    "        self.weight_decay = 1e-4\n",
    "        self.momentum = 0.9\n",
    "\n",
    "        # data - make sure it is the same size for quilted images\n",
    "        self.image_size = 500\n",
    "\n",
    "        self.prefix = \"tmp\"\n",
    "        \n",
    "        self.device = \"cpu\" # \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Paths\n",
    "        # =============================================================================\n",
    "        \n",
    "        # we read and write to an external directory!!\n",
    "        self.base_path = r\"C:/Users/Prinzessin/projects/decentnet\"\n",
    "        if not os.path.exists(self.base_path):\n",
    "            os.makedirs(self.base_path)\n",
    "        os.chdir(self.base_path) # this is now the main directory !!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        # input for decentblock\n",
    "        self.csv_filenames =     [f\"results/{self.prefix}/masks_info_label.csv\"]\n",
    "        self.concepts_path =     f\"data/{self.prefix}/concepts\"\n",
    "        \n",
    "        # input for decentnet\n",
    "        self.train_path =        r\"data/images/train\"\n",
    "        self.val_path =          r\"data/images/val\"\n",
    "        self.test_path =         r\"data/images/test\"\n",
    "\n",
    "        # output\n",
    "        self.ckpt_net_path =     f\"results/{self.prefix}/ckpts/decentnet\"\n",
    "        self.ckpt_blocks_path =  f\"results/{self.prefix}/ckpts/decentblock\"\n",
    "        self.results_path =      f\"results/{self.prefix}\"\n",
    "        \n",
    "        if not os.path.exists(self.ckpt_net_path):\n",
    "            os.makedirs(self.ckpt_net_path)\n",
    "        if not os.path.exists(self.ckpt_blocks_path):\n",
    "            os.makedirs(self.ckpt_blocks_path)\n",
    "            \n",
    "        # =============================================================================\n",
    "        # activate function calls\n",
    "        # =============================================================================    \n",
    "            \n",
    "        self.run_decentblocks = True\n",
    "        self.run_decentnet = False\n",
    "        self.run_baseline = False\n",
    "        self.run_visualisation = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZKOheDtVvdYn",
   "metadata": {
    "id": "ZKOheDtVvdYn"
   },
   "source": [
    "# ùîªùïñùïîùïñùïüùï•ùïåùïüùïöùï•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ATBINCQDj5nY",
   "metadata": {
    "id": "ATBINCQDj5nY"
   },
   "source": [
    "## DecentBlock Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f18c174",
   "metadata": {
    "id": "0f18c174"
   },
   "outputs": [],
   "source": [
    "class DecentBlock_MLP_routine():\n",
    "    \n",
    "    def __init__(self, configs):\n",
    "        \n",
    "        self.configs = configs\n",
    "        \n",
    "    def set_loader(self, mode=\"train\", ci_concept=0):\n",
    "        # =============================================================================\n",
    "        # construct data loader\n",
    "        # =============================================================================\n",
    "        \n",
    "        p_aug = 0.5\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            dataset = PosNegConceptDataset(mode=\"train\", channels=3, index_col=0, image_size=self.configs.image_size, csv_filenames=self.configs.csv_filenames, ci_concept=ci_concept, concepts_path=self.configs.concepts_path, p_aug=p_aug)\n",
    "            # dataset = EyeDataset(mode=\"train\", ci_concept=ci_concept, image_size=self.configs.image_size)\n",
    "            mbs = MixedBatchSampler(dataset.get_class_labels(), n_samples_per_class_per_batch=self.configs.n_samples_per_class_per_batch)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                dataset, \n",
    "                batch_sampler = mbs,\n",
    "                num_workers=self.configs.num_workers)\n",
    "        elif mode == \"val\":\n",
    "            dataset = PosNegConceptDataset(mode=\"val\", channels=3, index_col=0, image_size=self.configs.image_size, csv_filenames=self.configs.csv_filenames, ci_concept=ci_concept, concepts_path=self.configs.concepts_path, p_aug=p_aug)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                dataset, \n",
    "                batch_size=1, # should be 1\n",
    "                shuffle=False,\n",
    "                num_workers=self.configs.num_workers)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def set_optimizer(self, model):\n",
    "        # =============================================================================\n",
    "        # =============================================================================\n",
    "\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                                lr=self.configs.learning_rate,\n",
    "                                momentum=self.configs.momentum,\n",
    "                                weight_decay=self.configs.weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, model, optimizer, epoch, save_file):\n",
    "        # =============================================================================\n",
    "        # =============================================================================\n",
    "\n",
    "        state = {\n",
    "            'model': model.encoder.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, save_file)\n",
    "        del state\n",
    "\n",
    "    def train(self, loader, model, criterion, optimizer, epoch):\n",
    "        # =============================================================================\n",
    "        # DecentBlock\n",
    "        # one epoch training\n",
    "        # trainings pair (2) - 2 augmented versions\n",
    "        # batch size (8)\n",
    "        # image size (256 x 256)\n",
    "        # (8 x (2 x (256 x 256) ) )\n",
    "        \n",
    "        model.train()\n",
    "        loss_epoch = []    \n",
    "        for idx, batch in enumerate(loader):\n",
    "\n",
    "            #print(\"train \"*10)\n",
    "            \n",
    "            images, labels = batch[\"img\"], batch[\"lbl\"]\n",
    "            \n",
    "            images = torch.cat([images[0], images[1]], dim=0)\n",
    "\n",
    "            batch_size = labels.shape[0]\n",
    "            \n",
    "            features = model(images.to(self.configs.device))\n",
    "            f1, f2 = torch.split(features, [batch_size, batch_size], dim=0)\n",
    "            features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)        \n",
    "            loss = criterion(features, labels)\n",
    "\n",
    "            # SGD\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "            #print(\"+\"*50)\n",
    "            \n",
    "        return np.mean(loss_epoch)\n",
    "\n",
    "\n",
    "    def val(self, loader, model, criterion, epoch):\n",
    "        # =============================================================================\n",
    "        # validation decentblock\n",
    "        # trainings pair (2) - 2 augmented versions\n",
    "        # batch size (8)\n",
    "        # image size (256 x 256)\n",
    "        # (8 x (2 x (256 x 256) ) )\n",
    "        # =============================================================================\n",
    "        \n",
    "        model.eval()\n",
    "        loss_epoch = []    \n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(loader):\n",
    "            \n",
    "                images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                \n",
    "                images = torch.cat([images[0], images[1]], dim=0)\n",
    "\n",
    "                batch_size = labels.shape[0]\n",
    "\n",
    "                features = model(images.to(self.configs.device))\n",
    "                f1, f2 = torch.split(features, [batch_size, batch_size], dim=0)\n",
    "                features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)        \n",
    "                loss = criterion(features, labels)\n",
    "\n",
    "                loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "        return np.mean(loss_epoch)\n",
    "\n",
    "def decentblock_routine(configs):\n",
    "    \n",
    "    \n",
    "    temp = 0.07\n",
    "    criterion = SupConLoss(temperature=temp)\n",
    "\n",
    "    run = DecentBlock_MLP_routine(configs=configs)\n",
    "\n",
    "    # todo\n",
    "    concept_list = list(range(7))\n",
    "\n",
    "    for ci_concept in concept_list:\n",
    "        print(configs.concepts_path)\n",
    "        # for ci_concept in concept_list:\n",
    "\n",
    "        print(\"concept\", ci_concept)\n",
    "\n",
    "        decent_block_mlp = DecentBlock(None, None, 128, device=configs.device, mode=\"train_mlp\")\n",
    "        decent_block_mlp = decent_block_mlp.to(configs.device)\n",
    "        criterion = criterion.to(configs.device)\n",
    "\n",
    "        # build data loader\n",
    "        train_loader = run.set_loader(mode=\"train\", ci_concept=ci_concept)\n",
    "        # val_loader = run.set_loader(mode=\"val\", batch_size=1, ci_concept=ci_concept)\n",
    "\n",
    "        # print(\"train_loader:\", train_loader.__len__())\n",
    "        # print(\"val_loader:\", val_loader.__len__())\n",
    "        \n",
    "        # build optimizer\n",
    "        optimizer = run.set_optimizer(decent_block_mlp)\n",
    "\n",
    "        best_loss = 0\n",
    "        # training routine\n",
    "        for epoch in range(1, configs.epochs + 1):\n",
    "\n",
    "            # train for one epoch\n",
    "            loss_train_epoch = run.train(train_loader, decent_block_mlp, criterion, optimizer, epoch)        \n",
    "            #loss_val_epoch = run.val(val_loader, decent_block_mlp, criterion, iterations)\n",
    "\n",
    "            print(\"iter: \", iter)\n",
    "            print(\"loss_train_epoch\", loss_train_epoch)\n",
    "            #print(\"loss_val_epoch\", loss_val_epoch)\n",
    "            \n",
    "            #if epoch < 5: # for the first 5 epochs\n",
    "            #    best_loss = loss_val_epoch\n",
    "            #elif best_loss > loss_val_epoch: # then if loss is lower than best loss (aka better)\n",
    "            #    best_loss = loss_val_epoch\n",
    "\n",
    "            best_loss = loss_train_epoch\n",
    "            save_file = os.path.join(ckpt_blocks_path, f'{prefix}_mlp_{ci_concept}_ep{iter}_{round(best_loss, 4)}.ckpt')\n",
    "            run.save_model(decent_block_mlp, optimizer, iter, save_file)\n",
    "\n",
    "        # save the last model\n",
    "        #save_file = os.path.join(ckpt_net_path, f'mlp_{ci_concept}_last_{round(loss_val_epoch, 4)}.ckpt')\n",
    "        #run.save_model(decent_block_mlp, optimizer, iter, save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ktmPY-WevwxD",
   "metadata": {
    "id": "ktmPY-WevwxD"
   },
   "source": [
    "# ùîªùïñùïîùïñùïüùï•‚Ñïùïñùï•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11567602",
   "metadata": {
    "id": "11567602"
   },
   "source": [
    "**How to freeze layers**\n",
    "\n",
    "The layers of DecentBlocks need to be frozen while training the DecentNet\n",
    "\n",
    "* Just adding this here for completeness. You can also freeze parameters in place without iterating over them with requires_grad_ (API).\n",
    "\n",
    "* For example say you have a RetinaNet and want to just fine-tune on the heads\n",
    "\n",
    "```\n",
    "class RetinaNet(torch.nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        self.backbone = ResNet(...)\n",
    "        self.fpn = FPN(...)\n",
    "        self.box_head = torch.nn.Sequential(...)\n",
    "        self.cls_head = torch.nn.Sequential(...)\n",
    "```\n",
    "\n",
    "Then you could freeze the backbone and FPN like this:\n",
    "\n",
    "* Getting the model\n",
    "```\n",
    "retinanet = RetinaNet(...)\n",
    "```\n",
    "\n",
    "* Freezing backbone and FPN\n",
    "```\n",
    "retinanet.backbone.requires_grad_(False)\n",
    "retinanet.fpn.requires_grad_(False)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a7359",
   "metadata": {
    "id": "908a7359"
   },
   "source": [
    "**Fusing** \n",
    "\n",
    "Fusing methods: mostly element-wise sum or maximum operations have been studied for fusing CNN feature maps from multiple views for the purpose of classification. Correspondence between multiple views is thereby lost, while fusion by concatenation or convolution were found to efficiently model correspondences between different views for other learning tasks. Comparative evaluations of different strategies for image classification are either missing or yield contradicting results.\n",
    "\n",
    "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0245230\n",
    "\n",
    "In early fusion, convolutional feature maps from the different CNN branches are stacked and subsequently processed together. \n",
    "\n",
    "\n",
    "We consider two different approaches for depth reduction: \n",
    "* (1) early fusion (max): max-pooling of the stacked feature map across the nV views\n",
    "* (2) early fusion (conv): 1 √ó 1 convolution across the depth of the stacked feature maps.\n",
    "\n",
    "stacking, 1x1 conv || max\n",
    "\n",
    "\n",
    "\n",
    "* In order to further achieve effective fusion of local and global features of images, this paper uses gated fusion sub-networks to adaptively fuse multiple feature maps obtained based on branch networks\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/sheryl-ai/MVGCN/blob/master/models.py\n",
    "```\n",
    "    def _view_pool(self, view_features, name, method='max'):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
    "\n",
    "        vp = tf.expand_dims(view_features[0], 0) # eg. [100] -> [1, 100]\n",
    "        for v in view_features[1:]:\n",
    "            v = tf.expand_dims(v, 0)\n",
    "            vp = tf.concat([vp, v], axis=0)\n",
    "        print ('vp before reducing:', vp.get_shape().as_list())\n",
    "        if method == 'max':\n",
    "            vp = tf.reduce_max(vp, [0], name=name)\n",
    "        elif method == 'mean':\n",
    "            vp = tf.reduce_mean(vp, [0], name=name)\n",
    "        return vp\n",
    "```    \n",
    "    \n",
    "    \n",
    "https://github.com/VChristlein/dgmp/blob/master/clamm/pooling.py\n",
    "\n",
    "\n",
    "Fusion of features after layer three of a CNN. The features from two streams are passed through max pooling, convolution, batch normalization and ReLU layers. The two outputs are then concatenated and form the input for the fourth layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bR00xLAQw0Am",
   "metadata": {
    "id": "bR00xLAQw0Am"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a65599",
   "metadata": {
    "id": "f3a65599"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet50, shufflenet_v2_x1_0\n",
    "import os\n",
    "\n",
    "class DecentNet_v1(nn.Module):\n",
    "    \n",
    "    # for freezing in train code: retinanet.backbone.requires_grad_(False)\n",
    "        \n",
    "    def __init__(self, num_classes=3, plot=False):\n",
    "        super(DecentNet_v1, self).__init__()\n",
    "        \n",
    "        print(\"init decentnet start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # get ckpt files\n",
    "        block_ckpts = os.listdir(ckpt_blocks_path)\n",
    "\n",
    "        # loads blocks\n",
    "        self.decent_blocks = nn.ModuleList([])\n",
    "\n",
    "        decent_block_116 = \"\"\n",
    "        \n",
    "        amount_of_blocks = 0\n",
    "        for block_ckpt in block_ckpts:\n",
    "            \n",
    "            # we use a shufflenet pretrained on data from a previous step\n",
    "            # these layer should be frozen during training of this model\n",
    "            decent_block = shufflenet_v2_x1_0()\n",
    "            decent_block.fc = nn.Identity()\n",
    "\n",
    "            # load shuffle net weights here\n",
    "            checkpoint = torch.load(os.path.join(ckpt_blocks_path, block_ckpt))\n",
    "            decent_block.load_state_dict(checkpoint['model'])\n",
    "\n",
    "            if True:\n",
    "              # remove layers - this line might be wrong too\n",
    "              decent_block_116 = nn.Sequential(*(list(decent_block.children())[:-4]), \n",
    "                                              nn.Conv2d(116, 5, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1)),\n",
    "                                              nn.BatchNorm2d(5),\n",
    "                                              nn.ReLU()\n",
    "                                              ).to(device)\n",
    "            else:\n",
    "              # not working for some reason ... needs to be in nn.Seq\n",
    "              decent_block_116 = nn.Sequential(*(list(decent_block.children())[:-4])).to(device)\n",
    "              self.decent_block_reduction = nn.Conv2d(116, 5, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1))\n",
    "            \n",
    "            self.decent_blocks.append(decent_block_116) # 116 output filters\n",
    "            amount_of_blocks += 1\n",
    "\n",
    "\n",
    "        if plot:\n",
    "            print(\"original\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block)\n",
    "            print(\"116\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block_116)\n",
    "                \n",
    "        # layer between decent blocks and combined layers\n",
    "        if False:\n",
    "          # single conv\n",
    "          self.fusion_layer = nn.Conv2d(5*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1))\n",
    "        else:\n",
    "          # conv, batchnorm and relu\n",
    "          self.fusion_layer = nn.Sequential(\n",
    "                nn.Conv2d(5*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1)),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU()\n",
    "          )\n",
    "        \n",
    "        # combined layers\n",
    "        r50 = resnet50(pretrained=True)\n",
    "        \n",
    "        # remove early layers\n",
    "        r50.conv1 = nn.Identity()\n",
    "        r50.bn1 = nn.Identity()\n",
    "        r50.relu = nn.Identity()\n",
    "        r50.maxpool = nn.Identity()\n",
    "        r50.layer1 = nn.Identity()\n",
    "        r50.layer2 = nn.Identity()  \n",
    "                \n",
    "        # change classification head\n",
    "        in_features = r50.fc.in_features\n",
    "        r50.fc = nn.Linear(in_features, self.num_classes)\n",
    "\n",
    "        self.combined_layers = r50 \n",
    "        \n",
    "        print(\"init decentnet done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        \n",
    "        # idea: use combined very early layers (pretrained on any dataset): edges\n",
    "        # (maybe for later, needs to be taken into account for the decent block training)\n",
    "        \n",
    "        \n",
    "        # get output for each decent block\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.decent_blocks):\n",
    "            block_output = block(image)\n",
    "            # block_output = self.decent_block_reduction(block_output)\n",
    "            block_outputs.append(block_output)\n",
    "            \n",
    "            # print(\"block output shape:\", block_output.shape)\n",
    "            \n",
    "        # concat features\n",
    "        concat = torch.cat(block_outputs, dim=1)\n",
    "        # print(\"concat shape:\", concat.shape)\n",
    "\n",
    "        # fusion layer\n",
    "        fusion = self.fusion_layer(concat)\n",
    "\n",
    "        # print(\"fusion output:\", fusion.shape)\n",
    "        \n",
    "        # combined layers\n",
    "        feature_vector = self.combined_layers(fusion)\n",
    "            \n",
    "        # print(\"combined layers output shape\", feature_vector.shape) \n",
    "        # print(feature_vector.shape) \n",
    "                        \n",
    "        return feature_vector\n",
    "    \n",
    "\n",
    "class DecentNet_v2(nn.Module):\n",
    "    \n",
    "    # for freezing in train code: retinanet.backbone.requires_grad_(False)\n",
    "        \n",
    "    def __init__(self, num_classes=3, plot=False):\n",
    "        super(DecentNet_v2, self).__init__()\n",
    "        \n",
    "        print(\"init decentnet start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # get ckpt files\n",
    "        block_ckpts = os.listdir(ckpt_blocks_path)\n",
    "\n",
    "        # loads blocks\n",
    "        self.decent_blocks = nn.ModuleList([])\n",
    "\n",
    "        decent_block_116 = \"\"\n",
    "        \n",
    "        amount_of_blocks = 0\n",
    "        for block_ckpt in block_ckpts:\n",
    "            \n",
    "            # we use a shufflenet pretrained on data from a previous step\n",
    "            # these layer should be frozen during training of this model\n",
    "            decent_block = shufflenet_v2_x1_0()\n",
    "            decent_block.fc = nn.Identity()\n",
    "\n",
    "            # load shuffle net weights here\n",
    "            checkpoint = torch.load(os.path.join(ckpt_blocks_path, block_ckpt))\n",
    "            decent_block.load_state_dict(checkpoint['model'])\n",
    "\n",
    "            # remove layers - this line might be wrong too\n",
    "            decent_block_116 = nn.Sequential(*(list(decent_block.children())[:-4])).to(device)\n",
    "\n",
    "            \n",
    "            \n",
    "            self.decent_blocks.append(decent_block_116) # 116 output filters\n",
    "            amount_of_blocks += 1\n",
    "\n",
    "        if plot:\n",
    "            print(\"original\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block)\n",
    "            print(\"116\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block_116)\n",
    "                \n",
    "        # layer between decent blocks and combined layers\n",
    "        # todo, figure out how to get the 116 automatically - 58 * 2\n",
    "        # fusion_conv\n",
    "        if True:\n",
    "            self.fusion_layer = nn.Conv2d(116*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1))\n",
    "        else:\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Conv2d(116*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1)),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # combined layers\n",
    "        r50 = resnet50(pretrained=True)\n",
    "        \n",
    "        # remove early layers\n",
    "        r50.conv1 = nn.Identity()\n",
    "        r50.bn1 = nn.Identity()\n",
    "        r50.relu = nn.Identity()\n",
    "        r50.maxpool = nn.Identity()\n",
    "        r50.layer1 = nn.Identity()\n",
    "        r50.layer2 = nn.Identity()  \n",
    "        \n",
    "        # torch.nn.Sequential(*list(r50.children())[3:]) - this is not working\n",
    "        \n",
    "        # change classification head\n",
    "        in_features = r50.fc.in_features\n",
    "        r50.fc = nn.Linear(in_features, self.num_classes)\n",
    "      \n",
    "        self.combined_layers = r50 \n",
    "        \n",
    "        print(\"init decentnet done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        \n",
    "        # idea: use combined very early layers (pretrained on any dataset): edges\n",
    "        # (maybe for later, needs to be taken into account for the decent block training)\n",
    "        \n",
    "        \n",
    "        # get output for each decent block\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.decent_blocks):\n",
    "            block_output = block(image)\n",
    "            block_outputs.append(block_output)\n",
    "            # print(\"block output shape:\", block_output.shape)\n",
    "            \n",
    "        # concat features\n",
    "        concat = torch.cat(block_outputs, dim=1)\n",
    "        # print(\"concat shape:\", concat.shape)\n",
    "\n",
    "        # fusion layer\n",
    "        fusion = self.fusion_layer(concat)\n",
    "        \n",
    "        # combined layers\n",
    "        feature_vector = self.combined_layers(fusion)\n",
    "            \n",
    "        # print(\"combined layers output shape\", feature_vector.shape) \n",
    "        # print(feature_vector.shape) \n",
    "                        \n",
    "        return feature_vector\n",
    "    \n",
    "    \n",
    "\n",
    "class DecentNet_v3(nn.Module):\n",
    "    \n",
    "    # for freezing in train code: retinanet.backbone.requires_grad_(False)\n",
    "        \n",
    "    def __init__(self, num_classes=3):\n",
    "        super(DecentNet_v1, self).__init__()\n",
    "        \n",
    "        print(\"init decentnet start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # prepare early blocks list\n",
    "        ckpt_early_blocks = os.listdir(ckpt_early_blocks_path)\n",
    "        self.early_blocks = nn.ModuleList([])\n",
    "        early_block_116 = None\n",
    "        for i, early_block in enumerate(ckpt_early_blocks):            \n",
    "            early_block_116 = DecentBlock_Shuffle_EarlyBlock(ckpt_early_blocks_path, early_block, out_channels=5)\n",
    "            self.early_blocks.append(early_block_116) # 116 output filters\n",
    "                \n",
    "        # prepare early fusion (optional)\n",
    "        self.early_fusion_module = None\n",
    "        \n",
    "        # prepare late blocks list (optional)\n",
    "        self.late_blocks = nn.ModuleList([])\n",
    "        self.late_blocks.append(DecentBlock_ResNet_LateBlock())\n",
    "        \n",
    "        # prepare late fusion (optional)\n",
    "        self.late_fusion_module = None\n",
    "        \n",
    "        # prepare head blocks list\n",
    "        self.head_blocks = nn.ModuleList([])\n",
    "        self.late_blocks.append(DecentBlock_ResNet_LateBlock())\n",
    "\n",
    "        print(\"init decentnet done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # early block(s)\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.early_blocks):\n",
    "            block_output = block(x)\n",
    "            block_outputs.append(block_output)\n",
    "            \n",
    "        # concat + fusion\n",
    "        if block_outputs:\n",
    "            x = torch.cat(block_outputs, dim=1)\n",
    "            x = self.early_fusion_module(x)\n",
    "        \n",
    "        # late block(s)\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.late_blocks):\n",
    "            block_output = block(x)\n",
    "            block_outputs.append(block_output)\n",
    "            \n",
    "        # concat + fusion\n",
    "        if block_outputs:\n",
    "            x = torch.cat(block_outputs, dim=1)\n",
    "            x = self.late_fusion_module(x)\n",
    "        \n",
    "        # head block(s) (multi-task)            \n",
    "        model_outputs = {}\n",
    "        for i, block in enumerate(self.head_blocks):\n",
    "            # model output of head = pass feature vector through head\n",
    "            model_outputs[type(head).__name__] = fc_layer(x)\n",
    "        \n",
    "        return model_outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EZ6CElCnj-nL",
   "metadata": {
    "id": "EZ6CElCnj-nL"
   },
   "source": [
    "## DecentNet Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "oCM5QGTfyd-d",
   "metadata": {
    "id": "oCM5QGTfyd-d"
   },
   "outputs": [],
   "source": [
    "class DecentNet_routine():\n",
    "\n",
    "    def set_loader(self, mode=\"train\", batch_size=2, num_workers=0, image_size=500):\n",
    "        # construct data loader\n",
    "                    \n",
    "        if mode == \"train\":\n",
    "                train_transform = transforms.Compose([\n",
    "                    ResizeCrop(image_size=image_size),\n",
    "                    RandomAugmentations(),\n",
    "                    ToTensor(),\n",
    "                    Normalise()\n",
    "                ])\n",
    "\n",
    "                train_dataset = datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "\n",
    "                print(set(train_dataset.targets))\n",
    "\n",
    "                loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True,\n",
    "                    num_workers=num_workers)\n",
    "            \n",
    "        elif mode == \"val\":\n",
    "\n",
    "            val_transform = transforms.Compose([\n",
    "                    ResizeCrop(image_size=image_size),\n",
    "                    ToTensor(),\n",
    "                    Normalise()\n",
    "            ])\n",
    "\n",
    "            val_dataset = datasets.ImageFolder(root=val_path, transform=val_transform)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                    val_dataset, \n",
    "                    batch_size=batch_size, # should be 1\n",
    "                    shuffle=False,\n",
    "                    num_workers=num_workers)\n",
    "\n",
    "        elif mode == \"test\":\n",
    "\n",
    "            test_transforms = transforms.Compose([\n",
    "                    ResizeCrop(image_size=image_size),\n",
    "                    ToTensor(),\n",
    "                    Normalise()\n",
    "            ])\n",
    "\n",
    "            test_dataset = datasets.ImageFolder(root=test_path, transform=test_transforms)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                    test_dataset, \n",
    "                    batch_size=batch_size, # should be 1 \n",
    "                    shuffle=False,\n",
    "                    num_workers=num_workers)\n",
    "            \n",
    "\n",
    "        return loader\n",
    "\n",
    "    def set_optimizer(self, model):\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                                lr=learning_rate,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "    def save_model(self, model, optimizer, epoch, save_file):\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, save_file)\n",
    "        del state\n",
    "\n",
    "    def train(self, loader, model, criterion, optimizer, epoch):\n",
    "        \"\"\"one epoch training\"\"\"\n",
    "        \n",
    "        model.train()\n",
    "        loss_epoch = []\n",
    "        ground_truth_all = []\n",
    "        model_output_all = []  \n",
    "        for idx, (images, labels) in enumerate(loader):\n",
    "            \n",
    "\n",
    "            # images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                        \n",
    "            model_output = model(images.to(device))\n",
    "            loss = criterion(model_output, labels.to(device))\n",
    "\n",
    "            # SGD\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "            _, highest_class = torch.max(model_output, 1)    \n",
    "            highest_class = highest_class.detach().cpu().numpy()    \n",
    "\n",
    "            ground_truth_all.extend(labels.detach().cpu().numpy())\n",
    "            model_output_all.extend(highest_class)\n",
    "\n",
    "        print(ground_truth_all)\n",
    "        print(model_output_all)\n",
    "            \n",
    "        f_score_epoch = f1_score(y_true = ground_truth_all, y_pred = model_output_all, average=\"weighted\", labels=[0,1,2])\n",
    "\n",
    "        return np.mean(loss_epoch), f_score_epoch\n",
    "\n",
    "\n",
    "    def val(self, loader, model, criterion, epoch):\n",
    "        # decentnet\n",
    "        # with labels\n",
    "        \"\"\"validation\"\"\"\n",
    "        \n",
    "        model.eval()\n",
    "        loss_epoch = []   \n",
    "        ground_truth_all = []\n",
    "        model_output_all = [] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, labels) in enumerate(loader):\n",
    "            \n",
    "                # images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                        \n",
    "                model_output = model(images.to(device))\n",
    "                loss = criterion(model_output, labels.to(device))\n",
    "                        \n",
    "                loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "                _, highest_class = torch.max(model_output, 1)    \n",
    "                highest_class = highest_class.detach().cpu().numpy()    \n",
    "\n",
    "                ground_truth_all.extend(labels.detach().cpu().numpy())\n",
    "                model_output_all.extend(highest_class)\n",
    "\n",
    "        print(ground_truth_all)\n",
    "        print(model_output_all)\n",
    "\n",
    "        f_score_epoch = f1_score(y_true = ground_truth_all, y_pred = model_output_all, average=\"weighted\", labels=[0,1,2])\n",
    "\n",
    "        return np.mean(loss_epoch), f_score_epoch\n",
    "\n",
    "\n",
    "    def test(self, loader, model):\n",
    "        # decentnet - results for non existant labels\n",
    "        # without labels\n",
    "        \n",
    "        model.eval()\n",
    "        highest_classes = [] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, labels) in enumerate(loader):\n",
    "            \n",
    "                # images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                        \n",
    "                model_output = model(images.to(device))\n",
    "                _, highest_class = torch.max(model_output, 1)    \n",
    "                highest_class = highest_class.detach().cpu().numpy()\n",
    "\n",
    "                highest_classes.extend(highest_class)    \n",
    "\n",
    "\n",
    "        print(highest_classes)\n",
    "\n",
    "        df = pd.DataFrame({'Prediction': highest_classes})\n",
    "        df.to_csv(\"test_decentnet.csv\")\n",
    "\n",
    "\n",
    "    def visualise_with_labels(self, loader, model):\n",
    "        pass\n",
    "\n",
    "\n",
    "def decentnet_routine():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    run = DecentNet_routine()\n",
    "\n",
    "    decentnet = DecentNet_v2(num_classes=3).to(device)\n",
    "\n",
    "    # freeze decent blocks\n",
    "    if True:\n",
    "        for i, child in enumerate(decentnet.decent_blocks.children()):\n",
    "            # exclude the conv layer (or sequential???) - needs to be trainable\n",
    "            for param in child[:-1].parameters():\n",
    "                param.requires_grad = False\n",
    "    else:\n",
    "        # v1, without conv layer (that is trainable)\n",
    "        decentnet.decent_blocks.requires_grad_(False)\n",
    "\n",
    "    \n",
    "    decentnet = decentnet.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # set data loader\n",
    "    train_loader = run.set_loader(mode=\"train\", batch_size=8, num_workers=num_workers, image_size=image_size)\n",
    "    val_loader = run.set_loader(mode=\"val\", batch_size=1, num_workers=num_workers, image_size=image_size)\n",
    "\n",
    "    # set optimizer\n",
    "    optimizer = run.set_optimizer(decentnet)\n",
    "\n",
    "    best_loss = 0\n",
    "    iter = 0\n",
    "    # training routine\n",
    "    for iter in range(1, iterations + 1):\n",
    "\n",
    "            # train for one epoch\n",
    "            loss_train_epoch, fscore_train_epoch = run.train(loader=train_loader, model=decentnet, criterion=criterion, optimizer=optimizer, epoch=epoch)        \n",
    "            loss_val_epoch, fscore_val_epoch = run.val(loader=val_loader, model=decentnet, criterion=criterion, epoch=epoch)\n",
    "\n",
    "            print(\"iter: \", iter)\n",
    "            print(loss_train_epoch)\n",
    "            print(fscore_train_epoch)\n",
    "            print(loss_val_epoch)\n",
    "            print(fscore_val_epoch)\n",
    "            \n",
    "            if iter < 3: # for the first 3 epochs\n",
    "                best_loss = loss_val_epoch\n",
    "            elif best_loss > loss_val_epoch: # then if loss is lower than best loss (aka better)\n",
    "                best_loss = loss_val_epoch\n",
    "                save_file = os.path.join(ckpt_net_path, f'decentnet_epoch_{iter}_{round(best_loss, 4)}.ckpt')\n",
    "                run.save_model(decentnet, optimizer, iter, save_file)\n",
    "\n",
    "    # save the last model\n",
    "    save_file = os.path.join(ckpt_net_path, f'decentnet_last_{round(loss_val_epoch, 4)}.ckpt')\n",
    "    run.save_model(decentnet, optimizer, iter, save_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vg8CPRaSNdfv",
   "metadata": {
    "id": "Vg8CPRaSNdfv"
   },
   "source": [
    "# ùîπùïíùï§ùïñùïùùïöùïüùïñ ‚Ñùùïñùï§‚Ñïùïñùï•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aCIOrVGINkK-",
   "metadata": {
    "id": "aCIOrVGINkK-"
   },
   "source": [
    "## Model\n",
    "\n",
    "* early stopping: https://pythonguides.com/pytorch-early-stopping/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "APta2MnINdA7",
   "metadata": {
    "id": "APta2MnINdA7"
   },
   "outputs": [],
   "source": [
    "class DecentBaseline(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=3):\n",
    "        super(DecentBaseline, self).__init__()\n",
    "        \n",
    "        print(\"init baseline start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # resnet 50\n",
    "        r50 = resnet50(pretrained=True)\n",
    "\n",
    "        # change classification head\n",
    "        in_features = r50.fc.in_features\n",
    "        r50.fc = nn.Linear(in_features, self.num_classes)\n",
    "        \n",
    "        self.r50 = r50 \n",
    "        \n",
    "        print(\"init baseline done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        # resnet 50\n",
    "        feature_vector = self.r50(image)\n",
    "                        \n",
    "        return feature_vector\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rAN0jFnCQC0y",
   "metadata": {
    "id": "rAN0jFnCQC0y"
   },
   "source": [
    "## Baseline Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "TRVZHWPpQFFt",
   "metadata": {
    "id": "TRVZHWPpQFFt"
   },
   "outputs": [],
   "source": [
    "def baseline_routine():\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    run = DecentNet_routine()\n",
    "\n",
    "    ########## \n",
    "    # TRAIN and VAL Decent Baseline\n",
    "    ##########\n",
    "\n",
    "    baseline = DecentBaseline(num_classes=3).to(device)\n",
    "\n",
    "\n",
    "    baseline = baseline.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # build data loader\n",
    "    train_loader = run.set_loader(mode=\"train\", batch_size=8, num_workers=num_workers, image_size=image_size)\n",
    "    val_loader = run.set_loader(mode=\"val\", batch_size=1, num_workers=num_workers, image_size=image_size)\n",
    "\n",
    "    # build optimizer\n",
    "    optimizer = run.set_optimizer(baseline)\n",
    "\n",
    "    best_loss = 0\n",
    "    iter = 0\n",
    "    # training routine\n",
    "    for iter in range(1, iterations + 1):\n",
    "\n",
    "        # train for one epoch\n",
    "        loss_train_epoch, fscore_train_epoch = run.train(loader=train_loader, model=baseline, criterion=criterion, optimizer=optimizer, epoch=iter)        \n",
    "        loss_val_epoch, fscore_val_epoch = run.val(loader=val_loader, model=baseline, criterion=criterion, epoch=epoch)\n",
    "\n",
    "        print(\"iter: \", iter)\n",
    "        print(loss_train_epoch)\n",
    "        print(fscore_train_epoch)\n",
    "        print(loss_val_epoch)\n",
    "        print(fscore_val_epoch)\n",
    "        \n",
    "        if iter < 3: # for the first 3 epochs\n",
    "            best_loss = loss_val_epoch\n",
    "        elif best_loss > loss_val_epoch: # then if loss is lower than best loss (aka better)\n",
    "            best_loss = loss_val_epoch\n",
    "            save_file = os.path.join(ckpt_net_path, f'baseline_epoch_{iter}_{round(best_loss, 4)}.ckpt')\n",
    "            run.save_model(baseline, optimizer, iter, save_file)\n",
    "\n",
    "    # save the last model\n",
    "    save_file = os.path.join(ckpt_net_path, f'baseline_last_{round(loss_val_epoch, 4)}.ckpt')\n",
    "    run.save_model(baseline, optimizer, iter, save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5Tv2km0K77I",
   "metadata": {
    "id": "h5Tv2km0K77I"
   },
   "source": [
    "# Function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca581f7-22aa-4923-a1b9-22c23033aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c750b8b5-bf7e-47e2-b215-39b5f4d46df9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3ZIQeL3VK-BI",
    "outputId": "61172eba-eea9-4ed1-95f0-9d02836c332c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/tmp/concepts\n",
      "concept 0\n",
      "this concept: lbl_Scars\n",
      "this concept: lbl_Scars\n",
      "             lbl\n",
      "A0004_322  False\n",
      "A0004_200  False\n",
      "A0001_186  False\n",
      "A0001_9    False\n",
      "A0001_310  False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24148\\918848390.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_decentblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdecentblock_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24148\\654321660.py\u001b[0m in \u001b[0;36mdecentblock_routine\u001b[1;34m(configs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[1;31m# train for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mloss_train_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecent_block_mlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m             \u001b[1;31m#loss_val_epoch = run.val(val_loader, decent_block_mlp, criterion, iterations)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24148\\654321660.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\decentnet\\datasceyence\\helper\\model\\decentblock.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecent_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\projects\\decentnet\\datasceyence\\helper\\model\\block\\shuffle_block.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mblock_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torchvision\\models\\shufflenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torchvision\\models\\shufflenetv2.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torchvision\\models\\shufflenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbranch1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchannel_shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\feta\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# type: ignore[has-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[has-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# use cumulative moving average\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if configs.run_decentblocks:\n",
    "    decentblock_routine(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6vh627W0zPLo",
   "metadata": {
    "id": "6vh627W0zPLo"
   },
   "outputs": [],
   "source": [
    "if configs.run_decentnet:\n",
    "    decentnet_routine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pRGZLABtzQdE",
   "metadata": {
    "id": "pRGZLABtzQdE"
   },
   "outputs": [],
   "source": [
    "if configs.run_baseline:\n",
    "    baseline_routine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O-MGutwczQ7L",
   "metadata": {
    "id": "O-MGutwczQ7L"
   },
   "outputs": [],
   "source": [
    "if configs.run_visualisation:\n",
    "    visualisation_routine()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nPKap00_wfgL"
   ],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
