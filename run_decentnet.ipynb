{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ð”»ð•–ð•”ð•–ð•Ÿð•¥â„•ð•–ð•¥: ð••ð•šð•¤ð•–ð•Ÿð•¥ð•’ð•Ÿð•˜ð•ð•–ð•• ð•Ÿð•–ð•¥\n",
    "\n",
    "Goal: create a sparse and modular ConvNet\n",
    "\n",
    "Todos: \n",
    "* [ ] delete node (filter) if either no input or no output edges\n",
    "* [ ]\n",
    "\n",
    "\n",
    "Notes:\n",
    "* additionally needed: position, activated channels, connection between channels\n",
    "* within this layer, a whole filter can be deactivated\n",
    "* within a filter, single channels can be deactivated\n",
    "* within this layer, filters can be swapped\n",
    "     \n",
    "* pruning actually doesn\"t work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf\n",
    "\n",
    "pip install:\n",
    "* pytorch_lightning\n",
    "\n",
    "\n",
    "warnings:\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73a26-f239-466f-901d-559d192f61de",
   "metadata": {},
   "source": [
    "![uml of code](examples/example_vis/uml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# alphabetic order misc\n",
    "# =============================================================================\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys \n",
    "sys.path.insert(0, \"helper\")\n",
    "from typing import Optional, List, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "# =============================================================================\n",
    "# torch\n",
    "# =============================================================================\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# =============================================================================\n",
    "# datasceyence\n",
    "# =============================================================================\n",
    "from helper.visualisation.feature_map import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397b450-c5c6-4da1-b630-7bf80e46891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch 2.0.0 == False\n",
      "tl 2.1.0 == False\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(19)\n",
    "torch.cuda.manual_seed(19)\n",
    "random.seed(19)\n",
    "np.random.seed(19)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "debug_model = False\n",
    "\n",
    "print('torch 2.0.0 ==', torch.__version__=='2.0.0')\n",
    "print('tl 2.1.0 ==', pl.__version__=='2.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b0be-245d-49c0-88b4-d94167f7da90",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97670e82-0d27-4b7f-91df-874acf9974a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'n_classes': 10,\n",
    "    'out_dim' :  [1, 6, 6, 4], # [1, 8, 16, 32], #[1, 16, 24, 32]\n",
    "    'grid_size' : 18*18,\n",
    "    'criterion': torch.nn.CrossEntropyLoss(),# torch.nn.BCEWithLogitsLoss(),\n",
    "    'optimizer': \"sgd\", # sgd adamw\n",
    "    'base_lr': 0.001,\n",
    "    'min_lr' : 0.00001,\n",
    "    'momentum' : 0.9,\n",
    "    'lr_update' : 100,\n",
    "    'cc_weight': 10,\n",
    "    'cc_metric' : 'l2', # connection cost metric (for loss) - distance metric\n",
    "    'ci_metric' : 'l2', # channel importance metric (for pruning)\n",
    "    'cm_metric' : 'count', # crossing minimisation \n",
    "    'update_every_nth_epoch' : 1, # 10\n",
    "    'pretrain_epochs' : 0,\n",
    "    'prune_keep' : 0.8, # 0.97, # in each epoch\n",
    "    'prune_keep_total' : 0.5, # this number is not exact, depends on the prune_keep value\n",
    "}\n",
    "\n",
    "train_kwargs = {\n",
    "    'result_path': \"examples/example_results\", # \"example_results/lightning_logs\", # not in use??\n",
    "    'exp_name': \"tmp_testi_oct\", # must include oct or retina\n",
    "    'load_ckpt_file' : \"xversion_22/checkpoints/epoch=0-unpruned=10942-val_f1=0.06.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "    'epochs': 3, # including the pretrain epochs - no adding up\n",
    "    'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ... # MNIST/MedMNIST 28 Ã— 28 Pixel\n",
    "    'batch_size': 1, # 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "    'log_every_n_steps' : 1, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "    'device': \"cuda\",\n",
    "    'num_workers' : 0, # 18, # 18 for computer, 0 for laptop\n",
    "    'train_size' : (3 * 2), # total or percentage\n",
    "    'val_size' : (3 * 2), # total or percentage\n",
    "    'test_size' : 6, # total or percentage - 0 for all\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d5ff36-c015-4bd6-8b12-c4d0e3b64b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {},
   "source": [
    "# DecentNet trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Prinzessin\\.medmnist\\octmnist.npz\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(size=train_kwargs[\"img_size\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])   \n",
    "\n",
    "rgb_transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(size=train_kwargs[\"img_size\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])   \n",
    "    \n",
    "if 'oct' in train_kwargs['exp_name']:\n",
    "    from medmnist import OCTMNIST\n",
    "    dataset = OCTMNIST(split=\"train\", transform=transform, download=True)\n",
    "    testset = OCTMNIST(split=\"test\", transform=transform, download=True) \n",
    "    model_kwargs['n_classes'] = 4\n",
    "    \n",
    "    indices = np.arange(len(dataset))  \n",
    "    train_indices, val_indices = train_test_split(indices, train_size=train_kwargs[\"train_size\"], test_size=train_kwargs[\"val_size\"], stratify=dataset.labels)\n",
    "    \n",
    "elif 'retina' in train_kwargs['exp_name']:\n",
    "    from medmnist import RetinaMNIST\n",
    "    dataset = RetinaMNIST(split=\"train\", transform=rgb_transform, download=True)\n",
    "    testset = RetinaMNIST(split=\"test\", transform=rgb_transform, download=True) \n",
    "    model_kwargs['n_classes'] = 5\n",
    "    \n",
    "    indices = np.arange(len(dataset))  \n",
    "    train_indices, val_indices = train_test_split(indices, train_size=train_kwargs[\"train_size\"], test_size=train_kwargs[\"val_size\"], stratify=dataset.labels)\n",
    "    \n",
    "else:\n",
    "    dataset = torchvision.datasets.MNIST(root=\"examples/example_data\", train=True, transform=transform, download=True)\n",
    "    testset = torchvision.datasets.MNIST(root=\"examples/example_data\", train=False, transform=transform, download=True)\n",
    "    model_kwargs['n_classes'] = 10\n",
    "    \n",
    "    indices = np.arange(len(dataset))  \n",
    "    train_indices, val_indices = train_test_split(indices, train_size=train_kwargs[\"train_size\"], test_size=train_kwargs[\"val_size\"], stratify=dataset.targets)\n",
    "\n",
    "\n",
    "train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_subset, shuffle=True, batch_size=train_kwargs[\"batch_size\"], num_workers=train_kwargs[\"num_workers\"])\n",
    "\n",
    "val_subset = torch.utils.data.Subset(dataset, val_indices)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_subset, shuffle=False, batch_size=train_kwargs[\"batch_size\"], num_workers=train_kwargs[\"num_workers\"]) # , persistent_workers=True)\n",
    "\n",
    "# batch size has to be 1\n",
    "if train_kwargs[\"test_size\"] > 0:\n",
    "    testset = torch.utils.data.Subset(testset, range(train_kwargs[\"test_size\"]))\n",
    "xai_dataloader = torch.utils.data.DataLoader(testset, shuffle=False, batch_size=1, num_workers=train_kwargs[\"num_workers\"]) # , persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a63827d6-1e9f-4537-a86f-00515d8578cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a0bf5e-8191-4182-a682-d1c34495a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac8cc05-fbe0-4125-910d-55c171f68101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.tensor(dataset.labels).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # an object with image representations and their positions\n",
    "    # amout of channels need to have same length as m and n lists\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        self.data = data # list of tensors (image representations)\n",
    "        self.ms_x = ms_x # list of integers (m position of each image representation)\n",
    "        self.ns_x = ns_x # list of integers (n position of each image representation)\n",
    "                \n",
    "    def setter(self, data, ms_x, ns_x):\n",
    "        self.data = data\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        \n",
    "    def getter(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfa123-54e1-4053-94c3-52b45ec0859c",
   "metadata": {},
   "source": [
    "## DecentFilter\n",
    "* conv2d problem: https://stackoverflow.com/questions/61269421/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # convolution happens in here\n",
    "    # one filter has multiple channels (aka weights)\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, ms_in, ns_in, m_this, n_this,\n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 padding_mode=\"zeros\",\n",
    "                 dilation=3, \n",
    "                 # transposed=None, \n",
    "                 device=None, \n",
    "                 dtype=None):\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        \n",
    "        # padding\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        \n",
    "         \n",
    "        # convolution\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding_mode = padding_mode\n",
    "        self.padding = padding\n",
    "        self.dilation = _pair(dilation)\n",
    "        #self.transposed = transposed\n",
    "        \n",
    "        # weights\n",
    "        assert len(ms_in) == len(ns_in), \"ms_in and ns_in are not of same length\"\n",
    "        self.n_weights = len(ms_in)\n",
    "        \n",
    "        # position, currently not trainable \n",
    "        # self.non_trainable_param = nn.Parameter(torch.Tensor([1.0]), requires_grad=False)\n",
    "        self.ms_in = nn.Parameter(torch.Tensor(ms_in), requires_grad=False) # ms_in # list\n",
    "        self.ns_in = nn.Parameter(torch.Tensor(ns_in), requires_grad=False) # ns_in # list\n",
    "        self.m_this = nn.Parameter(torch.Tensor([m_this]), requires_grad=False) # m_this # single integer\n",
    "        self.n_this = nn.Parameter(torch.Tensor([n_this]), requires_grad=False) # n_this # single integer\n",
    "        \n",
    "        # weight\n",
    "        # filters x channels x kernel x kernel\n",
    "        # self.weights = torch.autograd.Variable(torch.randn(1,n_weights,*self.kernel_size)).to(\"cuda\")\n",
    "        # self.weights = torch.nn.Parameter(torch.randn(1,n_weights,*self.kernel_size))\n",
    "        self.weights = torch.nn.Parameter(torch.empty((1, self.n_weights, *self.kernel_size), **factory_kwargs))\n",
    "        \n",
    "        #print(\"weight shape init\")\n",
    "        #print(self.weights.shape)\n",
    "            \n",
    "        # bias    \n",
    "        if False: \n",
    "            # bias:\n",
    "            # where should the bias be???\n",
    "            self.bias = Parameter(torch.empty(1, **factory_kwargs))\n",
    "        else:\n",
    "            #self.bias = False\n",
    "            # we only use bias via instance normalisation\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # reset weights and bias in filter\n",
    "        self.reset_parameters()\n",
    "            \n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*self.kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        torch.nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))        \n",
    "        \n",
    "    def forward(self, x:X) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # first, we have to remove channels in X\n",
    "        # this is because some channels in the filter are pruned (aka gone)\n",
    "        # then we can apply convolution\n",
    "        # parameters:\n",
    "        #    x = batch x channels x width x height\n",
    "        # returns:\n",
    "        #    x_data: batch x filters x width x height\n",
    "        # saves:\n",
    "        #    self.weights = 1 filter x channels x kernel x kernel\n",
    "        # =============================================================================\n",
    "        \n",
    "        \n",
    "        # POSITION MATCHER\n",
    "        # Find the indices (IDs) of channel pairs that exist in both the X and then filter\n",
    "        common_pairs = [[i_in, i_x] for i_in, (m_in, n_in) in enumerate(zip(self.ms_in, self.ns_in)) for i_x, (m_x, n_x) in enumerate(zip(x.ms_x, x.ns_x)) if (m_in==m_x and n_in==n_x)]\n",
    "        \n",
    "        if False:\n",
    "            print(common_pairs)\n",
    "            print(len(self.ms_in))\n",
    "            print(len(self.ns_in))\n",
    "            print(len(x.ms_x))\n",
    "            print(len(x.ns_x))\n",
    "\n",
    "            for pair in common_pairs:\n",
    "                print(f\"Common pair at indices {pair}: {self.ms_in[pair[0]], tmp_ms[pair[1]]}, {self.ns_in[pair[0]], tmp_ns[pair[1]]}\")\n",
    "        \n",
    "        common_pairs_a = np.array(common_pairs)\n",
    "        try:\n",
    "            f_ids = common_pairs_a[:,0]\n",
    "            x_ids = common_pairs_a[:,1]\n",
    "        except Exception as e:\n",
    "            print(\"error: no common pairs\")\n",
    "            print(\"pairs\", common_pairs_a)\n",
    "            print(\"pairs shape\", common_pairs_a.shape)\n",
    "            print(\"len ms in\", len(self.ms_in))\n",
    "            print(\"len ns in\", len(self.ns_in))\n",
    "            print(\"len ms x\", len(x.ms_x))\n",
    "            print(\"len ns x\", len(x.ns_x))\n",
    "            print(e)\n",
    "            \n",
    "            # in this case the whole filter should be removed\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        # filter data and weights based on common pairs of data and weights\n",
    "        tmp_d = x.data[:, x_ids, :, :]\n",
    "        tmp_w = self.weights[:, f_ids, :, :]\n",
    "        \n",
    "        # the final convolution\n",
    "        if self.padding_mode != 'zeros':\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(F.pad(tmp_d, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            tmp_w, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, groups=1)\n",
    "        else:\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(tmp_d, tmp_w, self.bias, self.stride, self.padding, self.dilation, groups=1)\n",
    "        \n",
    "        #print(\"tmp_w\", tmp_w.shape)\n",
    "        \n",
    "        # print(x_data.shape, \"- batch x filters x width x height\")\n",
    "        return x_data\n",
    "    \n",
    "    def setter(self, value, m_this, n_this):\n",
    "        # preliminary, not in use\n",
    "        self.weights = value # weights in this filter\n",
    "        self.m_this = m_this # single integer\n",
    "        self.n_this = n_this # single integer\n",
    "    \n",
    "    def getter(self):\n",
    "        # preliminary, not in use\n",
    "        return self.weights, self.m_this, self.n_this\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'DecentFilter(weights: ' + str(self.weights.shape) + ' at position: m_this=' + str(self.m_this) + ', n_this=' + str(self.n_this) + ')' + \\\n",
    "    '\\n with inputs: ms_in= ' + ', '.join(str(int(m.item())) for m in self.ms_in) + ', ns_in= ' + ', '.join(str(int(n.item())) for n in self.ns_in) + ')'\n",
    "    __repr__ = __str__\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdab4-e0b0-4523-882f-dad3ebf06090",
   "metadata": {},
   "source": [
    "## DecentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fddb74c7-5940-424d-aab8-2c75efd914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLayer(torch.nn.Module):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # we save filters of the layer in the self.filter_list\n",
    "    # each filter has a position (m_this, n_this)\n",
    "    # each filter has input positions (ms_in, ns_in)\n",
    "    #    - these vary between filters, as some are pruned\n",
    "    # at the moment we have to loop through the filter list\n",
    "    # convolution is applied to each filter separately which makes this very slow\n",
    "    #\n",
    "    # =============================================================================\n",
    "    __constants__ = ['stride', 'padding', 'dilation', # 'groups',\n",
    "                     'padding_mode', # 'n_channels', #  'output_padding', # 'n_filters',\n",
    "                     'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "                \n",
    "    def __init__(self, ms_in:list, ns_in:list, n_filters:int,\n",
    "                 kernel_size: _size_2_t,  \n",
    "                 stride: _size_2_t = 1,  \n",
    "                 padding: Union[str, _size_2_t] = 0,  \n",
    "                 dilation: _size_2_t = 1,\n",
    "                 model_kwargs=None,\n",
    "                 layer_name=None,\n",
    "                 #prune_keep:float = 0.9,\n",
    "                 #prune_keep_total:float = 0.5,\n",
    "                 #transposed: bool = False, \n",
    "                 #grid_size:int=81,\n",
    "                 #ci_metric=\"l2\",\n",
    "                 #output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 #groups: int = 1,\n",
    "                 bias: bool = True,  # not in use\n",
    "                 padding_mode: str = \"zeros\",  # not in use\n",
    "                 device=None,  # not in use\n",
    "                 dtype=None) -> None:\n",
    "        # =============================================================================\n",
    "        # initialisation\n",
    "        # parameters:\n",
    "        #    a lot.\n",
    "        # =============================================================================\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_name = layer_name\n",
    "        \n",
    "        # prune numbers\n",
    "        self.prune_keep = model_kwargs[\"prune_keep\"] # in each update [0.0:1.0]\n",
    "        self.prune_keep_total = model_kwargs[\"prune_keep_total\"] # total [0.0:1.0]\n",
    "        \n",
    "        # importance metric for pruning\n",
    "        self.ci_metric = model_kwargs[\"ci_metric\"]\n",
    "        # distance metric for loss\n",
    "        self.cc_metric = model_kwargs[\"cc_metric\"]\n",
    "        \n",
    "        # from prev layer\n",
    "        self.ms_in = ms_in\n",
    "        self.ns_in = ns_in\n",
    "        \n",
    "        self.original_size = len(self.ms_in) * n_filters\n",
    "        \n",
    "        \n",
    "        self.grid_size = model_kwargs[\"grid_size\"]\n",
    "        self.grid_sqrt = math.sqrt(self.grid_size)\n",
    "        assert self.grid_sqrt == int(self.grid_sqrt), f\"square root ({self.grid_sqrt}) from grid size {self.grid_size} not possible; possible exampes: 81 (9*9), 144 (12*12)\"\n",
    "        self.grid_sqrt = int(self.grid_sqrt)\n",
    "        \n",
    "        # use techniques from coo matrix\n",
    "        self.geometry_array = np.full(self.grid_size, np.nan)\n",
    "        # plus 1 here cause of to_sparse array\n",
    "        self.geometry_array[0:n_filters] = range(1,n_filters+1)\n",
    "        np.random.shuffle(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.reshape((self.grid_sqrt,self.grid_sqrt), order='C')\n",
    "        self.geometry_array = torch.tensor(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.to_sparse(sparse_dim=2).to(\"cuda\")\n",
    "\n",
    "        #print(self.geometry_array)\n",
    "        #print(self.geometry_array.values())\n",
    "\n",
    "        self.filter_list = torch.nn.ModuleList([])\n",
    "        for i_filter in range(n_filters):\n",
    "            # minus 1 here cause of to_sparse array\n",
    "            index = (self.geometry_array.values()-1 == i_filter).nonzero(as_tuple=True)[0]\n",
    "            m_this = self.geometry_array.indices()[0][index]\n",
    "            n_this = self.geometry_array.indices()[1][index]\n",
    "            f = DecentFilter(ms_in, ns_in, m_this, n_this, \n",
    "                             kernel_size=kernel_size, \n",
    "                             stride=stride, padding=padding, dilation=dilation)\n",
    "            self.filter_list.append(f)\n",
    "            # self.register_parameter(f\"filter {i_filter}\", f.weights)\n",
    "            \n",
    "            #torch.nn.Parameter(torch.empty((1, n_channels, *kernel_size), **factory_kwargs))\n",
    "    \n",
    "    def run_layer_connection_cost(self) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # compute connection cost for this layer - based on distance\n",
    "        # returns:\n",
    "        #    connection cost for the loss function\n",
    "        # notes:\n",
    "        #    currently using l2 norm, doesn't work that well\n",
    "        # sources:\n",
    "        #    adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        #    https://stackoverflow.com/questions/74086766/how-to-find-total-cost-of-each-path-in-graph-using-dictionary-in-python\n",
    "        # nonsense?\n",
    "        #    i don't even know what the following comments are about ... \n",
    "        #    based on previous layer (cause I only have input ms_in, n_in information)\n",
    "        #    mean( sum( of connection cost between this filter and all incoming filters\n",
    "        #    need it for loss - aka all layers, all filters together\n",
    "        #    need it for swapping - this layer, all filters\n",
    "        #    only the active ones (we need to use the indices for that)\n",
    "        #    for swapping i need ??\n",
    "        # =============================================================================\n",
    "         \n",
    "        from scipy.spatial.distance import cdist\n",
    "        \n",
    "        # connection cost list\n",
    "        cc = []\n",
    "        \n",
    "        \n",
    "        for f in self.filter_list:\n",
    "            # for each filter we use the current position and all incoming positions\n",
    "\n",
    "            #mn = torch.cat([torch.tensor(f.m_this), torch.tensor(f.n_this)])\n",
    "            #print(mn.shape)\n",
    "            #msns = torch.cat([torch.tensor(f.ms_in), torch.tensor(f.ns_in)]) # .transpose(1,0)\n",
    "            #print(msns.shape)\n",
    "            #cc.append(torch.cdist(mn.unsqueeze(dim=0), msns.transpose(1,0), 'euclidean') / 8) # number comes from 9*9 = 81 [0-8]\n",
    "\n",
    "            mn = torch.cat([f.m_this.unsqueeze(0), f.n_this.unsqueeze(0)]).transpose(1,0)\n",
    "            msns = torch.cat([f.ms_in.unsqueeze(0), f.ns_in.unsqueeze(0)]).transpose(1,0)\n",
    "            #print(mn)\n",
    "            #print(msns)\n",
    "\n",
    "            # mean ( l2 norm as distance metric / normalisation term for l2 norm)\n",
    "            # mean of distances\n",
    "            # normalise with max=grid square root, min=0\n",
    "            # mean from all non-nan values\n",
    "            # \n",
    "            \n",
    "            if self.cc_metric == 'l1':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'cityblock') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'l2':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'euclidean') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'l2_torch':\n",
    "                cc.append(torch.nanmean( torch.cdist( a=mn.float(), b=msns.float(), p=2) /self.grid_sqrt ))\n",
    "            elif self.cc_metric == 'linf':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'chebyshev') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'cos':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'cosine') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'jac':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'jaccard') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'cor':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'correlation') /self.grid_sqrt ) ))\n",
    "                \n",
    "\n",
    "        # mean connection cost of a layer\n",
    "        # mean from all non-nan values\n",
    "        return torch.nanmean(torch.tensor(cc))\n",
    "    \n",
    "    def run_channel_importance(self, i_f:int) -> list:\n",
    "        # =============================================================================\n",
    "        # compute channel importance metric for pruning\n",
    "        # calculate the norm of each weight in filter with id i_f\n",
    "        # we need to call this in a loop to go through each filter\n",
    "        # returns:\n",
    "        #     ci: channel importance list of a filter\n",
    "        # notes:\n",
    "        #     based on l2 norm = magnitude = euclidean distance\n",
    "        # nonsense?\n",
    "        #    maybe the kernel trigger todo\n",
    "        #    print(self.filter_list[i_f].weights.shape)\n",
    "        #    print(self.filter_list[i_f].weights[:,i_w].shape)\n",
    "        # =============================================================================\n",
    "        \n",
    "        ci = []\n",
    "        \n",
    "        print(\"DECENT NOTE: weight shape\", self.filter_list[i_f].weights.shape)\n",
    "        \n",
    "        for i_w in range(self.filter_list[i_f].weights.shape[1]): # todo, sure this is 1 and not 0???\n",
    "            # importance of a kernel in a layer\n",
    "            \n",
    "            \n",
    "                \n",
    "            \n",
    "            if self.ci_metric == 'l1':\n",
    "                # weight dependent - filter norm\n",
    "                pass\n",
    "                print(\"nooooooooooooooooo\")\n",
    "                # ci.append(self.filter_list[i_f].weights[:,i_w].norm(2).detach().cpu().numpy())\n",
    "                \n",
    "            elif self.ci_metric == 'l2':\n",
    "                # weight dependent - filter norm\n",
    "                ci.append(self.filter_list[i_f].weights[:,i_w].norm(2).detach().cpu().numpy()) # .detach().cpu().numpy()\n",
    "                pass\n",
    "            \n",
    "            elif self.ci_metric == '':\n",
    "                # weight dependent - filter correlation\n",
    "                pass\n",
    "            \n",
    "            elif self.ci_metric == '':\n",
    "                # activation-based\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                # mutual information\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                # Hessian matrix / Taylor\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                \n",
    "                pass\n",
    "                \n",
    "                \n",
    "            elif self.ci_metric == 'random':\n",
    "                ci.append( np.array(random.random()) )\n",
    "\n",
    "        \n",
    "        return ci \n",
    "    \n",
    "    def run_swap_filter(self):\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # we swap filters within the layer\n",
    "        # based on connection cost\n",
    "        # filter can move a maximum of two positions per swap\n",
    "        # change positions\n",
    "        # change\n",
    "        # =============================================================================\n",
    "        print(\"swap here\")\n",
    "        self.m_this = self.m_this # single integer\n",
    "        self.n_this = self.n_this # single integer\n",
    "        pass\n",
    "    \n",
    "    def run_grow_filter(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # introduce new filters in a layer\n",
    "        # based on \n",
    "        # algorithmic growth process \n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def run_grow_channel(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # introduce new channel in a layer\n",
    "        # based on connection cost??\n",
    "        # algorithmic growth process \n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def run_prune_filter(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # delete filter in a layer\n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def run_prune_channel(self, i_f:int, keep_ids:list) -> None:\n",
    "        # =============================================================================\n",
    "        # delete channels in a filter based on keep_ids\n",
    "        # based on importance score\n",
    "        # only keep \"the best\" weights\n",
    "        # pruning based on a metric\n",
    "        # nonsense?\n",
    "        #    delete layer with id\n",
    "        #    delete channels in each layer with id\n",
    "        #    channel deactivation\n",
    "        #    require_grad = False/True for each channel\n",
    "        #    deactivate_ids = [1, 2, 6]\n",
    "        #    self.active[deactivate_ids] = False\n",
    "        #    print(\"weight\")\n",
    "        #    print(self.weight.shape)\n",
    "        #    print(self.weight[:,self.active,:,:].shape)\n",
    "        #    this is totally wrong - iterative will break after first iteration\n",
    "        #    print()\n",
    "        #    Good to hear itâ€™s working, although I would think youâ€™ll get an error at some point in your code, as the cuda() call creates a non-leaf tensor.\n",
    "        #    self.weight = torch.nn.Parameter(  self.weight[:,self.active,:,:] ) # .detach().cpu().numpy()\n",
    "        #    self.weight = self.weight.cuda()\n",
    "        #    print(self.weight.shape)\n",
    "        #    print(self.active)\n",
    "        #    print(\"prune here\")\n",
    "        #    for f in self.filter_list:\n",
    "        #        f.update()\n",
    "        # =============================================================================\n",
    "        \n",
    "        if False:\n",
    "            for i in keep_ids:\n",
    "                print(i)\n",
    "                print(self.filter_list[i_f].ms_in[i])\n",
    "                print(torch.nn.Parameter(self.filter_list[i_f].ms_in[keep_ids]) )\n",
    "        \n",
    "        if random.randint(1, 100) == 5:\n",
    "            print()\n",
    "            print(\"info at random intervals\")\n",
    "            print(keep_ids)\n",
    "            print(self.filter_list[i_f].weights[:, keep_ids, :, :].shape)\n",
    "            print(self.filter_list[i_f].weights.shape)        \n",
    "        \n",
    "        # todo: check, this may create more parameters ...\n",
    "        \n",
    "        # prune weights, ms and ns based on the 'keep ids'\n",
    "        self.filter_list[i_f].weights = torch.nn.Parameter(self.filter_list[i_f].weights[:, keep_ids, :, :])\n",
    "        self.filter_list[i_f].ms_in = torch.nn.Parameter(self.filter_list[i_f].ms_in[keep_ids], requires_grad=False) # this becomes a grad here, hence turn off again with False\n",
    "        #[self.filter_list[i_f].ms_in[i] for i in keep_ids] # self.ms_in[remove_ids]\n",
    "        self.filter_list[i_f].ns_in = torch.nn.Parameter(self.filter_list[i_f].ns_in[keep_ids], requires_grad=False)\n",
    "        # [self.filter_list[i_f].ns_in[i] for i in keep_ids] # self.ns_in[remove_ids]\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x: X) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # calculate representation x for each filter in this layer\n",
    "        # =============================================================================\n",
    "        \n",
    "        output_list = []\n",
    "        m_list = []\n",
    "        n_list = []\n",
    "        for f in self.filter_list:\n",
    "            # output = filter(input)\n",
    "            out = f(x)\n",
    "            # if filter has no channels left\n",
    "            if out is not None:\n",
    "                output_list.append(out)\n",
    "                m_list.append(f.m_this)\n",
    "                n_list.append(f.n_this)\n",
    "        x.ms_x = m_list\n",
    "        x.ns_x = n_list\n",
    "        x.data = torch.cat(output_list, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def get_filter_positions(self):\n",
    "        # =============================================================================\n",
    "        # in use for next layer input (initialisation of the model)\n",
    "        # =============================================================================\n",
    "        \n",
    "        ms_this = []\n",
    "        ns_this = []\n",
    "        for f in self.filter_list:\n",
    "            ms_this.append(f.m_this)\n",
    "            ns_this.append(f.n_this)\n",
    "        \n",
    "        return ms_this, ns_this\n",
    "    \n",
    "    def get_everything(self):\n",
    "        \n",
    "        sources = [] # source m,n,l-1\n",
    "        targets = [] # target m,n,l\n",
    "        target_groups = []\n",
    "        values = [] # connection value = ci value of channel in target connected to ms[i], ns[i] \n",
    "        \n",
    "        # for each filter\n",
    "        for i_f, f in enumerate(self.filter_list):\n",
    "            # for each channel\n",
    "            \n",
    "            print(\"ms_in shape\", f.ms_in.shape)\n",
    "            for i_s in range(len(f.ms_in)):\n",
    "                \n",
    "                s = str(int(f.ms_in[i_s].item()))+'_'+str(int(f.ns_in[i_s].item()))\n",
    "                sources.append(s)\n",
    "\n",
    "                t = str(int(f.m_this.item()))+'_'+str(int(f.n_this.item()))\n",
    "                targets.append(t)\n",
    "\n",
    "                target_groups.append(self.layer_name)\n",
    "            \n",
    "            # get all channel importances\n",
    "            ci = np.array(self.run_channel_importance(i_f)).flatten()\n",
    "            #print(\"CI\"*50)\n",
    "            #print(ci)\n",
    "            values.extend(ci)\n",
    "            \n",
    "            \"\"\"\n",
    "            try:\n",
    "                values.append( [val.item() for tmp in ci for val in tmp] )\n",
    "            except:\n",
    "                try:\n",
    "                    a = [val.item() for val in ci]\n",
    "                    values.append(  [val for tmp in a for val in tmp])\n",
    "                except:\n",
    "                    print(\"empty channel importance??\")\n",
    "                    values = []\n",
    "                    \n",
    "            \"\"\"\n",
    "            \n",
    "        print(\"lengths note:\", len(sources), len(targets), len(target_groups), len(values))\n",
    "        \n",
    "        return {'source':sources, 'target':targets, 'target_group':target_groups, 'value':values}\n",
    "            \n",
    "    \n",
    "    def update(self):\n",
    "        # =============================================================================\n",
    "        # currently: calculate importance metric for the prune_channel method\n",
    "        # remove channels based on self.prune_keep\n",
    "        # =============================================================================\n",
    "        \n",
    "        all_ci = []\n",
    "        all_len = 0\n",
    "        for i_f in range(len(self.filter_list)):\n",
    "            all_len += len(self.filter_list[i_f].ms_in)\n",
    "            # list of lists\n",
    "            all_ci.append(self.run_channel_importance(i_f))\n",
    "            #tmp_ids = sorted(range(len(all_ci)), key=lambda sub: all_ci[sub])\n",
    "          \n",
    "        #print(all_len) # this is the size of the previous pruning\n",
    "        #print(self.original_size)\n",
    "        #print(self.prune_keep_total)\n",
    "        #print(int(self.original_size * self.prune_keep_total))\n",
    "        \n",
    "        #self.log(f'{self.original_size}_active_channels', all_len, on_step=True, on_epoch=True)\n",
    "        \n",
    "        if all_len < int(self.original_size * self.prune_keep_total):\n",
    "            # if n percent have been pruned, stop this layer\n",
    "            print(\"pruning done for this layer\")\n",
    "        else:\n",
    "            # pruning\n",
    "            n = int(all_len*self.prune_keep)\n",
    "            all_ci_flatten = [item for row in all_ci for item in row] # don't have equal lengths, so no numpy possible\n",
    "            index = sorted(range(all_len), key=lambda sub: all_ci_flatten[sub])[-n] # error, out of range\n",
    "            threshold_value = all_ci_flatten[index]\n",
    "\n",
    "            for i_f in range(len(self.filter_list)):\n",
    "\n",
    "                # channel importance list for this filter\n",
    "                ci = all_ci[i_f] # self.run_channel_importance(i_f)\n",
    "\n",
    "                #print(ci)\n",
    "                #print(threshold_value)\n",
    "                # torch.where()\n",
    "                            \n",
    "                indices = np.where(ci >= threshold_value)[0] # just need the x axis\n",
    "\n",
    "                # indices should be list/np/detached\n",
    "                self.run_prune_channel(i_f, indices)\n",
    "                \n",
    "                #print(\"prune done\")\n",
    "                # ci = ci[indices] # probably not useful\n",
    "        \n",
    "            \n",
    "            # print(\"channel importance ci\", ci)\n",
    "            # keep_ids = random.sample(range(0, 8), 5)\n",
    "            #keep_ids = sorted(range(len(ci)), key=lambda sub: ci[sub])[amout_remove:]\n",
    "            #print(keep_ids)\n",
    "\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31232158-b0fb-4e80-856e-94f8808cf414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[324, 23, 324, 23]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "x.extend([324, 23]) # Nothing is printed because the return value is None\n",
    "x.extend([324, 23])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "109208ab-bed3-4f9b-aa56-eb61aebc3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(5):\n",
    "    a.append(np.array(random.random()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af56675d-9098-4244-8170-519cc3d4076a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.67712583),\n",
       " array(0.78491136),\n",
       " array(0.52046616),\n",
       " array(0.5114917),\n",
       " array(0.39353466)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0510096-312f-4ae3-8192-8165b649199c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6771258268002703,\n",
       " 0.7849113560871108,\n",
       " 0.5204661572030815,\n",
       " 0.5114917024932601,\n",
       " 0.39353466292596484]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "[tmp.item() for tmp in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c186cf1-c7db-45da-b779-f7ab88f3896f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentNet(nn.Module):\n",
    "    def __init__(self, model_kwargs, log_dir=\"\") -> None:\n",
    "        super(DecentNet, self).__init__()\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        out_dim = model_kwargs[\"out_dim\"]\n",
    "        out_dim.append(self.n_classes) # out_dim = [1, 32, 48, 64, 10]     \n",
    "        \n",
    "        grid_size = model_kwargs[\"grid_size\"]\n",
    "        assert not any(i > grid_size for i in out_dim), f\"filters need to be less than {grid_size}\"\n",
    "        self.grid_sqrt = int(math.sqrt(grid_size))\n",
    "        \n",
    "        self.ci_metric = model_kwargs[\"ci_metric\"]\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        # backbone\n",
    "        \n",
    "        # initialise input positions of first layer\n",
    "        ms_in_1 = [torch.tensor(0)]\n",
    "        ns_in_1 = [torch.tensor(0)]\n",
    "        assert out_dim[0] == len(ms_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ms_in_1={len(ms_in_1)})\"\n",
    "        assert out_dim[0] == len(ns_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ns_in_1={len(ns_in_1)})\"\n",
    "        self.decent1 = DecentLayer(ms_in=ms_in_1, ns_in=ns_in_1, n_filters=out_dim[1], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs, layer_name='decent1')\n",
    "        \n",
    "        # get position of previous layer as input for this layer\n",
    "        ms_in_2,ns_in_2 = self.decent1.get_filter_positions()\n",
    "        assert out_dim[1] == len(ms_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ms_in_2={len(ms_in_2)})\"\n",
    "        assert out_dim[1] == len(ns_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ns_in_2={len(ns_in_2)})\"\n",
    "        self.decent2 = DecentLayer(ms_in=ms_in_2, ns_in=ns_in_2, n_filters=out_dim[2], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs, layer_name='decent2')\n",
    "        \n",
    "        ms_in_3,ns_in_3 = self.decent2.get_filter_positions()\n",
    "        assert out_dim[2] == len(ms_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ms_in_3={len(ms_in_3)})\"\n",
    "        assert out_dim[2] == len(ns_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ns_in_3={len(ns_in_3)})\"\n",
    "        self.decent3 = DecentLayer(ms_in=ms_in_3, ns_in=ns_in_3, n_filters=out_dim[3], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs, layer_name='decent3')\n",
    "        \n",
    "        ms_in_1x1,ns_in_1x1 = self.decent3.get_filter_positions()\n",
    "        assert out_dim[3] == len(ms_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ms_in_1x1={len(ms_in_1x1)})\"\n",
    "        assert out_dim[3] == len(ns_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ns_in_1x1={len(ns_in_1x1)})\"\n",
    "        self.decent1x1 = DecentLayer(ms_in=ms_in_1x1, ns_in=ns_in_1x1, n_filters=out_dim[-1], kernel_size=1, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs, layer_name='decent1x1')\n",
    "        \n",
    "        #self.tmp = torchvision.models.squeezenet1_0(torchvision.models.SqueezeNet1_0_Weights.IMAGENET1K_V1)\n",
    "        #self.tmp.classifier[1] = torch.nn.Conv2d(512, 10, kernel_size=(3,3))\n",
    "        \n",
    "        # head\n",
    "        self.fc = torch.nn.Linear(out_dim[-1], out_dim[-1])\n",
    "    \n",
    "        # activation\n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        # bias\n",
    "        self.bias1 = torch.nn.InstanceNorm2d(out_dim[1])\n",
    "        self.bias2 = torch.nn.InstanceNorm2d(out_dim[2])\n",
    "        self.bias3 = torch.nn.InstanceNorm2d(out_dim[3])\n",
    "        self.bias1x1 = torch.nn.InstanceNorm2d(out_dim[-1])\n",
    "        \n",
    "        # activation\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # init connection cost\n",
    "        self.cc = []\n",
    "        self.update_connection_cost()\n",
    "        \n",
    "        # get a position in filter list\n",
    "        self.m_plot = self.decent2.filter_list[0].m_this.detach().cpu().numpy()\n",
    "        self.n_plot = self.decent2.filter_list[0].n_this.detach().cpu().numpy()  \n",
    "        # self.plot_layer_of_1_channel(current_epoch=0) - not working here, dir not created yet\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        # hook on the data (for gradcam or something similar)\n",
    "        # https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
    "        if mode == 'explain':\n",
    "            output = x.data.register_hook(self.activations_hook)\n",
    "            #'cannot register a hook on a tensor that doesn't require gradient'\n",
    "        \n",
    "        \n",
    "        # global max pooling for MIL\n",
    "        x.data = F.max_pool2d(x.data, kernel_size=x.data.size()[2:])\n",
    "        \n",
    "        x.data = x.data.reshape(x.data.size(0), -1)\n",
    "        x.data = self.fc(x.data) \n",
    "        \n",
    "        # x.data = self.sigmoid(x.data)\n",
    "        \n",
    "        # x.data = self.tmp(x.data)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    \n",
    "    def activations_hook(self, grad):\n",
    "        # hook for the gradients of the activations\n",
    "        self.gradients = grad\n",
    "    def get_activations_gradient(self):\n",
    "        # method for the gradient extraction\n",
    "        return self.gradients\n",
    "    def get_activations(self, x):\n",
    "        # method for the activation exctraction\n",
    "        \n",
    "        #print('0', x)\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        #print('1', x)\n",
    "\n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        #print('2', x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        #print('3', x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        #print('1x1', x)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    def plot_layer_of_1_channel(self, current_epoch=0):\n",
    "        \n",
    "        # get each filter position that has a channel that matches\n",
    "        ms = []; ns = []\n",
    "        \n",
    "        #print(self.decent2.filter_list)\n",
    "        #print(\"**********************\")\n",
    "        #print(self.decent3.filter_list)\n",
    "\n",
    "        \n",
    "        # go through all filters in this layer\n",
    "        for f in self.decent3.filter_list:\n",
    "            \n",
    "            # if filter position in prev layer matches any channel in this layer\n",
    "            if any(pair == (self.m_plot, self.n_plot) for pair in zip(f.ms_in.detach().cpu().numpy(), f.ns_in.detach().cpu().numpy())):\n",
    "                \n",
    "                #print('match', f.m_this, f.n_this)\n",
    "                \n",
    "                # save position of each filter in this layer\n",
    "                ms.append(f.m_this.detach().cpu().numpy())\n",
    "                ns.append(f.n_this.detach().cpu().numpy())\n",
    "              \n",
    "            if False:\n",
    "                    print(\"nooooooooooooooo\")\n",
    "                    print(f.ms_in)\n",
    "                    print(self.m_plot)\n",
    "                    print(f.ns_in)\n",
    "                    print(self.n_plot)\n",
    "\n",
    "                    print((self.m_plot, self.n_plot))\n",
    "                \n",
    "        # visualising the previous and current layer neurons\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=100000, color='blue', alpha=0.1) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=50000, color='blue',alpha=0.2) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=25000, color='blue',alpha=0.3) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=500, color='blue') # previous layer\n",
    "        ax.scatter(ms, ns, color='red') # next layer\n",
    "        plt.xlim(0, self.grid_sqrt) # m coordinate of grid_size field\n",
    "        plt.ylim(0, self.grid_sqrt) # n coordinate of grid_size field\n",
    "        ax.grid() # enable grid line\n",
    "        fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_plot[0])}_n{int(self.n_plot[0])}_{str(current_epoch)}.png\"))\n",
    "    \n",
    "    def update_connection_cost(self):\n",
    "        self.cc = []\n",
    "        # self.cc.append(self.decent1.run_layer_connection_cost()) # maybe not even needed ...\n",
    "        self.cc.append(self.decent2.run_layer_connection_cost())\n",
    "        self.cc.append(self.decent3.run_layer_connection_cost())\n",
    "        self.cc.append(self.decent1x1.run_layer_connection_cost())\n",
    "        self.cc = torch.mean(torch.tensor(self.cc))\n",
    "\n",
    "    def update(self, current_epoch):\n",
    "        # =============================================================================\n",
    "        # update_every_nth_epoch\n",
    "        # adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        # =============================================================================\n",
    "        \n",
    "        # update decent layers\n",
    "        \n",
    "        #self.decent1.update()\n",
    "        self.decent2.update()\n",
    "        self.decent3.update()\n",
    "        #self.decent1x1.update()\n",
    "        \n",
    "        # visualisation\n",
    "        self.plot_layer_of_1_channel(current_epoch)\n",
    "    \n",
    "        # connection cost has to be calculated after pruning\n",
    "        # self.cc which is updated is used for loss function\n",
    "        self.update_connection_cost()\n",
    "        \n",
    "    def get_everything(self):\n",
    "        \n",
    "        d1 = self.decent1.get_everything()\n",
    "        d2 = self.decent2.get_everything()\n",
    "        d3 = self.decent3.get_everything()\n",
    "        d1x1 = self.decent1x1.get_everything()\n",
    "        \n",
    "        return d1, d2, d3, d1x1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "\n",
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint\n",
    "        # Save a checkpoint at the end of the training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "            print(\"DECENT NOTE: callback on_train_epoch_end\", monitor_candidates[\"epoch\"])\n",
    "            if monitor_candidates[\"epoch\"] > 0:\n",
    "                if monitor_candidates[\"unpruned_state\"] != -1:\n",
    "                    print(\"DECENT NOTE: save model\", monitor_candidates[\"epoch\"])\n",
    "                    if self._every_n_epochs >= 1 and ((trainer.current_epoch + 1) % self._every_n_epochs) == 0:\n",
    "                        self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                    self._save_last_checkpoint(trainer, monitor_candidates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "## DecentLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # Lightning Module consists of functions that define the training routine\n",
    "    # train, val, test: before epoch, step, after epoch, ...\n",
    "    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n",
    "    # order for the instance methods:\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\n",
    "    # \n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, kwargs, log_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "    \n",
    "        # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "        self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        self.pretrain_epochs = model_kwargs[\"pretrain_epochs\"]\n",
    "        \n",
    "        # needed for hparams.yaml file\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if False:\n",
    "            self.metric = { \"train_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"train_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"val_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"val_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "                   }\n",
    "        else:\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "\n",
    "            \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        # =============================================================================\n",
    "        # we make it possible to use model_output = self(image)\n",
    "        # =============================================================================\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def load_from_checkpoint(cls, checkpoint_path, **kwargs):\n",
    "        # todo, need to overwrite this somehow\n",
    "    \n",
    "        #Always use self for the first argument to instance methods.\n",
    "        #Always use cls for the first argument to class methods.\n",
    "    \n",
    "        loaded = cls._load_from_checkpoint(\n",
    "            cls,  # type: ignore[arg-type]\n",
    "            checkpoint_path,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return cast(Self, loaded)\n",
    "    \n",
    "    # todo, need to overwrite this somehow\n",
    "    @classmethod\n",
    "    def _load_from_checkpoint(\n",
    "        cls,\n",
    "        checkpoint_path,\n",
    "        **kwargs):\n",
    "        \n",
    "        map_location = None\n",
    "        with pl_legacy_patch():\n",
    "            checkpoint = pl_load(checkpoint_path, map_location=map_location)\n",
    "\n",
    "        # convert legacy checkpoints to the new format\n",
    "        checkpoint = _pl_migrate_checkpoint(\n",
    "            checkpoint, checkpoint_path=(checkpoint_path if isinstance(checkpoint_path, (str, Path)) else None)\n",
    "        )\n",
    "\n",
    "        if hparams_file is not None:\n",
    "            extension = str(hparams_file).split(\".\")[-1]\n",
    "            if extension.lower() == \"csv\":\n",
    "                hparams = load_hparams_from_tags_csv(hparams_file)\n",
    "            elif extension.lower() in (\"yml\", \"yaml\"):\n",
    "                hparams = load_hparams_from_yaml(hparams_file)\n",
    "            else:\n",
    "                raise ValueError(\".csv, .yml or .yaml is required for `hparams_file`\")\n",
    "\n",
    "            # overwrite hparams by the given file\n",
    "            checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY] = hparams\n",
    "\n",
    "        # TODO: make this a migration:\n",
    "        # for past checkpoint need to add the new key\n",
    "        checkpoint.setdefault(cls.CHECKPOINT_HYPER_PARAMS_KEY, {})\n",
    "        # override the hparams with values that were passed in\n",
    "        checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)\n",
    "\n",
    "        if issubclass(cls, pl.LightningDataModule):\n",
    "            return _load_state(cls, checkpoint, **kwargs)\n",
    "        if issubclass(cls, pl.LightningModule):\n",
    "            model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "            if not state_dict:\n",
    "                rank_zero_warn(f\"The state dict in {checkpoint_path!r} contains no parameters.\")\n",
    "                return model\n",
    "\n",
    "            device = next((t for t in state_dict.values() if isinstance(t, torch.Tensor)), torch.tensor(0)).device\n",
    "            assert isinstance(model, pl.LightningModule)\n",
    "            return model.to(device)\n",
    "\n",
    "        raise NotImplementedError(f\"Unsupported {cls}\")\n",
    "    \"\"\"\n",
    "           \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # =============================================================================\n",
    "        # returns:\n",
    "        #    optimiser and lr scheduler\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimiser = optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        else:\n",
    "            optimiser = optim.SGD(self.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        # initial plot of circular layer\n",
    "        # updates model every nth epoch\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: on_train_epoch_start\", self.current_epoch)\n",
    "        \n",
    "        # plot random layer (the circular plot)\n",
    "        if self.current_epoch == 0:\n",
    "            self.model.plot_layer_of_1_channel(current_epoch=0)\n",
    "        \n",
    "        # update model\n",
    "         # don't update unless pretrain epochs is reached\n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch >= self.pretrain_epochs:\n",
    "            print(\"DECENT NOTE: update model\", self.current_epoch)        \n",
    "            if debug_model:\n",
    "                print(\"DECENT NOTE: before update\")\n",
    "                print(self.model)\n",
    "            self.model.update(current_epoch = self.current_epoch)\n",
    "            if debug_model: \n",
    "                print(\"DECENT NOTE: after update\")\n",
    "                print(self.model)\n",
    "                \n",
    "            print(\"DECENT NOTE: model updated\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculates loss for a batch # 1\n",
    "        # parameters:\n",
    "        #    batch\n",
    "        #    batch id\n",
    "        # returns:\n",
    "        #    loss\n",
    "        # notes:\n",
    "        #    calling gradcam like self.gradcam(batch) is dangerous cause changes gradients\n",
    "        # =============================================================================     \n",
    "        if False: # batch_idx < 2: # print first two steps\n",
    "            print(\"DECENT NOTE: training_step\", batch_idx)\n",
    "\n",
    "        # calculate loss\n",
    "        # loss = torch.tensor(1)\n",
    "        loss = self.run_loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging # 2\n",
    "        # =============================================================================\n",
    "        if False: # batch_idx < 2:\n",
    "            print(\"DECENT NOTE: validation_step\", batch_idx)\n",
    "        \n",
    "        self.run_loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing # 3\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_validation_epoch_end\")\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # save model if next iteration model is pruned # 4 \n",
    "        # this needs to be called before callback \n",
    "        # - if internal pytorch lightning convention changes, this will stop working\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_train_epoch_end\", self.current_epoch)\n",
    "               \n",
    "        if False:\n",
    "            print(\"current epoch\")\n",
    "            print(((self.current_epoch+1) % self.update_every_nth_epoch) == 0)\n",
    "            print(self.current_epoch+1)\n",
    "            print(self.current_epoch)\n",
    "            print(self.update_every_nth_epoch)\n",
    "        \n",
    "        # numel: returns the total number of elements in the input tensor\n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.log(f'unpruned', unpruned, on_step=False, on_epoch=True) \n",
    "        \n",
    "        if ((self.current_epoch+1) % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            # if next epoch is an update, set unpruned flag            \n",
    "            self.log(f'unpruned_state', 1, on_step=False, on_epoch=True)\n",
    "            \n",
    "            # save file\n",
    "            with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "                f.write(\"#\"*50)\n",
    "                for p in self.model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        f.write(str(p.shape))\n",
    "            \n",
    "        else:\n",
    "            # else set unpruned flag to -1, then model won't be saved\n",
    "            self.log(f'unpruned_state', -1, on_step=False, on_epoch=True)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        d1, d2, d3, d1x1 = self.model.get_everything()\n",
    "        \n",
    "        print(d1)\n",
    "        print(d2)\n",
    "        print(d3)\n",
    "        \n",
    "        df1 = pd.DataFrame(d1)\n",
    "        \n",
    "        print(df1.head())\n",
    "        \n",
    "        df2 = pd.DataFrame(d2)\n",
    "        \n",
    "        print(df2.head())\n",
    "        \n",
    "        \n",
    "        df3 = pd.DataFrame(d3)\n",
    "        \n",
    "        print(df3.head())\n",
    "        \n",
    "        \n",
    "        df1x1 = pd.DataFrame(d1x1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        frames = [df1, df2, df3, df1x1]\n",
    "        result = pd.concat(frames)\n",
    "        \n",
    "        result.to_csv('out.csv', index=False)  \n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging, plot gradcam\n",
    "        # =============================================================================\n",
    "        if batch_idx < 2:\n",
    "            print(\"DECENT NOTE: test_step\", batch_idx)\n",
    "\n",
    "        self.run_loss_n_metrics(batch, mode=\"test\")\n",
    "\n",
    "                # .requires_grad_()\n",
    "        \n",
    "                \n",
    "        \"\"\"\n",
    "        with torch.enable_grad():\n",
    "            grad_preds = preds.requires_grad_()\n",
    "            preds2 = self.layer2(grad_preds)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            with torch.set_grad_enabled(True): # torch.set_grad_enabled(True):\n",
    "                self.run_xai_gradcam(batch, batch_idx, mode='explain')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"batch size has to be 1\")\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            layer = self.model.decent1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent2\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent2' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent3\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent3' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent1x1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1x1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_test_epoch_end\", self.current_epoch)\n",
    "        pass\n",
    "    \n",
    "    def run_xai_feature_map(self, batch, batch_idx, layer, layer_str, device='cuda'):\n",
    " \n",
    "        # img, label = testset.__getitem__(0) # batch x channel x width x height, class\n",
    "\n",
    "        # img = X(img.to(device).unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        \n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        # print(img.data.shape)\n",
    "\n",
    "        # run feature map\n",
    "        # model, layer, layer_str, log_dir, device=\"cpu\"\n",
    "        fm = FeatureMap(model=self.model, layer=layer, layer_str=layer_str, log_dir=self.log_dir, device=device)\n",
    "        fm.run(tmp_img, batch_idx)\n",
    "        fm.log()\n",
    "    \n",
    "    def run_xai_gradcam(self, batch, batch_idx, mode='explain'):\n",
    "        # =============================================================================\n",
    "        # grad cam - or just cam?? idk\n",
    "        # todo error: RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n",
    "        # BATCH SIZE HAS TO BE ONE!!!\n",
    "        # grad enable in test mode:\n",
    "        # https://github.com/Project-MONAI/MONAI/discussions/1598\n",
    "        # https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "        # =============================================================================\n",
    "    \n",
    "        img, ground_truth = batch\n",
    "\n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)]) # .requires_grad_()\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        #print(\"nooooooooooo grad, whyyyyy\")\n",
    "        #print(tmp_img1)\n",
    "        #print(img)\n",
    "\n",
    "        #print('b1', tmp_img1)\n",
    "        #print('b2', tmp_img2)\n",
    "\n",
    "        model_output = self(tmp_img1, mode)\n",
    "\n",
    "        #print('c1', tmp_img1)\n",
    "        #print('c2', tmp_img2)\n",
    "\n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "\n",
    "        # get prediction value\n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "\n",
    "        #print('d1', tmp_img1)\n",
    "\n",
    "        #print(\"mo\", model_output)\n",
    "        #print(\"max\", pred_max)\n",
    "        #print(\"backprop\", model_output[:, pred_max])\n",
    "\n",
    "        # backpropagate for gradient tracking\n",
    "        model_output[:, pred_max].backward()\n",
    "\n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        #print('e2', tmp_img2)\n",
    "\n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # draw the heatmap\n",
    "        # plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "        # fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_plot[0])}_n{int(self.n_plot[0])}_{str(current_epoch)}.png\"))\n",
    "\n",
    "        plt.imsave(os.path.join( self.log_dir, f\"plt_cam_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" ), heatmap.detach().cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "        heatmap *= 255.0 / heatmap.max()\n",
    "        pil_heatmap = Image.fromarray(heatmap.detach().cpu().numpy().squeeze()).convert('RGB')\n",
    "        pil_heatmap.save(os.path.join( self.log_dir, f\"pil_cam_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" ) ) \n",
    "            \n",
    "    def run_loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        # =============================================================================\n",
    "        # put image through model, calculate loss and metrics\n",
    "        # use cc term that has been calculated previously\n",
    "        # =============================================================================\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"gt\", ground_truth)\n",
    "        print(\"gt shape\", ground_truth.shape)\n",
    "        print(\"gt type\", ground_truth.type())\n",
    "        print(torch.zeros(ground_truth.size(0), self.n_classes))\n",
    "        \n",
    "        if len(ground_truth.shape) < 2:\n",
    "            ground_truth_tmp_tmp = ground_truth.unsqueeze(1)\n",
    "        else:\n",
    "            ground_truth = ground_truth.transpose(1, 0)\n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth_tmp.size(0), self.n_classes).scatter_(1, ground_truth_tmp.to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \"\"\"\n",
    "\n",
    "        ground_truth = ground_truth.squeeze()\n",
    "        if len(ground_truth.shape) < 1:\n",
    "            ground_truth = ground_truth.unsqueeze(0)\n",
    "        loss = self.criterion(model_output, ground_truth.long()) # ground_truth_multi_hot)\n",
    "        cc = torch.mean(self.model.cc) * self.cc_weight\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "        \n",
    "        #print(model_output)\n",
    "        #print(pred_i)\n",
    "        #print(ground_truth)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            ta = self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            tf = self.train_f1(preds=pred_i, target=ground_truth) \n",
    "            tp = self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"train info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", ta)\n",
    "                print(\"f\", tf)\n",
    "                print(\"p\", tp)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        else:\n",
    "            va = self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            vf = self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            vp = self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"val info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", va)\n",
    "                print(\"f\", vf)\n",
    "                print(\"p\", vp)\n",
    "                print(\"l\", loss)\n",
    "            \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afaef32c-9dc5-40c3-813b-0b9a9c69b7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([5]).squeeze().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2906-61b6-4f3a-b2fa-7aeeaa12e7e1",
   "metadata": {},
   "source": [
    "## run dev routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'result_path': 'examples/example_results', 'exp_name': 'tmp_testi_oct', 'load_ckpt_file': 'xversion_22/checkpoints/epoch=0-unpruned=10942-val_f1=0.06.ckpt', 'epochs': 3, 'img_size': 28, 'batch_size': 1, 'log_every_n_steps': 1, 'device': 'cuda', 'num_workers': 0, 'train_size': 6, 'val_size': 6, 'test_size': 6}\n",
      "model kwargs {'n_classes': 4, 'out_dim': [1, 6, 6, 4], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 10, 'cc_metric': 'l2', 'ci_metric': 'l2', 'cm_metric': 'count', 'update_every_nth_epoch': 1, 'pretrain_epochs': 0, 'prune_keep': 0.8, 'prune_keep_total': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 19\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type                | Params\n",
      "---------------------------------------------------\n",
      "0 | model      | DecentNet           | 834   \n",
      "1 | criterion  | CrossEntropyLoss    | 0     \n",
      "2 | train_acc  | MulticlassAccuracy  | 0     \n",
      "3 | train_f1   | MulticlassF1Score   | 0     \n",
      "4 | train_prec | MulticlassPrecision | 0     \n",
      "5 | val_acc    | MulticlassAccuracy  | 0     \n",
      "6 | val_f1     | MulticlassF1Score   | 0     \n",
      "7 | val_prec   | MulticlassPrecision | 0     \n",
      "---------------------------------------------------\n",
      "630       Trainable params\n",
      "204       Non-trainable params\n",
      "834       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: configure_optimizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: on_validation_epoch_end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da1627a1a134d618955dd0496474170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: on_train_epoch_start 0\n",
      "DECENT NOTE: update model 0\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: model updated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: on_validation_epoch_end\n",
      "DECENT NOTE: on_train_epoch_end 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: UserWarning: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:212: UserWarning: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: callback on_train_epoch_end tensor(0)\n",
      "DECENT NOTE: on_train_epoch_start 1\n",
      "DECENT NOTE: update model 1\n",
      "DECENT NOTE: weight shape torch.Size([1, 1, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 5, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 4, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 5, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 4, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 4, 3, 3])\n",
      "DECENT NOTE: model updated\n",
      "\n",
      "train info at random intervals\n",
      "p tensor([3], device='cuda:0')\n",
      "g tensor([1], device='cuda:0', dtype=torch.int32)\n",
      "a tensor(0., device='cuda:0')\n",
      "f tensor(0., device='cuda:0')\n",
      "p tensor(0., device='cuda:0')\n",
      "l tensor(11.8476, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: on_validation_epoch_end\n",
      "DECENT NOTE: on_train_epoch_end 1\n",
      "DECENT NOTE: callback on_train_epoch_end tensor(1)\n",
      "DECENT NOTE: save model tensor(1)\n",
      "DECENT NOTE: on_train_epoch_start 2\n",
      "DECENT NOTE: update model 2\n",
      "DECENT NOTE: weight shape torch.Size([1, 1, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 3, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 6, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 2, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 5, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 5, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 4, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 5, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 3, 3, 3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 3, 3, 3])\n",
      "DECENT NOTE: model updated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: on_validation_epoch_end\n",
      "DECENT NOTE: on_train_epoch_end 2\n",
      "DECENT NOTE: callback on_train_epoch_end tensor(2)\n",
      "DECENT NOTE: save model tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# this is the main function, run this cell!!!\n",
    "\n",
    "# dataset\n",
    "# logger\n",
    "# trainer\n",
    "# trainer.fit\n",
    "# trainer.test\n",
    "# =============================================================================\n",
    "\n",
    "print(\"train kwargs\", train_kwargs)\n",
    "print(\"model kwargs\", model_kwargs)\n",
    "\n",
    "kwargs = {'train_kwargs':train_kwargs, 'model_kwargs':model_kwargs}\n",
    "\n",
    "# \"examples/example_results/lightning_logs\"\n",
    "logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"])\n",
    "trainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                     accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                     devices=[0],\n",
    "                     # inference_mode=False, # do grad manually\n",
    "                     log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                     logger=logger,\n",
    "                     check_val_every_n_epoch=1,\n",
    "                     max_epochs=train_kwargs[\"epochs\"],\n",
    "                     callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1\",\n",
    "                                               filename='{epoch}-{val_f1:.2f}-{unpruned:.0f}'),\n",
    "                                DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                                filename='{epoch}-{unpruned:.0f}-{val_f1:.2f}'),\n",
    "                                LearningRateMonitor(\"epoch\")])\n",
    "\n",
    "trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "# Check whether pretrained model exists. If yes, load it and skip training\n",
    "pretrained_filename = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "if os.path.isfile(pretrained_filename):\n",
    "    print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "    light = DecentLightning.load_from_checkpoint(pretrained_filename, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n",
    "else:\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "\n",
    "    # Initialize the LightningModule and LightningDataModule\n",
    "    light = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "\n",
    "    # Train the model using a Trainer\n",
    "    trainer.fit(light, train_dataloader, val_dataloader)\n",
    "\n",
    "    # we don't save the positions here ...\n",
    "    # light = DecentLightning.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, kwargs=kwargs) # Load best checkpoint after training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66262489-1c4a-439e-9673-32a40c5c52f1",
   "metadata": {},
   "source": [
    "## run test routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73f9f7ff-a6f1-4947-bcf9-07b9141fd3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Prinzessin\\anaconda3\\envs\\feta\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e63a26098b248b4b08d417c187c3ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms_in shape torch.Size([1])\n",
      "DECENT NOTE: weight shape torch.Size([1, 1, 3, 3])\n",
      "ms_in shape torch.Size([1])\n",
      "DECENT NOTE: weight shape torch.Size([1, 1, 3, 3])\n",
      "ms_in shape torch.Size([1])\n",
      "DECENT NOTE: weight shape torch.Size([1, 1, 3, 3])\n",
      "ms_in shape torch.Size([1])\n",
      "DECENT NOTE: weight shape torch.Size([1, 1, 3, 3])\n",
      "ms_in shape torch.Size([1])\n",
      "DECENT NOTE: weight shape torch.Size([1, 1, 3, 3])\n",
      "ms_in shape torch.Size([1])\n",
      "DECENT NOTE: weight shape torch.Size([1, 1, 3, 3])\n",
      "lengths note: 6 6 6 6\n",
      "ms_in shape torch.Size([1])\n",
      "DECENT NOTE: weight shape torch.Size([1, 1, 3, 3])\n",
      "ms_in shape torch.Size([3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 3, 3, 3])\n",
      "ms_in shape torch.Size([2])\n",
      "DECENT NOTE: weight shape torch.Size([1, 2, 3, 3])\n",
      "ms_in shape torch.Size([2])\n",
      "DECENT NOTE: weight shape torch.Size([1, 2, 3, 3])\n",
      "ms_in shape torch.Size([4])\n",
      "DECENT NOTE: weight shape torch.Size([1, 4, 3, 3])\n",
      "ms_in shape torch.Size([5])\n",
      "DECENT NOTE: weight shape torch.Size([1, 5, 3, 3])\n",
      "lengths note: 17 17 17 17\n",
      "ms_in shape torch.Size([3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 3, 3, 3])\n",
      "ms_in shape torch.Size([4])\n",
      "DECENT NOTE: weight shape torch.Size([1, 4, 3, 3])\n",
      "ms_in shape torch.Size([2])\n",
      "DECENT NOTE: weight shape torch.Size([1, 2, 3, 3])\n",
      "ms_in shape torch.Size([3])\n",
      "DECENT NOTE: weight shape torch.Size([1, 3, 3, 3])\n",
      "lengths note: 12 12 12 12\n",
      "ms_in shape torch.Size([4])\n",
      "DECENT NOTE: weight shape torch.Size([1, 4, 1, 1])\n",
      "ms_in shape torch.Size([4])\n",
      "DECENT NOTE: weight shape torch.Size([1, 4, 1, 1])\n",
      "ms_in shape torch.Size([4])\n",
      "DECENT NOTE: weight shape torch.Size([1, 4, 1, 1])\n",
      "ms_in shape torch.Size([4])\n",
      "DECENT NOTE: weight shape torch.Size([1, 4, 1, 1])\n",
      "lengths note: 16 16 16 16\n",
      "{'source': ['0_0', '0_0', '0_0', '0_0', '0_0', '0_0'], 'target': ['10_10', '1_6', '2_1', '13_11', '14_1', '12_2'], 'target_group': ['decent1', 'decent1', 'decent1', 'decent1', 'decent1', 'decent1'], 'value': [0.71538395, 0.69958556, 0.50922984, 0.6845003, 0.63364434, 0.8870881]}\n",
      "{'source': ['14_1', '10_10', '1_6', '12_2', '2_1', '14_1', '10_10', '13_11', '10_10', '1_6', '14_1', '12_2', '10_10', '2_1', '13_11', '14_1', '12_2'], 'target': ['10_5', '13_0', '13_0', '13_0', '13_6', '13_6', '9_7', '9_7', '0_16', '0_16', '0_16', '0_16', '12_6', '12_6', '12_6', '12_6', '12_6'], 'target_group': ['decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2', 'decent2'], 'value': [0.23511426, 0.24823669, 0.23601817, 0.2908662, 0.2301396, 0.30872014, 0.29476297, 0.26055497, 0.23221657, 0.3092166, 0.2318513, 0.24117418, 0.30394855, 0.2366417, 0.27975342, 0.25033087, 0.2491101]}\n",
      "{'source': ['10_5', '13_0', '9_7', '10_5', '13_0', '9_7', '0_16', '10_5', '12_6', '10_5', '0_16', '12_6'], 'target': ['7_3', '7_3', '7_3', '17_14', '17_14', '17_14', '17_14', '5_8', '5_8', '0_7', '0_7', '0_7'], 'target_group': ['decent3', 'decent3', 'decent3', 'decent3', 'decent3', 'decent3', 'decent3', 'decent3', 'decent3', 'decent3', 'decent3', 'decent3'], 'value': [0.28471807, 0.29257622, 0.25615892, 0.24937508, 0.29441422, 0.27433097, 0.24310859, 0.30998996, 0.2568822, 0.28166717, 0.2940434, 0.26794517]}\n",
      "  source target target_group     value\n",
      "0    0_0  10_10      decent1  0.715384\n",
      "1    0_0    1_6      decent1  0.699586\n",
      "2    0_0    2_1      decent1  0.509230\n",
      "3    0_0  13_11      decent1  0.684500\n",
      "4    0_0   14_1      decent1  0.633644\n",
      "  source target target_group     value\n",
      "0   14_1   10_5      decent2  0.235114\n",
      "1  10_10   13_0      decent2  0.248237\n",
      "2    1_6   13_0      decent2  0.236018\n",
      "3   12_2   13_0      decent2  0.290866\n",
      "4    2_1   13_6      decent2  0.230140\n",
      "  source target target_group     value\n",
      "0   10_5    7_3      decent3  0.284718\n",
      "1   13_0    7_3      decent3  0.292576\n",
      "2    9_7    7_3      decent3  0.256159\n",
      "3   10_5  17_14      decent3  0.249375\n",
      "4   13_0  17_14      decent3  0.294414\n",
      "DECENT NOTE: test_step 0\n",
      "DECENT NOTE: test_step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prinzessin\\projects\\decentnet\\datasceyence\\helper\\visualisation\\feature_map.py:111: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axarr = plt.subplots(x_axis, y_axis)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: on_test_epoch_end 0\n",
      "DecentError: layer not working, run not defined\n",
      "name 'run_explain' is not defined\n",
      "{'test accuracy on valset': 0.1666666716337204}\n"
     ]
    }
   ],
   "source": [
    "# Test best model on test set\n",
    "\n",
    "# we want the grad to work in test, hence: inference_mode=False\n",
    "logger_x = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                     accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                     devices=[0],\n",
    "                     logger=logger_x,\n",
    "                     inference_mode=False)\n",
    "\n",
    "test_result = explainer.test(light, xai_dataloader, verbose=False)\n",
    "\n",
    "try:\n",
    "    result = {\"test accuracy on valset\": test_result[0][\"test_acc\"]}\n",
    "except:\n",
    "    result = 0\n",
    "\n",
    "try:     \n",
    "    layer = light.model.decent2 # .filter_list[7]weights\n",
    "    run_explain(light, layer, device='cuda')\n",
    "except Exception as e:\n",
    "    print(\"DecentError: layer not working, run not defined\" )\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ccb396-da0e-4942-998b-676d1fb8234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7212581\n",
      "0.7065036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7212581, 0.7065036]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[np.array(0.7212581)], [np.array(0.7065036)]]\n",
    "\n",
    "for e in a:\n",
    "    for i in e:\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "flattened = [val.item() for tmp in a for val in tmp] \n",
    " \n",
    "\n",
    "a\n",
    "a\n",
    "\n",
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6ee6f3b-11df-4ef3-9fd9-a110fa996159",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [[0.2847180664539337, 0.29257622361183167, 0.2561589181423187, 0.2416810840368271, 0.2306821644306183], [0.24937507510185242, 0.294414222240448, 0.2743309736251831, 0.2431085854768753, 0.23851336538791656], [0.30998995900154114, 0.2402188777923584, 0.22844929993152618, 0.2568821907043457], [0.28166717290878296, 0.22486495971679688, 0.2940433919429779, 0.26794517040252686]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d48391f-1f3e-4d78-aa61-79dc7eeac5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    a=np.array([np.array(0.69074845)])\n",
    "\n",
    "    print(a.flatten())\n",
    "\n",
    "    flattened = [val for tmp in a for val in tmp] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f165e1d-6d72-47d1-bf97-bfcd334b0a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>target_group</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0</td>\n",
       "      <td>10_10</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.715384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_0</td>\n",
       "      <td>1_6</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.699586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_0</td>\n",
       "      <td>2_1</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.509230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_0</td>\n",
       "      <td>13_11</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_0</td>\n",
       "      <td>14_1</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.633644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0_0</td>\n",
       "      <td>12_2</td>\n",
       "      <td>decent1</td>\n",
       "      <td>0.887088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14_1</td>\n",
       "      <td>10_5</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.235114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10_10</td>\n",
       "      <td>13_0</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.248237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1_6</td>\n",
       "      <td>13_0</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.236018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12_2</td>\n",
       "      <td>13_0</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.290866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2_1</td>\n",
       "      <td>13_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.230140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14_1</td>\n",
       "      <td>13_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.308720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10_10</td>\n",
       "      <td>9_7</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.294763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13_11</td>\n",
       "      <td>9_7</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.260555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10_10</td>\n",
       "      <td>0_16</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.232217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1_6</td>\n",
       "      <td>0_16</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.309217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14_1</td>\n",
       "      <td>0_16</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.231851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12_2</td>\n",
       "      <td>0_16</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.241174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10_10</td>\n",
       "      <td>12_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.303949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2_1</td>\n",
       "      <td>12_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.236642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13_11</td>\n",
       "      <td>12_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.279753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14_1</td>\n",
       "      <td>12_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.250331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12_2</td>\n",
       "      <td>12_6</td>\n",
       "      <td>decent2</td>\n",
       "      <td>0.249110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10_5</td>\n",
       "      <td>7_3</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.284718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13_0</td>\n",
       "      <td>7_3</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.292576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9_7</td>\n",
       "      <td>7_3</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.256159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10_5</td>\n",
       "      <td>17_14</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.249375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13_0</td>\n",
       "      <td>17_14</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.294414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9_7</td>\n",
       "      <td>17_14</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.274331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0_16</td>\n",
       "      <td>17_14</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.243109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10_5</td>\n",
       "      <td>5_8</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.309990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>12_6</td>\n",
       "      <td>5_8</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.256882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10_5</td>\n",
       "      <td>0_7</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.281667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0_16</td>\n",
       "      <td>0_7</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.294043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12_6</td>\n",
       "      <td>0_7</td>\n",
       "      <td>decent3</td>\n",
       "      <td>0.267945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7_3</td>\n",
       "      <td>5_14</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.428868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>17_14</td>\n",
       "      <td>5_14</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.507207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5_8</td>\n",
       "      <td>5_14</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.150944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0_7</td>\n",
       "      <td>5_14</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.384323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7_3</td>\n",
       "      <td>7_9</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.337815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>17_14</td>\n",
       "      <td>7_9</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.387488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5_8</td>\n",
       "      <td>7_9</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0_7</td>\n",
       "      <td>7_9</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.020934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7_3</td>\n",
       "      <td>8_16</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.410962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>17_14</td>\n",
       "      <td>8_16</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.311382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5_8</td>\n",
       "      <td>8_16</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.442951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0_7</td>\n",
       "      <td>8_16</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.212011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>7_3</td>\n",
       "      <td>2_7</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.464132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>17_14</td>\n",
       "      <td>2_7</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.198097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5_8</td>\n",
       "      <td>2_7</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.328148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0_7</td>\n",
       "      <td>2_7</td>\n",
       "      <td>decent1x1</td>\n",
       "      <td>0.277719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source target target_group     value\n",
       "0     0_0  10_10      decent1  0.715384\n",
       "1     0_0    1_6      decent1  0.699586\n",
       "2     0_0    2_1      decent1  0.509230\n",
       "3     0_0  13_11      decent1  0.684500\n",
       "4     0_0   14_1      decent1  0.633644\n",
       "5     0_0   12_2      decent1  0.887088\n",
       "6    14_1   10_5      decent2  0.235114\n",
       "7   10_10   13_0      decent2  0.248237\n",
       "8     1_6   13_0      decent2  0.236018\n",
       "9    12_2   13_0      decent2  0.290866\n",
       "10    2_1   13_6      decent2  0.230140\n",
       "11   14_1   13_6      decent2  0.308720\n",
       "12  10_10    9_7      decent2  0.294763\n",
       "13  13_11    9_7      decent2  0.260555\n",
       "14  10_10   0_16      decent2  0.232217\n",
       "15    1_6   0_16      decent2  0.309217\n",
       "16   14_1   0_16      decent2  0.231851\n",
       "17   12_2   0_16      decent2  0.241174\n",
       "18  10_10   12_6      decent2  0.303949\n",
       "19    2_1   12_6      decent2  0.236642\n",
       "20  13_11   12_6      decent2  0.279753\n",
       "21   14_1   12_6      decent2  0.250331\n",
       "22   12_2   12_6      decent2  0.249110\n",
       "23   10_5    7_3      decent3  0.284718\n",
       "24   13_0    7_3      decent3  0.292576\n",
       "25    9_7    7_3      decent3  0.256159\n",
       "26   10_5  17_14      decent3  0.249375\n",
       "27   13_0  17_14      decent3  0.294414\n",
       "28    9_7  17_14      decent3  0.274331\n",
       "29   0_16  17_14      decent3  0.243109\n",
       "30   10_5    5_8      decent3  0.309990\n",
       "31   12_6    5_8      decent3  0.256882\n",
       "32   10_5    0_7      decent3  0.281667\n",
       "33   0_16    0_7      decent3  0.294043\n",
       "34   12_6    0_7      decent3  0.267945\n",
       "35    7_3   5_14    decent1x1  0.428868\n",
       "36  17_14   5_14    decent1x1  0.507207\n",
       "37    5_8   5_14    decent1x1  0.150944\n",
       "38    0_7   5_14    decent1x1  0.384323\n",
       "39    7_3    7_9    decent1x1  0.337815\n",
       "40  17_14    7_9    decent1x1  0.387488\n",
       "41    5_8    7_9    decent1x1  0.144600\n",
       "42    0_7    7_9    decent1x1  0.020934\n",
       "43    7_3   8_16    decent1x1  0.410962\n",
       "44  17_14   8_16    decent1x1  0.311382\n",
       "45    5_8   8_16    decent1x1  0.442951\n",
       "46    0_7   8_16    decent1x1  0.212011\n",
       "47    7_3    2_7    decent1x1  0.464132\n",
       "48  17_14    2_7    decent1x1  0.198097\n",
       "49    5_8    2_7    decent1x1  0.328148\n",
       "50    0_7    2_7    decent1x1  0.277719"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696de4a-76ea-4b95-9d11-e1cf8a733a42",
   "metadata": {},
   "source": [
    "# random nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3d0e6-ce41-4a30-b68f-7c70bbf92113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2005a8f1-b68b-42f8-a653-cd905542361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing yet - currently part of the main running dev thingi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5d6dd-be92-4db7-bdd8-40d886f10adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43468c27-f130-4118-8d11-275282921c0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16008\\2355735933.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d301c-cbad-4154-a053-137a3263c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# useless, always the same filters\n",
    "\n",
    "for i_filter in range(100):\n",
    "    try:\n",
    "        layer = model.model.decent2.filter_list[i_filter] # i_filter] # .filter_list[7]weights\n",
    "        run_explain(model, layer, device='cuda')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2568e15-95ba-4c8c-9a51-b8cb5050ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "layer = model.model.decent2 # .filter_list[7]weights\n",
    "run_explain(model, layer, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f4513-419a-433b-97ff-6f54c5a6d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b09e4-644a-4cca-9da3-a47986de1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca471c2c-819f-44aa-a8f9-e318aacf3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['loops'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['state_dict'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
