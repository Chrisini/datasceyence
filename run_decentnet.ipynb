{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ð”»ð•–ð•”ð•–ð•Ÿð•¥â„•ð•–ð•¥: ð••ð•šð•¤ð•–ð•Ÿð•¥ð•’ð•Ÿð•˜ð•ð•–ð•• ð•Ÿð•–ð•¥\n",
    "\n",
    "Goal: create a sparse and modular ConvNet\n",
    "\n",
    "\n",
    "Notes:\n",
    "* additionally needed: position, activated channels, connection between channels\n",
    "* within this layer, a whole filter can be deactivated\n",
    "* within a filter, single channels can be deactivated\n",
    "* within this layer, filters can be swapped\n",
    "     \n",
    "* pruning actually doesn\"t work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf\n",
    "\n",
    "pip install:\n",
    "* pytorch_lightning\n",
    "\n",
    "\n",
    "warnings:\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73a26-f239-466f-901d-559d192f61de",
   "metadata": {},
   "source": [
    "![uml of code](examples/example_vis/uml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "from typing import Optional, List, Tuple, Union\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "import sys \n",
    "sys.path.insert(0, \"helper\")\n",
    "from visualisation.feature_map import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397b450-c5c6-4da1-b630-7bf80e46891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch 2.0.0 == True\n",
      "tl 2.1.0 == True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(19)\n",
    "torch.cuda.manual_seed(19)\n",
    "random.seed(19)\n",
    "np.random.seed(19)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "debug_model = False\n",
    "\n",
    "print('torch 2.0.0 ==', torch.__version__=='2.0.0')\n",
    "print('tl 2.1.0 ==', pl.__version__=='2.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b0be-245d-49c0-88b4-d94167f7da90",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97670e82-0d27-4b7f-91df-874acf9974a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'n_classes': 10,\n",
    "    'out_dim' :  [1, 8, 16, 32], # [1, 8, 16, 32], #[1, 16, 24, 32]\n",
    "    'grid_size' : 18*18,\n",
    "    'criterion': torch.nn.CrossEntropyLoss(),# torch.nn.BCEWithLogitsLoss(),\n",
    "    'optimizer': \"sgd\", # sgd adamw\n",
    "    'base_lr': 0.001,\n",
    "    'min_lr' : 0.00001,\n",
    "    'momentum' : 0.9,\n",
    "    'lr_update' : 100,\n",
    "    'cc_weight': 10,\n",
    "    'cc_metric' : 'l2', # connection cost metric (for loss) - distance metric\n",
    "    'ci_metric' : 'l2', # importance metric (for pruning)\n",
    "    'update_every_nth_epoch' : 2, # 10\n",
    "    'pretrain_epochs' : 1,\n",
    "    'prune_keep' : 0.95, # 0.97, # in each epoch\n",
    "    'prune_keep_total' : 0.5, # this number is not exact, depends on the prune_keep value\n",
    "}\n",
    "\n",
    "train_kwargs = {\n",
    "    'result_path': \"examples/example_results\", # \"example_results/lightning_logs\", # not in use??\n",
    "    'exp_name': \"tmp_testi\",\n",
    "    'load_ckpt_file' : \"xversion_22/checkpoints/epoch=0-unpruned=10942-val_f1=0.06.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "    'epochs': 3,\n",
    "    'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ...\n",
    "    'batch_size': 128, # 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "    'log_every_n_steps' : 4, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "    # 'test_batch_size': 1,\n",
    "    'device': \"cuda\",\n",
    "    'num_workers' : 18, # 18, # 18 for computer, 0 for laptop\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {},
   "source": [
    "# DecentNet trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    \n",
    "    transform=transforms.Compose([\n",
    "        #transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(size=train_kwargs[\"img_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST('examples/example_data', train=False, download=True, transform=transform)\n",
    "    # val_set = datasets.MNIST('example_data', train=False, transform=transform)\n",
    "    \n",
    "    print(len(dataset))\n",
    "    \n",
    "    # Split the indices in a stratified way\n",
    "    indices = np.arange(len(dataset))\n",
    "    # train_indices, val_indices = train_test_split(indices, train_size=0.8, test_size=0.2, stratify=dataset.targets)\n",
    "    train_indices, val_indices = train_test_split(indices, train_size=200, test_size=200, stratify=dataset.targets)\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_subset, shuffle=True, batch_size=train_kwargs[\"batch_size\"], num_workers=train_kwargs[\"num_workers\"])\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_subset, shuffle=False, batch_size=train_kwargs[\"batch_size\"], num_workers=train_kwargs[\"num_workers\"]) # , persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # an object with image representations and their positions\n",
    "    # amout of channels need to have same length as m and n lists\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        self.data = data # list of tensors (image representations)\n",
    "        self.ms_x = ms_x # list of integers (m position of each image representation)\n",
    "        self.ns_x = ns_x # list of integers (n position of each image representation)\n",
    "                \n",
    "    def setter(self, data, ms_x, ns_x):\n",
    "        self.data = data\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        \n",
    "    def getter(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfa123-54e1-4053-94c3-52b45ec0859c",
   "metadata": {},
   "source": [
    "## DecentFilter\n",
    "* conv2d problem: https://stackoverflow.com/questions/61269421/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # convolution happens in here\n",
    "    # one filter has multiple channels (aka weights)\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, ms_in, ns_in, m_this, n_this,\n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 padding_mode=\"zeros\",\n",
    "                 dilation=3, \n",
    "                 # transposed=None, \n",
    "                 device=None, \n",
    "                 dtype=None):\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        \n",
    "        # padding\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        \n",
    "         \n",
    "        # convolution\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding_mode = padding_mode\n",
    "        self.padding = padding\n",
    "        self.dilation = _pair(dilation)\n",
    "        #self.transposed = transposed\n",
    "        \n",
    "        # weights\n",
    "        assert len(ms_in) == len(ns_in), \"ms_in and ns_in are not of same length\"\n",
    "        self.n_weights = len(ms_in)\n",
    "        \n",
    "        # position, currently not trainable \n",
    "        # self.non_trainable_param = nn.Parameter(torch.Tensor([1.0]), requires_grad=False)\n",
    "        self.ms_in = nn.Parameter(torch.Tensor(ms_in), requires_grad=False) # ms_in # list\n",
    "        self.ns_in = nn.Parameter(torch.Tensor(ns_in), requires_grad=False) # ns_in # list\n",
    "        self.m_this = nn.Parameter(torch.Tensor([m_this]), requires_grad=False) # m_this # single integer\n",
    "        self.n_this = nn.Parameter(torch.Tensor([n_this]), requires_grad=False) # n_this # single integer\n",
    "        \n",
    "        # weight\n",
    "        # filters x channels x kernel x kernel\n",
    "        # self.weights = torch.autograd.Variable(torch.randn(1,n_weights,*self.kernel_size)).to(\"cuda\")\n",
    "        # self.weights = torch.nn.Parameter(torch.randn(1,n_weights,*self.kernel_size))\n",
    "        self.weights = torch.nn.Parameter(torch.empty((1, self.n_weights, *self.kernel_size), **factory_kwargs))\n",
    "        \n",
    "        #print(\"weight shape init\")\n",
    "        #print(self.weights.shape)\n",
    "            \n",
    "        # bias    \n",
    "        if False: \n",
    "            # bias:\n",
    "            # where should the bias be???\n",
    "            self.bias = Parameter(torch.empty(1, **factory_kwargs))\n",
    "        else:\n",
    "            #self.bias = False\n",
    "            # we only use bias via instance normalisation\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # reset weights and bias in filter\n",
    "        self.reset_parameters()\n",
    "            \n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*self.kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        torch.nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))        \n",
    "        \n",
    "    def forward(self, x:X) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # first, we have to remove channels in X\n",
    "        # this is because some channels in the filter are pruned (aka gone)\n",
    "        # then we can apply convolution\n",
    "        # parameters:\n",
    "        #    x = batch x channels x width x height\n",
    "        # returns:\n",
    "        #    x_data: batch x filters x width x height\n",
    "        # saves:\n",
    "        #    self.weights = 1 filter x channels x kernel x kernel\n",
    "        # =============================================================================\n",
    "\n",
    "        # Find the indices (IDs) of channel pairs that exist in both the X and then filter\n",
    "        common_pairs = [[i_in, i_x] for i_in, (m_in, n_in) in enumerate(zip(self.ms_in, self.ns_in)) for i_x, (m_x, n_x) in enumerate(zip(x.ms_x, x.ns_x)) if (m_in==m_x and n_in==n_x)]\n",
    "        \n",
    "        if False:\n",
    "            print(common_pairs)\n",
    "            print(len(self.ms_in))\n",
    "            print(len(self.ns_in))\n",
    "            print(len(x.ms_x))\n",
    "            print(len(x.ns_x))\n",
    "\n",
    "            for pair in common_pairs:\n",
    "                print(f\"Common pair at indices {pair}: {self.ms_in[pair[0]], tmp_ms[pair[1]]}, {self.ns_in[pair[0]], tmp_ns[pair[1]]}\")\n",
    "        \n",
    "        common_pairs_a = np.array(common_pairs)\n",
    "        try:\n",
    "            f_ids = common_pairs_a[:,0]\n",
    "            x_ids = common_pairs_a[:,1]\n",
    "        except Exception as e:\n",
    "            print(\"error: no common pairs\")\n",
    "            print(\"pairs\", common_pairs_a)\n",
    "            print(\"pairs shape\", common_pairs_a.shape)\n",
    "            print(\"len ms in\", len(self.ms_in))\n",
    "            print(\"len ns in\", len(self.ns_in))\n",
    "            print(\"len ms x\", len(x.ms_x))\n",
    "            print(\"len ns x\", len(x.ns_x))\n",
    "            print(e)\n",
    "            \n",
    "            # in this case the whole filter should be removed\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        # filter data and weights based on common pairs of data and weights\n",
    "        tmp_x = x.data[:, x_ids, :, :]\n",
    "        tmp_w = self.weights[:, f_ids, :, :]\n",
    "        \n",
    "        # the final convolution\n",
    "        if self.padding_mode != 'zeros':\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(F.pad(tmp_x, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            tmp_w, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, groups=1)\n",
    "        else:\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(tmp_x, tmp_w, self.bias, self.stride, self.padding, self.dilation, groups=1)\n",
    "        \n",
    "        #print(\"tmp_w\", tmp_w.shape)\n",
    "        \n",
    "        # print(x_data.shape, \"- batch x filters x width x height\")\n",
    "        return x_data\n",
    "    \n",
    "    def setter(self, value, m_this, n_this):\n",
    "        # preliminary, not in use\n",
    "        self.weights = value # weights in this filter\n",
    "        self.m_this = m_this # single integer\n",
    "        self.n_this = n_this # single integer\n",
    "    \n",
    "    def getter(self):\n",
    "        # preliminary, not in use\n",
    "        return self.weights, self.m_this, self.n_this\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'DecentFilter(weights: ' + str(self.weights.shape) + ' at position: m_this=' + str(self.m_this) + ', n_this=' + str(self.n_this) + ')' + \\\n",
    "    '\\n with inputs: ms_in= ' + ', '.join(str(int(m.item())) for m in self.ms_in) + ', ns_in= ' + ', '.join(str(int(n.item())) for n in self.ns_in) + ')'\n",
    "    __repr__ = __str__\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdab4-e0b0-4523-882f-dad3ebf06090",
   "metadata": {},
   "source": [
    "## DecentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fddb74c7-5940-424d-aab8-2c75efd914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLayer(torch.nn.Module):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # we save filters of the layer in the self.filter_list\n",
    "    # each filter has a position (m_this, n_this)\n",
    "    # each filter has input positions (ms_in, ns_in)\n",
    "    #    - these vary between filters, as some are pruned\n",
    "    # at the moment we have to loop through the filter list\n",
    "    # convolution is applied to each filter separately which makes this very slow\n",
    "    #\n",
    "    # =============================================================================\n",
    "    __constants__ = ['stride', 'padding', 'dilation', # 'groups',\n",
    "                     'padding_mode', # 'n_channels', #  'output_padding', # 'n_filters',\n",
    "                     'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "                \n",
    "    def __init__(self, ms_in:list, ns_in:list, n_filters:int,\n",
    "                 kernel_size: _size_2_t,  \n",
    "                 stride: _size_2_t = 1,  \n",
    "                 padding: Union[str, _size_2_t] = 0,  \n",
    "                 dilation: _size_2_t = 1,\n",
    "                 model_kwargs=None,\n",
    "                 #prune_keep:float = 0.9,\n",
    "                 #prune_keep_total:float = 0.5,\n",
    "                 #transposed: bool = False, \n",
    "                 #grid_size:int=81,\n",
    "                 #ci_metric=\"l2\",\n",
    "                 #output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 #groups: int = 1,\n",
    "                 bias: bool = True,  # not in use\n",
    "                 padding_mode: str = \"zeros\",  # not in use\n",
    "                 device=None,  # not in use\n",
    "                 dtype=None) -> None:\n",
    "        # =============================================================================\n",
    "        # initialisation\n",
    "        # parameters:\n",
    "        #    a lot.\n",
    "        # =============================================================================\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # prune numbers\n",
    "        self.prune_keep = model_kwargs[\"prune_keep\"] # in each update [0.0:1.0]\n",
    "        self.prune_keep_total = model_kwargs[\"prune_keep_total\"] # total [0.0:1.0]\n",
    "        \n",
    "        # importance metric for pruning\n",
    "        self.ci_metric = model_kwargs[\"ci_metric\"]\n",
    "        # distance metric for loss\n",
    "        self.cc_metric = model_kwargs[\"cc_metric\"]\n",
    "        \n",
    "        # from prev layer\n",
    "        self.ms_in = ms_in\n",
    "        self.ns_in = ns_in\n",
    "        \n",
    "        self.original_size = len(self.ms_in) * n_filters\n",
    "        \n",
    "        \n",
    "        self.grid_size = model_kwargs[\"grid_size\"]\n",
    "        self.grid_sqrt = math.sqrt(self.grid_size)\n",
    "        assert self.grid_sqrt == int(self.grid_sqrt), f\"square root ({self.grid_sqrt}) from grid size {self.grid_size} not possible; possible exampes: 81 (9*9), 144 (12*12)\"\n",
    "        self.grid_sqrt = int(self.grid_sqrt)\n",
    "        \n",
    "        # use techniques from coo matrix\n",
    "        self.geometry_array = np.full(self.grid_size, np.nan)\n",
    "        # plus 1 here cause of to_sparse array\n",
    "        self.geometry_array[0:n_filters] = range(1,n_filters+1)\n",
    "        np.random.shuffle(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.reshape((self.grid_sqrt,self.grid_sqrt), order='C')\n",
    "        self.geometry_array = torch.tensor(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.to_sparse(sparse_dim=2).to(\"cuda\")\n",
    "\n",
    "        #print(self.geometry_array)\n",
    "        #print(self.geometry_array.values())\n",
    "\n",
    "        self.filter_list = torch.nn.ModuleList([])\n",
    "        for i_filter in range(n_filters):\n",
    "            # minus 1 here cause of to_sparse array\n",
    "            index = (self.geometry_array.values()-1 == i_filter).nonzero(as_tuple=True)[0]\n",
    "            m_this = self.geometry_array.indices()[0][index]\n",
    "            n_this = self.geometry_array.indices()[1][index]\n",
    "            f = DecentFilter(ms_in, ns_in, m_this, n_this, \n",
    "                             kernel_size=kernel_size, \n",
    "                             stride=stride, padding=padding, dilation=dilation)\n",
    "            self.filter_list.append(f)\n",
    "            # self.register_parameter(f\"filter {i_filter}\", f.weights)\n",
    "            \n",
    "            #torch.nn.Parameter(torch.empty((1, n_channels, *kernel_size), **factory_kwargs))\n",
    "    \n",
    "    def compute_layer_connection_cost(self) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # compute connection cost for this layer - based on distance\n",
    "        # returns:\n",
    "        #    connection cost for the loss function\n",
    "        # notes:\n",
    "        #    currently using l2 norm, doesn't work that well\n",
    "        # sources:\n",
    "        #    adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        #    https://stackoverflow.com/questions/74086766/how-to-find-total-cost-of-each-path-in-graph-using-dictionary-in-python\n",
    "        # nonsense?\n",
    "        #    i don't even know what the following comments are about ... \n",
    "        #    based on previous layer (cause I only have input ms_in, n_in information)\n",
    "        #    mean( sum( of connection cost between this filter and all incoming filters\n",
    "        #    need it for loss - aka all layers, all filters together\n",
    "        #    need it for swapping - this layer, all filters\n",
    "        #    only the active ones (we need to use the indices for that)\n",
    "        #    for swapping i need ??\n",
    "        # =============================================================================\n",
    "         \n",
    "        from scipy.spatial.distance import cdist\n",
    "        \n",
    "        # connection cost list\n",
    "        cc = []\n",
    "        \n",
    "        \n",
    "        for f in self.filter_list:\n",
    "            # for each filter we use the current position and all incoming positions\n",
    "\n",
    "            #mn = torch.cat([torch.tensor(f.m_this), torch.tensor(f.n_this)])\n",
    "            #print(mn.shape)\n",
    "            #msns = torch.cat([torch.tensor(f.ms_in), torch.tensor(f.ns_in)]) # .transpose(1,0)\n",
    "            #print(msns.shape)\n",
    "            #cc.append(torch.cdist(mn.unsqueeze(dim=0), msns.transpose(1,0), 'euclidean') / 8) # number comes from 9*9 = 81 [0-8]\n",
    "\n",
    "            mn = torch.cat([f.m_this.unsqueeze(0), f.n_this.unsqueeze(0)]).transpose(1,0)\n",
    "            msns = torch.cat([f.ms_in.unsqueeze(0), f.ns_in.unsqueeze(0)]).transpose(1,0)\n",
    "            #print(mn)\n",
    "            #print(msns)\n",
    "\n",
    "            # mean ( l2 norm as distance metric / normalisation term for l2 norm)\n",
    "            # mean of distances\n",
    "            # normalise with max=grid square root, min=0\n",
    "            # mean from all non-nan values\n",
    "            # \n",
    "            \n",
    "            if self.cc_metric == 'l1':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'cityblock') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'l2':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'euclidean') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'l2_torch':\n",
    "                cc.append(torch.nanmean( torch.cdist( a=mn.float(), b=msns.float(), p=2) /self.grid_sqrt ))\n",
    "            elif self.cc_metric == 'linf':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'chebyshev') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'cos':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'cosine') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'jac':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'jaccard') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'cor':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'correlation') /self.grid_sqrt ) ))\n",
    "                \n",
    "\n",
    "        # mean connection cost of a layer\n",
    "        # mean from all non-nan values\n",
    "        return torch.nanmean(torch.tensor(cc))\n",
    "    \n",
    "    def compute_channel_importance(self, i_f:int) -> list:\n",
    "        # =============================================================================\n",
    "        # compute channel importance metric for pruning\n",
    "        # calculate the norm of each weight in filter with id i_f\n",
    "        # we need to call this in a loop to go through each filter\n",
    "        # returns:\n",
    "        #     ci: channel importance list of a filter\n",
    "        # notes:\n",
    "        #     based on l2 norm = magnitude = euclidean distance\n",
    "        # nonsense?\n",
    "        #    maybe the kernel trigger todo\n",
    "        #    print(self.filter_list[i_f].weights.shape)\n",
    "        #    print(self.filter_list[i_f].weights[:,i_w].shape)\n",
    "        # =============================================================================\n",
    "        \n",
    "        ci = []\n",
    "        \n",
    "        for i_w in range(self.filter_list[i_f].weights.shape[1]):\n",
    "            # importance of a kernel in a layer\n",
    "            \n",
    "            \n",
    "                \n",
    "            \n",
    "            if self.ci_metric == 'l1':\n",
    "                # weight dependent - filter norm\n",
    "                pass\n",
    "                print(\"nooooooooooooooooo\")\n",
    "                # ci.append(self.filter_list[i_f].weights[:,i_w].norm(2).detach().cpu().numpy())\n",
    "                \n",
    "            elif self.ci_metric == 'l2':\n",
    "                # weight dependent - filter norm\n",
    "                ci.append(self.filter_list[i_f].weights[:,i_w].norm(2).detach().cpu().numpy()) # .detach().cpu().numpy()\n",
    "                pass\n",
    "            \n",
    "            elif self.ci_metric == '':\n",
    "                # weight dependent - filter correlation\n",
    "                pass\n",
    "            \n",
    "            elif self.ci_metric == '':\n",
    "                # activation-based\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                # mutual information\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                # Hessian matrix / Taylor\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                \n",
    "                pass\n",
    "                \n",
    "                \n",
    "            elif self.ci_metric == 'random':\n",
    "                ci.append( np.array(random.random()) )\n",
    "\n",
    "        \n",
    "        return ci \n",
    "    \n",
    "    def swap_filter(self):\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # we swap filters within the layer\n",
    "        # based on connection cost\n",
    "        # filter can move a maximum of two positions per swap\n",
    "        # change positions\n",
    "        # change\n",
    "        # =============================================================================\n",
    "        print(\"swap here\")\n",
    "        self.m_this = self.m_this # single integer\n",
    "        self.n_this = self.n_this # single integer\n",
    "        pass\n",
    "    \n",
    "    def grow_filter(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # introduce new filters in a layer\n",
    "        # based on \n",
    "        # algorithmic growth process \n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def grow_channel(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # introduce new channel in a layer\n",
    "        # based on connection cost??\n",
    "        # algorithmic growth process \n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def prune_filter(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # delete filter in a layer\n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def prune_channel(self, i_f:int, keep_ids:list) -> None:\n",
    "        # =============================================================================\n",
    "        # delete channels in a filter based on keep_ids\n",
    "        # based on importance score\n",
    "        # only keep \"the best\" weights\n",
    "        # pruning based on a metric\n",
    "        # nonsense?\n",
    "        #    delete layer with id\n",
    "        #    delete channels in each layer with id\n",
    "        #    channel deactivation\n",
    "        #    require_grad = False/True for each channel\n",
    "        #    deactivate_ids = [1, 2, 6]\n",
    "        #    self.active[deactivate_ids] = False\n",
    "        #    print(\"weight\")\n",
    "        #    print(self.weight.shape)\n",
    "        #    print(self.weight[:,self.active,:,:].shape)\n",
    "        #    this is totally wrong - iterative will break after first iteration\n",
    "        #    print()\n",
    "        #    Good to hear itâ€™s working, although I would think youâ€™ll get an error at some point in your code, as the cuda() call creates a non-leaf tensor.\n",
    "        #    self.weight = torch.nn.Parameter(  self.weight[:,self.active,:,:] ) # .detach().cpu().numpy()\n",
    "        #    self.weight = self.weight.cuda()\n",
    "        #    print(self.weight.shape)\n",
    "        #    print(self.active)\n",
    "        #    print(\"prune here\")\n",
    "        #    for f in self.filter_list:\n",
    "        #        f.update()\n",
    "        # =============================================================================\n",
    "        \n",
    "        if False:\n",
    "            for i in keep_ids:\n",
    "                print(i)\n",
    "                print(self.filter_list[i_f].ms_in[i])\n",
    "                print(torch.nn.Parameter(self.filter_list[i_f].ms_in[keep_ids]) )\n",
    "        \n",
    "        if random.randint(1, 100) == 5:\n",
    "            print()\n",
    "            print(\"info at random intervals\")\n",
    "            print(keep_ids)\n",
    "            print(self.filter_list[i_f].weights[:, keep_ids, :, :].shape)\n",
    "            print(self.filter_list[i_f].weights.shape)        \n",
    "        \n",
    "        # todo: check, this may create more parameters ...\n",
    "        \n",
    "        # prune weights, ms and ns based on the 'keep ids'\n",
    "        self.filter_list[i_f].weights = torch.nn.Parameter(self.filter_list[i_f].weights[:, keep_ids, :, :])\n",
    "        self.filter_list[i_f].ms_in = torch.nn.Parameter(self.filter_list[i_f].ms_in[keep_ids], requires_grad=False) # this becomes a grad here, hence turn off again with False\n",
    "        #[self.filter_list[i_f].ms_in[i] for i in keep_ids] # self.ms_in[remove_ids]\n",
    "        self.filter_list[i_f].ns_in = torch.nn.Parameter(self.filter_list[i_f].ns_in[keep_ids], requires_grad=False)\n",
    "        # [self.filter_list[i_f].ns_in[i] for i in keep_ids] # self.ns_in[remove_ids]\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x: X) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # calculate representation x for each filter in this layer\n",
    "        # =============================================================================\n",
    "        \n",
    "        output_list = []\n",
    "        m_list = []\n",
    "        n_list = []\n",
    "        for f in self.filter_list:\n",
    "            # output = filter(input)\n",
    "            out = f(x)\n",
    "            # if filter has no channels left\n",
    "            if out is not None:\n",
    "                output_list.append(out)\n",
    "                m_list.append(f.m_this)\n",
    "                n_list.append(f.n_this)\n",
    "        x.ms_x = m_list\n",
    "        x.ns_x = n_list\n",
    "        x.data = torch.cat(output_list, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def get_filter_positions(self):\n",
    "        # =============================================================================\n",
    "        # in use for next layer input (initialisation of the model)\n",
    "        # =============================================================================\n",
    "        \n",
    "        ms_this = []\n",
    "        ns_this = []\n",
    "        for f in self.filter_list:\n",
    "            ms_this.append(f.m_this)\n",
    "            ns_this.append(f.n_this)\n",
    "        \n",
    "        return ms_this, ns_this\n",
    "    \n",
    "    def update(self):\n",
    "        # =============================================================================\n",
    "        # currently: calculate importance metric for the prune_channel method\n",
    "        # remove channels based on self.prune_keep\n",
    "        # =============================================================================\n",
    "        \n",
    "        all_ci = []\n",
    "        all_len = 0\n",
    "        for i_f in range(len(self.filter_list)):\n",
    "            all_len += len(self.filter_list[i_f].ms_in)\n",
    "            # list of lists\n",
    "            all_ci.append(self.compute_channel_importance(i_f))\n",
    "            #tmp_ids = sorted(range(len(all_ci)), key=lambda sub: all_ci[sub])\n",
    "          \n",
    "        #print(all_len) # this is the size of the previous pruning\n",
    "        #print(self.original_size)\n",
    "        #print(self.prune_keep_total)\n",
    "        #print(int(self.original_size * self.prune_keep_total))\n",
    "        \n",
    "        #self.log(f'{self.original_size}_active_channels', all_len, on_step=True, on_epoch=True)\n",
    "        \n",
    "        if all_len < int(self.original_size * self.prune_keep_total):\n",
    "            # if n percent have been pruned, stop this layer\n",
    "            print(\"pruning done for this layer\")\n",
    "        else:\n",
    "            # pruning\n",
    "            n = int(all_len*self.prune_keep)\n",
    "            all_ci_flatten = [item for row in all_ci for item in row] # don't have equal lengths, so no numpy possible\n",
    "            index = sorted(range(all_len), key=lambda sub: all_ci_flatten[sub])[-n] # error, out of range\n",
    "            threshold_value = all_ci_flatten[index]\n",
    "\n",
    "            for i_f in range(len(self.filter_list)):\n",
    "\n",
    "                # channel importance list for this filter\n",
    "                ci = all_ci[i_f] # self.compute_channel_importance(i_f)\n",
    "\n",
    "                #print(ci)\n",
    "                #print(threshold_value)\n",
    "                # torch.where()\n",
    "                            \n",
    "                indices = np.where(ci >= threshold_value)[0] # just need the x axis\n",
    "\n",
    "                # indices should be list/np/detached\n",
    "                self.prune_channel(i_f, indices)\n",
    "                \n",
    "                #print(\"prune done\")\n",
    "                # ci = ci[indices] # probably not useful\n",
    "        \n",
    "            \n",
    "            # print(\"channel importance ci\", ci)\n",
    "            # keep_ids = random.sample(range(0, 8), 5)\n",
    "            #keep_ids = sorted(range(len(ci)), key=lambda sub: ci[sub])[amout_remove:]\n",
    "            #print(keep_ids)\n",
    "\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "109208ab-bed3-4f9b-aa56-eb61aebc3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(5):\n",
    "    a.append(np.array(random.random()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af56675d-9098-4244-8170-519cc3d4076a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.67712583),\n",
       " array(0.78491136),\n",
       " array(0.52046616),\n",
       " array(0.5114917),\n",
       " array(0.39353466)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0510096-312f-4ae3-8192-8165b649199c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c186cf1-c7db-45da-b779-f7ab88f3896f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentNet(nn.Module):\n",
    "    def __init__(self, model_kwargs, log_dir=\"\") -> None:\n",
    "        super(DecentNet, self).__init__()\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        out_dim = model_kwargs[\"out_dim\"]\n",
    "        out_dim.append(self.n_classes) # out_dim = [1, 32, 48, 64, 10]     \n",
    "        \n",
    "        grid_size = model_kwargs[\"grid_size\"]\n",
    "        assert not any(i > grid_size for i in out_dim), f\"filters need to be less than {grid_size}\"\n",
    "        self.grid_sqrt = int(math.sqrt(grid_size))\n",
    "        \n",
    "        self.ci_metric = model_kwargs[\"ci_metric\"]\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        # backbone\n",
    "        \n",
    "        # initialise input positions of first layer\n",
    "        ms_in_1 = [torch.tensor(0)]\n",
    "        ns_in_1 = [torch.tensor(0)]\n",
    "        assert out_dim[0] == len(ms_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ms_in_1={len(ms_in_1)})\"\n",
    "        assert out_dim[0] == len(ns_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ns_in_1={len(ns_in_1)})\"\n",
    "        self.decent1 = DecentLayer(ms_in=ms_in_1, ns_in=ns_in_1, n_filters=out_dim[1], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        # get position of previous layer as input for this layer\n",
    "        ms_in_2,ns_in_2 = self.decent1.get_filter_positions()\n",
    "        assert out_dim[1] == len(ms_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ms_in_2={len(ms_in_2)})\"\n",
    "        assert out_dim[1] == len(ns_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ns_in_2={len(ns_in_2)})\"\n",
    "        self.decent2 = DecentLayer(ms_in=ms_in_2, ns_in=ns_in_2, n_filters=out_dim[2], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        ms_in_3,ns_in_3 = self.decent2.get_filter_positions()\n",
    "        assert out_dim[2] == len(ms_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ms_in_3={len(ms_in_3)})\"\n",
    "        assert out_dim[2] == len(ns_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ns_in_3={len(ns_in_3)})\"\n",
    "        self.decent3 = DecentLayer(ms_in=ms_in_3, ns_in=ns_in_3, n_filters=out_dim[3], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        ms_in_1x1,ns_in_1x1 = self.decent3.get_filter_positions()\n",
    "        assert out_dim[3] == len(ms_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ms_in_1x1={len(ms_in_1x1)})\"\n",
    "        assert out_dim[3] == len(ns_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ns_in_1x1={len(ns_in_1x1)})\"\n",
    "        self.decent1x1 = DecentLayer(ms_in=ms_in_1x1, ns_in=ns_in_1x1, n_filters=out_dim[-1], kernel_size=1, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        #self.tmp = torchvision.models.squeezenet1_0(torchvision.models.SqueezeNet1_0_Weights.IMAGENET1K_V1)\n",
    "        #self.tmp.classifier[1] = torch.nn.Conv2d(512, 10, kernel_size=(3,3))\n",
    "        \n",
    "        # head\n",
    "        self.fc = torch.nn.Linear(out_dim[-1], out_dim[-1])\n",
    "    \n",
    "        # activation\n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        # bias\n",
    "        self.bias1 = torch.nn.InstanceNorm2d(out_dim[1])\n",
    "        self.bias2 = torch.nn.InstanceNorm2d(out_dim[2])\n",
    "        self.bias3 = torch.nn.InstanceNorm2d(out_dim[3])\n",
    "        self.bias1x1 = torch.nn.InstanceNorm2d(out_dim[-1])\n",
    "        \n",
    "        # activation\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # init connection cost\n",
    "        self.cc = []\n",
    "        self.update_connection_cost()\n",
    "        \n",
    "        # get a position in filter list\n",
    "        self.m_plot = self.decent2.filter_list[0].m_this.detach().cpu().numpy()\n",
    "        self.n_plot = self.decent2.filter_list[0].n_this.detach().cpu().numpy()  \n",
    "        # self.plot_layer_of_1_channel(current_epoch=0) - not working here, dir not created yet\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        # hook on the data (for gradcam or something similar)\n",
    "        # https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
    "        if mode == 'explain':\n",
    "            output = x.data.register_hook(self.activations_hook)\n",
    "            #'cannot register a hook on a tensor that doesn't require gradient'\n",
    "        \n",
    "        \n",
    "        # global max pooling for MIL\n",
    "        x.data = F.max_pool2d(x.data, kernel_size=x.data.size()[2:])\n",
    "        \n",
    "        x.data = x.data.reshape(x.data.size(0), -1)\n",
    "        x.data = self.fc(x.data) \n",
    "        \n",
    "        # x.data = self.sigmoid(x.data)\n",
    "        \n",
    "        # x.data = self.tmp(x.data)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    \n",
    "    def activations_hook(self, grad):\n",
    "        # hook for the gradients of the activations\n",
    "        self.gradients = grad\n",
    "    def get_activations_gradient(self):\n",
    "        # method for the gradient extraction\n",
    "        return self.gradients\n",
    "    def get_activations(self, x):\n",
    "        # method for the activation exctraction\n",
    "        \n",
    "        #print('0', x)\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        #print('1', x)\n",
    "\n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        #print('2', x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        #print('3', x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        #print('1x1', x)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    def plot_layer_of_1_channel(self, current_epoch=0):\n",
    "        \n",
    "        # get each filter position that has a channel that matches\n",
    "        ms = []; ns = []\n",
    "        \n",
    "        #print(self.decent2.filter_list)\n",
    "        #print(\"**********************\")\n",
    "        #print(self.decent3.filter_list)\n",
    "\n",
    "        \n",
    "        # go through all filters in this layer\n",
    "        for f in self.decent3.filter_list:\n",
    "            \n",
    "            # if filter position in prev layer matches any channel in this layer\n",
    "            if any(pair == (self.m_plot, self.n_plot) for pair in zip(f.ms_in.detach().cpu().numpy(), f.ns_in.detach().cpu().numpy())):\n",
    "                \n",
    "                #print('match', f.m_this, f.n_this)\n",
    "                \n",
    "                # save position of each filter in this layer\n",
    "                ms.append(f.m_this.detach().cpu().numpy())\n",
    "                ns.append(f.n_this.detach().cpu().numpy())\n",
    "              \n",
    "            if False:\n",
    "                    print(\"nooooooooooooooo\")\n",
    "                    print(f.ms_in)\n",
    "                    print(self.m_plot)\n",
    "                    print(f.ns_in)\n",
    "                    print(self.n_plot)\n",
    "\n",
    "                    print((self.m_plot, self.n_plot))\n",
    "                \n",
    "        # visualising the previous and current layer neurons\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=100000, color='blue', alpha=0.1) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=50000, color='blue',alpha=0.2) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=25000, color='blue',alpha=0.3) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=500, color='blue') # previous layer\n",
    "        ax.scatter(ms, ns, color='red') # next layer\n",
    "        plt.xlim(0, self.grid_sqrt) # m coordinate of grid_size field\n",
    "        plt.ylim(0, self.grid_sqrt) # n coordinate of grid_size field\n",
    "        ax.grid() # enable grid line\n",
    "        fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_plot[0])}_n{int(self.n_plot[0])}_{str(current_epoch)}.png\"))\n",
    "    \n",
    "    def update_connection_cost(self):\n",
    "        self.cc = []\n",
    "        # self.cc.append(self.decent1.compute_layer_connection_cost()) # maybe not even needed ...\n",
    "        self.cc.append(self.decent2.compute_layer_connection_cost())\n",
    "        self.cc.append(self.decent3.compute_layer_connection_cost())\n",
    "        self.cc.append(self.decent1x1.compute_layer_connection_cost())\n",
    "        self.cc = torch.mean(torch.tensor(self.cc))\n",
    "\n",
    "    def update(self, current_epoch):\n",
    "        # =============================================================================\n",
    "        # update_every_nth_epoch\n",
    "        # adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        # =============================================================================\n",
    "        \n",
    "        # update decent layers\n",
    "        \n",
    "        #self.decent1.update()\n",
    "        self.decent2.update()\n",
    "        self.decent3.update()\n",
    "        #self.decent1x1.update()\n",
    "        \n",
    "        # visualisation\n",
    "        self.plot_layer_of_1_channel(current_epoch)\n",
    "    \n",
    "        # connection cost has to be calculated after pruning\n",
    "        # self.cc which is updated is used for loss function\n",
    "        self.update_connection_cost()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "\n",
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint\n",
    "        # Save a checkpoint at the end of the training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "            print(\"DECENT NOTE: callback on_train_epoch_end\", monitor_candidates[\"epoch\"])\n",
    "            if monitor_candidates[\"epoch\"] > 0:\n",
    "                if monitor_candidates[\"unpruned_state\"] != -1:\n",
    "                    print(\"DECENT NOTE: save model\", monitor_candidates[\"epoch\"])\n",
    "                    if self._every_n_epochs >= 1 and ((trainer.current_epoch + 1) % self._every_n_epochs) == 0:\n",
    "                        self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                    self._save_last_checkpoint(trainer, monitor_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6be2dce9-a8d6-42c2-aed5-e25199d0a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(ModelCheckpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81d937-90ac-4b68-b255-5cbddabe624b",
   "metadata": {},
   "source": [
    "## explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a78f1c9b-367f-4bf1-9af8-ad5856cbd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisation.hook import Hook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class FeatureMap1():\n",
    "    # =============================================================================\n",
    "    # ??? https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030\n",
    "    # ??? https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c\n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, model, layer, device=\"cpu\", ckpt_net_path=None, iterations=200, lr=1):\n",
    "        # =============================================================================\n",
    "        # Initialise iter, lr, model, layer\n",
    "        # =============================================================================\n",
    "\n",
    "        # settings for dreams\n",
    "        self.iterations=iterations\n",
    "        self.lr=lr\n",
    "        self.device = device\n",
    "\n",
    "        # model\n",
    "        if ckpt_net_path is not None:\n",
    "            model.load_state_dict(torch.load(ckpt_net_path)[\"model\"]) # 'dir/decentnet_epoch_19_0.3627.ckpt'\n",
    "        self.model = model.eval()\n",
    "        \n",
    "        # the (conv) layer to be visualised\n",
    "        self.layer = layer\n",
    "        print(\"\")\n",
    "        print(\"Layer:\", self.layer)\n",
    "        print(\"\")\n",
    "\n",
    "    def run(self, img_tensor):\n",
    "        # =============================================================================\n",
    "        # Feature map visualisation using hooks       \n",
    "        # A high activation means a certain feature was found. \n",
    "        # A feature map is called the activations of a layer after the convolutional operation.\n",
    "        # =============================================================================\n",
    "        \n",
    "        #print(\"i\", img_tensor.data.shape)\n",
    "        self.ii = img_tensor.data\n",
    "            \n",
    "        hook = Hook(self.layer)\n",
    "        output = self.model(img_tensor, mode='explain')\n",
    "        self.feature_maps = hook.output.data.squeeze()\n",
    "        \n",
    "        #print('o', output.shape)\n",
    "        #print('i', self.ii.shape)\n",
    "        #print('f', self.feature_maps.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def plot(self, path=None):\n",
    "        # =============================================================================\n",
    "        # plot 15 random feature maps + original image\n",
    "        # =============================================================================\n",
    "        fig, axarr = plt.subplots(4, 4)\n",
    "        # plt.figure(figsize=(100,100))\n",
    "        amount = self.feature_maps.shape[0]\n",
    "        print(\"amount of feature maps:\", amount)\n",
    "        if amount < 16:\n",
    "            sample_amount = amount\n",
    "        else:\n",
    "            sample_amount = 16\n",
    "        random_samples = random.sample(range(0, amount), sample_amount)\n",
    "        counter = 0  \n",
    "        idx,idx2 = [0, 0]\n",
    "        for idx in range(0, 4):\n",
    "            for idx2 in range(0, 4):\n",
    "                axarr[idx, idx2].axis('off')\n",
    "                try:\n",
    "                    axarr[idx, idx2].imshow(self.feature_maps[random_samples[counter]].cpu().detach().numpy())\n",
    "                    counter += 1\n",
    "                except:\n",
    "                    try:\n",
    "                        axarr[idx, idx2].imshow(self.feature_maps.cpu().detach().numpy())\n",
    "                        counter += 1\n",
    "                    except Exception as e:\n",
    "                        print(\"not possible to show feature maps image\")\n",
    "                        print(self.feature_maps.shape)\n",
    "                        print(e)\n",
    "\n",
    "        # overwrite first image with original image\n",
    "        try:\n",
    "            axarr[idx,idx2].imshow(self.ii.cpu().detach().numpy().transpose(1, 2, 0))\n",
    "        except:\n",
    "            try:\n",
    "                axarr[idx,idx2].imshow(self.ii.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "            except:\n",
    "                try: \n",
    "                    axarr[idx,idx2].imshow(self.ii.squeeze(1).cpu().detach().numpy().transpose(1, 2, 0))\n",
    "                except Exception as e:\n",
    "                    print(\"not possible to show original image\")\n",
    "                    print(e)\n",
    "        \n",
    "        if path is not None:\n",
    "            fig.savefig(path)\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "        # print(self.ii.shape)\n",
    "        # plt.imshow(self.ii.squeeze(0).cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cd9fbda-b8d3-43ba-9b86-f28c9eaedaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_explain(model, layer, device='cuda'):\n",
    "\n",
    "    ichallenge_data = torchvision.datasets.ImageFolder('example_data/eye')\n",
    "    img, label = ichallenge_data.__getitem__(1)\n",
    "\n",
    "    # tensor preparation\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    resize = transforms.Resize(28)\n",
    "    img = to_tensor(img)\n",
    "    img = resize(img)\n",
    "    \n",
    "    import torch\n",
    "\n",
    "    tmp = torch.autograd.Variable(torch.randn(1, 1, 28, 28)) # batch x channel x width x height\n",
    "    # dense_input.shape\n",
    "\n",
    "    # todo: ms need to have same size as channel\n",
    "\n",
    "    img = X(img.unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "    \n",
    "    print(img.data.shape)\n",
    "    \n",
    "    # run feature map\n",
    "    dd = FeatureMap1(model=model, layer=layer, device=device, iterations=10, lr=0.1)\n",
    "    dd.run(img)\n",
    "    dd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "### DecentLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # Lightning Module consists of functions that define the training routine\n",
    "    # train, val, test: before epoch, step, after epoch, ...\n",
    "    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n",
    "    # order for the instance methods:\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\n",
    "    # \n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, kwargs, log_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "    \n",
    "        # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "        self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        self.pretrain_epochs = model_kwargs[\"pretrain_epochs\"]\n",
    "        \n",
    "        # needed for hparams.yaml file\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if False:\n",
    "            self.metric = { \"train_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"train_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"val_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"val_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "                   }\n",
    "        else:\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "\n",
    "            \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        # =============================================================================\n",
    "        # we make it possible to use model_output = self(image)\n",
    "        # =============================================================================\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_checkpoint(cls, checkpoint_path, **kwargs):\n",
    "        # todo, need to overwrite this somehow\n",
    "    \n",
    "        #Always use self for the first argument to instance methods.\n",
    "        #Always use cls for the first argument to class methods.\n",
    "    \n",
    "        loaded = cls._load_from_checkpoint(\n",
    "            cls,  # type: ignore[arg-type]\n",
    "            checkpoint_path,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return cast(Self, loaded)\n",
    "    \n",
    "    # todo, need to overwrite this somehow\n",
    "    @classmethod\n",
    "    def _load_from_checkpoint(\n",
    "        cls,\n",
    "        checkpoint_path,\n",
    "        **kwargs):\n",
    "        \n",
    "        map_location = None\n",
    "        with pl_legacy_patch():\n",
    "            checkpoint = pl_load(checkpoint_path, map_location=map_location)\n",
    "\n",
    "        # convert legacy checkpoints to the new format\n",
    "        checkpoint = _pl_migrate_checkpoint(\n",
    "            checkpoint, checkpoint_path=(checkpoint_path if isinstance(checkpoint_path, (str, Path)) else None)\n",
    "        )\n",
    "\n",
    "        if hparams_file is not None:\n",
    "            extension = str(hparams_file).split(\".\")[-1]\n",
    "            if extension.lower() == \"csv\":\n",
    "                hparams = load_hparams_from_tags_csv(hparams_file)\n",
    "            elif extension.lower() in (\"yml\", \"yaml\"):\n",
    "                hparams = load_hparams_from_yaml(hparams_file)\n",
    "            else:\n",
    "                raise ValueError(\".csv, .yml or .yaml is required for `hparams_file`\")\n",
    "\n",
    "            # overwrite hparams by the given file\n",
    "            checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY] = hparams\n",
    "\n",
    "        # TODO: make this a migration:\n",
    "        # for past checkpoint need to add the new key\n",
    "        checkpoint.setdefault(cls.CHECKPOINT_HYPER_PARAMS_KEY, {})\n",
    "        # override the hparams with values that were passed in\n",
    "        checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)\n",
    "\n",
    "        if issubclass(cls, pl.LightningDataModule):\n",
    "            return _load_state(cls, checkpoint, **kwargs)\n",
    "        if issubclass(cls, pl.LightningModule):\n",
    "            model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "            if not state_dict:\n",
    "                rank_zero_warn(f\"The state dict in {checkpoint_path!r} contains no parameters.\")\n",
    "                return model\n",
    "\n",
    "            device = next((t for t in state_dict.values() if isinstance(t, torch.Tensor)), torch.tensor(0)).device\n",
    "            assert isinstance(model, pl.LightningModule)\n",
    "            return model.to(device)\n",
    "\n",
    "        raise NotImplementedError(f\"Unsupported {cls}\")\n",
    "\n",
    "           \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # =============================================================================\n",
    "        # returns:\n",
    "        #    optimiser and lr scheduler\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimiser = optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        else:\n",
    "            optimiser = optim.SGD(self.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        # initial plot of circular layer\n",
    "        # updates model every nth epoch\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: on_train_epoch_start\", self.current_epoch)\n",
    "        \n",
    "        # plot random layer (the circular plot)\n",
    "        if self.current_epoch == 0:\n",
    "            self.model.plot_layer_of_1_channel(current_epoch=0)\n",
    "        \n",
    "        # update model\n",
    "         # don't update unless pretrain epochs is reached\n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch >= self.pretrain_epochs:\n",
    "            print(\"DECENT NOTE: update model\", self.current_epoch)        \n",
    "            if debug_model:\n",
    "                print(\"DECENT NOTE: before update\")\n",
    "                print(self.model)\n",
    "            self.model.update(current_epoch = self.current_epoch)\n",
    "            if debug_model: \n",
    "                print(\"DECENT NOTE: after update\")\n",
    "                print(self.model)\n",
    "                \n",
    "            print(\"DECENT NOTE: model updated\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculates loss for a batch # 1\n",
    "        # parameters:\n",
    "        #    batch\n",
    "        #    batch id\n",
    "        # returns:\n",
    "        #    loss\n",
    "        # notes:\n",
    "        #    calling gradcam like self.gradcam(batch) is dangerous cause changes gradients\n",
    "        # =============================================================================     \n",
    "        if False: # batch_idx < 2: # print first two steps\n",
    "            print(\"DECENT NOTE: training_step\", batch_idx)\n",
    "\n",
    "        # calculate loss\n",
    "        # loss = torch.tensor(1)\n",
    "        loss = self._loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging # 2\n",
    "        # =============================================================================\n",
    "        if False: # batch_idx < 2:\n",
    "            print(\"DECENT NOTE: validation_step\", batch_idx)\n",
    "        \n",
    "        self._loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing # 3\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_validation_epoch_end\")\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # save model if next iteration model is pruned # 4 \n",
    "        # this needs to be called before callback \n",
    "        # - if internal pytorch lightning convention changes, this will stop working\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_train_epoch_end\", self.current_epoch)\n",
    "               \n",
    "        if False:\n",
    "            print(\"current epoch\")\n",
    "            print(((self.current_epoch+1) % self.update_every_nth_epoch) == 0)\n",
    "            print(self.current_epoch+1)\n",
    "            print(self.current_epoch)\n",
    "            print(self.update_every_nth_epoch)\n",
    "        \n",
    "        # numel: returns the total number of elements in the input tensor\n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.log(f'unpruned', unpruned, on_step=False, on_epoch=True) \n",
    "        \n",
    "        if ((self.current_epoch+1) % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            # if next epoch is an update, set unpruned flag            \n",
    "            self.log(f'unpruned_state', 1, on_step=False, on_epoch=True)\n",
    "            \n",
    "            # save file\n",
    "            with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "                f.write(\"#\"*50)\n",
    "                for p in self.model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        f.write(str(p.shape))\n",
    "            \n",
    "        else:\n",
    "            # else set unpruned flag to -1, then model won't be saved\n",
    "            self.log(f'unpruned_state', -1, on_step=False, on_epoch=True)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging, plot gradcam\n",
    "        # =============================================================================\n",
    "        if batch_idx < 2:\n",
    "            print(\"DECENT NOTE: test_step\", batch_idx)\n",
    "\n",
    "        self._loss_n_metrics(batch, mode=\"test\")\n",
    "        self.gradcam(batch, mode='explain')\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_test_epoch_end\", self.current_epoch)\n",
    "        pass\n",
    "    \n",
    "    def gradcam(self, batch, mode='explain'):\n",
    "        # =============================================================================\n",
    "        # grad cam\n",
    "        # todo error: RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n",
    "        # =============================================================================\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        \n",
    "        # make it an X object\n",
    "        # init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        print(\"nooooooooooo grad, whyyyyy\")\n",
    "        print(tmp_img1)\n",
    "        print(img)\n",
    "        \n",
    "        #print('b1', tmp_img1)\n",
    "        #print('b2', tmp_img2)\n",
    "        \n",
    "        model_output = self(tmp_img1, mode)\n",
    "        \n",
    "        #print('c1', tmp_img1)\n",
    "        #print('c2', tmp_img2)\n",
    "        \n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "        \n",
    "        # get prediction value\n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "        \n",
    "        #print('d1', tmp_img1)\n",
    "        \n",
    "        #print(\"mo\", model_output)\n",
    "        #print(\"max\", pred_max)\n",
    "        \n",
    "        # backpropagate for gradient tracking\n",
    "        model_output[:, pred_max].backward()\n",
    "        \n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        #print('e2', tmp_img2)\n",
    "        \n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "        \n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "        \n",
    "        # draw the heatmap\n",
    "        plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "    \n",
    "    def _loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        # =============================================================================\n",
    "        # put image through model, calculate loss and metrics\n",
    "        # use cc term that has been calculated previously\n",
    "        # =============================================================================\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth.unsqueeze(1).size(0), self.n_classes).scatter_(1, ground_truth.unsqueeze(1).to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \n",
    "        loss = self.criterion(model_output, ground_truth) # ground_truth_multi_hot)\n",
    "        cc = torch.mean(self.model.cc) * self.cc_weight\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "        \n",
    "        #print(model_output)\n",
    "        #print(pred_i)\n",
    "        #print(ground_truth)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            self.train_f1(preds=pred_i, target=ground_truth) \n",
    "            self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", self.train_acc)\n",
    "                print(\"f\", self.train_f1)\n",
    "                print(\"p\", self.train_prec)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        else:\n",
    "            self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2906-61b6-4f3a-b2fa-7aeeaa12e7e1",
   "metadata": {},
   "source": [
    "### Dev routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_routine(**kwargs):\n",
    "    # =============================================================================\n",
    "    # dataset\n",
    "    # logger\n",
    "    # trainer\n",
    "    # trainer.fit\n",
    "    # trainer.test\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"train kwargs\", kwargs['train_kwargs'])\n",
    "    print(\"model kwargs\", kwargs['model_kwargs'])\n",
    "    \n",
    "    train_kwargs = kwargs['train_kwargs']\n",
    "    model_kwargs = kwargs['model_kwargs']\n",
    "    \n",
    "    # \"examples/example_results/lightning_logs\"\n",
    "    logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"])\n",
    "    \n",
    "    trainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         # inference_mode=False, # do grad manually\n",
    "                         log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                         logger=logger,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         max_epochs=train_kwargs[\"epochs\"],\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1\",\n",
    "                                                   filename='{epoch}-{val_f1:.2f}-{unpruned:.0f}'),\n",
    "                                    DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                                    filename='{epoch}-{unpruned:.0f}-{val_f1:.2f}'),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    \n",
    "    # we want the grad to work in test\n",
    "    explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                         accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=[0],\n",
    "                         inference_mode=False)\n",
    "    \n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = DecentLightning.load_from_checkpoint(pretrained_filename, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(19) # To be reproducable\n",
    "                \n",
    "        # Initialize the LightningModule and LightningDataModule\n",
    "        model = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "        \n",
    "\n",
    "        # Train the model using a Trainer\n",
    "        trainer.fit(model, train_dataloader, val_dataloader)\n",
    "        \n",
    "        # we don't save the positions here ...\n",
    "        # model = DecentLightning.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, kwargs=kwargs) # Load best checkpoint after training\n",
    "\n",
    "        \n",
    "    # Test best model on validation and test set\n",
    "    val_result = explainer.test(model, val_dataloader, verbose=False)\n",
    "    # test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    try:\n",
    "        result = {\"test accuracy on valset\": val_result[0][\"test_acc\"]}\n",
    "    except:\n",
    "        result = 0\n",
    "        \n",
    "    try:     \n",
    "        layer = model.model.decent2 # .filter_list[7]weights\n",
    "        run_explain(model, layer, device='cuda')\n",
    "    except Exception as e:\n",
    "        print(\" layer not working \" )\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "    print(result)\n",
    "\n",
    "    return model, result, val_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02da990-f5b1-4cef-86cb-403eb1d6fcc2",
   "metadata": {},
   "source": [
    "### run lightning ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ad8b29-48de-4f62-88da-381f60d698f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'result_path': 'examples/example_results', 'exp_name': 'tmp_testi', 'load_ckpt_file': 'xversion_22/checkpoints/epoch=0-unpruned=10942-val_f1=0.06.ckpt', 'epochs': 3, 'img_size': 28, 'batch_size': 128, 'log_every_n_steps': 4, 'device': 'cuda', 'num_workers': 18}\n",
      "model kwargs {'n_classes': 10, 'out_dim': [1, 8, 16, 32], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 10, 'cc_metric': 'l2', 'ci_metric': 'l2', 'update_every_nth_epoch': 2, 'pretrain_epochs': 1, 'prune_keep': 0.95, 'prune_keep_total': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 19\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type                | Params\n",
      "---------------------------------------------------\n",
      "0 | model      | DecentNet           | 8.3 K \n",
      "1 | criterion  | CrossEntropyLoss    | 0     \n",
      "2 | train_acc  | MulticlassAccuracy  | 0     \n",
      "3 | train_f1   | MulticlassF1Score   | 0     \n",
      "4 | train_prec | MulticlassPrecision | 0     \n",
      "5 | val_acc    | MulticlassAccuracy  | 0     \n",
      "6 | val_f1     | MulticlassF1Score   | 0     \n",
      "7 | val_prec   | MulticlassPrecision | 0     \n",
      "---------------------------------------------------\n",
      "6.3 K     Trainable params\n",
      "2.1 K     Non-trainable params\n",
      "8.3 K     Total params\n",
      "0.033     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: configure_optimizers\n",
      "Sanity Checking: |                                                                               | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  0.51it/s]DECENT NOTE: on_validation_epoch_end\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                   | 0/2 [00:00<?, ?it/s]DECENT NOTE: on_train_epoch_start 0\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  0.36it/s, v_num=9]\n",
      "Validation: |                                                                                    | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 1/2 [00:02<00:02,  0.50it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  0.59it/s]\u001b[ADECENT NOTE: on_validation_epoch_end\n",
      "\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  0.15it/s, v_num=9]\u001b[ADECENT NOTE: on_train_epoch_end 0\n",
      "DECENT NOTE: callback on_train_epoch_end tensor(0)\n",
      "Epoch 1:   0%|                                                                          | 0/2 [00:00<?, ?it/s, v_num=9]DECENT NOTE: on_train_epoch_start 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  0.25it/s, v_num=9]\n",
      "Validation: |                                                                                    | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 1/2 [00:02<00:02,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  0.57it/s]\u001b[ADECENT NOTE: on_validation_epoch_end\n",
      "\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  0.13it/s, v_num=9]\u001b[ADECENT NOTE: on_train_epoch_end 1\n",
      "DECENT NOTE: callback on_train_epoch_end tensor(1)\n",
      "DECENT NOTE: save model tensor(1)\n",
      "Epoch 2:   0%|                                                                          | 0/2 [00:00<?, ?it/s, v_num=9]DECENT NOTE: on_train_epoch_start 2\n",
      "DECENT NOTE: update model 2\n",
      "DECENT NOTE: model updated\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  0.25it/s, v_num=9]\n",
      "Validation: |                                                                                    | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 1/2 [00:02<00:02,  0.48it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  0.59it/s]\u001b[ADECENT NOTE: on_validation_epoch_end\n",
      "\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  0.13it/s, v_num=9]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: on_train_epoch_end 2\n",
      "DECENT NOTE: callback on_train_epoch_end tensor(2)\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  0.13it/s, v_num=9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|                                                                      | 0/2 [00:00<?, ?it/s]DECENT NOTE: test_step 0\n",
      "nooooooooooo grad, whyyyyy\n",
      "X(data: torch.Size([128, 1, 28, 28]) at positions: ms_x= 0, ns_x= 0)\n",
      "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot register a hook on a tensor that doesn't require gradient",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# this is the main function, run this cell!!!\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model, results, v_res \u001b[38;5;241m=\u001b[39m \u001b[43mdev_routine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 62\u001b[0m, in \u001b[0;36mdev_routine\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloader, val_dataloader)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# we don't save the positions here ...\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# model = DecentLightning.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, kwargs=kwargs) # Load best checkpoint after training\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m     \n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Test best model on validation and test set\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m val_result \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# test_result = trainer.test(model, test_loader, verbose=False)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:755\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:795\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    794\u001b[0m )\n\u001b[1;32m--> 795\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    797\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 990\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    995\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1029\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mzero_grad(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mzero_grad_kwargs)\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[1;32m-> 1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:416\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtest_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[16], line 250\u001b[0m, in \u001b[0;36mDecentLightning.test_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDECENT NOTE: test_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_idx)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_n_metrics(batch, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexplain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 279\u001b[0m, in \u001b[0;36mDecentLightning.gradcam\u001b[1;34m(self, batch, mode)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m(img)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m#print('b1', tmp_img1)\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m#print('b2', tmp_img2)\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtmp_img1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m#print('c1', tmp_img1)\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m#print('c2', tmp_img2)\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m \n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# get prediction value\u001b[39;00m\n\u001b[0;32m    288\u001b[0m pred_max \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[16], line 58\u001b[0m, in \u001b[0;36mDecentLightning.forward\u001b[1;34m(self, x, mode)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# we make it possible to use model_output = self(image)\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[11], line 106\u001b[0m, in \u001b[0;36mDecentNet.forward\u001b[1;34m(self, x, mode)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m#print(x)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# hook on the data (for gradcam or something similar)\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations_hook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m#'cannot register a hook on a tensor that doesn't require gradient'\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# global max pooling for MIL\u001b[39;00m\n\u001b[0;32m    111\u001b[0m x\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x\u001b[38;5;241m.\u001b[39mdata, kernel_size\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torch\\_tensor.py:527\u001b[0m, in \u001b[0;36mTensor.register_hook\u001b[1;34m(self, hook)\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39mregister_hook, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, hook)\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m--> 527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    528\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot register a hook on a tensor that \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt require gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    529\u001b[0m     )\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cannot register a hook on a tensor that doesn't require gradient"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# this is the main function, run this cell!!!\n",
    "# =============================================================================\n",
    "\n",
    "model, results, v_res = dev_routine(model_kwargs=model_kwargs, train_kwargs=train_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc96b36-0633-4ecb-ae79-57287d70e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "(4%2)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe1e41-94f2-4a2b-ab3f-d3aedd24efbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5d6dd-be92-4db7-bdd8-40d886f10adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43468c27-f130-4118-8d11-275282921c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d301c-cbad-4154-a053-137a3263c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "for l in range(100):\n",
    "    try:\n",
    "        layer = model.model.decent2.filter_list[l] # .filter_list[7]weights\n",
    "        run_explain(model, layer, device='cuda')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2568e15-95ba-4c8c-9a51-b8cb5050ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "layer = model.model.decent2 # .filter_list[7]weights\n",
    "run_explain(model, layer, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f4513-419a-433b-97ff-6f54c5a6d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b09e4-644a-4cca-9da3-a47986de1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca471c2c-819f-44aa-a8f9-e318aacf3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['loops'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['state_dict'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
