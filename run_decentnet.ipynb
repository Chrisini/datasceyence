{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Aaq4vTX6lxUS",
   "metadata": {
    "id": "Aaq4vTX6lxUS"
   },
   "source": [
    "# 𝔻𝕖𝕔𝕖𝕟𝕥ℕ𝕖𝕥: 𝕕𝕚𝕤-𝕖𝕟𝕥-𝕒𝕟𝕘𝕝𝕖𝕕 𝕟𝕖𝕥\n",
    "Goal: create a neural network with disentangled early hidden layers. We want to analyse which textures contribute to the predictions.\n",
    "\n",
    "In this notebook you can\n",
    "\n",
    "1) train one or multiple DecentBlocks\n",
    "  * we use a supervised contrastive loss function\n",
    "\n",
    "2) train a DecentNet\n",
    "  * we use the cross entropy loss\n",
    "  * we freeze and use the DecentBlocks as part of the DecentNet\n",
    "  * a fusion layer is the bridge between the DecentBlocks and the combined layers\n",
    "\n",
    "3) Baseline\n",
    "\n",
    "4) NOT HERE RIGHT NOW visualise the DecentBlocks and DecentNet (work in progress)\n",
    "  * DeepDreams\n",
    "  * Feature maps\n",
    "  * Filters\n",
    "\n",
    "Todos:\n",
    "* [ ] metrics / tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pYryiJ8XKkm4",
   "metadata": {
    "id": "pYryiJ8XKkm4"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae53756",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eae53756",
    "outputId": "1392d54c-c059-4187-80fb-2bbb803edb84"
   },
   "outputs": [],
   "source": [
    "# basics\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import shufflenet_v2_x1_0, resnet50\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"helper\")\n",
    "from helper.dataset.transform.transform import ToTensor, ResizeCrop, RandomAugmentations, Normalise\n",
    "from helper.dataset.transform.two_crop import *\n",
    "from helper.compute.loss.supcon import SupConLoss\n",
    "from helper.sampler.mixed_batch import MixedBatchSampler\n",
    "from helper.dataset.concept import ClusterConceptDataset, PosNegConceptDataset\n",
    "from helper.model.decentblock import *\n",
    "\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "try:\n",
    "    from torchvision.models import ShuffleNet_V2_X1_0_Weights\n",
    "except:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244d212",
   "metadata": {
    "id": "4244d212"
   },
   "source": [
    "# Experiment Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TMN4hZi_yFAD",
   "metadata": {
    "id": "TMN4hZi_yFAD"
   },
   "outputs": [],
   "source": [
    "class Configs():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # general (half of these not in use)\n",
    "        self.num_workers = 0\n",
    "        self.epochs = 50\n",
    "        self.n_samples_per_class_per_batch = 10\n",
    "\n",
    "        # optimisation\n",
    "        self.learning_rate = 0.01\n",
    "        self.weight_decay = 1e-4\n",
    "        self.momentum = 0.9\n",
    "\n",
    "        # data - make sure it is the same size for quilted images\n",
    "        self.image_size = 500\n",
    "\n",
    "        self.prefix = \"tmp\"\n",
    "        \n",
    "        self.device = \"cpu\" # \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # =============================================================================\n",
    "        # Paths\n",
    "        # =============================================================================\n",
    "        \n",
    "        # we read and write to an external directory!!\n",
    "        self.base_path = r\"C:/Users/Prinzessin/projects/decentnet\"\n",
    "        if not os.path.exists(self.base_path):\n",
    "            os.makedirs(self.base_path)\n",
    "        os.chdir(self.base_path) # this is now the main directory !!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "        # input for decentblock\n",
    "        self.csv_filenames =     [f\"results/{self.prefix}/masks_info_label.csv\"]\n",
    "        self.concepts_path =     f\"data/{self.prefix}/concepts\"\n",
    "        \n",
    "        # input for decentnet\n",
    "        self.train_path =        r\"data/images/train\"\n",
    "        self.val_path =          r\"data/images/val\"\n",
    "        self.test_path =         r\"data/images/test\"\n",
    "\n",
    "        # output\n",
    "        self.ckpt_net_path =     f\"results/{self.prefix}/ckpts/decentnet\"\n",
    "        self.ckpt_blocks_path =  f\"results/{self.prefix}/ckpts/decentblock\"\n",
    "        self.results_path =      f\"results/{self.prefix}\"\n",
    "        \n",
    "        if not os.path.exists(self.ckpt_net_path):\n",
    "            os.makedirs(self.ckpt_net_path)\n",
    "        if not os.path.exists(self.ckpt_blocks_path):\n",
    "            os.makedirs(self.ckpt_blocks_path)\n",
    "            \n",
    "        # =============================================================================\n",
    "        # activate function calls\n",
    "        # =============================================================================    \n",
    "            \n",
    "        self.run_decentblocks = True\n",
    "        self.run_decentnet = False\n",
    "        self.run_baseline = False\n",
    "        self.run_visualisation = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZKOheDtVvdYn",
   "metadata": {
    "id": "ZKOheDtVvdYn"
   },
   "source": [
    "# 𝔻𝕖𝕔𝕖𝕟𝕥𝕌𝕟𝕚𝕥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ATBINCQDj5nY",
   "metadata": {
    "id": "ATBINCQDj5nY"
   },
   "source": [
    "## DecentBlock Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18c174",
   "metadata": {
    "id": "0f18c174"
   },
   "outputs": [],
   "source": [
    "class DecentBlock_MLP_routine():\n",
    "    \n",
    "    def __init__(self, configs):\n",
    "        \n",
    "        self.configs = configs\n",
    "        \n",
    "    def set_loader(self, mode=\"train\", ci_concept=0):\n",
    "        # =============================================================================\n",
    "        # construct data loader\n",
    "        # =============================================================================\n",
    "        \n",
    "        p_aug = 0.5\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            dataset = PosNegConceptDataset(mode=\"train\", channels=3, index_col=0, image_size=self.configs.image_size, csv_filenames=self.configs.csv_filenames, ci_concept=ci_concept, concepts_path=self.configs.concepts_path, p_aug=p_aug)\n",
    "            # dataset = EyeDataset(mode=\"train\", ci_concept=ci_concept, image_size=self.configs.image_size)\n",
    "            mbs = MixedBatchSampler(dataset.get_class_labels(), n_samples_per_class_per_batch=self.configs.n_samples_per_class_per_batch)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                dataset, \n",
    "                batch_sampler = mbs,\n",
    "                num_workers=self.configs.num_workers)\n",
    "        elif mode == \"val\":\n",
    "            dataset = PosNegConceptDataset(mode=\"val\", channels=3, index_col=0, image_size=self.configs.image_size, csv_filenames=self.configs.csv_filenames, ci_concept=ci_concept, concepts_path=self.configs.concepts_path, p_aug=p_aug)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                dataset, \n",
    "                batch_size=1, # should be 1\n",
    "                shuffle=False,\n",
    "                num_workers=self.configs.num_workers)\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def set_optimizer(self, model):\n",
    "        # =============================================================================\n",
    "        # =============================================================================\n",
    "\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                                lr=self.configs.learning_rate,\n",
    "                                momentum=self.configs.momentum,\n",
    "                                weight_decay=self.configs.weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, model, optimizer, epoch, save_file):\n",
    "        # =============================================================================\n",
    "        # =============================================================================\n",
    "\n",
    "        state = {\n",
    "            'model': model.encoder.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, save_file)\n",
    "        del state\n",
    "\n",
    "    def train(self, loader, model, criterion, optimizer, epoch):\n",
    "        # =============================================================================\n",
    "        # DecentBlock\n",
    "        # one epoch training\n",
    "        # trainings pair (2) - 2 augmented versions\n",
    "        # batch size (8)\n",
    "        # image size (256 x 256)\n",
    "        # (8 x (2 x (256 x 256) ) )\n",
    "        \n",
    "        model.train()\n",
    "        loss_epoch = []    \n",
    "        for idx, batch in enumerate(loader):\n",
    "\n",
    "            #print(\"train \"*10)\n",
    "            \n",
    "            images, labels = batch[\"img\"], batch[\"lbl\"]\n",
    "            \n",
    "            images = torch.cat([images[0], images[1]], dim=0)\n",
    "\n",
    "            batch_size = labels.shape[0]\n",
    "            \n",
    "            features = model(images.to(self.configs.device))\n",
    "            f1, f2 = torch.split(features, [batch_size, batch_size], dim=0)\n",
    "            features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)        \n",
    "            loss = criterion(features, labels)\n",
    "\n",
    "            # SGD\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "            #print(\"+\"*50)\n",
    "            \n",
    "        return np.mean(loss_epoch)\n",
    "\n",
    "\n",
    "    def val(self, loader, model, criterion, epoch):\n",
    "        # =============================================================================\n",
    "        # validation decentblock\n",
    "        # trainings pair (2) - 2 augmented versions\n",
    "        # batch size (8)\n",
    "        # image size (256 x 256)\n",
    "        # (8 x (2 x (256 x 256) ) )\n",
    "        # =============================================================================\n",
    "        \n",
    "        model.eval()\n",
    "        loss_epoch = []    \n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(loader):\n",
    "            \n",
    "                images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                \n",
    "                images = torch.cat([images[0], images[1]], dim=0)\n",
    "\n",
    "                batch_size = labels.shape[0]\n",
    "\n",
    "                features = model(images.to(self.configs.device))\n",
    "                f1, f2 = torch.split(features, [batch_size, batch_size], dim=0)\n",
    "                features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)        \n",
    "                loss = criterion(features, labels)\n",
    "\n",
    "                loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "        return np.mean(loss_epoch)\n",
    "\n",
    "def decentblock_routine(configs):\n",
    "    \n",
    "    \n",
    "    temp = 0.07\n",
    "    criterion = SupConLoss(temperature=temp)\n",
    "\n",
    "    run = DecentBlock_MLP_routine(configs=configs)\n",
    "\n",
    "    # todo\n",
    "    concept_list = list(range(7))\n",
    "\n",
    "    for ci_concept in concept_list:\n",
    "        print(configs.concepts_path)\n",
    "        # for ci_concept in concept_list:\n",
    "\n",
    "        #print(\"concept\", ci_concept)\n",
    "\n",
    "        decent_block_mlp = DecentBlock(None, None, 128, device=configs.device, mode=\"train_mlp\")\n",
    "        decent_block_mlp = decent_block_mlp.to(configs.device)\n",
    "        criterion = criterion.to(configs.device)\n",
    "\n",
    "        # build data loader\n",
    "        train_loader = run.set_loader(mode=\"train\", ci_concept=ci_concept)\n",
    "        # val_loader = run.set_loader(mode=\"val\", batch_size=1, ci_concept=ci_concept)\n",
    "\n",
    "        # print(\"train_loader:\", train_loader.__len__())\n",
    "        # print(\"val_loader:\", val_loader.__len__())\n",
    "        \n",
    "        # build optimizer\n",
    "        optimizer = run.set_optimizer(decent_block_mlp)\n",
    "\n",
    "        best_loss = 0\n",
    "        # training routine\n",
    "        for epoch in range(1, configs.epochs + 1):\n",
    "\n",
    "            # train for one epoch\n",
    "            loss_train_epoch = run.train(train_loader, decent_block_mlp, criterion, optimizer, epoch)        \n",
    "            #loss_val_epoch = run.val(val_loader, decent_block_mlp, criterion, iterations)\n",
    "\n",
    "            #print(\"iter: \", iter)\n",
    "            #print(\"loss_train_epoch\", loss_train_epoch)\n",
    "            #print(\"loss_val_epoch\", loss_val_epoch)\n",
    "            \n",
    "            #if epoch < 5: # for the first 5 epochs\n",
    "            #    best_loss = loss_val_epoch\n",
    "            #elif best_loss > loss_val_epoch: # then if loss is lower than best loss (aka better)\n",
    "            #    best_loss = loss_val_epoch\n",
    "\n",
    "            best_loss = loss_train_epoch\n",
    "            save_file = os.path.join(ckpt_blocks_path, f'{prefix}_mlp_{ci_concept}_ep{iter}_{round(best_loss, 4)}.ckpt')\n",
    "            run.save_model(decent_block_mlp, optimizer, iter, save_file)\n",
    "\n",
    "        # save the last model\n",
    "        #save_file = os.path.join(ckpt_net_path, f'mlp_{ci_concept}_last_{round(loss_val_epoch, 4)}.ckpt')\n",
    "        #run.save_model(decent_block_mlp, optimizer, iter, save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ktmPY-WevwxD",
   "metadata": {
    "id": "ktmPY-WevwxD"
   },
   "source": [
    "# 𝔻𝕖𝕔𝕖𝕟𝕥ℕ𝕖𝕥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11567602",
   "metadata": {
    "id": "11567602"
   },
   "source": [
    "**How to freeze layers**\n",
    "\n",
    "The layers of DecentBlocks need to be frozen while training the DecentNet\n",
    "\n",
    "* Just adding this here for completeness. You can also freeze parameters in place without iterating over them with requires_grad_ (API).\n",
    "\n",
    "* For example say you have a RetinaNet and want to just fine-tune on the heads\n",
    "\n",
    "```\n",
    "class RetinaNet(torch.nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        self.backbone = ResNet(...)\n",
    "        self.fpn = FPN(...)\n",
    "        self.box_head = torch.nn.Sequential(...)\n",
    "        self.cls_head = torch.nn.Sequential(...)\n",
    "```\n",
    "\n",
    "Then you could freeze the backbone and FPN like this:\n",
    "\n",
    "* Getting the model\n",
    "```\n",
    "retinanet = RetinaNet(...)\n",
    "```\n",
    "\n",
    "* Freezing backbone and FPN\n",
    "```\n",
    "retinanet.backbone.requires_grad_(False)\n",
    "retinanet.fpn.requires_grad_(False)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a7359",
   "metadata": {
    "id": "908a7359"
   },
   "source": [
    "**Fusing** \n",
    "\n",
    "Fusing methods: mostly element-wise sum or maximum operations have been studied for fusing CNN feature maps from multiple views for the purpose of classification. Correspondence between multiple views is thereby lost, while fusion by concatenation or convolution were found to efficiently model correspondences between different views for other learning tasks. Comparative evaluations of different strategies for image classification are either missing or yield contradicting results.\n",
    "\n",
    "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0245230\n",
    "\n",
    "In early fusion, convolutional feature maps from the different CNN branches are stacked and subsequently processed together. \n",
    "\n",
    "\n",
    "We consider two different approaches for depth reduction: \n",
    "* (1) early fusion (max): max-pooling of the stacked feature map across the nV views\n",
    "* (2) early fusion (conv): 1 × 1 convolution across the depth of the stacked feature maps.\n",
    "\n",
    "stacking, 1x1 conv || max\n",
    "\n",
    "\n",
    "\n",
    "* In order to further achieve effective fusion of local and global features of images, this paper uses gated fusion sub-networks to adaptively fuse multiple feature maps obtained based on branch networks\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/sheryl-ai/MVGCN/blob/master/models.py\n",
    "```\n",
    "    def _view_pool(self, view_features, name, method='max'):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
    "\n",
    "        vp = tf.expand_dims(view_features[0], 0) # eg. [100] -> [1, 100]\n",
    "        for v in view_features[1:]:\n",
    "            v = tf.expand_dims(v, 0)\n",
    "            vp = tf.concat([vp, v], axis=0)\n",
    "        print ('vp before reducing:', vp.get_shape().as_list())\n",
    "        if method == 'max':\n",
    "            vp = tf.reduce_max(vp, [0], name=name)\n",
    "        elif method == 'mean':\n",
    "            vp = tf.reduce_mean(vp, [0], name=name)\n",
    "        return vp\n",
    "```    \n",
    "    \n",
    "    \n",
    "https://github.com/VChristlein/dgmp/blob/master/clamm/pooling.py\n",
    "\n",
    "\n",
    "Fusion of features after layer three of a CNN. The features from two streams are passed through max pooling, convolution, batch normalization and ReLU layers. The two outputs are then concatenated and form the input for the fourth layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bR00xLAQw0Am",
   "metadata": {
    "id": "bR00xLAQw0Am"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a65599",
   "metadata": {
    "id": "f3a65599"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet50, shufflenet_v2_x1_0\n",
    "import os\n",
    "\n",
    "class DecentNet_v1(nn.Module):\n",
    "    \n",
    "    # for freezing in train code: retinanet.backbone.requires_grad_(False)\n",
    "        \n",
    "    def __init__(self, num_classes=3, plot=False):\n",
    "        super(DecentNet_v1, self).__init__()\n",
    "        \n",
    "        print(\"init decentnet start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # get ckpt files\n",
    "        block_ckpts = os.listdir(ckpt_blocks_path)\n",
    "\n",
    "        # loads blocks\n",
    "        self.decent_blocks = nn.ModuleList([])\n",
    "\n",
    "        decent_block_116 = \"\"\n",
    "        \n",
    "        amount_of_blocks = 0\n",
    "        for block_ckpt in block_ckpts:\n",
    "            \n",
    "            # we use a shufflenet pretrained on data from a previous step\n",
    "            # these layer should be frozen during training of this model\n",
    "            decent_block = shufflenet_v2_x1_0()\n",
    "            decent_block.fc = nn.Identity()\n",
    "\n",
    "            # load shuffle net weights here\n",
    "            checkpoint = torch.load(os.path.join(ckpt_blocks_path, block_ckpt))\n",
    "            decent_block.load_state_dict(checkpoint['model'])\n",
    "\n",
    "            if True:\n",
    "              # remove layers - this line might be wrong too\n",
    "              decent_block_116 = nn.Sequential(*(list(decent_block.children())[:-4]), \n",
    "                                              nn.Conv2d(116, 5, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1)),\n",
    "                                              nn.BatchNorm2d(5),\n",
    "                                              nn.ReLU()\n",
    "                                              ).to(device)\n",
    "            else:\n",
    "              # not working for some reason ... needs to be in nn.Seq\n",
    "              decent_block_116 = nn.Sequential(*(list(decent_block.children())[:-4])).to(device)\n",
    "              self.decent_block_reduction = nn.Conv2d(116, 5, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1))\n",
    "            \n",
    "            self.decent_blocks.append(decent_block_116) # 116 output filters\n",
    "            amount_of_blocks += 1\n",
    "\n",
    "\n",
    "        if plot:\n",
    "            print(\"original\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block)\n",
    "            print(\"116\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block_116)\n",
    "                \n",
    "        # layer between decent blocks and combined layers\n",
    "        if False:\n",
    "          # single conv\n",
    "          self.fusion_layer = nn.Conv2d(5*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1))\n",
    "        else:\n",
    "          # conv, batchnorm and relu\n",
    "          self.fusion_layer = nn.Sequential(\n",
    "                nn.Conv2d(5*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1)),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU()\n",
    "          )\n",
    "        \n",
    "        # combined layers\n",
    "        r50 = resnet50(pretrained=True)\n",
    "        \n",
    "        # remove early layers\n",
    "        r50.conv1 = nn.Identity()\n",
    "        r50.bn1 = nn.Identity()\n",
    "        r50.relu = nn.Identity()\n",
    "        r50.maxpool = nn.Identity()\n",
    "        r50.layer1 = nn.Identity()\n",
    "        r50.layer2 = nn.Identity()  \n",
    "                \n",
    "        # change classification head\n",
    "        in_features = r50.fc.in_features\n",
    "        r50.fc = nn.Linear(in_features, self.num_classes)\n",
    "\n",
    "        self.combined_layers = r50 \n",
    "        \n",
    "        print(\"init decentnet done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        \n",
    "        # idea: use combined very early layers (pretrained on any dataset): edges\n",
    "        # (maybe for later, needs to be taken into account for the decent block training)\n",
    "        \n",
    "        \n",
    "        # get output for each decent block\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.decent_blocks):\n",
    "            block_output = block(image)\n",
    "            # block_output = self.decent_block_reduction(block_output)\n",
    "            block_outputs.append(block_output)\n",
    "            \n",
    "            # print(\"block output shape:\", block_output.shape)\n",
    "            \n",
    "        # concat features\n",
    "        concat = torch.cat(block_outputs, dim=1)\n",
    "        # print(\"concat shape:\", concat.shape)\n",
    "\n",
    "        # fusion layer\n",
    "        fusion = self.fusion_layer(concat)\n",
    "\n",
    "        # print(\"fusion output:\", fusion.shape)\n",
    "        \n",
    "        # combined layers\n",
    "        feature_vector = self.combined_layers(fusion)\n",
    "            \n",
    "        # print(\"combined layers output shape\", feature_vector.shape) \n",
    "        # print(feature_vector.shape) \n",
    "                        \n",
    "        return feature_vector\n",
    "    \n",
    "\n",
    "class DecentNet_v2(nn.Module):\n",
    "    \n",
    "    # for freezing in train code: retinanet.backbone.requires_grad_(False)\n",
    "        \n",
    "    def __init__(self, num_classes=3, plot=False):\n",
    "        super(DecentNet_v2, self).__init__()\n",
    "        \n",
    "        print(\"init decentnet start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # get ckpt files\n",
    "        block_ckpts = os.listdir(ckpt_blocks_path)\n",
    "\n",
    "        # loads blocks\n",
    "        self.decent_blocks = nn.ModuleList([])\n",
    "\n",
    "        decent_block_116 = \"\"\n",
    "        \n",
    "        amount_of_blocks = 0\n",
    "        for block_ckpt in block_ckpts:\n",
    "            \n",
    "            # we use a shufflenet pretrained on data from a previous step\n",
    "            # these layer should be frozen during training of this model\n",
    "            decent_block = shufflenet_v2_x1_0()\n",
    "            decent_block.fc = nn.Identity()\n",
    "\n",
    "            # load shuffle net weights here\n",
    "            checkpoint = torch.load(os.path.join(ckpt_blocks_path, block_ckpt))\n",
    "            decent_block.load_state_dict(checkpoint['model'])\n",
    "\n",
    "            # remove layers - this line might be wrong too\n",
    "            decent_block_116 = nn.Sequential(*(list(decent_block.children())[:-4])).to(device)\n",
    "\n",
    "            \n",
    "            \n",
    "            self.decent_blocks.append(decent_block_116) # 116 output filters\n",
    "            amount_of_blocks += 1\n",
    "\n",
    "        if plot:\n",
    "            print(\"original\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block)\n",
    "            print(\"116\")\n",
    "            print(\"*\"*50)\n",
    "            print(decent_block_116)\n",
    "                \n",
    "        # layer between decent blocks and combined layers\n",
    "        # todo, figure out how to get the 116 automatically - 58 * 2\n",
    "        # fusion_conv\n",
    "        if True:\n",
    "            self.fusion_layer = nn.Conv2d(116*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1))\n",
    "        else:\n",
    "            self.fusion_layer = nn.Sequential(\n",
    "                nn.Conv2d(116*amount_of_blocks, 512, (1, 1), stride=(1, 1), padding=(0, 0), dilation=(1, 1)),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # combined layers\n",
    "        r50 = resnet50(pretrained=True)\n",
    "        \n",
    "        # remove early layers\n",
    "        r50.conv1 = nn.Identity()\n",
    "        r50.bn1 = nn.Identity()\n",
    "        r50.relu = nn.Identity()\n",
    "        r50.maxpool = nn.Identity()\n",
    "        r50.layer1 = nn.Identity()\n",
    "        r50.layer2 = nn.Identity()  \n",
    "        \n",
    "        # torch.nn.Sequential(*list(r50.children())[3:]) - this is not working\n",
    "        \n",
    "        # change classification head\n",
    "        in_features = r50.fc.in_features\n",
    "        r50.fc = nn.Linear(in_features, self.num_classes)\n",
    "      \n",
    "        self.combined_layers = r50 \n",
    "        \n",
    "        print(\"init decentnet done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        \n",
    "        # idea: use combined very early layers (pretrained on any dataset): edges\n",
    "        # (maybe for later, needs to be taken into account for the decent block training)\n",
    "        \n",
    "        \n",
    "        # get output for each decent block\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.decent_blocks):\n",
    "            block_output = block(image)\n",
    "            block_outputs.append(block_output)\n",
    "            # print(\"block output shape:\", block_output.shape)\n",
    "            \n",
    "        # concat features\n",
    "        concat = torch.cat(block_outputs, dim=1)\n",
    "        # print(\"concat shape:\", concat.shape)\n",
    "\n",
    "        # fusion layer\n",
    "        fusion = self.fusion_layer(concat)\n",
    "        \n",
    "        # combined layers\n",
    "        feature_vector = self.combined_layers(fusion)\n",
    "            \n",
    "        # print(\"combined layers output shape\", feature_vector.shape) \n",
    "        # print(feature_vector.shape) \n",
    "                        \n",
    "        return feature_vector\n",
    "    \n",
    "    \n",
    "\n",
    "class DecentNet_v3(nn.Module):\n",
    "    \n",
    "    # for freezing in train code: retinanet.backbone.requires_grad_(False)\n",
    "        \n",
    "    def __init__(self, num_classes=3):\n",
    "        super(DecentNet_v1, self).__init__()\n",
    "        \n",
    "        print(\"init decentnet start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # prepare early blocks list\n",
    "        ckpt_early_blocks = os.listdir(ckpt_early_blocks_path)\n",
    "        self.early_blocks = nn.ModuleList([])\n",
    "        early_block_116 = None\n",
    "        for i, early_block in enumerate(ckpt_early_blocks):            \n",
    "            early_block_116 = DecentBlock_Shuffle_EarlyBlock(ckpt_early_blocks_path, early_block, out_channels=5)\n",
    "            self.early_blocks.append(early_block_116) # 116 output filters\n",
    "                \n",
    "        # prepare early fusion (optional)\n",
    "        self.early_fusion_module = None\n",
    "        \n",
    "        # prepare late blocks list (optional)\n",
    "        self.late_blocks = nn.ModuleList([])\n",
    "        self.late_blocks.append(DecentBlock_ResNet_LateBlock())\n",
    "        \n",
    "        # prepare late fusion (optional)\n",
    "        self.late_fusion_module = None\n",
    "        \n",
    "        # prepare head blocks list\n",
    "        self.head_blocks = nn.ModuleList([])\n",
    "        self.late_blocks.append(DecentBlock_ResNet_LateBlock())\n",
    "\n",
    "        print(\"init decentnet done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # early block(s)\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.early_blocks):\n",
    "            block_output = block(x)\n",
    "            block_outputs.append(block_output)\n",
    "            \n",
    "        # concat + fusion\n",
    "        if block_outputs:\n",
    "            x = torch.cat(block_outputs, dim=1)\n",
    "            x = self.early_fusion_module(x)\n",
    "        \n",
    "        # late block(s)\n",
    "        block_outputs = []\n",
    "        for i, block in enumerate(self.late_blocks):\n",
    "            block_output = block(x)\n",
    "            block_outputs.append(block_output)\n",
    "            \n",
    "        # concat + fusion\n",
    "        if block_outputs:\n",
    "            x = torch.cat(block_outputs, dim=1)\n",
    "            x = self.late_fusion_module(x)\n",
    "        \n",
    "        # head block(s) (multi-task)            \n",
    "        model_outputs = {}\n",
    "        for i, block in enumerate(self.head_blocks):\n",
    "            # model output of head = pass feature vector through head\n",
    "            model_outputs[type(head).__name__] = fc_layer(x)\n",
    "        \n",
    "        return model_outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EZ6CElCnj-nL",
   "metadata": {
    "id": "EZ6CElCnj-nL"
   },
   "source": [
    "## DecentNet Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oCM5QGTfyd-d",
   "metadata": {
    "id": "oCM5QGTfyd-d"
   },
   "outputs": [],
   "source": [
    "class DecentNet_routine():\n",
    "\n",
    "    def set_loader(self, mode=\"train\", batch_size=2, num_workers=0, image_size=500):\n",
    "        # construct data loader\n",
    "                    \n",
    "        if mode == \"train\":\n",
    "                train_transform = transforms.Compose([\n",
    "                    ResizeCrop(image_size=image_size),\n",
    "                    RandomAugmentations(),\n",
    "                    ToTensor(),\n",
    "                    Normalise()\n",
    "                ])\n",
    "\n",
    "                train_dataset = datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "\n",
    "                print(set(train_dataset.targets))\n",
    "\n",
    "                loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True,\n",
    "                    num_workers=num_workers)\n",
    "            \n",
    "        elif mode == \"val\":\n",
    "\n",
    "            val_transform = transforms.Compose([\n",
    "                    ResizeCrop(image_size=image_size),\n",
    "                    ToTensor(),\n",
    "                    Normalise()\n",
    "            ])\n",
    "\n",
    "            val_dataset = datasets.ImageFolder(root=val_path, transform=val_transform)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                    val_dataset, \n",
    "                    batch_size=batch_size, # should be 1\n",
    "                    shuffle=False,\n",
    "                    num_workers=num_workers)\n",
    "\n",
    "        elif mode == \"test\":\n",
    "\n",
    "            test_transforms = transforms.Compose([\n",
    "                    ResizeCrop(image_size=image_size),\n",
    "                    ToTensor(),\n",
    "                    Normalise()\n",
    "            ])\n",
    "\n",
    "            test_dataset = datasets.ImageFolder(root=test_path, transform=test_transforms)\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                    test_dataset, \n",
    "                    batch_size=batch_size, # should be 1 \n",
    "                    shuffle=False,\n",
    "                    num_workers=num_workers)\n",
    "            \n",
    "\n",
    "        return loader\n",
    "\n",
    "    def set_optimizer(self, model):\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                                lr=learning_rate,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "    def save_model(self, model, optimizer, epoch, save_file):\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, save_file)\n",
    "        del state\n",
    "\n",
    "    def train(self, loader, model, criterion, optimizer, epoch):\n",
    "        \"\"\"one epoch training\"\"\"\n",
    "        \n",
    "        model.train()\n",
    "        loss_epoch = []\n",
    "        ground_truth_all = []\n",
    "        model_output_all = []  \n",
    "        for idx, (images, labels) in enumerate(loader):\n",
    "            \n",
    "\n",
    "            # images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                        \n",
    "            model_output = model(images.to(device))\n",
    "            loss = criterion(model_output, labels.to(device))\n",
    "\n",
    "            # SGD\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                    \n",
    "            loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "            _, highest_class = torch.max(model_output, 1)    \n",
    "            highest_class = highest_class.detach().cpu().numpy()    \n",
    "\n",
    "            ground_truth_all.extend(labels.detach().cpu().numpy())\n",
    "            model_output_all.extend(highest_class)\n",
    "\n",
    "        print(ground_truth_all)\n",
    "        print(model_output_all)\n",
    "            \n",
    "        f_score_epoch = f1_score(y_true = ground_truth_all, y_pred = model_output_all, average=\"weighted\", labels=[0,1,2])\n",
    "\n",
    "        return np.mean(loss_epoch), f_score_epoch\n",
    "\n",
    "\n",
    "    def val(self, loader, model, criterion, epoch):\n",
    "        # decentnet\n",
    "        # with labels\n",
    "        \"\"\"validation\"\"\"\n",
    "        \n",
    "        model.eval()\n",
    "        loss_epoch = []   \n",
    "        ground_truth_all = []\n",
    "        model_output_all = [] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, labels) in enumerate(loader):\n",
    "            \n",
    "                # images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                        \n",
    "                model_output = model(images.to(device))\n",
    "                loss = criterion(model_output, labels.to(device))\n",
    "                        \n",
    "                loss_epoch.append(loss.detach().cpu().numpy().item())\n",
    "\n",
    "                _, highest_class = torch.max(model_output, 1)    \n",
    "                highest_class = highest_class.detach().cpu().numpy()    \n",
    "\n",
    "                ground_truth_all.extend(labels.detach().cpu().numpy())\n",
    "                model_output_all.extend(highest_class)\n",
    "\n",
    "        print(ground_truth_all)\n",
    "        print(model_output_all)\n",
    "\n",
    "        f_score_epoch = f1_score(y_true = ground_truth_all, y_pred = model_output_all, average=\"weighted\", labels=[0,1,2])\n",
    "\n",
    "        return np.mean(loss_epoch), f_score_epoch\n",
    "\n",
    "\n",
    "    def test(self, loader, model):\n",
    "        # decentnet - results for non existant labels\n",
    "        # without labels\n",
    "        \n",
    "        model.eval()\n",
    "        highest_classes = [] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, labels) in enumerate(loader):\n",
    "            \n",
    "                # images, labels = batch[\"image\"], batch[\"label\"]\n",
    "                        \n",
    "                model_output = model(images.to(device))\n",
    "                _, highest_class = torch.max(model_output, 1)    \n",
    "                highest_class = highest_class.detach().cpu().numpy()\n",
    "\n",
    "                highest_classes.extend(highest_class)    \n",
    "\n",
    "\n",
    "        print(highest_classes)\n",
    "\n",
    "        df = pd.DataFrame({'Prediction': highest_classes})\n",
    "        df.to_csv(\"test_decentnet.csv\")\n",
    "\n",
    "\n",
    "    def visualise_with_labels(self, loader, model):\n",
    "        pass\n",
    "\n",
    "\n",
    "def decentnet_routine():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    run = DecentNet_routine()\n",
    "\n",
    "    decentnet = DecentNet_v2(num_classes=3).to(device)\n",
    "\n",
    "    # freeze decent blocks\n",
    "    if True:\n",
    "        for i, child in enumerate(decentnet.decent_blocks.children()):\n",
    "            # exclude the conv layer (or sequential???) - needs to be trainable\n",
    "            for param in child[:-1].parameters():\n",
    "                param.requires_grad = False\n",
    "    else:\n",
    "        # v1, without conv layer (that is trainable)\n",
    "        decentnet.decent_blocks.requires_grad_(False)\n",
    "\n",
    "    \n",
    "    decentnet = decentnet.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # set data loader\n",
    "    train_loader = run.set_loader(mode=\"train\", batch_size=8, num_workers=num_workers, image_size=image_size)\n",
    "    val_loader = run.set_loader(mode=\"val\", batch_size=1, num_workers=num_workers, image_size=image_size)\n",
    "\n",
    "    # set optimizer\n",
    "    optimizer = run.set_optimizer(decentnet)\n",
    "\n",
    "    best_loss = 0\n",
    "    iter = 0\n",
    "    # training routine\n",
    "    for iter in range(1, iterations + 1):\n",
    "\n",
    "            # train for one epoch\n",
    "            loss_train_epoch, fscore_train_epoch = run.train(loader=train_loader, model=decentnet, criterion=criterion, optimizer=optimizer, epoch=epoch)        \n",
    "            loss_val_epoch, fscore_val_epoch = run.val(loader=val_loader, model=decentnet, criterion=criterion, epoch=epoch)\n",
    "\n",
    "            print(\"iter: \", iter)\n",
    "            print(loss_train_epoch)\n",
    "            print(fscore_train_epoch)\n",
    "            print(loss_val_epoch)\n",
    "            print(fscore_val_epoch)\n",
    "            \n",
    "            if iter < 3: # for the first 3 epochs\n",
    "                best_loss = loss_val_epoch\n",
    "            elif best_loss > loss_val_epoch: # then if loss is lower than best loss (aka better)\n",
    "                best_loss = loss_val_epoch\n",
    "                save_file = os.path.join(ckpt_net_path, f'decentnet_epoch_{iter}_{round(best_loss, 4)}.ckpt')\n",
    "                run.save_model(decentnet, optimizer, iter, save_file)\n",
    "\n",
    "    # save the last model\n",
    "    save_file = os.path.join(ckpt_net_path, f'decentnet_last_{round(loss_val_epoch, 4)}.ckpt')\n",
    "    run.save_model(decentnet, optimizer, iter, save_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vg8CPRaSNdfv",
   "metadata": {
    "id": "Vg8CPRaSNdfv"
   },
   "source": [
    "# 𝔹𝕒𝕤𝕖𝕝𝕚𝕟𝕖 ℝ𝕖𝕤ℕ𝕖𝕥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aCIOrVGINkK-",
   "metadata": {
    "id": "aCIOrVGINkK-"
   },
   "source": [
    "## Model\n",
    "\n",
    "* early stopping: https://pythonguides.com/pytorch-early-stopping/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "APta2MnINdA7",
   "metadata": {
    "id": "APta2MnINdA7"
   },
   "outputs": [],
   "source": [
    "class DecentBaseline(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=3):\n",
    "        super(DecentBaseline, self).__init__()\n",
    "        \n",
    "        print(\"init baseline start\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # resnet 50\n",
    "        r50 = resnet50(pretrained=True)\n",
    "\n",
    "        # change classification head\n",
    "        in_features = r50.fc.in_features\n",
    "        r50.fc = nn.Linear(in_features, self.num_classes)\n",
    "        \n",
    "        self.r50 = r50 \n",
    "        \n",
    "        print(\"init baseline done\")\n",
    "        \n",
    "        \n",
    "    def forward(self, image):\n",
    "        \n",
    "        # resnet 50\n",
    "        feature_vector = self.r50(image)\n",
    "                        \n",
    "        return feature_vector\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rAN0jFnCQC0y",
   "metadata": {
    "id": "rAN0jFnCQC0y"
   },
   "source": [
    "## Baseline Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRVZHWPpQFFt",
   "metadata": {
    "id": "TRVZHWPpQFFt"
   },
   "outputs": [],
   "source": [
    "def baseline_routine():\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    run = DecentNet_routine()\n",
    "\n",
    "    ########## \n",
    "    # TRAIN and VAL Decent Baseline\n",
    "    ##########\n",
    "\n",
    "    baseline = DecentBaseline(num_classes=3).to(device)\n",
    "\n",
    "\n",
    "    baseline = baseline.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # build data loader\n",
    "    train_loader = run.set_loader(mode=\"train\", batch_size=8, num_workers=num_workers, image_size=image_size)\n",
    "    val_loader = run.set_loader(mode=\"val\", batch_size=1, num_workers=num_workers, image_size=image_size)\n",
    "\n",
    "    # build optimizer\n",
    "    optimizer = run.set_optimizer(baseline)\n",
    "\n",
    "    best_loss = 0\n",
    "    iter = 0\n",
    "    # training routine\n",
    "    for iter in range(1, iterations + 1):\n",
    "\n",
    "        # train for one epoch\n",
    "        loss_train_epoch, fscore_train_epoch = run.train(loader=train_loader, model=baseline, criterion=criterion, optimizer=optimizer, epoch=iter)        \n",
    "        loss_val_epoch, fscore_val_epoch = run.val(loader=val_loader, model=baseline, criterion=criterion, epoch=epoch)\n",
    "\n",
    "        print(\"iter: \", iter)\n",
    "        print(loss_train_epoch)\n",
    "        print(fscore_train_epoch)\n",
    "        print(loss_val_epoch)\n",
    "        print(fscore_val_epoch)\n",
    "        \n",
    "        if iter < 3: # for the first 3 epochs\n",
    "            best_loss = loss_val_epoch\n",
    "        elif best_loss > loss_val_epoch: # then if loss is lower than best loss (aka better)\n",
    "            best_loss = loss_val_epoch\n",
    "            save_file = os.path.join(ckpt_net_path, f'baseline_epoch_{iter}_{round(best_loss, 4)}.ckpt')\n",
    "            run.save_model(baseline, optimizer, iter, save_file)\n",
    "\n",
    "    # save the last model\n",
    "    save_file = os.path.join(ckpt_net_path, f'baseline_last_{round(loss_val_epoch, 4)}.ckpt')\n",
    "    run.save_model(baseline, optimizer, iter, save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fab557-104b-4da0-a34d-986ea6c04e58",
   "metadata": {},
   "source": [
    "# 𝔹𝕣𝕒𝕚𝕟-𝕀𝕟𝕤𝕡𝕚𝕣𝕖𝕕 𝕄𝕠𝕕𝕦𝕝𝕒𝕣 𝕋𝕣𝕒𝕚𝕟𝕚𝕟𝕘 (𝔹𝕀𝕄𝕋)\n",
    "* https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b3062-183f-4b34-86c6-defec7e04e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0f106-921f-4b2b-afa0-1ce1693f0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "class BioLinear2D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, in_fold=1, out_fold=1, out_ring=False):\n",
    "        super(BioLinear2D, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.in_fold = in_fold\n",
    "        self.out_fold = out_fold\n",
    "        assert in_dim % in_fold == 0\n",
    "        assert out_dim % out_fold == 0\n",
    "        \n",
    "        #compute in_cor, shape: (in_dim_sqrt, in_dim_sqrt)\n",
    "        in_dim_fold = int(in_dim/in_fold)\n",
    "        out_dim_fold = int(out_dim/out_fold)\n",
    "        in_dim_sqrt = int(np.sqrt(in_dim_fold))\n",
    "        out_dim_sqrt = int(np.sqrt(out_dim_fold))\n",
    "        x = np.linspace(1/(2*in_dim_sqrt), 1-1/(2*in_dim_sqrt), num=in_dim_sqrt)\n",
    "        X, Y = np.meshgrid(x, x)\n",
    "        self.in_coordinates = torch.tensor(np.transpose(np.array([X.reshape(-1,), Y.reshape(-1,)])), dtype=torch.float)\n",
    "        \n",
    "        # compute out_cor, shape: (out_dim_sqrt, out_dim_sqrt)\n",
    "        if out_ring:\n",
    "            thetas = np.linspace(1/(2*out_dim_fold)*2*np.pi, (1-1/(2*out_dim_fold))*2*np.pi, num=out_dim_fold)\n",
    "            self.out_coordinates = 0.5+torch.tensor(np.transpose(np.array([np.cos(thetas), np.sin(thetas)]))/4, dtype=torch.float)\n",
    "        else:\n",
    "            x = np.linspace(1/(2*out_dim_sqrt), 1-1/(2*out_dim_sqrt), num=out_dim_sqrt)\n",
    "            X, Y = np.meshgrid(x, x)\n",
    "            self.out_coordinates = torch.tensor(np.transpose(np.array([X.reshape(-1,), Y.reshape(-1,)])), dtype=torch.float)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d27cc-079f-4133-8f22-aeb59eacfefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioMLP2D(nn.Module):\n",
    "    def __init__(self, in_dim=2, out_dim=2, w=2, depth=2, shp=None, token_embedding=False, embedding_size=None):\n",
    "        super(BioMLP2D, self).__init__()\n",
    "        if shp == None:\n",
    "            shp = [in_dim] + [w]*(depth-1) + [out_dim]\n",
    "            self.in_dim = in_dim\n",
    "            self.out_dim = out_dim\n",
    "            self.depth = depth\n",
    "                 \n",
    "        else:\n",
    "            self.in_dim = shp[0]\n",
    "            self.out_dim = shp[-1]\n",
    "            self.depth = len(shp) - 1\n",
    "        linear_list = []\n",
    "        for i in range(self.depth):\n",
    "            if i == 0:\n",
    "                # for modular addition\n",
    "                #linear_list.append(BioLinear(shp[i], shp[i+1], in_fold=2))\n",
    "                # for regression\n",
    "                linear_list.append(BioLinear2D(shp[i], shp[i+1], in_fold=1))\n",
    "            elif i == self.depth - 1:\n",
    "                linear_list.append(BioLinear2D(shp[i], shp[i+1], in_fold=1, out_ring=True))\n",
    "            else:\n",
    "                linear_list.append(BioLinear2D(shp[i], shp[i+1]))\n",
    "        self.linears = nn.ModuleList(linear_list)\n",
    "        \n",
    "        \n",
    "        if token_embedding == True:\n",
    "            # embedding size: number of tokens * embedding dimension\n",
    "            self.embedding = torch.nn.Parameter(torch.normal(0,1,size=embedding_size))\n",
    "        \n",
    "        self.shp = shp\n",
    "        # parameters for the bio-inspired trick\n",
    "        self.l0 = 0.5 # distance between two nearby layers\n",
    "        self.in_perm = nn.Parameter(torch.tensor(np.arange(int(self.in_dim/self.linears[0].in_fold)), dtype=torch.float))\n",
    "        self.out_perm = nn.Parameter(torch.tensor(np.arange(int(self.out_dim/self.linears[-1].out_fold)), dtype=torch.float))\n",
    "        self.top_k = 30\n",
    "        self.token_embedding = token_embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"************************************\")\n",
    "        #print(\"next input\")\n",
    "        \n",
    "        #print(\"input size\", x.shape)\n",
    "        \n",
    "        shp = x.shape\n",
    "        x = x.reshape(shp[0],-1)\n",
    "        #print(\"reshape size\", x.shape)\n",
    "        \n",
    "        shp = x.shape\n",
    "        in_fold = self.linears[0].in_fold\n",
    "        x = x.reshape(shp[0], in_fold, int(shp[1]/in_fold))\n",
    "        x = x[:,:,self.in_perm.long()]\n",
    "        x = x.reshape(shp[0], shp[1])\n",
    "        #print(\"morde reshape size\", x.shape)\n",
    "        \n",
    "        f = torch.nn.SiLU()\n",
    "        for i in range(self.depth-1):\n",
    "            x = f(self.linears[i](x))\n",
    "            #print(\"linear output size\", x.shape)\n",
    "            \n",
    "        x = self.linears[-1](x)\n",
    "        #print(\"final linear output size\", x.shape)\n",
    "        \n",
    "        out_perm_inv = torch.zeros(self.out_dim, dtype=torch.long)\n",
    "        out_perm_inv[self.out_perm.long()] = torch.arange(self.out_dim)\n",
    "        x = x[:,out_perm_inv]\n",
    "        \n",
    "        #print(\"result\", x.shape)\n",
    "        \n",
    "        #x = x[:,self.out_perm]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_linear_layers(self):\n",
    "        return self.linears\n",
    "    \n",
    "    def get_cc(self, weight_factor=2.0, bias_penalize=True, no_penalize_last=False):\n",
    "        # compute connection cost\n",
    "        cc = 0\n",
    "        num_linear = len(self.linears)\n",
    "        for i in range(num_linear):\n",
    "            if i == num_linear - 1 and no_penalize_last:\n",
    "                weight_factor = 0.\n",
    "            biolinear = self.linears[i]\n",
    "            dist = torch.sum(torch.abs(biolinear.out_coordinates.unsqueeze(dim=1) - biolinear.in_coordinates.unsqueeze(dim=0)),dim=2)\n",
    "            \n",
    "            #print(\"biolinear.linear.weight\", biolinear.linear.weight)\n",
    "            #print(\"weight_factor\", weight_factor)\n",
    "            #print(\"dist\", dist)\n",
    "            #print(\"self.l0\", self.l0)\n",
    "            \n",
    "            \n",
    "            cc += torch.mean(torch.abs(biolinear.linear.weight)*(weight_factor*dist+self.l0).to(\"cuda:0\"))\n",
    "            if bias_penalize == True:\n",
    "                cc += torch.mean(torch.abs(biolinear.linear.bias)*(self.l0))\n",
    "        if self.token_embedding:\n",
    "            cc += torch.mean(torch.abs(self.embedding)*(self.l0))\n",
    "            #pass\n",
    "        return cc\n",
    "    \n",
    "    def swap_weight(self, weights, j, k, swap_type=\"out\"):\n",
    "        with torch.no_grad():  \n",
    "            if swap_type == \"in\":\n",
    "                temp = weights[:,j].clone()\n",
    "                weights[:,j] = weights[:,k].clone()\n",
    "                weights[:,k] = temp\n",
    "            elif swap_type == \"out\":\n",
    "                temp = weights[j].clone()\n",
    "                weights[j] = weights[k].clone()\n",
    "                weights[k] = temp\n",
    "            else:\n",
    "                raise Exception(\"Swap type {} is not recognized!\".format(swap_type))\n",
    "            \n",
    "    def swap_bias(self, biases, j, k):\n",
    "        with torch.no_grad():  \n",
    "            temp = biases[j].clone()\n",
    "            biases[j] = biases[k].clone()\n",
    "            biases[k] = temp\n",
    "    \n",
    "    def swap(self, i, j, k):\n",
    "        # in the ith layer (of neurons), swap the jth and the kth neuron. \n",
    "        # Note: n layers of weights means n+1 layers of neurons.\n",
    "        # (incoming, outgoing) * weights + biases are swapped. \n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i == 0:\n",
    "            return\n",
    "            # for images, do not allow input_perm\n",
    "            # input layer, only has outgoing weights; update in_perm\n",
    "            weights = linears[i].linear.weight\n",
    "            infold = linears[i].in_fold\n",
    "            fold_dim = int(weights.shape[1]/infold)\n",
    "            for l in range(infold):\n",
    "                self.swap_weight(weights, j+fold_dim*l, k+fold_dim*l, swap_type=\"in\")\n",
    "            # change input_perm. do not allow input_perm for images\n",
    "            self.swap_bias(self.in_perm, j, k)\n",
    "        elif i == num_linear:\n",
    "            # output layer, only has incoming weights and biases; update out_perm\n",
    "            weights = linears[i-1].linear.weight\n",
    "            biases = linears[i-1].linear.bias\n",
    "            self.swap_weight(weights, j, k, swap_type=\"out\")\n",
    "            self.swap_bias(biases, j, k)\n",
    "            # change output_perm\n",
    "            self.swap_bias(self.out_perm, j, k)\n",
    "        else:\n",
    "            # middle layer : (incoming, outgoing) * weights, and biases\n",
    "            weights_in = linears[i-1].linear.weight\n",
    "            weights_out = linears[i].linear.weight\n",
    "            biases = linears[i-1].linear.bias\n",
    "            self.swap_weight(weights_in, j, k, swap_type=\"out\")\n",
    "            self.swap_weight(weights_out, j, k, swap_type=\"in\")\n",
    "            self.swap_bias(biases, j, k)\n",
    "\n",
    "    def get_top_id(self, i, top_k=20):\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i == 0:\n",
    "            # input layer\n",
    "            weights = linears[i].linear.weight\n",
    "            score = torch.sum(torch.abs(weights), dim=0)\n",
    "            in_fold = linears[0].in_fold\n",
    "            #print(score.shape)\n",
    "            score = torch.sum(score.reshape(in_fold, int(score.shape[0]/in_fold)), dim=0)\n",
    "        elif i == num_linear:\n",
    "            # output layer\n",
    "            weights = linears[i-1].linear.weight\n",
    "            score = torch.sum(torch.abs(weights), dim=1)\n",
    "        else:\n",
    "            weights_in = linears[i-1].linear.weight\n",
    "            weights_out = linears[i].linear.weight\n",
    "            score = torch.sum(torch.abs(weights_out), dim=0) + torch.sum(torch.abs(weights_in), dim=1)\n",
    "        #print(score.shape)\n",
    "        top_index = torch.flip(torch.argsort(score),[0])[:top_k]\n",
    "        return top_index, score\n",
    "    \n",
    "    def relocate_ij(self, i, j):\n",
    "        # In the ith layer (of neurons), relocate the jth neuron\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i < num_linear:\n",
    "            num_neuron = int(linears[i].linear.weight.shape[1]/linears[i].in_fold)\n",
    "        else:\n",
    "            num_neuron = linears[i-1].linear.weight.shape[0]\n",
    "        ccs = []\n",
    "        for k in range(num_neuron):\n",
    "            self.swap(i,j,k)\n",
    "            ccs.append(self.get_cc())\n",
    "            self.swap(i,j,k)\n",
    "        k = torch.argmin(torch.stack(ccs))\n",
    "        self.swap(i,j,k)\n",
    "            \n",
    "    def relocate_i(self, i):\n",
    "        # Relocate neurons in the ith layer\n",
    "        top_id = self.get_top_id(i, top_k=self.top_k)\n",
    "        for j in top_id[0]:\n",
    "            self.relocate_ij(i,j)\n",
    "            \n",
    "    def relocate(self):\n",
    "        # Relocate neurons in the whole model\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        for i in range(num_linear+1):\n",
    "            self.relocate_i(i)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f46ac-1078-4c54-8478-144cfbb8125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# i think steps need to be smaller than data size\n",
    "steps_oct = 5000\n",
    "data_size_oct = 50000\n",
    "steps = 5000\n",
    "data_size = 1000\n",
    "batch_size = 50\n",
    "\n",
    "log = 1000\n",
    "lamb = 0.01\n",
    "swap_log = 1000\n",
    "plot_log = 1000\n",
    "\n",
    "\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# todo change here\n",
    "if True:\n",
    "    from medmnist import OCTMNIST\n",
    "    train = OCTMNIST(split=\"train\", transform=torchvision.transforms.ToTensor(), download=True)\n",
    "    test = OCTMNIST(split=\"test\", transform=torchvision.transforms.ToTensor(), download=True) \n",
    "    class_amount = 4\n",
    "elif True:\n",
    "    from medmnist import RetinaMNIST\n",
    "    t = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        torchvision.transforms.ToTensor()\n",
    "         ])\n",
    "    # torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "    train = RetinaMNIST(split=\"train\", transform=t, download=True)\n",
    "    test = RetinaMNIST(split=\"test\", transform=t, download=True) \n",
    "    class_amount = 5\n",
    "else:\n",
    "    train = torchvision.datasets.MNIST(root=\"/tmp\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "    test = torchvision.datasets.MNIST(root=\"/tmp\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "    class_amount = 10\n",
    "\n",
    "    \n",
    "train = torch.utils.data.Subset(train, range(data_size))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(network, dataset, device, N=2000, batch_size=5):\n",
    "    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, labels in islice(dataset_loader, N // batch_size):\n",
    "                \n",
    "        #print(x.shape)\n",
    "        logits = network(x.to(device))\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        correct += torch.sum(predicted_labels == labels.to(device))\n",
    "        total += x.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def loss_f(network, dataset, device, N=2000, batch_size=5):\n",
    "    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    for x, labels in islice(dataset_loader, N // batch_size):\n",
    "        labels = labels.squeeze().long()\n",
    "        logits = network(x.to(device))\n",
    "        #print(\"logits\", logits)\n",
    "        #print(\"torch eye\", torch.eye(10,)[labels]) \n",
    "        loss += torch.sum(( logits-torch.eye(class_amount,)[labels].to(device) )**2)\n",
    "        total += x.size(0)\n",
    "    return loss / total\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "if True:\n",
    "    import medmnist\n",
    "    #from medmnist import INFO\n",
    "    #info = INFO['retinamnist']\n",
    "    #DataClass = getattr(medmnist, info['python_class'])\n",
    "    #train = DataClass(split='train', download=True)    \n",
    "    #test = DataClass(split='test', download=True)    \n",
    "    from medmnist import RetinaMNIST\n",
    "    train = RetinaMNIST(split=\"train\", transform=torchvision.transforms.ToTensor(), download=True)\n",
    "    test = RetinaMNIST(split=\"test\", transform=torchvision.transforms.ToTensor(), download=True)\n",
    "else:\n",
    "    #train = torchvision.datasets.MNIST(root=\"/tmp\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "    #test = torchvision.datasets.MNIST(root=\"/tmp\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "    train = torchvision.datasets.MNIST(root=\"/tmp\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "    test = torchvision.datasets.MNIST(root=\"/tmp\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "train = torch.utils.data.Subset(train, range(data_size))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def L2(model):\n",
    "    L2_ = 0.\n",
    "    for p in mlp.parameters():\n",
    "        L2_ += torch.sum(p**2)\n",
    "    return L2_\n",
    "\n",
    "def rescale(model, alpha):\n",
    "    for p in mlp.parameters():\n",
    "        p.data = alpha * p.data\n",
    "\n",
    "\n",
    "width = 200\n",
    "mlp = BioMLP2D(shp=(784,100,100,class_amount))\n",
    "mlp.to(device)\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(mlp.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "\n",
    "one_hots = torch.eye(class_amount, class_amount).to(device)\n",
    "\n",
    "mlp.eval()\n",
    "print(\"Initial accuracy: {0:.4f}\".format(accuracy(mlp, test, device)))\n",
    "\n",
    "# todo create this directory\n",
    "# or change in all following\n",
    "torch.save(mlp.state_dict(), \"../results/oct_mnist/init.cptk\")\n",
    "\n",
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "\n",
    "\n",
    "step = 0\n",
    "mlp.train()\n",
    "pbar = tqdm(islice(cycle(train_loader), steps), total=steps)\n",
    "\n",
    "best_train_loss = 1e4\n",
    "best_test_loss = 1e4\n",
    "best_train_acc = 0.\n",
    "best_test_acc = 0.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd29611-a95c-4603-8d80-eef3bd56692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd2dbcb-26c0-40b4-9bc5-1a902f6be086",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083aeb06-6ced-498d-ac39-089b48eafcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if True:\n",
    "#for i_batch, (x, label) enumerate(train_loader):\n",
    "\n",
    "    for x, label in pbar:\n",
    "\n",
    "        if step == int(steps/4):\n",
    "            lamb *= class_amount\n",
    "        elif step == int(steps/2):\n",
    "            lamb *= class_amount\n",
    "\n",
    "        label = label.squeeze().long()\n",
    "\n",
    "        #print(\"image\", x.type)\n",
    "        #print(label)\n",
    "        #print(\"label\", label.shape)\n",
    "        #print(\"one_hots[label]\", one_hots[label])\n",
    "        #print(\"one_hots[label]\", one_hots[label].shape)\n",
    "\n",
    "        mlp.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print(label)\n",
    "        loss_train = loss_fn(mlp(x.to(device)), one_hots[label]) # .long()\n",
    "        cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        total_loss = loss_train + lamb*cc\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % log == 0:\n",
    "            with torch.no_grad():\n",
    "                mlp.eval()\n",
    "                train_acc = accuracy(mlp, train, device).item()\n",
    "                test_acc = accuracy(mlp, test, device).item()\n",
    "                train_loss = loss_f(mlp, train, device).item()\n",
    "                test_loss = loss_f(mlp, test, device).item()\n",
    "\n",
    "                if train_acc > best_train_acc:\n",
    "                    best_train_acc = train_acc\n",
    "                if test_acc > best_test_acc:\n",
    "                    best_test_acc = test_acc\n",
    "                if train_loss < best_train_loss:\n",
    "                    best_train_loss = train_loss\n",
    "                if test_loss < best_test_loss:\n",
    "                    best_test_loss = test_loss\n",
    "                mlp.train()\n",
    "                pbar.set_description(\"{:3.3f} | {:3.3f} | {:3.3f} | {:3.3f} | {:3.3f} \".format(train_acc, test_acc, train_loss, test_loss, cc))\n",
    "        step += 1\n",
    "\n",
    "        if step % swap_log == 0:\n",
    "            mlp.relocate()\n",
    "\n",
    "        if (step-1) % plot_log == 0:\n",
    "\n",
    "            fig=plt.figure(figsize=(30,15))\n",
    "            ax=fig.add_subplot(projection='3d')\n",
    "\n",
    "            ax.get_proj = lambda: np.dot(Axes3D.get_proj(ax), np.diag([0.5, 0.5, 2, 1]))\n",
    "            ax.scatter(mlp.linears[0].in_coordinates[:,0].detach().cpu().numpy(), mlp.linears[0].in_coordinates[:,1].detach().cpu().numpy(),[0]*784, s=5, alpha=0.5, c=train[46][0][0].detach().cpu().numpy()[:,::-1].reshape(-1,))\n",
    "            ax.scatter(mlp.linears[1].in_coordinates[:,0].detach().cpu().numpy(), mlp.linears[1].in_coordinates[:,1].detach().cpu().numpy(),[1]*100, s=5, alpha=0.5, color=\"black\")\n",
    "            ax.scatter(mlp.linears[2].in_coordinates[:,0].detach().cpu().numpy(), mlp.linears[2].in_coordinates[:,1].detach().cpu().numpy(),[2]*100, s=5, alpha=0.5, color=\"black\")\n",
    "            ax.scatter(mlp.linears[2].out_coordinates[:,0].detach().cpu().numpy(), mlp.linears[2].out_coordinates[:,1].detach().cpu().numpy(),[3]*class_amount, s=5, alpha=0.5, color=\"black\")\n",
    "            ax.set_zlim(-0.5,5)\n",
    "            ax.set_xlim(-0.2,1.2)\n",
    "            ax.set_ylim(-0.2,1.2)\n",
    "\n",
    "\n",
    "            for ii in range(3):\n",
    "                biolinear = mlp.linears[ii]\n",
    "                p = biolinear.linear.weight.clone()\n",
    "                p_shp = p.shape\n",
    "                p = p/torch.abs(p).max()\n",
    "\n",
    "                for i in range(p_shp[0]):\n",
    "                    #if i % 20 == 0:\n",
    "                    #    print(i)\n",
    "                    for j in range(p_shp[1]):\n",
    "                        out_xy = biolinear.out_coordinates[i].detach().cpu().numpy()\n",
    "                        in_xy = biolinear.in_coordinates[j].detach().cpu().numpy()\n",
    "                        plt.plot([out_xy[0], in_xy[0]], [out_xy[1], in_xy[1]], [ii+1,ii], lw=1*np.abs(p[i,j].detach().cpu().numpy()), color=\"blue\" if p[i,j]>0 else \"red\")\n",
    "\n",
    "\n",
    "            ring = mlp.linears[2].out_coordinates.detach().cpu().numpy()\n",
    "            for i in range(class_amount):\n",
    "                ax.text(ring[i,0], ring[i,1], 3.05, \"{}\".format(mlp.out_perm.long()[i].detach().cpu().numpy()))\n",
    "\n",
    "\n",
    "            ax.view_init(30,class_amount)\n",
    "\n",
    "            ax.text(0.3,0.25,3.5,\"step={}\".format(step-1), fontsize=15)\n",
    "\n",
    "            ax.axis('off')\n",
    "\n",
    "            plt.savefig('../results/oct_mnist/{0:06d}.png'.format(step-1))\n",
    "            torch.save(mlp.state_dict(), \"../results/oct_mnist/{0:06d}.cptk\".format(step-1))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    torch.save(mlp.state_dict(), \"../results/oct_mnist/final.cptk\")\n",
    "\n",
    "    # 50, 3, 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f058a-f91f-42bf-930f-2fe1c9f4028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    mlp.load_state_dict(torch.load(\"../results/oct_mnist/final.cptk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519ff04-6a6d-46ed-91db-bb56defe6420",
   "metadata": {},
   "outputs": [],
   "source": [
    "43313/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9de26-a027-428b-abed-5645f4e39f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699c42e-d9ca-483f-8002-79368f8bad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "3 * 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5454336-537c-4b02-aba5-7174e045cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "39200/2352/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e9e96-e202-4b52-be5f-7d1a1d3f4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef3eca-ff20-4a99-9ce1-623acb50e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "\n",
    "for i in range(1,3):\n",
    "    top_k = 784\n",
    "    linears = mlp.get_linear_layers()\n",
    "    num_linear = len(linears)\n",
    "    if i == 0:\n",
    "        # input layer\n",
    "        weights = linears[i].linear.weight\n",
    "        score = torch.sum(torch.abs(weights), dim=0)\n",
    "        in_fold = linears[0].in_fold\n",
    "        #print(score.shape)\n",
    "        score = torch.sum(score.reshape(in_fold, int(score.shape[0]/in_fold)), dim=0)\n",
    "    elif i == num_linear:\n",
    "        # output layer\n",
    "        weights = linears[i-1].linear.weight\n",
    "        score = torch.sum(torch.abs(weights), dim=1)\n",
    "    else:\n",
    "        weights_in = linears[i-1].linear.weight\n",
    "        weights_out = linears[i].linear.weight\n",
    "        score = torch.sum(torch.abs(weights_out), dim=0) + torch.sum(torch.abs(weights_in), dim=1)\n",
    "    #print(score.shape)\n",
    "    top_index = torch.flip(torch.argsort(score),[0])[:top_k]\n",
    "    score = score[top_index]\n",
    "    \n",
    "    num = score.shape[0]\n",
    "    \n",
    "    plt.plot(np.arange(num)+1, score.detach().cpu().numpy(), marker=\"o\", markersize=3)\n",
    "    \n",
    "#plt.xscale('log')\n",
    "\n",
    "plt.legend([\"hidden layer 1\", \"hidden layer 2\"], fontsize=15)\n",
    "plt.xlabel(\"Rank\", fontsize=15)\n",
    "plt.ylabel(\"Score\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8169890-24c7-4737-840d-82ffa1163740",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d42893-2370-4a82-9700-7e14a9aaa604",
   "metadata": {},
   "source": [
    "### Seems like this is only one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8ff7b-23d2-462a-8048-f8b880aa5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "top_k = 784\n",
    "linears = mlp.get_linear_layers()\n",
    "num_linear = len(linears)\n",
    "if i == 0:\n",
    "    # input layer\n",
    "    weights = linears[i].linear.weight\n",
    "    score = torch.sum(torch.abs(weights), dim=0)\n",
    "    in_fold = linears[0].in_fold\n",
    "    #print(score.shape)\n",
    "    score = torch.sum(score.reshape(in_fold, int(score.shape[0]/in_fold)), dim=0)\n",
    "elif i == num_linear:\n",
    "    # output layer\n",
    "    weights = linears[i-1].linear.weight\n",
    "    score = torch.sum(torch.abs(weights), dim=1)\n",
    "else:\n",
    "    weights_in = linears[i-1].linear.weight\n",
    "    weights_out = linears[i].linear.weight\n",
    "    score = torch.sum(torch.abs(weights_out), dim=0) + torch.sum(torch.abs(weights_in), dim=1)\n",
    "#print(score.shape)\n",
    "top_index = torch.flip(torch.argsort(score),[0])[:top_k]\n",
    "score = score[top_index]\n",
    "\n",
    "num = score.shape[0]\n",
    "\n",
    "features = mlp.linears[0].linear.weight[top_index].reshape(100,28,28).detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "\n",
    "    plt.imshow(features[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"score=%.2f\"%(score[i]), color=\"red\", fontsize=15,y=0.8)\n",
    "    \n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "plt.savefig(\"../results/oct_mnist/final_mnist_features.pdf\", bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a481a7-437e-4420-8506-7a36250e5a2a",
   "metadata": {},
   "source": [
    "### Seems like these are 3 layers combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd20982-b469-400b-b9a1-b66c4f0e4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# im_f2 = torch.argsort(mlp.linears[2].linear.weight[4])[-1]\n",
    "im_f2 = torch.argsort(mlp.linears[2].linear.weight[3])[-1] # [0, n_classes-1]\n",
    "im_f1 = torch.argsort(mlp.linears[1].linear.weight[im_f2,:])\n",
    "\n",
    "features = mlp.linears[0].linear.weight[im_f1].reshape(100,28,28).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "mlp.linears[1].linear.weight[im_f2,:]\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "\n",
    "    plt.imshow(features[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"weight=%.2f\"%(mlp.linears[1].linear.weight[im_f2,im_f1[i]]), color=\"red\", fontsize=15,y=0.8)\n",
    "    \n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b6f07-f8a8-47e7-9199-7dc5eb497904",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7efc443-2e67-4389-9997-bedb6930ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# im_f2 = torch.argsort(mlp.linears[2].linear.weight[4])[-1]\n",
    "im_f2 = torch.argsort(mlp.linears[2].linear.weight[0])[-1]\n",
    "features = torch.argsort(mlp.linears[1].linear.weight).reshape(100,10,10).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "mlp.linears[1].linear.weight[im_f2,:]\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "\n",
    "    plt.imshow(features[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"weight=%.2f\"%(mlp.linears[1].linear.weight[im_f2,im_f1[i]]), color=\"red\", fontsize=15,y=0.8)\n",
    "    \n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b837a2-3e45-49f7-95d9-62ab5ce69275",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argsort(mlp.linears[2].linear.weight[0]) # [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba48fe4-621e-4a93-b9ec-1f356ccd3a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268d6c3-0442-4ec2-b09b-e7c745e260e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.linears[2].linear.weight[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171bae8-5a6f-47f8-8513-28efc6a951b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.linears[2].linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3770f-e651-405c-a7d6-587d47063c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.linears[1].linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fc2a6-1241-40ca-8776-fc12b1083117",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.linears[0].linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a2445-d196-4adc-989b-80c0f20d4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe195d0-906d-4014-a0cd-d08a4ae1ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12439c4-2e31-4ee2-b900-70679baedbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchsummary.summary(mlp, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c39da-ad71-4f26-a88f-35eebb6cf206",
   "metadata": {},
   "outputs": [],
   "source": [
    "100*28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3633e4-7f7c-42c0-87bc-3974fb83da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "100*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1acc67-0b1f-430a-9fe5-a3bfa41575e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.linears[0].linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6f342-5dcf-43f5-9491-1a22dbd4b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.sqrt(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d92ae2-ce87-4ad1-b5a9-a4b643f63a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f1928-5c97-40ed-88cf-cc1dee77091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_f1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f693bf-036f-44ea-a845-c7c568a2eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17ad99-f3a0-4015-b5ec-192f5827e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dee3c7-d7f0-498c-a83b-216944d1f558",
   "metadata": {},
   "source": [
    "## Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd6dca-1789-4a03-a3b0-2b0c424a48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "\n",
    "nums = [2000,100,20]\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    weights = mlp.linears[i].linear.weight.reshape(-1,)\n",
    "    weights = weights[torch.argsort(weights)]\n",
    "    plt.plot(-weights[:nums[i]].detach().cpu().numpy(), marker=\"o\", markersize=3)\n",
    "    plt.plot(weights[-nums[i]:].detach().cpu().numpy()[::-1], marker=\"o\", markersize=3)\n",
    "    plt.xlabel(\"rank\", fontsize=15)\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"abs(weight)\", fontsize=15)\n",
    "        plt.legend([\"positive\", \"negative\"])\n",
    "    plt.title(\"Layer {}\".format(i+1))\n",
    "    \n",
    "    \n",
    "plt.savefig(\"../results/mnist/mnist_weights.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5Tv2km0K77I",
   "metadata": {
    "id": "h5Tv2km0K77I"
   },
   "source": [
    "# 𝔽𝕦𝕟𝕔𝕥𝕚𝕠𝕟 𝕔𝕒𝕝𝕝𝕤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca581f7-22aa-4923-a1b9-22c23033aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750b8b5-bf7e-47e2-b215-39b5f4d46df9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3ZIQeL3VK-BI",
    "outputId": "61172eba-eea9-4ed1-95f0-9d02836c332c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if configs.run_decentblocks:\n",
    "    decentblock_routine(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6vh627W0zPLo",
   "metadata": {
    "id": "6vh627W0zPLo"
   },
   "outputs": [],
   "source": [
    "if configs.run_decentnet:\n",
    "    decentnet_routine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pRGZLABtzQdE",
   "metadata": {
    "id": "pRGZLABtzQdE"
   },
   "outputs": [],
   "source": [
    "if configs.run_baseline:\n",
    "    baseline_routine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O-MGutwczQ7L",
   "metadata": {
    "id": "O-MGutwczQ7L"
   },
   "outputs": [],
   "source": [
    "if configs.run_visualisation:\n",
    "    visualisation_routine()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nPKap00_wfgL"
   ],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
