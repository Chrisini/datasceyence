{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d8ea6b-7e2f-4257-9875-25f651882f81",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ð”»ð•–ð•”ð•–ð•Ÿð•¥â„•ð•–ð•¥: ð••ð•šð•¤ð•–ð•Ÿð•¥ð•’ð•Ÿð•˜ð•ð•–ð•• ð•Ÿð•–ð•¥\n",
    "\n",
    "Goal: create a sparse and modular ConvNet\n",
    "\n",
    "\n",
    "Notes:\n",
    "* additionally needed: position, activated channels, connection between channels\n",
    "* within this layer, a whole filter can be deactivated\n",
    "* within a filter, single channels can be deactivated\n",
    "* within this layer, filters can be swapped\n",
    "     \n",
    "* pruning actually doesn\"t work: https://discuss.pytorch.org/t/pruning-doesnt-affect-speed-nor-memory-for-resnet-101/75814   \n",
    "* fine tune a pruned model: https://stackoverflow.com/questions/73103144/how-to-fine-tune-the-pruned-model-in-pytorch\n",
    "* an actual pruning mechanism: https://arxiv.org/pdf/2002.08258.pdf\n",
    "\n",
    "pip install:\n",
    "* pytorch_lightning\n",
    "\n",
    "\n",
    "warnings:\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
    "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73a26-f239-466f-901d-559d192f61de",
   "metadata": {},
   "source": [
    "![uml of code](examples/example_vis/uml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd77a1f-a306-47cc-9905-993793aee5be",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f2bf02-f53b-4688-bf18-5b8075397e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "from typing import Optional, List, Tuple, Union\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch._torch_docs import reproducibility_notes\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "import sys \n",
    "sys.path.insert(0, \"helper\")\n",
    "\n",
    "# Explainability of datasceyence\n",
    "from helper.visualisation.feature_map import *\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b397b450-c5c6-4da1-b630-7bf80e46891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "torch 2.0.0 == True\n",
      "tl 2.1.0 == True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(19)\n",
    "torch.cuda.manual_seed(19)\n",
    "random.seed(19)\n",
    "np.random.seed(19)\n",
    "\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "debug_model = False\n",
    "\n",
    "print('torch 2.0.0 ==', torch.__version__=='2.0.0')\n",
    "print('tl 2.1.0 ==', pl.__version__=='2.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419b0be-245d-49c0-88b4-d94167f7da90",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97670e82-0d27-4b7f-91df-874acf9974a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'n_classes': 10,\n",
    "    'out_dim' :  [1, 8, 16, 32], # [1, 8, 16, 32], #[1, 16, 24, 32]\n",
    "    'grid_size' : 18*18,\n",
    "    'criterion': torch.nn.CrossEntropyLoss(),# torch.nn.BCEWithLogitsLoss(),\n",
    "    'optimizer': \"sgd\", # sgd adamw\n",
    "    'base_lr': 0.001,\n",
    "    'min_lr' : 0.00001,\n",
    "    'momentum' : 0.9,\n",
    "    'lr_update' : 100,\n",
    "    'cc_weight': 10,\n",
    "    'cc_metric' : 'l2', # connection cost metric (for loss) - distance metric\n",
    "    'ci_metric' : 'l2', # channel importance metric (for pruning)\n",
    "    'cm_metric' : 'count', # crossing minimisation \n",
    "    'update_every_nth_epoch' : 4, # 10\n",
    "    'pretrain_epochs' : 20,\n",
    "    'prune_keep' : 0.95, # 0.97, # in each epoch\n",
    "    'prune_keep_total' : 0.5, # this number is not exact, depends on the prune_keep value\n",
    "}\n",
    "\n",
    "train_kwargs = {\n",
    "    'result_path': \"examples/example_results\", # \"example_results/lightning_logs\", # not in use??\n",
    "    'exp_name': \"tmp_testi_oct\", # must include oct or retina\n",
    "    'load_ckpt_file' : \"xversion_22/checkpoints/epoch=0-unpruned=10942-val_f1=0.06.ckpt\", # 'version_94/checkpoints/epoch=26-step=1080.ckpt', # change this for loading a file and using \"test\", if you want training, keep None\n",
    "    'epochs': 1, # including the pretrain epochs - no adding up\n",
    "    'img_size' : 28, #168, # keep mnist at original size, training didn't work when i increased the size ... # MNIST/MedMNIST 28 Ã— 28 Pixel\n",
    "    'batch_size': 128, # 128, # the higher the batch_size the faster the training - every iteration adds A LOT OF comp cost\n",
    "    'log_every_n_steps' : 4, # lightning default: 50 # needs to be bigger than the amount of steps in an epoch (based on trainset size and batchsize)\n",
    "    'device': \"cuda\",\n",
    "    'num_workers' : 18, # 18, # 18 for computer, 0 for laptop\n",
    "    'train_size' : (130 * 2), # total or percentage\n",
    "    'val_size' : (130 * 2), # total or percentage\n",
    "    'test_size' : 24, # total or percentage - 0 for all\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d5ff36-c015-4bd6-8b12-c4d0e3b64b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f898ba-2323-4628-a0e3-70ee44ce0b0d",
   "metadata": {},
   "source": [
    "# DecentNet trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd6da2-32d7-4727-af6d-cee380d01b5f",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d607d8fb-4ac1-4fad-848c-11d189a62637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Christina\\.medmnist\\octmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Christina\\.medmnist\\octmnist.npz\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(size=train_kwargs[\"img_size\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])   \n",
    "\n",
    "rgb_transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(size=train_kwargs[\"img_size\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])   \n",
    "    \n",
    "if 'oct' in train_kwargs['exp_name']:\n",
    "    from medmnist import OCTMNIST\n",
    "    dataset = OCTMNIST(split=\"train\", transform=transform, download=True)\n",
    "    testset = OCTMNIST(split=\"test\", transform=transform, download=True) \n",
    "    model_kwargs['n_classes'] = 4\n",
    "    \n",
    "    indices = np.arange(len(dataset))  \n",
    "    train_indices, val_indices = train_test_split(indices, train_size=train_kwargs[\"train_size\"], test_size=train_kwargs[\"val_size\"], stratify=dataset.labels)\n",
    "    \n",
    "elif 'retina' in train_kwargs['exp_name']:\n",
    "    from medmnist import RetinaMNIST\n",
    "    dataset = RetinaMNIST(split=\"train\", transform=rgb_transform, download=True)\n",
    "    testset = RetinaMNIST(split=\"test\", transform=rgb_transform, download=True) \n",
    "    model_kwargs['n_classes'] = 5\n",
    "    \n",
    "    indices = np.arange(len(dataset))  \n",
    "    train_indices, val_indices = train_test_split(indices, train_size=train_kwargs[\"train_size\"], test_size=train_kwargs[\"val_size\"], stratify=dataset.labels)\n",
    "    \n",
    "else:\n",
    "    dataset = torchvision.datasets.MNIST(root=\"examples/example_data\", train=True, transform=transform, download=True)\n",
    "    testset = torchvision.datasets.MNIST(root=\"examples/example_data\", train=False, transform=transform, download=True)\n",
    "    model_kwargs['n_classes'] = 10\n",
    "    \n",
    "    indices = np.arange(len(dataset))  \n",
    "    train_indices, val_indices = train_test_split(indices, train_size=train_kwargs[\"train_size\"], test_size=train_kwargs[\"val_size\"], stratify=dataset.targets)\n",
    "\n",
    "\n",
    "train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_subset, shuffle=True, batch_size=train_kwargs[\"batch_size\"], num_workers=train_kwargs[\"num_workers\"])\n",
    "\n",
    "val_subset = torch.utils.data.Subset(dataset, val_indices)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_subset, shuffle=False, batch_size=train_kwargs[\"batch_size\"], num_workers=train_kwargs[\"num_workers\"]) # , persistent_workers=True)\n",
    "\n",
    "# batch size has to be 1\n",
    "if train_kwargs[\"test_size\"] > 0:\n",
    "    testset = torch.utils.data.Subset(testset, range(train_kwargs[\"test_size\"]))\n",
    "xai_dataloader = torch.utils.data.DataLoader(testset, shuffle=False, batch_size=1, num_workers=train_kwargs[\"num_workers\"]) # , persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a63827d6-1e9f-4537-a86f-00515d8578cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a0bf5e-8191-4182-a682-d1c34495a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac8cc05-fbe0-4125-910d-55c171f68101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.tensor(dataset.labels).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd40000-1da6-4f92-b9e4-ace7a184a19a",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c8a8868-9d8f-47cf-a42b-5ab72f44b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class X:\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # an object with image representations and their positions\n",
    "    # amout of channels need to have same length as m and n lists\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, data, ms_x, ns_x):\n",
    "        self.data = data # list of tensors (image representations)\n",
    "        self.ms_x = ms_x # list of integers (m position of each image representation)\n",
    "        self.ns_x = ns_x # list of integers (n position of each image representation)\n",
    "                \n",
    "    def setter(self, data, ms_x, ns_x):\n",
    "        self.data = data\n",
    "        self.ms_x = ms_x\n",
    "        self.ns_x = ns_x\n",
    "        \n",
    "    def getter(self):\n",
    "        return self.data, self.m, self.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'X(data: ' + str(self.data.shape) +' at positions: ms_x= ' + ', '.join(str(m.item()) for m in self.ms_x) + ', ns_x= ' + ', '.join(str(n.item()) for n in self.ns_x) + ')'\n",
    "    __repr__ = __str__\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfa123-54e1-4053-94c3-52b45ec0859c",
   "metadata": {},
   "source": [
    "## DecentFilter\n",
    "* conv2d problem: https://stackoverflow.com/questions/61269421/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62bc1f3c-5f40-445c-b5f1-4ca6387eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentFilter(torch.nn.Module):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # convolution happens in here\n",
    "    # one filter has multiple channels (aka weights)\n",
    "    #\n",
    "    # =============================================================================\n",
    "    \n",
    "    def __init__(self, ms_in, ns_in, m_this, n_this,\n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 padding_mode=\"zeros\",\n",
    "                 dilation=3, \n",
    "                 # transposed=None, \n",
    "                 device=None, \n",
    "                 dtype=None):\n",
    "        \n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        \n",
    "        # padding\n",
    "        padding = padding if isinstance(padding, str) else _pair(padding)\n",
    "        valid_padding_strings = {'same', 'valid'}\n",
    "        if isinstance(padding, str):\n",
    "            if padding not in valid_padding_strings:\n",
    "                raise ValueError(\n",
    "                    \"Invalid padding string {!r}, should be one of {}\".format(\n",
    "                        padding, valid_padding_strings))\n",
    "            if padding == 'same' and any(s != 1 for s in stride):\n",
    "                raise ValueError(\"padding='same' is not supported for strided convolutions\")\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        \n",
    "         \n",
    "        # convolution\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding_mode = padding_mode\n",
    "        self.padding = padding\n",
    "        self.dilation = _pair(dilation)\n",
    "        #self.transposed = transposed\n",
    "        \n",
    "        # weights\n",
    "        assert len(ms_in) == len(ns_in), \"ms_in and ns_in are not of same length\"\n",
    "        self.n_weights = len(ms_in)\n",
    "        \n",
    "        # position, currently not trainable \n",
    "        # self.non_trainable_param = nn.Parameter(torch.Tensor([1.0]), requires_grad=False)\n",
    "        self.ms_in = nn.Parameter(torch.Tensor(ms_in), requires_grad=False) # ms_in # list\n",
    "        self.ns_in = nn.Parameter(torch.Tensor(ns_in), requires_grad=False) # ns_in # list\n",
    "        self.m_this = nn.Parameter(torch.Tensor([m_this]), requires_grad=False) # m_this # single integer\n",
    "        self.n_this = nn.Parameter(torch.Tensor([n_this]), requires_grad=False) # n_this # single integer\n",
    "        \n",
    "        # weight\n",
    "        # filters x channels x kernel x kernel\n",
    "        # self.weights = torch.autograd.Variable(torch.randn(1,n_weights,*self.kernel_size)).to(\"cuda\")\n",
    "        # self.weights = torch.nn.Parameter(torch.randn(1,n_weights,*self.kernel_size))\n",
    "        self.weights = torch.nn.Parameter(torch.empty((1, self.n_weights, *self.kernel_size), **factory_kwargs))\n",
    "        \n",
    "        #print(\"weight shape init\")\n",
    "        #print(self.weights.shape)\n",
    "            \n",
    "        # bias    \n",
    "        if False: \n",
    "            # bias:\n",
    "            # where should the bias be???\n",
    "            self.bias = Parameter(torch.empty(1, **factory_kwargs))\n",
    "        else:\n",
    "            #self.bias = False\n",
    "            # we only use bias via instance normalisation\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # reset weights and bias in filter\n",
    "        self.reset_parameters()\n",
    "            \n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*self.kernel_size)\n",
    "        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\n",
    "        torch.nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))        \n",
    "        \n",
    "    def forward(self, x:X) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # first, we have to remove channels in X\n",
    "        # this is because some channels in the filter are pruned (aka gone)\n",
    "        # then we can apply convolution\n",
    "        # parameters:\n",
    "        #    x = batch x channels x width x height\n",
    "        # returns:\n",
    "        #    x_data: batch x filters x width x height\n",
    "        # saves:\n",
    "        #    self.weights = 1 filter x channels x kernel x kernel\n",
    "        # =============================================================================\n",
    "        \n",
    "        \n",
    "        # POSITION MATCHER\n",
    "        # Find the indices (IDs) of channel pairs that exist in both the X and then filter\n",
    "        common_pairs = [[i_in, i_x] for i_in, (m_in, n_in) in enumerate(zip(self.ms_in, self.ns_in)) for i_x, (m_x, n_x) in enumerate(zip(x.ms_x, x.ns_x)) if (m_in==m_x and n_in==n_x)]\n",
    "        \n",
    "        if False:\n",
    "            print(common_pairs)\n",
    "            print(len(self.ms_in))\n",
    "            print(len(self.ns_in))\n",
    "            print(len(x.ms_x))\n",
    "            print(len(x.ns_x))\n",
    "\n",
    "            for pair in common_pairs:\n",
    "                print(f\"Common pair at indices {pair}: {self.ms_in[pair[0]], tmp_ms[pair[1]]}, {self.ns_in[pair[0]], tmp_ns[pair[1]]}\")\n",
    "        \n",
    "        common_pairs_a = np.array(common_pairs)\n",
    "        try:\n",
    "            f_ids = common_pairs_a[:,0]\n",
    "            x_ids = common_pairs_a[:,1]\n",
    "        except Exception as e:\n",
    "            print(\"error: no common pairs\")\n",
    "            print(\"pairs\", common_pairs_a)\n",
    "            print(\"pairs shape\", common_pairs_a.shape)\n",
    "            print(\"len ms in\", len(self.ms_in))\n",
    "            print(\"len ns in\", len(self.ns_in))\n",
    "            print(\"len ms x\", len(x.ms_x))\n",
    "            print(\"len ns x\", len(x.ns_x))\n",
    "            print(e)\n",
    "            \n",
    "            # in this case the whole filter should be removed\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        # filter data and weights based on common pairs of data and weights\n",
    "        tmp_d = x.data[:, x_ids, :, :]\n",
    "        tmp_w = self.weights[:, f_ids, :, :]\n",
    "        \n",
    "        # the final convolution\n",
    "        if self.padding_mode != 'zeros':\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(F.pad(tmp_d, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            tmp_w, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, groups=1)\n",
    "        else:\n",
    "            # this is written in c++\n",
    "            x_data = torch.nn.functional.conv2d(tmp_d, tmp_w, self.bias, self.stride, self.padding, self.dilation, groups=1)\n",
    "        \n",
    "        #print(\"tmp_w\", tmp_w.shape)\n",
    "        \n",
    "        # print(x_data.shape, \"- batch x filters x width x height\")\n",
    "        return x_data\n",
    "    \n",
    "    def setter(self, value, m_this, n_this):\n",
    "        # preliminary, not in use\n",
    "        self.weights = value # weights in this filter\n",
    "        self.m_this = m_this # single integer\n",
    "        self.n_this = n_this # single integer\n",
    "    \n",
    "    def getter(self):\n",
    "        # preliminary, not in use\n",
    "        return self.weights, self.m_this, self.n_this\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'DecentFilter(weights: ' + str(self.weights.shape) + ' at position: m_this=' + str(self.m_this) + ', n_this=' + str(self.n_this) + ')' + \\\n",
    "    '\\n with inputs: ms_in= ' + ', '.join(str(int(m.item())) for m in self.ms_in) + ', ns_in= ' + ', '.join(str(int(n.item())) for n in self.ns_in) + ')'\n",
    "    __repr__ = __str__\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffdab4-e0b0-4523-882f-dad3ebf06090",
   "metadata": {},
   "source": [
    "## DecentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fddb74c7-5940-424d-aab8-2c75efd914bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLayer(torch.nn.Module):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # we save filters of the layer in the self.filter_list\n",
    "    # each filter has a position (m_this, n_this)\n",
    "    # each filter has input positions (ms_in, ns_in)\n",
    "    #    - these vary between filters, as some are pruned\n",
    "    # at the moment we have to loop through the filter list\n",
    "    # convolution is applied to each filter separately which makes this very slow\n",
    "    #\n",
    "    # =============================================================================\n",
    "    __constants__ = ['stride', 'padding', 'dilation', # 'groups',\n",
    "                     'padding_mode', # 'n_channels', #  'output_padding', # 'n_filters',\n",
    "                     'kernel_size']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "                \n",
    "    def __init__(self, ms_in:list, ns_in:list, n_filters:int,\n",
    "                 kernel_size: _size_2_t,  \n",
    "                 stride: _size_2_t = 1,  \n",
    "                 padding: Union[str, _size_2_t] = 0,  \n",
    "                 dilation: _size_2_t = 1,\n",
    "                 model_kwargs=None,\n",
    "                 #prune_keep:float = 0.9,\n",
    "                 #prune_keep_total:float = 0.5,\n",
    "                 #transposed: bool = False, \n",
    "                 #grid_size:int=81,\n",
    "                 #ci_metric=\"l2\",\n",
    "                 #output_padding: Tuple[int, ...] = _pair(0),\n",
    "                 #groups: int = 1,\n",
    "                 bias: bool = True,  # not in use\n",
    "                 padding_mode: str = \"zeros\",  # not in use\n",
    "                 device=None,  # not in use\n",
    "                 dtype=None) -> None:\n",
    "        # =============================================================================\n",
    "        # initialisation\n",
    "        # parameters:\n",
    "        #    a lot.\n",
    "        # =============================================================================\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # prune numbers\n",
    "        self.prune_keep = model_kwargs[\"prune_keep\"] # in each update [0.0:1.0]\n",
    "        self.prune_keep_total = model_kwargs[\"prune_keep_total\"] # total [0.0:1.0]\n",
    "        \n",
    "        # importance metric for pruning\n",
    "        self.ci_metric = model_kwargs[\"ci_metric\"]\n",
    "        # distance metric for loss\n",
    "        self.cc_metric = model_kwargs[\"cc_metric\"]\n",
    "        \n",
    "        # from prev layer\n",
    "        self.ms_in = ms_in\n",
    "        self.ns_in = ns_in\n",
    "        \n",
    "        self.original_size = len(self.ms_in) * n_filters\n",
    "        \n",
    "        \n",
    "        self.grid_size = model_kwargs[\"grid_size\"]\n",
    "        self.grid_sqrt = math.sqrt(self.grid_size)\n",
    "        assert self.grid_sqrt == int(self.grid_sqrt), f\"square root ({self.grid_sqrt}) from grid size {self.grid_size} not possible; possible exampes: 81 (9*9), 144 (12*12)\"\n",
    "        self.grid_sqrt = int(self.grid_sqrt)\n",
    "        \n",
    "        # use techniques from coo matrix\n",
    "        self.geometry_array = np.full(self.grid_size, np.nan)\n",
    "        # plus 1 here cause of to_sparse array\n",
    "        self.geometry_array[0:n_filters] = range(1,n_filters+1)\n",
    "        np.random.shuffle(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.reshape((self.grid_sqrt,self.grid_sqrt), order='C')\n",
    "        self.geometry_array = torch.tensor(self.geometry_array)\n",
    "        self.geometry_array = self.geometry_array.to_sparse(sparse_dim=2).to(\"cuda\")\n",
    "\n",
    "        #print(self.geometry_array)\n",
    "        #print(self.geometry_array.values())\n",
    "\n",
    "        self.filter_list = torch.nn.ModuleList([])\n",
    "        for i_filter in range(n_filters):\n",
    "            # minus 1 here cause of to_sparse array\n",
    "            index = (self.geometry_array.values()-1 == i_filter).nonzero(as_tuple=True)[0]\n",
    "            m_this = self.geometry_array.indices()[0][index]\n",
    "            n_this = self.geometry_array.indices()[1][index]\n",
    "            f = DecentFilter(ms_in, ns_in, m_this, n_this, \n",
    "                             kernel_size=kernel_size, \n",
    "                             stride=stride, padding=padding, dilation=dilation)\n",
    "            self.filter_list.append(f)\n",
    "            # self.register_parameter(f\"filter {i_filter}\", f.weights)\n",
    "            \n",
    "            #torch.nn.Parameter(torch.empty((1, n_channels, *kernel_size), **factory_kwargs))\n",
    "    \n",
    "    def run_layer_connection_cost(self) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # compute connection cost for this layer - based on distance\n",
    "        # returns:\n",
    "        #    connection cost for the loss function\n",
    "        # notes:\n",
    "        #    currently using l2 norm, doesn't work that well\n",
    "        # sources:\n",
    "        #    adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        #    https://stackoverflow.com/questions/74086766/how-to-find-total-cost-of-each-path-in-graph-using-dictionary-in-python\n",
    "        # nonsense?\n",
    "        #    i don't even know what the following comments are about ... \n",
    "        #    based on previous layer (cause I only have input ms_in, n_in information)\n",
    "        #    mean( sum( of connection cost between this filter and all incoming filters\n",
    "        #    need it for loss - aka all layers, all filters together\n",
    "        #    need it for swapping - this layer, all filters\n",
    "        #    only the active ones (we need to use the indices for that)\n",
    "        #    for swapping i need ??\n",
    "        # =============================================================================\n",
    "         \n",
    "        from scipy.spatial.distance import cdist\n",
    "        \n",
    "        # connection cost list\n",
    "        cc = []\n",
    "        \n",
    "        \n",
    "        for f in self.filter_list:\n",
    "            # for each filter we use the current position and all incoming positions\n",
    "\n",
    "            #mn = torch.cat([torch.tensor(f.m_this), torch.tensor(f.n_this)])\n",
    "            #print(mn.shape)\n",
    "            #msns = torch.cat([torch.tensor(f.ms_in), torch.tensor(f.ns_in)]) # .transpose(1,0)\n",
    "            #print(msns.shape)\n",
    "            #cc.append(torch.cdist(mn.unsqueeze(dim=0), msns.transpose(1,0), 'euclidean') / 8) # number comes from 9*9 = 81 [0-8]\n",
    "\n",
    "            mn = torch.cat([f.m_this.unsqueeze(0), f.n_this.unsqueeze(0)]).transpose(1,0)\n",
    "            msns = torch.cat([f.ms_in.unsqueeze(0), f.ns_in.unsqueeze(0)]).transpose(1,0)\n",
    "            #print(mn)\n",
    "            #print(msns)\n",
    "\n",
    "            # mean ( l2 norm as distance metric / normalisation term for l2 norm)\n",
    "            # mean of distances\n",
    "            # normalise with max=grid square root, min=0\n",
    "            # mean from all non-nan values\n",
    "            # \n",
    "            \n",
    "            if self.cc_metric == 'l1':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'cityblock') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'l2':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'euclidean') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'l2_torch':\n",
    "                cc.append(torch.nanmean( torch.cdist( a=mn.float(), b=msns.float(), p=2) /self.grid_sqrt ))\n",
    "            elif self.cc_metric == 'linf':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'chebyshev') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'cos':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'cosine') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'jac':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'jaccard') /self.grid_sqrt ) ))\n",
    "            elif self.cc_metric == 'cor':\n",
    "                cc.append(torch.nanmean( torch.tensor( scipy.spatial.distance.cdist(mn.detach().cpu().numpy(), msns.detach().cpu().numpy(), 'correlation') /self.grid_sqrt ) ))\n",
    "                \n",
    "\n",
    "        # mean connection cost of a layer\n",
    "        # mean from all non-nan values\n",
    "        return torch.nanmean(torch.tensor(cc))\n",
    "    \n",
    "    def run_channel_importance(self, i_f:int) -> list:\n",
    "        # =============================================================================\n",
    "        # compute channel importance metric for pruning\n",
    "        # calculate the norm of each weight in filter with id i_f\n",
    "        # we need to call this in a loop to go through each filter\n",
    "        # returns:\n",
    "        #     ci: channel importance list of a filter\n",
    "        # notes:\n",
    "        #     based on l2 norm = magnitude = euclidean distance\n",
    "        # nonsense?\n",
    "        #    maybe the kernel trigger todo\n",
    "        #    print(self.filter_list[i_f].weights.shape)\n",
    "        #    print(self.filter_list[i_f].weights[:,i_w].shape)\n",
    "        # =============================================================================\n",
    "        \n",
    "        ci = []\n",
    "        \n",
    "        for i_w in range(self.filter_list[i_f].weights.shape[1]):\n",
    "            # importance of a kernel in a layer\n",
    "            \n",
    "            \n",
    "                \n",
    "            \n",
    "            if self.ci_metric == 'l1':\n",
    "                # weight dependent - filter norm\n",
    "                pass\n",
    "                print(\"nooooooooooooooooo\")\n",
    "                # ci.append(self.filter_list[i_f].weights[:,i_w].norm(2).detach().cpu().numpy())\n",
    "                \n",
    "            elif self.ci_metric == 'l2':\n",
    "                # weight dependent - filter norm\n",
    "                ci.append(self.filter_list[i_f].weights[:,i_w].norm(2).detach().cpu().numpy()) # .detach().cpu().numpy()\n",
    "                pass\n",
    "            \n",
    "            elif self.ci_metric == '':\n",
    "                # weight dependent - filter correlation\n",
    "                pass\n",
    "            \n",
    "            elif self.ci_metric == '':\n",
    "                # activation-based\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                # mutual information\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                # Hessian matrix / Taylor\n",
    "                pass\n",
    "                \n",
    "            elif self.ci_metric == '':\n",
    "                \n",
    "                pass\n",
    "                \n",
    "                \n",
    "            elif self.ci_metric == 'random':\n",
    "                ci.append( np.array(random.random()) )\n",
    "\n",
    "        \n",
    "        return ci \n",
    "    \n",
    "    def run_swap_filter(self):\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # we swap filters within the layer\n",
    "        # based on connection cost\n",
    "        # filter can move a maximum of two positions per swap\n",
    "        # change positions\n",
    "        # change\n",
    "        # =============================================================================\n",
    "        print(\"swap here\")\n",
    "        self.m_this = self.m_this # single integer\n",
    "        self.n_this = self.n_this # single integer\n",
    "        pass\n",
    "    \n",
    "    def run_grow_filter(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # introduce new filters in a layer\n",
    "        # based on \n",
    "        # algorithmic growth process \n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def run_grow_channel(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # introduce new channel in a layer\n",
    "        # based on connection cost??\n",
    "        # algorithmic growth process \n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def run_prune_filter(self) -> None:\n",
    "        # =============================================================================\n",
    "        # not working yet\n",
    "        # delete filter in a layer\n",
    "        # =============================================================================\n",
    "        pass\n",
    "    \n",
    "    def run_prune_channel(self, i_f:int, keep_ids:list) -> None:\n",
    "        # =============================================================================\n",
    "        # delete channels in a filter based on keep_ids\n",
    "        # based on importance score\n",
    "        # only keep \"the best\" weights\n",
    "        # pruning based on a metric\n",
    "        # nonsense?\n",
    "        #    delete layer with id\n",
    "        #    delete channels in each layer with id\n",
    "        #    channel deactivation\n",
    "        #    require_grad = False/True for each channel\n",
    "        #    deactivate_ids = [1, 2, 6]\n",
    "        #    self.active[deactivate_ids] = False\n",
    "        #    print(\"weight\")\n",
    "        #    print(self.weight.shape)\n",
    "        #    print(self.weight[:,self.active,:,:].shape)\n",
    "        #    this is totally wrong - iterative will break after first iteration\n",
    "        #    print()\n",
    "        #    Good to hear itâ€™s working, although I would think youâ€™ll get an error at some point in your code, as the cuda() call creates a non-leaf tensor.\n",
    "        #    self.weight = torch.nn.Parameter(  self.weight[:,self.active,:,:] ) # .detach().cpu().numpy()\n",
    "        #    self.weight = self.weight.cuda()\n",
    "        #    print(self.weight.shape)\n",
    "        #    print(self.active)\n",
    "        #    print(\"prune here\")\n",
    "        #    for f in self.filter_list:\n",
    "        #        f.update()\n",
    "        # =============================================================================\n",
    "        \n",
    "        if False:\n",
    "            for i in keep_ids:\n",
    "                print(i)\n",
    "                print(self.filter_list[i_f].ms_in[i])\n",
    "                print(torch.nn.Parameter(self.filter_list[i_f].ms_in[keep_ids]) )\n",
    "        \n",
    "        if random.randint(1, 100) == 5:\n",
    "            print()\n",
    "            print(\"info at random intervals\")\n",
    "            print(keep_ids)\n",
    "            print(self.filter_list[i_f].weights[:, keep_ids, :, :].shape)\n",
    "            print(self.filter_list[i_f].weights.shape)        \n",
    "        \n",
    "        # todo: check, this may create more parameters ...\n",
    "        \n",
    "        # prune weights, ms and ns based on the 'keep ids'\n",
    "        self.filter_list[i_f].weights = torch.nn.Parameter(self.filter_list[i_f].weights[:, keep_ids, :, :])\n",
    "        self.filter_list[i_f].ms_in = torch.nn.Parameter(self.filter_list[i_f].ms_in[keep_ids], requires_grad=False) # this becomes a grad here, hence turn off again with False\n",
    "        #[self.filter_list[i_f].ms_in[i] for i in keep_ids] # self.ms_in[remove_ids]\n",
    "        self.filter_list[i_f].ns_in = torch.nn.Parameter(self.filter_list[i_f].ns_in[keep_ids], requires_grad=False)\n",
    "        # [self.filter_list[i_f].ns_in[i] for i in keep_ids] # self.ns_in[remove_ids]\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x: X) -> Tensor:\n",
    "        # =============================================================================\n",
    "        # calculate representation x for each filter in this layer\n",
    "        # =============================================================================\n",
    "        \n",
    "        output_list = []\n",
    "        m_list = []\n",
    "        n_list = []\n",
    "        for f in self.filter_list:\n",
    "            # output = filter(input)\n",
    "            out = f(x)\n",
    "            # if filter has no channels left\n",
    "            if out is not None:\n",
    "                output_list.append(out)\n",
    "                m_list.append(f.m_this)\n",
    "                n_list.append(f.n_this)\n",
    "        x.ms_x = m_list\n",
    "        x.ns_x = n_list\n",
    "        x.data = torch.cat(output_list, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def get_filter_positions(self):\n",
    "        # =============================================================================\n",
    "        # in use for next layer input (initialisation of the model)\n",
    "        # =============================================================================\n",
    "        \n",
    "        ms_this = []\n",
    "        ns_this = []\n",
    "        for f in self.filter_list:\n",
    "            ms_this.append(f.m_this)\n",
    "            ns_this.append(f.n_this)\n",
    "        \n",
    "        return ms_this, ns_this\n",
    "    \n",
    "    def update(self):\n",
    "        # =============================================================================\n",
    "        # currently: calculate importance metric for the prune_channel method\n",
    "        # remove channels based on self.prune_keep\n",
    "        # =============================================================================\n",
    "        \n",
    "        all_ci = []\n",
    "        all_len = 0\n",
    "        for i_f in range(len(self.filter_list)):\n",
    "            all_len += len(self.filter_list[i_f].ms_in)\n",
    "            # list of lists\n",
    "            all_ci.append(self.run_channel_importance(i_f))\n",
    "            #tmp_ids = sorted(range(len(all_ci)), key=lambda sub: all_ci[sub])\n",
    "          \n",
    "        #print(all_len) # this is the size of the previous pruning\n",
    "        #print(self.original_size)\n",
    "        #print(self.prune_keep_total)\n",
    "        #print(int(self.original_size * self.prune_keep_total))\n",
    "        \n",
    "        #self.log(f'{self.original_size}_active_channels', all_len, on_step=True, on_epoch=True)\n",
    "        \n",
    "        if all_len < int(self.original_size * self.prune_keep_total):\n",
    "            # if n percent have been pruned, stop this layer\n",
    "            print(\"pruning done for this layer\")\n",
    "        else:\n",
    "            # pruning\n",
    "            n = int(all_len*self.prune_keep)\n",
    "            all_ci_flatten = [item for row in all_ci for item in row] # don't have equal lengths, so no numpy possible\n",
    "            index = sorted(range(all_len), key=lambda sub: all_ci_flatten[sub])[-n] # error, out of range\n",
    "            threshold_value = all_ci_flatten[index]\n",
    "\n",
    "            for i_f in range(len(self.filter_list)):\n",
    "\n",
    "                # channel importance list for this filter\n",
    "                ci = all_ci[i_f] # self.run_channel_importance(i_f)\n",
    "\n",
    "                #print(ci)\n",
    "                #print(threshold_value)\n",
    "                # torch.where()\n",
    "                            \n",
    "                indices = np.where(ci >= threshold_value)[0] # just need the x axis\n",
    "\n",
    "                # indices should be list/np/detached\n",
    "                self.prune_channel(i_f, indices)\n",
    "                \n",
    "                #print(\"prune done\")\n",
    "                # ci = ci[indices] # probably not useful\n",
    "        \n",
    "            \n",
    "            # print(\"channel importance ci\", ci)\n",
    "            # keep_ids = random.sample(range(0, 8), 5)\n",
    "            #keep_ids = sorted(range(len(ci)), key=lambda sub: ci[sub])[amout_remove:]\n",
    "            #print(keep_ids)\n",
    "\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "109208ab-bed3-4f9b-aa56-eb61aebc3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(5):\n",
    "    a.append(np.array(random.random()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af56675d-9098-4244-8170-519cc3d4076a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.67712583),\n",
       " array(0.78491136),\n",
       " array(0.52046616),\n",
       " array(0.5114917),\n",
       " array(0.39353466)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0510096-312f-4ae3-8192-8165b649199c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c186cf1-c7db-45da-b779-f7ab88f3896f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## DecentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd6a2811-6313-41cc-82a0-78cdb812c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentNet(nn.Module):\n",
    "    def __init__(self, model_kwargs, log_dir=\"\") -> None:\n",
    "        super(DecentNet, self).__init__()\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        out_dim = model_kwargs[\"out_dim\"]\n",
    "        out_dim.append(self.n_classes) # out_dim = [1, 32, 48, 64, 10]     \n",
    "        \n",
    "        grid_size = model_kwargs[\"grid_size\"]\n",
    "        assert not any(i > grid_size for i in out_dim), f\"filters need to be less than {grid_size}\"\n",
    "        self.grid_sqrt = int(math.sqrt(grid_size))\n",
    "        \n",
    "        self.ci_metric = model_kwargs[\"ci_metric\"]\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        # backbone\n",
    "        \n",
    "        # initialise input positions of first layer\n",
    "        ms_in_1 = [torch.tensor(0)]\n",
    "        ns_in_1 = [torch.tensor(0)]\n",
    "        assert out_dim[0] == len(ms_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ms_in_1={len(ms_in_1)})\"\n",
    "        assert out_dim[0] == len(ns_in_1), f\"x data (out_dim[0]={out_dim[0]}) needs to match (ns_in_1={len(ns_in_1)})\"\n",
    "        self.decent1 = DecentLayer(ms_in=ms_in_1, ns_in=ns_in_1, n_filters=out_dim[1], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        # get position of previous layer as input for this layer\n",
    "        ms_in_2,ns_in_2 = self.decent1.get_filter_positions()\n",
    "        assert out_dim[1] == len(ms_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ms_in_2={len(ms_in_2)})\"\n",
    "        assert out_dim[1] == len(ns_in_2), f\"x data (out_dim[1]={out_dim[1]}) needs to match (ns_in_2={len(ns_in_2)})\"\n",
    "        self.decent2 = DecentLayer(ms_in=ms_in_2, ns_in=ns_in_2, n_filters=out_dim[2], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        ms_in_3,ns_in_3 = self.decent2.get_filter_positions()\n",
    "        assert out_dim[2] == len(ms_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ms_in_3={len(ms_in_3)})\"\n",
    "        assert out_dim[2] == len(ns_in_3), f\"x data (out_dim[2]={out_dim[2]}) needs to match (ns_in_3={len(ns_in_3)})\"\n",
    "        self.decent3 = DecentLayer(ms_in=ms_in_3, ns_in=ns_in_3, n_filters=out_dim[3], kernel_size=3, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        ms_in_1x1,ns_in_1x1 = self.decent3.get_filter_positions()\n",
    "        assert out_dim[3] == len(ms_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ms_in_1x1={len(ms_in_1x1)})\"\n",
    "        assert out_dim[3] == len(ns_in_1x1), f\"x data (out_dim[3]={out_dim[3]}) needs to match (ns_in_1x1={len(ns_in_1x1)})\"\n",
    "        self.decent1x1 = DecentLayer(ms_in=ms_in_1x1, ns_in=ns_in_1x1, n_filters=out_dim[-1], kernel_size=1, stride=1, padding=0, dilation=1, model_kwargs=model_kwargs)\n",
    "        \n",
    "        #self.tmp = torchvision.models.squeezenet1_0(torchvision.models.SqueezeNet1_0_Weights.IMAGENET1K_V1)\n",
    "        #self.tmp.classifier[1] = torch.nn.Conv2d(512, 10, kernel_size=(3,3))\n",
    "        \n",
    "        # head\n",
    "        self.fc = torch.nn.Linear(out_dim[-1], out_dim[-1])\n",
    "    \n",
    "        # activation\n",
    "        self.mish1 = torch.nn.Mish()\n",
    "        self.mish2 = torch.nn.Mish()\n",
    "        self.mish3 = torch.nn.Mish()\n",
    "        self.mish1x1 = torch.nn.Mish()\n",
    "        \n",
    "        # bias\n",
    "        self.bias1 = torch.nn.InstanceNorm2d(out_dim[1])\n",
    "        self.bias2 = torch.nn.InstanceNorm2d(out_dim[2])\n",
    "        self.bias3 = torch.nn.InstanceNorm2d(out_dim[3])\n",
    "        self.bias1x1 = torch.nn.InstanceNorm2d(out_dim[-1])\n",
    "        \n",
    "        # activation\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # init connection cost\n",
    "        self.cc = []\n",
    "        self.update_connection_cost()\n",
    "        \n",
    "        # get a position in filter list\n",
    "        self.m_plot = self.decent2.filter_list[0].m_this.detach().cpu().numpy()\n",
    "        self.n_plot = self.decent2.filter_list[0].n_this.detach().cpu().numpy()  \n",
    "        # self.plot_layer_of_1_channel(current_epoch=0) - not working here, dir not created yet\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        \n",
    "        #print(x)\n",
    "        \n",
    "        # hook on the data (for gradcam or something similar)\n",
    "        # https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
    "        if mode == 'explain':\n",
    "            output = x.data.register_hook(self.activations_hook)\n",
    "            #'cannot register a hook on a tensor that doesn't require gradient'\n",
    "        \n",
    "        \n",
    "        # global max pooling for MIL\n",
    "        x.data = F.max_pool2d(x.data, kernel_size=x.data.size()[2:])\n",
    "        \n",
    "        x.data = x.data.reshape(x.data.size(0), -1)\n",
    "        x.data = self.fc(x.data) \n",
    "        \n",
    "        # x.data = self.sigmoid(x.data)\n",
    "        \n",
    "        # x.data = self.tmp(x.data)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    \n",
    "    def activations_hook(self, grad):\n",
    "        # hook for the gradients of the activations\n",
    "        self.gradients = grad\n",
    "    def get_activations_gradient(self):\n",
    "        # method for the gradient extraction\n",
    "        return self.gradients\n",
    "    def get_activations(self, x):\n",
    "        # method for the activation exctraction\n",
    "        \n",
    "        #print('0', x)\n",
    "        \n",
    "        x = self.decent1(x)\n",
    "        x.data = self.mish1(x.data)\n",
    "        x.data = self.bias1(x.data)\n",
    "        #print('1', x)\n",
    "\n",
    "        x = self.decent2(x)\n",
    "        x.data = self.mish2(x.data)\n",
    "        x.data = self.bias2(x.data)\n",
    "        #print('2', x)\n",
    "        \n",
    "        x = self.decent3(x)\n",
    "        x.data = self.mish3(x.data)\n",
    "        x.data = self.bias3(x.data)\n",
    "        #print('3', x)\n",
    "        \n",
    "        x = self.decent1x1(x)\n",
    "        x.data = self.mish1x1(x.data)\n",
    "        x.data = self.bias1x1(x.data)\n",
    "        #print('1x1', x)\n",
    "        \n",
    "        return x.data\n",
    "    \n",
    "    def plot_layer_of_1_channel(self, current_epoch=0):\n",
    "        \n",
    "        # get each filter position that has a channel that matches\n",
    "        ms = []; ns = []\n",
    "        \n",
    "        #print(self.decent2.filter_list)\n",
    "        #print(\"**********************\")\n",
    "        #print(self.decent3.filter_list)\n",
    "\n",
    "        \n",
    "        # go through all filters in this layer\n",
    "        for f in self.decent3.filter_list:\n",
    "            \n",
    "            # if filter position in prev layer matches any channel in this layer\n",
    "            if any(pair == (self.m_plot, self.n_plot) for pair in zip(f.ms_in.detach().cpu().numpy(), f.ns_in.detach().cpu().numpy())):\n",
    "                \n",
    "                #print('match', f.m_this, f.n_this)\n",
    "                \n",
    "                # save position of each filter in this layer\n",
    "                ms.append(f.m_this.detach().cpu().numpy())\n",
    "                ns.append(f.n_this.detach().cpu().numpy())\n",
    "              \n",
    "            if False:\n",
    "                    print(\"nooooooooooooooo\")\n",
    "                    print(f.ms_in)\n",
    "                    print(self.m_plot)\n",
    "                    print(f.ns_in)\n",
    "                    print(self.n_plot)\n",
    "\n",
    "                    print((self.m_plot, self.n_plot))\n",
    "                \n",
    "        # visualising the previous and current layer neurons\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=100000, color='blue', alpha=0.1) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=50000, color='blue',alpha=0.2) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=25000, color='blue',alpha=0.3) # previous layer\n",
    "        ax.scatter(self.m_plot, self.n_plot, s=500, color='blue') # previous layer\n",
    "        ax.scatter(ms, ns, color='red') # next layer\n",
    "        plt.xlim(0, self.grid_sqrt) # m coordinate of grid_size field\n",
    "        plt.ylim(0, self.grid_sqrt) # n coordinate of grid_size field\n",
    "        ax.grid() # enable grid line\n",
    "        fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_plot[0])}_n{int(self.n_plot[0])}_{str(current_epoch)}.png\"))\n",
    "    \n",
    "    def update_connection_cost(self):\n",
    "        self.cc = []\n",
    "        # self.cc.append(self.decent1.run_layer_connection_cost()) # maybe not even needed ...\n",
    "        self.cc.append(self.decent2.run_layer_connection_cost())\n",
    "        self.cc.append(self.decent3.run_layer_connection_cost())\n",
    "        self.cc.append(self.decent1x1.run_layer_connection_cost())\n",
    "        self.cc = torch.mean(torch.tensor(self.cc))\n",
    "\n",
    "    def update(self, current_epoch):\n",
    "        # =============================================================================\n",
    "        # update_every_nth_epoch\n",
    "        # adapted from BIMT: https://github.com/KindXiaoming/BIMT/blob/main/mnist_3.5.ipynb\n",
    "        # =============================================================================\n",
    "        \n",
    "        # update decent layers\n",
    "        \n",
    "        #self.decent1.update()\n",
    "        self.decent2.update()\n",
    "        self.decent3.update()\n",
    "        #self.decent1x1.update()\n",
    "        \n",
    "        # visualisation\n",
    "        self.plot_layer_of_1_channel(current_epoch)\n",
    "    \n",
    "        # connection cost has to be calculated after pruning\n",
    "        # self.cc which is updated is used for loss function\n",
    "        self.update_connection_cost()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238bf19-b053-46ce-b0b4-228ead91f827",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b6ec5f3-4f08-421c-9137-53ab5908d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.model_checkpoint import *\n",
    "\n",
    "class DecentModelCheckpoint(ModelCheckpoint):\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        # =============================================================================\n",
    "        # costum model checkpoint\n",
    "        # Save a checkpoint at the end of the training epoch.\n",
    "        # parameters:\n",
    "        #    trainer\n",
    "        #    module\n",
    "        # saves:\n",
    "        #    the checkpoint model\n",
    "        # sources:\n",
    "        #    https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/callbacks/model_checkpoint.py\n",
    "        # =============================================================================\n",
    "        \n",
    "        if (\n",
    "            not self._should_skip_saving_checkpoint(trainer) \n",
    "            and self._should_save_on_train_epoch_end(trainer)\n",
    "        ):\n",
    "            monitor_candidates = self._monitor_candidates(trainer)\n",
    "            monitor_candidates[\"epoch\"] = monitor_candidates[\"epoch\"]\n",
    "            print(\"DECENT NOTE: callback on_train_epoch_end\", monitor_candidates[\"epoch\"])\n",
    "            if monitor_candidates[\"epoch\"] > 0:\n",
    "                if monitor_candidates[\"unpruned_state\"] != -1:\n",
    "                    print(\"DECENT NOTE: save model\", monitor_candidates[\"epoch\"])\n",
    "                    if self._every_n_epochs >= 1 and ((trainer.current_epoch + 1) % self._every_n_epochs) == 0:\n",
    "                        self._save_topk_checkpoint(trainer, monitor_candidates)\n",
    "                    self._save_last_checkpoint(trainer, monitor_candidates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bdf0-147c-4bfe-83c0-4a330c7f9bfc",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90dfcd-c00f-4f9d-8c24-bbac8c6af17b",
   "metadata": {},
   "source": [
    "## DecentLightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92509f06-c9fb-4084-843e-b80b3552c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecentLightning(pl.LightningModule):\n",
    "    # =============================================================================\n",
    "    #\n",
    "    # Lightning Module consists of functions that define the training routine\n",
    "    # train, val, test: before epoch, step, after epoch, ...\n",
    "    # https://github.com/Lightning-AI/pytorch-lightning/blob/master/src/lightning/pytorch/core/module.py\n",
    "    # order for the instance methods:\n",
    "    # https://pytorch-lightning.readthedocs.io/en/1.7.2/common/lightning_module.html#hooks\n",
    "    # \n",
    "    # =============================================================================\n",
    "\n",
    "    def __init__(self, kwargs, log_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # print(\"the kwargs: \", kwargs)\n",
    "        \n",
    "        # keep kwargs for saving hyperparameters\n",
    "        model_kwargs = kwargs['model_kwargs']\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "    \n",
    "        # n_classes=self.n_classes, grid_size=self.grid_size, out_dim=self.out_dim, prune_keep=self.prune_keep, prune_keep_total=self.prune_keep_total, cc_metric=self.cc_metric\n",
    "        self.model = DecentNet(model_kwargs=model_kwargs, log_dir=log_dir).to(\"cuda\")\n",
    "        \n",
    "        self.n_classes = model_kwargs[\"n_classes\"]\n",
    "        self.cc_weight = model_kwargs[\"cc_weight\"]\n",
    "        self.criterion = model_kwargs[\"criterion\"]\n",
    "        self.optimizer = model_kwargs[\"optimizer\"]\n",
    "        self.base_lr = model_kwargs[\"base_lr\"]\n",
    "        self.min_lr = model_kwargs[\"min_lr\"]\n",
    "        self.lr_update = model_kwargs[\"lr_update\"]\n",
    "        self.momentum = model_kwargs[\"momentum\"]\n",
    "        self.update_every_nth_epoch = model_kwargs[\"update_every_nth_epoch\"]\n",
    "        self.pretrain_epochs = model_kwargs[\"pretrain_epochs\"]\n",
    "        \n",
    "        # needed for hparams.yaml file\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if False:\n",
    "            self.metric = { \"train_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"train_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"val_acc\" : torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes),\n",
    "                     \"val_f1\" : torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "                   }\n",
    "        else:\n",
    "            self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.train_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "            self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=self.n_classes)\n",
    "            self.val_prec =  torchmetrics.Precision(task=\"multiclass\", average='macro', num_classes=self.n_classes)\n",
    "\n",
    "            \n",
    "    def forward(self, x, mode=\"grad\"):\n",
    "        # =============================================================================\n",
    "        # we make it possible to use model_output = self(image)\n",
    "        # =============================================================================\n",
    "        return self.model(x, mode)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def load_from_checkpoint(cls, checkpoint_path, **kwargs):\n",
    "        # todo, need to overwrite this somehow\n",
    "    \n",
    "        #Always use self for the first argument to instance methods.\n",
    "        #Always use cls for the first argument to class methods.\n",
    "    \n",
    "        loaded = cls._load_from_checkpoint(\n",
    "            cls,  # type: ignore[arg-type]\n",
    "            checkpoint_path,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return cast(Self, loaded)\n",
    "    \n",
    "    # todo, need to overwrite this somehow\n",
    "    @classmethod\n",
    "    def _load_from_checkpoint(\n",
    "        cls,\n",
    "        checkpoint_path,\n",
    "        **kwargs):\n",
    "        \n",
    "        map_location = None\n",
    "        with pl_legacy_patch():\n",
    "            checkpoint = pl_load(checkpoint_path, map_location=map_location)\n",
    "\n",
    "        # convert legacy checkpoints to the new format\n",
    "        checkpoint = _pl_migrate_checkpoint(\n",
    "            checkpoint, checkpoint_path=(checkpoint_path if isinstance(checkpoint_path, (str, Path)) else None)\n",
    "        )\n",
    "\n",
    "        if hparams_file is not None:\n",
    "            extension = str(hparams_file).split(\".\")[-1]\n",
    "            if extension.lower() == \"csv\":\n",
    "                hparams = load_hparams_from_tags_csv(hparams_file)\n",
    "            elif extension.lower() in (\"yml\", \"yaml\"):\n",
    "                hparams = load_hparams_from_yaml(hparams_file)\n",
    "            else:\n",
    "                raise ValueError(\".csv, .yml or .yaml is required for `hparams_file`\")\n",
    "\n",
    "            # overwrite hparams by the given file\n",
    "            checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY] = hparams\n",
    "\n",
    "        # TODO: make this a migration:\n",
    "        # for past checkpoint need to add the new key\n",
    "        checkpoint.setdefault(cls.CHECKPOINT_HYPER_PARAMS_KEY, {})\n",
    "        # override the hparams with values that were passed in\n",
    "        checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)\n",
    "\n",
    "        if issubclass(cls, pl.LightningDataModule):\n",
    "            return _load_state(cls, checkpoint, **kwargs)\n",
    "        if issubclass(cls, pl.LightningModule):\n",
    "            model = _load_state(cls, checkpoint, strict=strict, **kwargs)\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "            if not state_dict:\n",
    "                rank_zero_warn(f\"The state dict in {checkpoint_path!r} contains no parameters.\")\n",
    "                return model\n",
    "\n",
    "            device = next((t for t in state_dict.values() if isinstance(t, torch.Tensor)), torch.tensor(0)).device\n",
    "            assert isinstance(model, pl.LightningModule)\n",
    "            return model.to(device)\n",
    "\n",
    "        raise NotImplementedError(f\"Unsupported {cls}\")\n",
    "    \"\"\"\n",
    "           \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # =============================================================================\n",
    "        # returns:\n",
    "        #    optimiser and lr scheduler\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: configure_optimizers\")\n",
    "        \n",
    "        if self.optimizer == \"adamw\":\n",
    "            optimiser = optim.AdamW(self.parameters(), lr=self.base_lr)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimiser, milestones=[50,100], gamma=0.1)\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        else:\n",
    "            optimiser = optim.SGD(self.parameters(), lr=self.base_lr, momentum=self.momentum)\n",
    "            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, \n",
    "                                                                              T_0 = self.lr_update, # number of iterations for the first restart.\n",
    "                                                                              eta_min = self.min_lr\n",
    "                                                                               )\n",
    "            return [optimiser], [lr_scheduler]\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        # =============================================================================\n",
    "        # initial plot of circular layer\n",
    "        # updates model every nth epoch\n",
    "        # =============================================================================  \n",
    "        print(\"DECENT NOTE: on_train_epoch_start\", self.current_epoch)\n",
    "        \n",
    "        # plot random layer (the circular plot)\n",
    "        if self.current_epoch == 0:\n",
    "            self.model.plot_layer_of_1_channel(current_epoch=0)\n",
    "        \n",
    "        # update model\n",
    "         # don't update unless pretrain epochs is reached\n",
    "        if (self.current_epoch % self.update_every_nth_epoch) == 0 and self.current_epoch >= self.pretrain_epochs:\n",
    "            print(\"DECENT NOTE: update model\", self.current_epoch)        \n",
    "            if debug_model:\n",
    "                print(\"DECENT NOTE: before update\")\n",
    "                print(self.model)\n",
    "            self.model.update(current_epoch = self.current_epoch)\n",
    "            if debug_model: \n",
    "                print(\"DECENT NOTE: after update\")\n",
    "                print(self.model)\n",
    "                \n",
    "            print(\"DECENT NOTE: model updated\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculates loss for a batch # 1\n",
    "        # parameters:\n",
    "        #    batch\n",
    "        #    batch id\n",
    "        # returns:\n",
    "        #    loss\n",
    "        # notes:\n",
    "        #    calling gradcam like self.gradcam(batch) is dangerous cause changes gradients\n",
    "        # =============================================================================     \n",
    "        if False: # batch_idx < 2: # print first two steps\n",
    "            print(\"DECENT NOTE: training_step\", batch_idx)\n",
    "\n",
    "        # calculate loss\n",
    "        # loss = torch.tensor(1)\n",
    "        loss = self.run_loss_n_metrics(batch, mode=\"train\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging # 2\n",
    "        # =============================================================================\n",
    "        if False: # batch_idx < 2:\n",
    "            print(\"DECENT NOTE: validation_step\", batch_idx)\n",
    "        \n",
    "        self.run_loss_n_metrics(batch, mode=\"val\")\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing # 3\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_validation_epoch_end\")\n",
    "        pass\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # save model if next iteration model is pruned # 4 \n",
    "        # this needs to be called before callback \n",
    "        # - if internal pytorch lightning convention changes, this will stop working\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_train_epoch_end\", self.current_epoch)\n",
    "               \n",
    "        if False:\n",
    "            print(\"current epoch\")\n",
    "            print(((self.current_epoch+1) % self.update_every_nth_epoch) == 0)\n",
    "            print(self.current_epoch+1)\n",
    "            print(self.current_epoch)\n",
    "            print(self.update_every_nth_epoch)\n",
    "        \n",
    "        # numel: returns the total number of elements in the input tensor\n",
    "        unpruned = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.log(f'unpruned', unpruned, on_step=False, on_epoch=True) \n",
    "        \n",
    "        if ((self.current_epoch+1) % self.update_every_nth_epoch) == 0 and self.current_epoch != 0:\n",
    "            # if next epoch is an update, set unpruned flag            \n",
    "            self.log(f'unpruned_state', 1, on_step=False, on_epoch=True)\n",
    "            \n",
    "            # save file\n",
    "            with open(os.path.join(self.log_dir, 'logger.txt'), 'a') as f:\n",
    "                f.write(\"#\"*50)\n",
    "                for p in self.model.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        f.write(str(p.shape))\n",
    "            \n",
    "        else:\n",
    "            # else set unpruned flag to -1, then model won't be saved\n",
    "            self.log(f'unpruned_state', -1, on_step=False, on_epoch=True)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # =============================================================================\n",
    "        # calculate loss for logging, plot gradcam\n",
    "        # =============================================================================\n",
    "        if batch_idx < 2:\n",
    "            print(\"DECENT NOTE: test_step\", batch_idx)\n",
    "\n",
    "        self.run_loss_n_metrics(batch, mode=\"test\")\n",
    "\n",
    "                # .requires_grad_()\n",
    "        \n",
    "                \n",
    "        \"\"\"\n",
    "        with torch.enable_grad():\n",
    "            grad_preds = preds.requires_grad_()\n",
    "            preds2 = self.layer2(grad_preds)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            with torch.set_grad_enabled(True): # torch.set_grad_enabled(True):\n",
    "                self.run_xai_gradcam(batch, batch_idx, mode='explain')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"batch size has to be 1\")\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            layer = self.model.decent1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent2\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent2' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent3\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent3' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "            layer = self.model.decent1x1\n",
    "            # this line seems to be useless, always same output no matter what\n",
    "            layer_str = 'decent1x1' # 'decent3'  model.model.decent3' # .filter_list[7]weights\n",
    "            self.run_xai_feature_map(batch, batch_idx, layer, layer_str, device='cuda')\n",
    "            \n",
    "    def on_test_epoch_end(self):\n",
    "        # =============================================================================\n",
    "        # currently nothing\n",
    "        # =============================================================================\n",
    "        print(\"DECENT NOTE: on_test_epoch_end\", self.current_epoch)\n",
    "        pass\n",
    "    \n",
    "    def run_xai_feature_map(self, batch, batch_idx, layer, layer_str, device='cuda'):\n",
    " \n",
    "        # img, label = testset.__getitem__(0) # batch x channel x width x height, class\n",
    "\n",
    "        # img = X(img.to(device).unsqueeze(0), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        \n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        # print(img.data.shape)\n",
    "\n",
    "        # run feature map\n",
    "        # model, layer, layer_str, log_dir, device=\"cpu\"\n",
    "        dd = FeatureMap(model=self.model, layer=layer, layer_str=layer_str, log_dir=self.log_dir, device=device)\n",
    "        dd.run(tmp_img, batch_idx)\n",
    "        dd.log()\n",
    "    \n",
    "    def run_xai_gradcam(self, batch, batch_idx, mode='explain'):\n",
    "        # =============================================================================\n",
    "        # grad cam - or just cam?? idk\n",
    "        # todo error: RuntimeError: cannot register a hook on a tensor that doesn't require gradient\n",
    "        # BATCH SIZE HAS TO BE ONE!!!\n",
    "        # grad enable in test mode:\n",
    "        # https://github.com/Project-MONAI/MONAI/discussions/1598\n",
    "        # https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "        # =============================================================================\n",
    "    \n",
    "        img, ground_truth = batch\n",
    "\n",
    "        # make it an X object, init with position 0/0 as input for first layer\n",
    "        tmp_img1 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)]) # .requires_grad_()\n",
    "        tmp_img2 = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "\n",
    "        #print(\"nooooooooooo grad, whyyyyy\")\n",
    "        #print(tmp_img1)\n",
    "        #print(img)\n",
    "\n",
    "        #print('b1', tmp_img1)\n",
    "        #print('b2', tmp_img2)\n",
    "\n",
    "        model_output = self(tmp_img1, mode)\n",
    "\n",
    "        #print('c1', tmp_img1)\n",
    "        #print('c2', tmp_img2)\n",
    "\n",
    "        # get the gradient of the output with respect to the parameters of the model\n",
    "        #pred[:, 386].backward()\n",
    "\n",
    "        # get prediction value\n",
    "        pred_max = model_output.argmax(dim=1)\n",
    "\n",
    "        #print('d1', tmp_img1)\n",
    "\n",
    "        #print(\"mo\", model_output)\n",
    "        #print(\"max\", pred_max)\n",
    "        #print(\"backprop\", model_output[:, pred_max])\n",
    "\n",
    "        # backpropagate for gradient tracking\n",
    "        model_output[:, pred_max].backward()\n",
    "\n",
    "        # pull the gradients out of the model\n",
    "        gradients = self.model.get_activations_gradient()\n",
    "\n",
    "        # pool the gradients across the channels\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "        #print('e2', tmp_img2)\n",
    "\n",
    "        # get the activations of the last convolutional layer\n",
    "        activations = self.model.get_activations(tmp_img2).detach()\n",
    "\n",
    "        # weight the channels by corresponding gradients\n",
    "        for i in range(self.n_classes):\n",
    "            activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "        # average the channels of the activations\n",
    "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # relu on top of the heatmap\n",
    "        # expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "        #heatmap = torch.max(heatmap, 0)\n",
    "\n",
    "        # normalize the heatmap\n",
    "        #heatmap /= torch.max(heatmap)\n",
    "\n",
    "        #print(\"hm\", heatmap.shape)\n",
    "\n",
    "        # draw the heatmap\n",
    "        # plt.matshow(heatmap.detach().cpu().numpy().squeeze())\n",
    "        # fig.savefig(os.path.join(self.log_dir, f\"{self.ci_metric}_m{int(self.m_plot[0])}_n{int(self.n_plot[0])}_{str(current_epoch)}.png\"))\n",
    "\n",
    "        plt.imsave(os.path.join( self.log_dir, f\"plt_cam_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" ), heatmap.detach().cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "        heatmap *= 255.0 / heatmap.max()\n",
    "        pil_heatmap = Image.fromarray(heatmap.detach().cpu().numpy().squeeze()).convert('RGB')\n",
    "        pil_heatmap.save(os.path.join( self.log_dir, f\"pil_cam_id{batch_idx}_mo{pred_max.detach().cpu().numpy().squeeze()}_gt{ground_truth.detach().cpu().numpy().squeeze()}.png\" ) ) \n",
    "            \n",
    "    def run_loss_n_metrics(self, batch, mode=\"train\"):\n",
    "        # =============================================================================\n",
    "        # put image through model, calculate loss and metrics\n",
    "        # use cc term that has been calculated previously\n",
    "        # =============================================================================\n",
    "        \n",
    "        img, ground_truth = batch\n",
    "        # make it an X object\n",
    "        \n",
    "        #print(img.shape)\n",
    "        \n",
    "        # init with position 0/0 as input for first layer\n",
    "        img = X(img.to(\"cuda\"), [torch.tensor(0)], [torch.tensor(0)])\n",
    "        \n",
    "        model_output = self(img, mode) # cause of the forward function\n",
    "        \n",
    "        # ground_truth = ground_truth\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"gt\", ground_truth)\n",
    "        print(\"gt shape\", ground_truth.shape)\n",
    "        print(\"gt type\", ground_truth.type())\n",
    "        print(torch.zeros(ground_truth.size(0), self.n_classes))\n",
    "        \n",
    "        if len(ground_truth.shape) < 2:\n",
    "            ground_truth_tmp_tmp = ground_truth.unsqueeze(1)\n",
    "        else:\n",
    "            ground_truth = ground_truth.transpose(1, 0)\n",
    "        ground_truth_multi_hot = torch.zeros(ground_truth_tmp.size(0), self.n_classes).scatter_(1, ground_truth_tmp.to(\"cpu\"), 1.).to(\"cuda\")\n",
    "        \n",
    "        # this needs fixing\n",
    "        # ground_truth_multi_hot = torch.zeros(ground_truth.size(0), 10).to(\"cuda\").scatter_(torch.tensor(1).to(\"cuda\"), ground_truth.to(\"cuda\"), torch.tensor(1.).to(\"cuda\")).to(\"cuda\")\n",
    "        \"\"\"\n",
    "\n",
    "        ground_truth = ground_truth.squeeze()\n",
    "        if len(ground_truth.shape) < 1:\n",
    "            ground_truth = ground_truth.unsqueeze(0)\n",
    "        loss = self.criterion(model_output, ground_truth.long()) # ground_truth_multi_hot)\n",
    "        cc = torch.mean(self.model.cc) * self.cc_weight\n",
    "        \n",
    "        # print(cc)\n",
    "        # from BIMT\n",
    "        # loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "        # cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "        # total_loss = loss_train + lamb*cc\n",
    "        \n",
    "        pred_value, pred_i  = torch.max(model_output, 1)\n",
    "        \n",
    "        #print(model_output)\n",
    "        #print(pred_i)\n",
    "        #print(ground_truth)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            self.train_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            self.train_f1(preds=pred_i, target=ground_truth) \n",
    "            self.train_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.train_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.train_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "            if random.randint(1, 50) == 5:\n",
    "                print()\n",
    "                print(\"info at random intervals\")\n",
    "                print(\"p\", pred_i)\n",
    "                print(\"g\", ground_truth)\n",
    "                print(\"a\", self.train_acc)\n",
    "                print(\"f\", self.train_f1)\n",
    "                print(\"p\", self.train_prec)\n",
    "                print(\"l\", loss)\n",
    "                \n",
    "        else:\n",
    "            self.val_acc(preds=pred_i, target=ground_truth) # (model_output.argmax(dim=-1) == ground_truth).float().mean()\n",
    "            self.val_f1(preds=pred_i, target=ground_truth) \n",
    "            self.val_prec(preds=pred_i, target=ground_truth) \n",
    "            \n",
    "            self.log(f'{mode}_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_f1', self.val_f1, on_step=False, on_epoch=True)\n",
    "            self.log(f'{mode}_prec', self.val_prec, on_step=False, on_epoch=True)\n",
    "            \n",
    "        self.log(f'{mode}_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_cc', cc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # loss + connection cost term\n",
    "        return loss + cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afaef32c-9dc5-40c3-813b-0b9a9c69b7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([5]).squeeze().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed2906-61b6-4f3a-b2fa-7aeeaa12e7e1",
   "metadata": {},
   "source": [
    "## run dev routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85d36c8d-87e6-4f01-8e52-cdcea768df24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train kwargs {'result_path': 'examples/example_results', 'exp_name': 'tmp_testi_oct', 'load_ckpt_file': 'xversion_22/checkpoints/epoch=0-unpruned=10942-val_f1=0.06.ckpt', 'epochs': 1, 'img_size': 28, 'batch_size': 128, 'log_every_n_steps': 4, 'device': 'cuda', 'num_workers': 18, 'train_size': 260, 'val_size': 260, 'test_size': 24}\n",
      "model kwargs {'n_classes': 4, 'out_dim': [1, 8, 16, 32], 'grid_size': 324, 'criterion': CrossEntropyLoss(), 'optimizer': 'sgd', 'base_lr': 0.001, 'min_lr': 1e-05, 'momentum': 0.9, 'lr_update': 100, 'cc_weight': 10, 'cc_metric': 'l2', 'ci_metric': 'l2', 'cm_metric': 'count', 'update_every_nth_epoch': 4, 'pretrain_epochs': 20, 'prune_keep': 0.95, 'prune_keep_total': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 19\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type                | Params\n",
      "---------------------------------------------------\n",
      "0 | model      | DecentNet           | 7.7 K \n",
      "1 | criterion  | CrossEntropyLoss    | 0     \n",
      "2 | train_acc  | MulticlassAccuracy  | 0     \n",
      "3 | train_f1   | MulticlassF1Score   | 0     \n",
      "4 | train_prec | MulticlassPrecision | 0     \n",
      "5 | val_acc    | MulticlassAccuracy  | 0     \n",
      "6 | val_f1     | MulticlassF1Score   | 0     \n",
      "7 | val_prec   | MulticlassPrecision | 0     \n",
      "---------------------------------------------------\n",
      "6.0 K     Trainable params\n",
      "1.7 K     Non-trainable params\n",
      "7.7 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: configure_optimizers\n",
      "Sanity Checking: |                                                                               | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  0.60it/s]DECENT NOTE: on_validation_epoch_end\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:293: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=4). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                   | 0/3 [00:00<?, ?it/s]DECENT NOTE: on_train_epoch_start 0\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  0.74it/s, v_num=22]\n",
      "Validation: |                                                                                    | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                       | 1/3 [00:01<00:02,  0.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 2/3 [00:02<00:01,  0.80it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  0.79it/s]\u001b[ADECENT NOTE: on_validation_epoch_end\n",
      "\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:35<00:00,  0.03it/s, v_num=22]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('unpruned_state', ...)` in your `on_train_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECENT NOTE: on_train_epoch_end 0\n",
      "DECENT NOTE: callback on_train_epoch_end tensor(0)\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:35<00:00,  0.03it/s, v_num=22]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGyCAYAAAB9ZmrWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABszUlEQVR4nO2deZRU5Zn/v7durb1v0AvdTaNBNhEMIFEW4UQUJAr2tJhgDAnnjDGDEzrMYRwTUfRnYjQzDsR4NM75JcZjMHFIw5igZPgZEHBBgQAujSBbN72vVd1dXUvfur8/Hm9X71R33aq71PM5p0513a6uevre932/93ne531eQZZlGQzDMAxjYixaG8AwDMMwsYbFjmEYhjE9LHYMwzCM6WGxYxiGYUwPix3DMAxjeljsGIZhGNPDYscwDMOYHhY7hmEYxvSw2DEMwzCmh8WOYRiGMT2jFruDBw/ijjvuQEFBAQRBwO7du/v9vrOzEw8++CAKCwvhcrkwffp0vPjii2rZyzAMwzCjZtRi19XVhVmzZuH5558f8vebNm3C3r178eqrr6KyshLl5eV48MEH8cYbb0RtLMMwDMOMBSGaQtCCIGDXrl1YvXp177Frr70W99xzD7Zs2dJ7bM6cOVixYgWefPLJqIxlGIZhmLFgVfsDb7rpJrzxxhtYv349CgoKcODAAZw5cwb/+Z//OeT7/X4//H5/7+tQKITW1lZkZ2dDEAS1zWMYhmEMgizL6OjoQEFBASyWKFNM5CgAIO/atavfMZ/PJ3/nO9+RAchWq1W22+3y7373u2E/47HHHpMB8IMf/OAHP/gx5KO6ujoaqZJlWZZV9+yee+45fPDBB3jjjTcwceJEHDx4EBs2bEBBQQFuueWWQe9/+OGHsWnTpt7XbrcbxcXFOHPmDLKystQ2T3WCwSD279+PpUuXwmazaW3OiLCtsYFtjQ1sa2wwkq2tra245pprkJqaGvVnqSp23d3d+PGPf4xdu3Zh5cqVAIDrrrsOJ06cwL//+78PKXYOhwMOh2PQ8aysLGRnZ6tpXkwIBoNISkpCdna27hsO2xob2NbYwLbGBiPZqqDGlJaq6+yCwSCCweCg2KooigiFQmp+FcMwDMNEzKg9u87OTnzxxRe9ry9cuIATJ04gKysLxcXFuPnmm7F582a4XC5MnDgR77zzDl555RU8++yzqhrOMAzDMJEyarE7evQoli5d2vtamW9bt24dXn75ZfzhD3/Aww8/jHvvvRetra2YOHEifvrTn+KBBx5Qz2qGYRiGGQWjFrslS5ZAHmFpXl5eHn77299GZRTDMAzDqAnXxmQYhmFMD4sdwzAMY3pY7BiGYRjTw2LHMAzDmB4WO4ZhGMb0sNgxDMMwpofFjmEYhjE9LHYMwzCM6WGxYxiGYUwPix3DMAxjeljsGIZhGNPDYscwDMOYHhY7hmEYxvSw2DEMwzCmh8WOYRiGMT0sdgzDMIzpYbFjGIZhTA+LHcMwDGN6WOwYhmEY08NixzAMw5geFjuGYRjG9LDYMQzDMKaHxY5hGIYxPSx2DMMwjOlhsWMYhmFMD4sdwzAMY3pY7BiGYRjTw2LHMAzDmB4WO4ZhGMb0sNgxDMMwpofFjmEYhjE9Vq0NYBjGwEgScOgQUFcH5OcDixYBoqi1VQwziFF7dgcPHsQdd9yBgoICCIKA3bt3D3pPZWUl7rzzTqSnpyM5ORnz5s1DVVWVGvYyDKMXKiqAkhJg6VJg7Vp6Limh4wyjM0Ytdl1dXZg1axaef/75IX9/7tw5LFy4EFOnTsWBAwdw6tQpbNmyBU6nM2pjGYbRCRUVQFkZcPly/+M1NXScBY/RGaMOY65YsQIrVqwY9vc/+clPcPvtt+OZZ57pPXb11VePzTqGYfSHJAEbNwKyPPh3sgwIAlBeDqxaxSFNRjeoOmcXCoWwZ88e/Ou//ituu+02/P3vf8ekSZPw8MMPY/Xq1UP+jd/vh9/v733t8XgAAMFgEMFgUE3zYoJiI9uqLmxrbFDF1sOHgZYWwOUa/j3NzcDBg8DChWP+moQ7r3HCiLaqgSDLQ92eRfjHgoBdu3b1Cll9fT3y8/ORlJSEJ598EkuXLsXevXvx4x//GPv378fNN9886DO2bt2Kxx9/fNDxHTt2ICkpaaymMQzDMAbH6/Vi7dq1cLvdSEtLi+qzVBW72tpaTJgwAd/61rewY8eO3vfdeeedSE5OxmuvvTboM4by7IqKilBXV4fs7OyxmhY3gsEg9u3bh2XLlsFms2ltzoiwrbEh4Ww9fBhYufLK79uzJ2rPLqHOa5wwkq0tLS3Iz89XRexUDWPm5OTAarVi+vTp/Y5PmzYNhw8fHvJvHA4HHA7HoOM2m033F6IvRrKXbY0NCWPr4sVAdjYlowx1rywIQGEhvU+FObuEOa9xxgi2qmmfqovK7XY75s2bh88//7zf8TNnzmDixIlqfhXDMFohisD27fSzIPT/nfJ62zZOTmF0xag9u87OTnzxxRe9ry9cuIATJ04gKysLxcXF2Lx5M+655x4sXry4d87uz3/+Mw4cOKCm3QzDaElpKbBzJ2Vl9l1+UFhIQldaqplpDDMUoxa7o0ePYunSpb2vN23aBABYt24dXn75Zdx111148cUX8dRTT+GHP/whpkyZgj/96U9YGEXsnmEYHVJaSssLuIIKYwBGLXZLlizBlXJa1q9fj/Xr14/ZKIZhDIIoAkuWaG0Fw1wRLgTNMAzDmB4WO4ZhGMb0sNgxDMMwpofFjmEYhjE9LHYMwzCM6WGxYxiGYUwPix3DMAxjeljsGIZhGNOjaiFohmHUJxSi/VIliX6W5fBz359DocHv7fsZAB0HgKoqwGoNl7IUBFofbrHQQ/lZEOgx8Gfl96I4uDwmw+gRFrtokCTa7gSgZ5WqvDOJgSxTE+rpGSxSgQAdDwTC71PeMxyKGPUVp4G/V/5e+c6+tgwUz+Fs7it2FguJps0G2O10vK8QiiL93sIxJH2QwGMWi91YqaigIrgtLcBrr9H+XtnZVA2ei+AyfVAETXnu6QF8PnooAjdQxAYKisMR/jkaT0r5nqSksY9xfT1IRZiV/yUUIvv6iqIihk4nCaLVSg/ld+wZxokEH7NY7MZCRQVQVkY92uUKH6+poeM7dyZE42H6I8s08AeD9Oz3h0VAETkg7Hkpg77Taayb674e3UjIcn9x7+wMe4zK34siCaAihDYbPa702cwo4TGLxW7USBLdHQ0V55FlGsnKy6kavJFGMGZUyDKJWjAIeL107NIl8mwUUes7oLtciTe/JQhh8RqIcp4kCejqAtxuOqeiGA6J2u30Xp+PPosFcIzwmAWAxW70HDrUf/+ugcgyUF1N7+Nq8KZBmT9TvLXubhK6vnNfskweCofmrozFEhazvoRCdF59PhJAgG4iFM8vKYmeHQ565vMcATxmAWCxGz11deq+j9ElfcWtq4sGX7+fxoW+c1BWa1jsFO+NGTvK/KTDEY62pabSOfb5gI4OugaK95ecTO9TPEEWvyHgMQsAi93oyc9X932MLpAkEjO/n8KS3d1hcVNCcRkZPJhqgcVC59/hCB9Tbkaam0kIFfFLSQl7gSx+X8JjFgAWu9GzaBFQWEgTu0PFwAWBfr9oUfxtYyJGSSbx+chz83rptTJw2mxAejqnzOsVJbknKYleK+LX2Bi+QXE4SPxcLuMlAakKj1kAWOxGjyhSqm5Z2dALmQBg27YE7ln6JRQKz7l1dITn3axW8gLS0ljcjMpA8VMyYhsa6LXTScKXlEQ/D5U0Y1p4zALA5cLGRmkppepOmND/eGFhQqTwGgkl26+5Gbh4kR41NSR4TieQlUUi53Sy0JkJm43m8zIzyUMHaHnZpUvhNuB2h0PVpofHLPbsxkxpKaXqHjwIeDzAnj0JVY1Az0gSeW1dXeTBKVVIlLAWX6LEwmKhUKbLFQ5fd3QAbW3hRKO0NPL6+s4Lmo4EH7NY7KJBFIGFC4E336TnBGk0ekTJ1uvqon6srM1yOunOnhMVGIDagZLtCVC40+8nT89mI8FLSwtneJqOBB6zWOwYwyLL4exJt5u8OWWtG2dOMpHQd9G7UiDA7Q5HAZQEF17Qbnz4EjKGIxCgQcnjoeeeHhqcUlMT6kaVUZm+wuf3A+3tNM/nclHbUtb08dyuMWGxYwyBLJOwdXSQyAUC4bAT33UzaqOEOmWZQuLNzUBTEwleRgY9J1RGpwngYYLRNUpoqb2d5uMAurtOSdHULCZBEIRwcksoRKHyy5dJCNPSyOPrW1eZ0S8sdozuUO6mASrZFwxSsgCHKRktsVjIo0tODnt7ra30mm++9A+LHaMbJCnsxXk8dEwQaK0UJ5swesLppEdPD3l77e10vKWFwpxOp5bWMUPBYsdoTk8P7XXW2kpi17cShsvFQsfoF6u1fyizqYmyOdPSaMkLt1/9wGLHaIayuLe9ne6Onc6wFzdw526G0TOKoGVkUNtta6N2rYhecjKLntaw2DFxx+8Pi5zPR3e/HKpkzIKy40JPD4XjFdFTsjh56YI2sNgxcUPZkNPtJq/O5aLalAxjRqxW8uqU+qweD4ldVhY9c7JVfGGxY8aOJNHuxnV1tBfWokVD9mBlAt/jobvdpCQNstckCfYjhyA21kEan4/A/KFtZXSCJMF25DBgA2xHDiM037g1HEWRPLtQiOakq6upD2RmmiTDOMJxQGtG7VAfPHgQd9xxBwoKCiAIAnbv3j3sex944AEIgoBt27ZFYSKjSyoqgJISYOlSYO1aei4poeNf4vPRFiuXLtEchsNBHTzexXadb1Ygd34Jcu5eiswNa5Fz91Lkzi+B882KK/8xE3eU65X9nZUAgOzvrDTF9bJY6CYvI4Nu+i5fBqqq6CZQ2e3ecEQwDuiFUYtdV1cXZs2aheeff37E9+3atQsffPABCgoKxmwco1MqKmhvrMuX+x+vqQHKyhB8vQJNTdSRm5rCtSq1KKzrfLMCmfeXwVLX31ZLfQ0y7y8z/ABqNhLheglCePuhYJD6SXU1zWMbaruhK4wDehO8UYvdihUr8OSTT+Kuu+4a9j01NTX453/+Z/z+97+HjWvqmAtJAjZuHLpXyjJkACgvR0OtBJuN5ic0qx4vSUh/lGwdmPsifGl/2mPlnPqpFxLseglC2NPz+Ujwampofk/3oneFcQAAUF6uq2ul+pxdKBTCfffdh82bN2PGjBlXfL/f74ff7+997flyNXEwGEQwGFTbPNVRbEwYWw8fDlfHHY72ZuScPYjg/IVjbuuSFOz3PBZsRw4j1N6C0Ei2tjXDcoRsHStq2Bov9GzrwOsVHPAMQJXrFQuiPa9JSaQL7e30SE+P3eL0uI0Dzc20d97CsV8rNcdVQZbHfg8hCAJ27dqF1atX9x576qmnsH//fvz1r3+FIAgoKSlBeXk5ysvLh/yMrVu34vHHHx90fMeOHUhSVhYzDMMwCYfX68XatWvhdruRlpYW1Wep6tkdO3YM27dvx/HjxyFEuGjq4YcfxqZNm3pfezweFBUVYenSpcjOzlbTvJgQDAaxb98+LFu2TPchW1VsPXwYWLnyim9reWVP1N7SyZP7MGvWMoji2Gy1HTncm+QwEnqwNV7o2daB1yvocmHfb36DZevXw9bd3Xs82usVC2JxXgMBCmna7TQdkJamTpJjPMcB7NkTlWfX0tIy5r8diKpid+jQITQ2NqK4uLj3mCRJ+Jd/+Rds27YNFy9eHPQ3DocDjiHS82w2m+7Foy9GsjcaW4M3LoYlMxuWupreeZS+yIIAKb8QofmLIarQM0XRNubBIzR/MSwZ2bDU69/WeKNHW4e7Xrbubti6u1W/XrFAzfOq7LbQ3Q00NtKyhZwc9aqxRDVmLV4MZGfTJONQwUFBAAoL6X1RXCs1x1RV1/Lfd999OHXqFE6cONH7KCgowObNm/HXv/5Vza9i4kwoRIvBq2pE1GzeDoDEoi/Ka8/j2/SxzkYU4X7CILYyfL2GQakw5PdT5mZ9Pf2sKaIIbKdrNUh5ldfbtunqWo1a7Do7O3uFDAAuXLiAEydOoKqqCtnZ2bj22mv7PWw2G/Ly8jBlyhS1bWfihNdLN3DV1SR64t2laHtpJ0J5E/q9T8ovRNtLO+G7vVQjSwfju904tjJ8vYZDEGgBekoKFUyvqqL8EE2THUtLgZ07gQn9rxUKC+l4qb6u1ajDmEePHsXSpUt7XyvzbevWrcPLL7+smmGM9gSDtBi8tZVELj09fKPmu70UvttWGaIqiZFsZcLXy3LkIAAPWl7ZY+gKKmpitZKX191NBUs6OiiamJKiUW3Z0lJg1SpDVFAZtdgtWbIEo0ngHGqejtE3skydqKmJOlVKyjBr5UQRgZuWxNu8sWEkWxlAFCkJ5fibCM5fqNs5Oq1wuWhZQmcneXlZWSR6mqxpFUVgyRINvnh0cG1Mph+BAIVH2trQuyicYRj9oYQ2e3rCe0GOG0fHeAeRwbDYMQD6e3M+H3UYK7cOhtE9Smizq4sqd2Vmaujl6RgezphB3lxmptYWMQwzWpKT2csbCRa7BIa9OYYxF+zlDQ8PbQkKe3MMY17YyxsMi10C0tlJnYC9OYYxLwO9vKwsqsCSqPAwl0AoC1AvXw5vpMowjLlRvLzmZrrBTdQMa1XLhTH6xeejMkMAbSeSkqKtPQzDxA/Fy/P5wnut6n7PPJVhsTM5sgx4PNTAOzroGE9WM0ziIQi0c4IybdHYSFWSEgX9it377+tql1sjIkmUaXn5MoleRob6X2B/7wBcu1+D/b0DfL0SEW4DgCTBduQwANqmSO/nQNlvtbWVxoauLm3tiRf6Fbu77wZKSoCKCq0tMSTd3VS8ubGRwpbJyep+vvPNCuTOL0HO3UuRuWEtcu5eitz5JXC+ydcrUeA2ED4Hyj582d9ZaZhzkJFBuydUV1NmdiiktUWxRb9iB9BoXVbGgjcKZJm24lHu2DIy1A9bOt+sQOb9ZbDUXe533FJfg8z7ywzR0Zno4DZg/HOghDXtdqrhXFdHS5LMir7FTplBLS/XfWhAD4RClHFVU0MNOT0dsKh9hSUJ6Y9uBGQZA5fsKBtupj1WztfLzHAbMNU5cDpprGhro7HD69Xaotigb7EDSPCqq2kLCWZYgkHKtmxooLBlUlJsvsd+5BDEusuDOriCIMuw1lbDfoSvl1nhNmC+cyCK4WzNmhpKajMb+hc7hbo6rS3QLUoDbW2lO7RYZluKjZFdh0jfxxgPbgPmPAdKNEgQaBqkudlc83jGWVSen6+1Bbqko4O8uUCA7sxiXQ5IGh/ZdYj0fYzx4DZg7nOQlESeXn09RYzGjTNHlSX9e3aCABQV0e63TC+yTBlUfZcVxKPuXWD+Ikj5hZCH+TJZENBTUEQ7gTOmhNuA+c+Bw0HJKy0tQG0tZW0aHX2LndKQtm3T5TbvWiFJtKSgro4apdrLCkZEFOF+YjsADOroymvP49v4epkZbgMJcQ6sVrqJ7uykm+rOTq0tig59i11hIbBzJ1BaqrUluiEQoDutpia683I642+D7/ZStL20E6G8Cf2OS/mFaHtpJ3y38/UyO9wGEuMcWCwkeD09lBfQ3m7cMmP6jcT+938DK1YY+s5IbZT6lp2d1ABVX1YwGltuL4XvtlWUldZYB2l8PoVs+HolDNwGwufAcuQgAA9aXtmD0PzFpjsHqalUqKK2loQvO9t42wXpV+xuvNF0DSYavF4KW/r98UlEiQhRROCmJVpbwWgJtwFAFBGcvxA4/iaC8xdCNOm45XLRDXZ9PWVp5uRoe8M9WvQrdkwvnZ3hzCjV61syDMNEiMNBAtfYSLkD48cbxydhsdM5bjcJnbIGhmEYRktstnCmZihEgmezaW3VlWGx0ymyTJPB9fXUkGJVEYVhGGa0KJmabW3k4eXl6X/rMBY7HaKsoWtspLCBsiUHwzCMXlAyNd1u8vDy8rTJDo8UA00vJgahEIlcfT2JHAsdwzB6RRE8ZUsxPReRZrHTEYrQNTVRqq/DobVFDMMwI6PkEwSDtDRBr5vBstjpBEmiGpfNzTT5a4QJX4ZhGIW0NBrH6ur0KXgsdjpAKf/V0kINxgxFVxmGSTxSU2k8q63VX3kxHlY1hoWOiRZZpqoWoVD4IUn0kOXw/qGyHN6ypamJ5luU4gSiSD+LIj0slvDDatVJEQPGEKSm0m4sdXW0WU1KitYWETy0akhfoUtPN87iTCZ+hEJUNcfvJ0ELBuk5EKDycd3d9Lu+QhcKhYWt70bZgkCPrCzg5El6T986h4rICUJ/sbNYwlnBTielmFutFGq3Wul3ymJjhgFI8Do79SV4oxa7gwcP4he/+AWOHTuGuro67Nq1C6tXrwYABINBPPLII3jzzTdx/vx5pKen45ZbbsHPf/5zFBQUqG27oVGSUVjoGCAsaH4/CVl3N90de730uqeHHoJAAqV4XKJIz4r31VegFE+tL7JMn5eXN9hbUzzBgaIpSTRwud1kgyKQshz+brud1oKmppIo2u1hERxTopUkwXbkMGADbEcOm7LepNlJSQl7eAUFcd6dZQhGLXZdXV2YNWsW1q9fj9IBuxF4vV4cP34cW7ZswaxZs9DW1oaNGzfizjvvxNGjR1Uz2uj0Fbq0NO7DiUYwSCLW3U0T+e3t9BwI0O8AEiu7nR7JyeRFxbqdjOXzJYlsDgTo/2hsDIdKbbaw/RkZ9OxykSiOlIDlfLMC6Y9uRKi9BXjtNWR/ZyUsGdlwP7HdFDsJJBIDQ5paCt6oxW7FihVYsWLFkL9LT0/Hvn37+h371a9+hRtuuAFVVVUoLi4em5UmQhE6JeuS5+jMjSSRqHm9JGhuN3V+v59EQgkRKvsSGi0LV5njG2oxsSKCHR3U3kMh+v8cDhoE09Ppf05KIhEURRK6zPvLAFlGqM8iU0t9DTLvLzPN1jmJRF/BKyjQrhpUzIdat9sNQRCQwRWMeyujsNCZF1kmYXO7gdZWevb5aNAXhHB4LzPTeMI2Wmw2evS9mw8G6Xy0tlLhBFmmc+J0AukpEpb8ZCMgyxiYDyPIMmRBQNpj5fDdtorDIQYjNRXweEjwJkzQptJKTIdbn8+Hhx56CN/61reQlpY25Hv8fj/8ffZ893g8AGj+L6jEdHSMYmMktra2kleXlEQDX9/kgXggScF+z3rGSLYGAmRjVVUQLS1hz00UyWNJSxu6bqAWm2DKcrDfc7yxWmkup2/CQiBA50s6fBhCRwt6vvToggOeAQBtzbAcOUhb6ugII7VXrWxNTqZQ9+XLFNKMpJammhogyPLYu5wgCP0SVPoSDAbxD//wD7h8+TIOHDgwrNht3boVjz/++KDjO3bsQBJXP2YYhklYvF4v1q5dC7fbPayGREpMxC4YDGLNmjU4f/48/va3vyE7O3vYzxjKsysqKkJdXd2If6cXgsEg9u3bh2XLlsE2TFxKiVfbbNrWupSkIE6e3IdZs5ZBFPUdQ9Ojrd3dFIppb6dwtFIH0OUKwuXaB5ttGQRBH7YOhywHEQzq09bMTw9jzqMre18HXS7s+81vsGz9eti6u3uPn3hyD4TFC5GWpp/asXpsr8Ohta3Kji4ZGVfeD6+lpQX5+fmqiJ3qYUxF6M6ePYv9+/dfUbAcDgccQ+Qm22y2YcVDjwxnb1cXzdEpWWl6QBRtuu+QClrbqmRLNjbS/Ft3N3XO5ORwR1XS+QXBpjsBGQ492to+bTF6krPhbK6BgPA9uK27G7bubsgQ0J1diM8yFqPnpAiXi5Jcxo8PZ3tqjdbtdTRoaWtWFvUrmw3IzR1+jaaaGjBqsevs7MQXX3zR+/rChQs4ceIEsrKykJ+fj7KyMhw/fhx/+ctfIEkS6uvrAQBZWVmw633DI5Xp7g5PwuthUSUTGZJEHbGhgR4+X/hmJTOTq4nEDFHE6Qe2Y/aTZRiYoqK8/vwH25BbIEKWqX8piS5OJw2aubkkfJy/om8slvAGsKIIjBsX+341arE7evQoli5d2vt606ZNAIB169Zh69ateOONNwAAs2fP7vd3+/fvx5IlS8ZuqcHw+6kT+v3U+Rj9owyeNTUkdgB1SANE001Dw4JSnHhkJ6a+uBHWrpbe475xhTj9/W1oWEDLDgSBEr2UaX2vF6iupuSHjAzK+MvK0k+YkxmM1UpZmo2NJH7Z2bEVvFGL3ZIlSzDSNF8UU4CmIRgkofN6Wej0jjJ/0NRE16yzkwbInBxeGqIVDQtK0fC1VcioPAjAg2NP7EH7tJErqCjCFwzSvOqpUxRNyc0lryEjgz1yPaIsTWlspMubmRm77+LurDLKovGODg556ZlAgLy42lp67umhu8wJE/ia6QJRRNuMhUDgTbTNWAhBiCwuabORhyDLdONy/jxQVUXH8vPJ20uw2RTd43DQuNnQQNcvVlM+LHYqoiwab2ujiXMeNPWHUs2jpoZ+ttvprp8HQHMhCHTzkpoavrGprw/f0OTk0M+MPnC5wnt6Wq2xWXTOYqcibjeFw1JSeIJcT8hy2ItraqJ51ORkutPnSv3mx26nUGYoRCHO06fJmxg3jspXZWXxjakeSEkJJ4ZNmKD+NAKLnUp4vXSRHA72EvREezslLihZsenpdFfPJB4WC3nxGRmUjFRXR+0iLw8oKuL5dT2Qnk6RsaYmmm9VExY7lWhooGfO/tIHHR2UmVdbS/NxWVlj3GqGMSUuFz38fmojjY3k5RUWcnhTSwQhvCTBZlPX42axi5KeHnr2+zlFXQ94vTR4VVfT+jhOP2dGwuGgcHZ3N3DhAnl6RUXaVudPdKzWcIammteAxS4KlIQUgO5GGO3w+Wigqq6mLLz0dL75YCLH5SKvrrMTOHuWQpxFRRTi1KJCf6LjcFDCSlOTep/JYhcFra30ADjRQSsCAboDvHSJEoTS0uiunBMOmLGQkkJeRUcH8NlnlLU7cSLNHxmoeqEpSEqizGm1YLEbI52dNMhyiEwbenro/FdX0w1HcjKJHN90MNGizBulpNAN1Mcfk+gVF1+5cDGjLmrOn7LYjYFAgBJShtuhmYkdskx3e5cuUYjD6aRQEw9AjNpYLFQYIi2NsnpPnKDlChMnUkYvRw9iDyeojBZJAg4dokB8fj6waNGYR8dQiAbZ7m5Kfoj3BqyJTHc3cPEiZVlaLBRa4pJeTKwRRZr/7emhtPgTJ2h+r6SEIztGwvxDRUUFsHEjjZAKhYXA9u1AaemoP66tLVwhhYkPskye9PnzdIedk8MeNRN/rFby7Hw+ytxsawOuukr99WBMbDD3DEdFBVBW1l/oAArAl5XR70dBVxd5dUlJHDaLF14vJQqcPEnh4wkTWOgYbXE6qR0GAtQuP/ssvJEvo1/MK3aSRB7dULswKMfKyyOOQwaDlBAhCDzYxoNQiJ5PnKD5uawsLuvE6AdBCLfJS5eonQLhdsvoD/OK3aFDgz26vsgypfIdOnTFj1Lm6bq6eBPWeNDZCVRW0s+hEN1Fc/UTRo84HNQ+FZGrrKT2y+gP887Z1dWp9j63m9LbeSeD2CJJtDD8/Hm6scjO5n3IGP0jCNROAwGq3uN201weZwnrC/N6dvn5qrzP6w2vp+OGGzu6u2nu49QpcrojvXwMoyfy86n9njpF7bm7W2uLGAXzenaLFlHWZU3N0PN2gkC/X7Ro2I9QytWEQpxiHEuam6lEU3s7ZbvZ7UNfMoYxAhkZlMR2+TKFNCdP5p029IB5PTtRpOUFwOA4mPJ627YR3bW2NiobxHUvY4MkUQr3iRPkQRcU8PZIjDmw26k9e72UsXnhAq/J1Rrzih1A6+h27qQZ5L4UFtLxEdbZdXWRx5GczHNGscDrBT75hCb0k5LIo+PzzJgJQaB27XLRhrGffMJLFLTEvGFMhdJSYNWqUVVQkSQSOlnmLMBY0NREYUu3mwvsMuYnJYXGkdpauomePJlEkIkv5hc7gIRtyZKI366ELzMzY2dSIhIK0TzG2bN018u7EzCJgs1G7b2lhZJXJk+mABMXLo8fiSF2o4DDl7EhGKQlBRcuUCVz3g2aSTQEgRJVlO2DurtpiQJHNuIDi10flPAlwOFLNfF6gTNnKIzDdS2ZRCc1lQTu3DkSvGuu4V3R4wGLXR84fKk+bW00Od/WRotseZcChglvTVVbS4Wlp0zhcSfWcMT4Szh8qT51dTQ/0dlJCbEsdAwTxmqlebyODuon9fVaW2RuWOxAiRMtLZx9qRayTIkon35KP+fm8g0EwwyFsi+jLNPShMuXuaBCrGCxA+Dx0IOTJqInFKIklM8+o1BNVpbWFjGM/snKov7y2WfUf3j3BPVJ+MBSIEDhS6eT04CjpaeHMi7PnaOSScnJWlvEMMYhLY1WSX3+OfWlq67i0L+aJPypbGujCWL2QKIjEKD1c1VVtFsBZ1wyzOhJTibBO3eOBO8rX+ESemqR0GLX1UVb9/AeddHh89HdaE0NMH48d06GiQankyqsXLxI61OnTOGbRzVI2MBdKERCB/DgHA0+Hy0tqKmhVGo+lwmGJCHr1AHkH3gNWacOJGa1Y0lC5qeHAYCeVTgHdjv1p5oa6l8+X9QfmfCMWuwOHjyIO+64AwUFBRAEAbt37+73e1mW8eijjyI/Px8ulwu33HILzp49q5a9qsFJKdGj7EFXW0slR3l+IbHIfbcCN3+3BDc8tBSznl6LGx5aipu/W4Lcdyu0Ni1uKOdgzqMrAQBzHl2p2jmwWqlf1dby3nhqMOrhqaurC7NmzcL69etROsSuAc888wx++ctf4ne/+x0mTZqELVu24LbbbsNnn30Gp0588UCAlho4HJyUMla6u2nHgvp66pC8sW386emhthwMhp+Vn7u7yRvw++m9kkTRDFEEvvY14OBBOmaxhK+dw0HhMpeLPAubjR7Kz3Z7+IYm990KzH6yDED/PHlncw1mP1mGE4/sRMOC4XcVMQN9z0FPnw0v1TwHokj9q66OXk+bxntrjpVRi92KFSuwYsWKIX8nyzK2bduGRx55BKtWrQIAvPLKK8jNzcXu3bvxzW9+MzprVaKtjQYDTkoZGz4fC108CQRofrmrixbou91Ugi0YJMGTJPq57/osi4WEyWKhNY7KOkdFrAIB+luA/k6WSQx7evqnvQsCCZ0o0t/abECyU8KPntsIQMbA5ZMCZMgQMPXX5Wj42irzNg5JwtQX43MOBgre9Ok8hzcWVA08XbhwAfX19bjlllt6j6Wnp2P+/Pl4//33hxQ7v98Pv3L7CcDj8QAAgsEggsGgmuYBoIG6uZnujtSYXpCkYL9nPaOGrT4f1blsaKA5BYslNotgZTnY71nPqGlrIEBC5vWSsLW10bPPF26vNhsJj9VKg54o0iOSKIXFQjZmZgYjWssVCtH3ShIJYTAIpJ8/DGegpZ83MxBrZzMyKg+ibcbCSP7tIdFzG8isPAxrV/gcBAc8A+qcAwWLhfpbQwPdgFxzzdgFz4hjlhoIsjz2oUoQBOzatQurV68GALz33ntYsGABamtrkZ+f3/u+NWvWQBAE/PGPfxz0GVu3bsXjjz8+6PiOHTuQxNVRGYZhEhav14u1a9fC7XYjLS0tqs/SPKXg4YcfxqZNm3pfezweFBUVYenSpcjOzlb1u7xeWgeWkqJeMoUkBXHy5D7MmrUMoqjvvTqisbWnh9bRVVfT8oJYJ6PIchDB4D7YbMsgCPo+r6O1tbsbaG+neePGRmqXkkRzZsojVtu+WCxBzJy5Dx9/vAyh0Ni+pLjqMO7dsfKK73vhG3vwWdZCiCJV9R8/ntZgZmRENu+k5zaQ+enh3qQUgDy6fb/5DZatXw9bn0ySY0/sUcWz60tPD7WboiLaF2+0fdFIY1Z7e4tqn6XqkJWXlwcAaGho6OfZNTQ0YPbs2UP+jcPhgGOIgpQ2mw02FXu8LFP2pdUam/qXomjTfcNRGK2toRBw6RIJ3bhx8d1/SxBsuhvohmM4W0MhKvbb1kaDVEsLCZwg0CLirKzB0zqxLhcVCtnGLHaXChbDa8tGmqcGAgYHhmQI8KQVom3qYuRZREgShWLPnqUQeFISiV5uLglfaurIIVg9toH2aYvRk5wNZ3P/c2Dr7oatuxsyBPjGFaJ92mIIgrrzljYb9cPqakoa+spXxpZoZ4QxS037VM1FnDRpEvLy8vD222/3HvN4PDhy5AhuvPFGNb9q1HR20oDDC8hHhyzT4tZz52iA4nV0kSHL5L2dPQu8+y5lPx49SkkGdjtVuy8oANLTjZfDIVtE7F2+nX4ekJ6hvN67fBtkC/1jokj/p/I/2+10Hj76iM7Lu+/SeWpvN1ARZFHE6QdGPgenv78tZhfXbqf+eO4c3Yga5rxpyKg9u87OTnzxxRe9ry9cuIATJ04gKysLxcXFKC8vx5NPPonJkyf3Lj0oKCjondfTAmUBuTKRz0ROTQ3wxRd0B84ZYFempwdoaqLq9Q0NFLJMSiLvZdw4ra1Tj8pppXh9zU4s37sR6Z7Lvcc9aYXYu3wbKqcNnXKveLNK3VSfjyIu9fUU2szNBQoL6Vzpva82LCjFiUd2YuqLG2HtCofbfOMKcfr722K+9MLppH559ix5e4WFMf06wzNqsTt69CiWLl3a+1qZb1u3bh1efvll/Ou//iu6urpw//33o729HQsXLsTevXs1XWPX0UGPjAzNTDAk9fVUvSEpiYs6R8LFi/Roa6NBPSODdmY3K5XTSnF6yipMrDqElI46dKbm41Lxol6PLhKcTnpkZ9ONweXLFJ7LzARKSsgT1DMNC0rR8LVVyKg8CMCDY0/sQfu0xXFT6uRkmu89fZqmaL6cSWKGYNRit2TJEoyUwCkIAp544gk88cQTURmmFpJEXp3dzgvIR0NbG9W7tFqpGjszGGUfxPp6SgU/eZLusOORwKMXZIuIiyVLVPksl4sePT20lvDkSRK7Tz6hQTw7W6d9WBQpCSXwJtpmLFR9ju5KpKXROfv8c8pH4B3Ph8b0XbKjgxbjcgOIHK+X7hQDAQorMf0JBEjgqqspZKmse8rP57kTNbBaSdiUhfDnz9MjJ4cyELkG62Cysihsfvo0MGsWRWOY/pha7Hp66M7b6eSdsiMlGKSMubY2YMIEra3RF5JEiRXnzpHI2e3hTTcBamMsduqh9Nn8fJrba2mh8z9uHHD11Vy9ZyDjxlEdzTNngBkz4ps1bQRMLXadnTQPwF5dZIRCdAddV0d3z3yDQIRCdNd8/jx5dHY7D7Txxm6n8LAyLXHkCLXRq66i6IMuw5txRqmyUltL4eDJk/m89MW0YidJ7NWNlsuXgQsXKISUKHNOIyHLVFpOuQEAaMDlO2btEEXyYIJB8q6bmmheb9IkCnMmel+3Wuk8XLhAgldcrLVF+sG0Qxp7daOjqYlSmFNTeYkBQGHc8+dp6YUk8RpDvWGzkRcTCNA1qq+n1PtJk7jPO53Uj8+eJcEz05KXaDCl2IVCNFjZ7XynFwleL3UMQeD9/To76a64qormibKyeEsVPaOElLu76brV1ZE3M2lSYheQSE2l9nv2LC1P4IQVk+5U3tlJD77AV0aSqEO43eS9JCqhEAnce+9RCrfTSQk6LHTGwOWi6+V00vV77z26nrEuu6ZncnLCVXwScQP5gZjOs1PKNNlsPDkbCVVVFAbKzU1cL7izk1K2q6pondKECYl7LoyO4sW0tlJ5tqYmYMqUxPTyBIH6dW0trcWbNElri7TFdGKnbHCZ6OG4SGhupjT6jIzETLoIhSgp5/RpKlk1blxsioQz8UUQKErh91Nos7WVBK+wMPFugG02qkt6/jwJXiJHb0x16RWvThA4LfxKdHeH5+kS8a63sxM4fpzu/oNB8uZY6MyF4qUHAnSd//53uu6JhtK/z5yhfp+omErsvF66Q+c6jkMgSbAdOQwAEN8/jC8+l9DertPajZKEzE/J1sxPD6s64dB3bu7iRbrT7VutQw8IIQklFw/g2o9fQ8nFAxBCPOEyVhQvLzubvLz330/MuTxl/u6LLxJ3/s5UYUyPh555jVh/nG9WIP3RjQi1twCvvYZx312JBc5sVH5/O1omxLYy+2jJfbeit4r8m6+9hjmPrkRPcjZOP7A96iryXV1AZaW+5+amVVYM2knAnVaIvcu3D7uTAHNllOvddy5v6tTEuTEWBArTX76cuAXxTePZ+f0kdpyB2R/nmxXIvL8MlrrL/Y4nt9Vg7s/LkPtuhUaWDSb33QrMfrIMzub+tjqbazD7yehsbW6mqht69eYAEro1r5chzdP//0/z1GDN62WYVqmfa2VEBnp5H35IhScSBbudxP3iRa0t0QbTiF1nJ8298MLfPkgS0h/dCMgDt5dE7+7KU39dro+4hiRh6osbMXgrzOhslWUq2PzRR3QzpNe5OSEkYfnekf//5XvLOaSpAg4HVV1xu0nwqqsTp6ZpRgZFOIDEC+WaQuwkieLRXPmjP/YjhyDWXR40eCoIkOFqqkbWp4fiatdQZH16CK5mdW1V9vk6dowGMz3X+5xYdQjpnpH//3RPNSZWaX+tzIBSR1KWqX18/rk+7vnigZKR2dCgrR3xxhRi5/VSlhEvAO6P2FgX0fscrZG9L5ZEakOk7/P5gBMngE8/pWUoWVlRGBcHUjoi+78ifR8TGVlZ1D4++YTai8+ntUWxR4l+XbhAY2eiYHixU5YbWK36vWvXCml8fkTv82dF9r5YEqkNkbzP7aYkhPPnqXCzEZIQOlMj+/8jfR8TOcnJ1E4uXKB243ZrbVF8UErjJUoI1/Bi5/NRDJoTUwYTmL8IUn7hELNAhAwB3eOK0DpjUZwtG0zrjEXozone1vp6modpaKB5GaPM4V4qXgR32sj/vzutCJeKtb9WZkSpsdnQQPO7iRDiy8mh7MxE+F8BE4hdZydt0srLDYZAFNH0yHYAg9MelNenv79NHyvwRRGnHxi7rbJMntzRoxTSLijQx78VKbJFxN7lI///e5dvg2wx0D9lMESR2o3XS4Jndq/HbqdknfPnE2OxuaHFrqeHQg48Vzc0sgx8OqUUb/9gJ3w5/bcd940rxIlHdka9dk1NGhaU4sQjo7c1FKLFsqdOUXmk8eONGdKunFaK19fshCet///vSSvE62t28jq7OCAI4T0LT56kdmXmrMXMTJoGunjR3MIOGHxReVcXhTETff+q4WhupjBF+q2leGflKmRUHgTgwbEn9qB92mJduj4NC0rR8LXIbQ2FqAzSZ59R7T+jlz6rnFaK01NWYWLVIaR01KEzNR+XihexRxdn0tMpY/OTTyhL85przFlXUxDC4cycHHPvfWdYsVMSU3jPuqHp6QEuXaIOSksyRLTNWAgE3kTbjIUQBB0PnmJktoZCtLTg9GlzCJ2CbBFxsWSJ1mYkPKmpNLZ89hmNN1OmmFPwnE5ag3rpEmWn6vAeWBUMe+l8Pooz89q6oWlqoodZvV5JotJflZV0F24WoWP0RUoKta/PPqO2ZtaQZlYWjReNjVpbEjsMK3ZeLyemDEcgQPUfnU5znp9QiBYBnz5NYm6EpQWMcUlOpnZ2+jS1OzMKntVK40VVFVWiMiOGFLtQiNxu9uqGprGRCt6aseCrMkd3+jT9f7zkhIkHSUnU3iorqf2ZUfAyMmjcMOtSBEOKXXc3hzCHw+ejzKrkZPPF3mWZNputrKQ5OvbomHiSnEzt7rPPqB2aLXtRFEnUL10yZyUZQ4qd10sNzYyTxdFSX09eb1qa1paoz4ULVP4rJYXn6BhtSEmhxJVPPzXn7gHp6bScq75ea0vUx3ByIUkcwhwOr5cquKelme9GoL6e7qhdLhpsGEYrUlOpHX76qflCfhYLjR/V1earm2m4IdHrJRebxW4wtbVUUcZsYuB2Ax9/TPMk6elaW8Mw1A5DISpkYLZamqmpQEcHjSdmwnBi19lJa194bV1/Ojrobiw93Vznxu8nofN4zL3glTEe48ZRu/z4Y2qnZkEQKFnl8mUaV8yCocQuGCSx4/Jgg7l8mTxes81lVVYCdXX63ouOSUwEgdplXR21UzORkkJRtMuXtbZEPQy1Cqu7m+6gOAuvP+3tFHKI+55tkoSsTw/B0VoHf1Y+7UigUgqokul26RKQm2u+zFIm/gghSfUybKJI7fPSJWD6dGq3qtyUxbBvRUp2No0r+fnmWMakuthJkoStW7fi1VdfRX19PQoKCvDd734XjzzyCIQoW0FXFw96A5FlCl/29MTX4819twJTX9wIV3P41q87pxCnH9iuSnHp2loKE2VkGGebHka/TKuswPK9G5HuCbdXd1oh9i7fHnWBbbs9LAa1tUBhYVQfF/O+FSkuF91Im2V6RPUw5tNPP40XXngBv/rVr1BZWYmnn34azzzzDJ577rmoPrenh8TO4VDJUJPQ2kqZivH06nLfrcDsJ8vgbO4f43A212D2k2XIfbciqs9vbqbMS4AXjTPRM62yAmteL0Oap397TfPUYM3rZZhWGV17BcLt9LPPgJaWsX9OrPvWaMnKovGltTWuXxsTVBe79957D6tWrcLKlStRUlKCsrIy3Hrrrfjwww+j+lyfj0KYLHb9qa0l7y5u50WSMPXFjRi86xoggGKPU39dTmtExkBXF034BwJRWckwACh0uXzvyO11+d5yCKGxtdeBBAKUodnVNYY/jnHfGgsOB40vZsjMVD2MedNNN+Gll17CmTNncM011+DkyZM4fPgwnn322SHf7/f74e+TyuTxeAAAwWAQwT5F2jo6KNVXb2V6JCnY7zmedHRQ8db09MiqOchysN/zWMisPAxrVwt6RoiZWjubkVF5kHYuGAVKzUu3G5gwgWy0WPRfqE+xkW1VFzVsLb58GEnBkdtrUrAZE2sPoqp4dO21L4qNublB1NRQSbHrrhvdetdY9q2+jHYcSE+naEt7e/yXNak5rgqyrG7Rm1AohB//+Md45plnIIoiJEnCT3/6Uzz88MNDvn/r1q14/PHHBx3fsWMHkjiGxTAMk7B4vV6sXbsWbrcbaVGWhVJd7P7whz9g8+bN+MUvfoEZM2bgxIkTKC8vx7PPPot169YNev9Qnl1RURHq6uqQnZ0NgEKYly5ROqzeElQkKYiTJ/dh1qxlEEVb3L43EACOHRvdQmtZDiIY3AebbRkEYWy2Zn56GHMeXXnF9x17Ys+o7j67uoAjR2h5SVYW3SnPnLkPH3+8DKFQ/M7rWGBbY4MathZXHca9O67cXn+/dk/Unl1fW1tbKXHlhhsizx6PVd8ayFjGAbebvNQ5c+KbMNbe3oLrrstXRexUD2Nu3rwZ//Zv/4ZvfvObAICZM2fi0qVLeOqpp4YUO4fDAccQE042mw02G12Izk46puesPFG0xVXs3G4KY+bnjz5LShBsYxa79mmL0ZOcDWdzTe88Ql9kCPCNK0T7tMURbxCrbMLa1gZMmNA/VB0K2XQ/KCuwrbEhGlsvFSyG15aNNM/w7dWTVohLBYshh6K/k1ZsTU8HamooLH/99ZGFM2PRt0ZiNONAaiolqrjdtLYwXqg5pqqeoOL1emEZcGVFUURojJNtskxVCvQsdPFGmTC22zWogSmKOP3AdrJjwDS68vr097eNygW/fJn20Ro3zvjpzYy+kC0i9i4fub3uXb4t6vV2AxEEas+XLo1iYXYM+pZaiCJgs9ECeqPu9qD6UHnHHXfgpz/9Kfbs2YOLFy9i165dePbZZ3HXXXeN6fMCAQpjchZmmPZ2SgXWqk5kw4JSnHhkJ3w5E/od940rxIlHdo5qLVBnJ3l1DgdfYyY2VE4rxetrdsKT1r+9etIK8fqanVGvsxsOpU1//nk4OnUl1OxbapOeTssqjFoLVPUw5nPPPYctW7bgn/7pn9DY2IiCggJ8//vfx6OPPjqmz/P5aB7HbMWNo6GpidYdauntNiwoRcPXVkVV5UEJX3o8FL5kmFhROa0Up6esUr2CypXIyhp9OFONvhULHA4adxobjVlRRXWxS01NxbZt27Bt2zZVPq+7W/NrrCu6u2lbEV2Ivyii9bolY/5zDl8y8US2iLhYsiSu39k3nDluHFBcHOEfRtm3YkVqKo0/RUXGq1Gs60LQoRBl6fF8XZjWVkpMMXrBZw5fMonCWMKZeiUlhf4HI1ZU0bXYBQL0YLEjJIlCIi6X8T2hCxcofBn34tUMowFZWTTXdeGC1pZEhyDQXqI1NXEt5KIKuhY7v5+8Ow5jEu3t9DD6BqZtbRS+zMw0vmgzTCQIArX3qipq/0YmLS08FhkJXYud16tBar2OaWigZ6uhNmbqjywD589T4hFv1cQkEsnJ1O4vXDBu+j5ASxBkOTweGQXdSkkoRGLHIUyiq4saV5RFBDSnuZlCIBy+ZBKRrCxKzGpu1tqS6EhPp/FoTAWvNUK3Ysfzdf1pb6e7QiOXCw2FyKuTJONlcjGMGrhc1P4vXNBfUfvRkJRE45GRQpm6FbtgkBoFz9cRjY3GF/6GBqrA8GXJU4ZJSJQdwBsbtbYkOux2WvNrFHQrdry+Lkx3N2VyGXmOS5LIqwOML9oMEw12OyWsnDtnvIzGviQnk2fX3a21JZGha7HjQZFwu+l8GDn0V1dHhWTZq2OY8A7gdXVaWzJ2XC4al77cglT36FbsgkEWO4W2NvJyjZqmHwjQXazdTplcDJPo2GzUH86fp/5hRASBxiWjLDDXrdjZj70PETr38SUJtiOHAYCeYxCTCAQoc8vIIcz6eortcwYmY0aEkITiKhoHiqsOQwhFNg5kZdG8XX19LK2LLcnJND4ZQbB1K3bF/3I3cueXwPlmhdamDInzzQrkzi9B9ndos8Xs76yMib0dHbQEw6hZmJJEC2ntdp6DZczHtMoKlG8v6d0g9t4dK1G+vQTTKq88Dogi9YvqauNmZiYl0fjU0aG1JVdGt2IHAJb6GmTeX6Y7wXO+WYHM+8tgqeu/UVUs7G1vpwWcRhWK1la68zNilXSGGYlplRVY83oZ0jz9x4E0Tw3WvF4WkeBlZFD/aGmJkZExRhRpfDLCEgRdi53wZZmBtMfK9ZO2JElIf3QjIA/cXlF9eyWJwhxG9eoASrGWJJ5/ZcyFEJKwfO9GDN5mFb27jC/fW37FkKbdTtvm1NbGxs54kJRE45Rehujh0LXYASQg1tpq2I8c0toUAID9yCGIdZcHNXAFNe3t6KAK40YVu64u6sRGr+XJMAOZWHUI6Z4RxgHISPdUY2LVlceB9HTqJ0aqRtKXpCQap/QeytS92CmIjfrI0Y3UDjXs7ejQfpPWaFDKCRk5uYZhhiKlI7L+Hcn7kpNJLIxWa1JB8U5Z7FRCGp+vtQkAIrcjWntlmUIDRt3rraeHNqw0w3ZEDDOQztTI+nck7xME8o4uXaJ+Y0QcDsq41nOBa92LnSwI6CkoQmD+Iq1NAQAE5i+ClF8IeZgRXC17vV5arGnUTVqbmmh9IIcwGTNyqXgR3GmFQ8zYETIEuNOKcKk4snEgPZ36i5HKb/UlJYWKX3i9WlsyPLoWO0VQPI9v0086oijC/cR2ABgkeGra63ZToVWnM6qP0QRZpsrugmDs7YgYZjhki4i9y78cBwYInvJ67/JtkC2RjQNWK/WXmhp17YwXTme4rKFe0bXYSfmFaHtpJ3y3l2ptSj98t5ei7aWdCOVN6HdcTXtbW40rFG43zT/wcgPGzFROK8Xra3bCk9Z/HPCkFeL1NTtROW1040B6Oi0w17NgjITNpu9qKrodTlt//d/wL12hH49uAL7bS+G7bRUsRw4C8KDllT0IzV+sir2SRA3eqLUwm5roLi8nR2tLGCa2VE4rxekpqzCx9iAy4cHv1+7BpYLFEXt0fUlKovV2jY3GDP+7XDRu6XW3Gt16dsG5N+rzjPVFFBGcvxAA6Fkle7u7jRvCDIXo7tSoyyUYZrTIFhFVxTQOVBUvHJPQKbhc1H+MWFHF6aRxS6+7IOhW7BIZr9e4G9d2dFA1BV5uwDCjJyWF+o/e0/iHwmYD/H79Jqmw2OmQri7jpuu3t1ODN6JXyjBa43RS/zFC+a2BCAJgsbDYMaPA7TamVwdQYgpv48MwY8dmM+4Cc5tNv0LNYqczgkEKYRhxMbnXSxPsRl0byDB6ICWF+pFePaSRcDpp/AoGtbZkMCx2OsPrNW4YsK3N2NsRMYweULbNaWvT2pLRo4Rh9ZikwmKnM7q76a7IiKHA5maK2xt1vpFh9IDSh5qbtbZk9NhsNH7p0StlsdMZXV00yWs0AgGaZ+AsTIaJnuRk6k9G2AF8IBaLPndwMOCwam7a2405X9feTg2c5+sYJnpSUmgnBL0me4yEw6FPu1nsdITfT4JhRLFra9Nv5QSGMRrKDuBGnLdzOGgc8/u1tqQ/LHY6wu837mLylhZjJtUwjF6x26lfGQ27ncaxhBC7mpoafPvb30Z2djZcLhdmzpyJo0ePxuKrTIXfb8zklEDAuMslGEavKGn8Rpu3U5JU9CZ2qheCbmtrw4IFC7B06VK89dZbGDduHM6ePYvMzEy1v8p0GK1RK3R1UU28rCytLWEY8+B00i4CXV3Gi/YIgv7GM9XF7umnn0ZRURF++9vf9h6bNGmS2l9jSrq7jZmJ2dVFOywbzSNlGD2jeEhdXYDRfAVB0N9aO9XF7o033sBtt92Gu+++G++88w4mTJiAf/qnf8I//uM/Dvl+v98Pfx9/1+PxAAAkKQhJ0uEy/AEoNqphq8dDd3Cx2tpeloP9ntWiq4v23lNTqC2WYL9nPcO2xga2lQTP61V3TIjVONAXu51CsJIU3eeoqQGCLKs7tDq/zFLYtGkT7r77bnz00UfYuHEjXnzxRaxbt27Q+7du3YrHH3980PEdO3YgiUtxMAzDJCxerxdr166F2+1GWlpaVJ+lutjZ7XbMnTsX7733Xu+xH/7wh/joo4/w/vvvD3r/UJ5dUVERTp2qQ0ZGtpqmxQRJCuLkyX2YNWsZRHHscbzubuCjj2gxaawSPWQ5iGBwH2y2ZRAEdWKOgQBw6BB5dWqusbNYgpg5cx8+/ngZQiF9x0fZ1tjAttJau1AIWLRIvXm7WIwDA1GWUc2bF90m1O3tLbjuunxVxE71MGZ+fj6mT5/e79i0adPwpz/9acj3OxwOOIYY3UXRFpV4xJto7e3pCc97xbrcliDYVGvkXi816qys2Gw4GQrZdD/QKbCtsSGRbbXZKEnF61X/JljNcWAgNlt4TItm7a2aGqB6OsSCBQvw+eef9zt25swZTJw4Ue2vMhVqNAwt4OQUhokdfZNUjIQohsc0vaC62P3oRz/CBx98gJ/97Gf44osvsGPHDrz00kvYsGGD2l9lKoJBYxZQ7uzU2gKGMTeCYDyxA8huPW31o3oYc968edi1axcefvhhPPHEE5g0aRK2bduGe++9V+2vMhU9PbHLwowlRt5oNhGQJCooXFdHj2AQmDUL+NOfyGvIz6dHbq7xogqJgt1O/cxoyLK+PDvVxQ4AvvGNb+Ab3/hGLD7atAQCxlxj5/XSsgNGX7S3A8eOAUeP0oJ/gNqXMu9z9iwlERw7Rq+dTmDuXGDOHCAjQwuLmeGwWvW5Zc6V0NvCch6mdILPZzzR6OkhT8FodpsZnw/Ytw84fpwGm77RgoEJRH1f+3zAu+8Chw8DX/0qcOutXP5NL1itJBo9Pcbqa1Zr+EZLDxjo1Jmb7m7jhZGUDhhNajGjHufOAbt3h+d3RhsWV97/978DZ84Aq1cDV1+tpoXMWBBFEg2j3VjqTewMGDgzH7JMISUjNWSAOh9v66MPPvwQePVVErpo535lmT7n1VfpcxltsVqpn+kpJBgJViuNa3rJRWCx0wE9PRRSMppoBALGu9s0Ix9+CLz1Fv2s1sCifM5bb7HgaY3VSv3MaGJnsZBI6yVJhcVOB4RC9DDa0oNgkAZFIybWmIVz58JCFyveeou+h9EGi4X6mZ7S+CPBYgmPbXqAhykdoDQIo4mG0Tqf2fD5aI4u9hV36Hv0tj9ZomG0/sZixwzCqGJntLCK2di3T505uiuhzOH97//G9nuY4ZFl4/U3FjtmEJJkzHCgUfffMwPt7bS8IF6T/7JM39feHp/vY/ojisbzrJXwa7Tb/KgFD1U6QJKMOWdnxLWBZuHYsfi3F0EIL0Jn4osRF5YLAnt2zACUux+jZWP6/ezZaYEkUWWUeKd0yzJ9r17u1BMJi8WYnh2LHdMPIw8eRvNGzUBDg3aLdX0+oLFRm+9OZIzYz5QKPix2jOGRJGN2QqNTV6ft99fWavv9iYgSEjQivKic6UWWjSkaRu18RqeuTrvwscWivdgmKtzfooPFjokKI4q00ens1G7gC4V4D0MtGFjU2yjoyW4WO2bM8J2mNmhdfknr709UuL9FB4sdM2Y4E1MbtF7uofX3Jyrc36KDTx8TFXoJUSQSKSnaztmlpGjz3YmMUef19WQ3i50O0FNcezTwnaY25OdrO2eXn6/Ndyc63N+ig08fM2ZE0ZgibXS0FpuCAm2/PxExYjlBBfbsmF6MVjmlLyx28Sc3F3A6tflupxMYP16b705kjNjPlBCmXkRaJ2YkNoJAgme0SioOB2eIaYEoAnPnalMbc+5cY9+cGZVQiPqbkVB2cmGxY3oRxXCFcCPhdHIaulbMmaNNbcw5c+L7nQzR0wMkJWltxehQQq8sdkwvomjMckAul/FsNgsZGcBXvxo/704Q6PsyMuLzfUx/JMmYnp0StdIDLHY6QLn7MZpw2O1aW5DY3HorkJwcn53Kk5Pp+xhtEATj9TcOYzKDMKrY2WxaW5DYOBzA6tXx2al89WrjeRZmw2j9jcWOGYTSIIw2Z2ezGTP8aiauvhpYsSK233H77fQ9jDYo4UAWu+jQiRmJjdVKDcJo2Zh2O3VATlLRlhtuCAueWiFN5XNuvx2YN0+dz2TGRk8P9TMjhjFFUT/l5XRiRmIjCBQiMlo1eZvNmEsmzMgNNwDZ2cDu3UBXV3RRAmWObvVq9uj0QE8P9TOjiV1PD5CayovKmQG4XMYTDbud7trYs9MHV18NbNgAXH89vR7tIKO8//rrgQcfZKHTC5JE/cxoYcyeHu2KHwwFe3Y6wYhr1pQO6PNpbQmj4HQCd9wBLFoEHDsGHD0avj4D5076JkU5nbRgfM4cXl6gN3p66GZYL+HASGGxY4bEbjdmokdSkvHCr4lARgbw9a8DS5YAjY1AbS3tMB4M0u8nT6Yblfx8qnU5frx+1kMx/THignKAQul6Cr3GPIz585//HIIgoLy8PNZfZWisVv3EtkdDejoQCGhtBTMcokiCNmcO8I1vAP/wD3T8H/6BXs+ZQ79nodMvgQD1M6MhCPryRmMqdh999BF+/etf47rrrovl15gCm814Sw8A3tuMYWKNLFPCkNGQZX3NM8ZM7Do7O3Hvvffiv/7rv5CZmRmrrzENVis9jJakkpxMdivhMYZh1CMYJMEwmtgpSTUJ4dlt2LABK1euxC233BKrrzAVDocxRSM5mSahOUmFYdTH56P+ZTSxCwZpPNNT1Z2Y6O4f/vAHHD9+HB999NEV3+v3++H3+3tfezweAIAkBSFJ+h/5FRujtdVqpcncQCB2DUSWg/2e1cBmo/mEtjZ1KyVYLMF+z3qGbY0NbCuJRlaWutMcsRgHBhIIhJcmRROtUlMDBFlWd6aouroac+fOxb59+3rn6pYsWYLZs2dj27Ztg96/detWPP7444OO79ixA0lGTEFiGIZhVMHr9WLt2rVwu91IS0uL6rNUF7vdu3fjrrvugtgnvUuSJAiCAIvFAr/f3+93Q3l2RUVFOHWqDhkZ2WqaFhMkKYiTJ/dh1qxlEMXoZmPPn6dHXp5Kxg1AloMIBvfBZlsGQVBv5ri2ltZz5eer9pGwWIKYOXMfPv54GUIhHc1yDwHbGhvYVupb8+bR8hC1iNU40Jf6euCqq+gRDe3tLbjuunxVxE71MObXv/51fPzxx/2Ofe9738PUqVPx0EMP9RM6AHA4HHAMEbcTRVvU4hFP1LA3KSlc9DWWCIJN1UauOOB+v/rZV6GQTfcDnQLbGhsS1dZgkMaCpKTYjAlqjwN9kWWyO9olLWpqgOpil5qaimuvvbbfseTkZGRnZw86zvRHTwswR0PfJBU9pRozjJExanIKoL8F5QDXxtQVDgeJhdEyMu12KvjaJxrNMEyU+HzUr/QmGldCWS6hp0xMIE7lwg4cOBCPrzE8Dkc4I9NoHlJ2Ns0vMAyjDoEA9SujoWRi6k3s2LPTEQ4HhSyM6CFlZvJ2PwyjFpJE83RGrMfh99M4xmLHjEhGhjHFLiODGjgXhWaY6OnspFJ8RtyBwu/Xp90sdjojOdmYux/Y7UBuLm0cyjBMdHR1UX8y2nwdQOOXHpNqWOx0hstlzCQVAMjJoSwsIxa0Zhi9oPShnBytLRk9SnKKHuuBsNjpjKQkinUbsdZkZibZ7/VqbQnDGBevl/qREefrfD4av1wurS0ZDIudzrDZjJvGn5RE2WM8b8cwY6ezk/qRHr2jK6Esl9BjNjmLnQ4x8oaoubnGDMEyjF4IBqkfGZFgUJ/JKQCLnS5JTjbuvFdGhnHDsAyjNUoYUK+CMRKyTMkpevVIWex0SFJSeHG50UhNpY7KWZkMM3o6O6n/pKZqbcnoCQZJqFnsmIhxuYy7IarFQrs2cJIKw4wen4/6j5p7Q8YLpZanHpNTgDiVC2NGhyjSvF1dHRDlrhaaMG4cNfjubv00fCEkYWLVIaR01KEzNR+XihdBtkRZkp0xFHpvA14vicX48VpbMja6u2mbr2h3OogVLHY6JSsLqK7W2oqxkZ5OE+yXL+tD7KZVVmD53o1I91zuPeZOK8Te5dtROa1UQ8uYeGGENuB2A0VF1H+MiLKrul4xoLOcGKSnGzeUKQhAYSFNWPf0aGvLtMoKrHm9DGl9BjkASPPUYM3rZZhWWaGRZUy8MEIb6Omh/jJhgtaWjA2fj25s9SzULHY6JSmJQphGXbM2bhwtinW7tbNBCElYvncjABkD974UQOmuy/eWQwhx9WqzYpQ24HZTfxk3TlMzxkxnJwmdXpNTAB2LnVFT79VCECh2b8TF5QBgtQITJ1IcX6trObHqENI9lwcNcgoCZKR7qjGx6lBc7WLihxHagCzTfN3EidRvjIjfT0Idix3V1UK3Yqd1+EsPpKZS4zfiEgSA5u2Sk7VbhpDSUafq+xjjYYQ20NVFOxwYdSF5IEDjlN6XS+hW7HhfNGo8KSnGTeNPTgYKCrQLZXam5qv6PsZ4GKENuN3UT/S4U0AkeL00TrHYjREuOUUpvOPHG1fsAOrEoqiNd3qpeBHcaYVDzNYQMgS404pwqXhRnC1j4oXe24DiFRUUaPL1quD10jil1yUHCroVOyPu6RYLMjIoDm5UTzc7m7YqaW+P/3fLFhF7l2+nnwcMdsrrvcu36WqtFaMuem8D7e3UP7KzNfn6qFF2VDdCeTPdih3ASSoAhQaMvG2OxQIUF9MdrBaCXTmtFK+v2QlPWv+cbk9aIV5fs1M3a6yY2KHXNiBJ1C+KioxZMQUIb0ek9xAmoONF5aJISSp63CointjtdOdXXW2MBjUUeXmUqdXaqk1qdeW0UpyeskrX1TOY2KLHNtDaSuG/vDzNTIiari4SayPsqK5bsbPbw7veJjqZmcDFi+Tp6jm1dzjsduDqq4EjR7S7prJFxMWSJfH/YkY36KkNBIPk1V11lTGEYihkmbxTPVdN6YtuneekJOOuMVOb9PRwrUmjkp9Pd7AtLVpbwjDa09pK/SHfwInASu1bo9Tv1a3YORxaW6AflDI8Rt42RxTpLhYw7rpBhlEDv5+8oquv1n8G40h0dVFiih7q30aCbsXObqeUXF6CQIwfb3yRyM2lO1n27phEprWVlhoYdXcDhUDAWOXNdCt2Nht5d0Yf4NUiI4MKQxs1KxOgjLOrrqK7WSOHZBlmrHR3U/ufNMm4GZhAeDsiIyw5UNDt6RYEWpXPnh2RnEyekcejtSXRkZNDld1bW7W2hGHiT2sr7QiSk6O1JdHhdofLARoF3YodQJ4dr7ULo9TOM3LdUEEg787pNPYcJMOMlq4uaveTJhkzq1ohGCT7jVbLU/dix/N2YTIy6KHltjlqkJlJC83b2vhmhkkMZJnae3ExtX8j4/GExyIjoWux43m7/ogihQC13DZHLSZNopRlDmcyiUBrK2VUT5qktSXRIcu0UeuECcbLJNW12PG83WCysqiSilE3dVVISQGmTqU0bF5PyZgZpY1PmULt3sh0dtL/YJSF5H3RtdgBFOOWZeN7MmrhclGsvKNDa0uip7CQNqxsauLry5gTWab2PXEitXej09FB449R1tb1RXWxe+qppzBv3jykpqZi/PjxWL16NT7//PPRf9D77wOSBKeT1txFFcqUJNjfOwDX7tdgf++AcbcQ+JJx44y9qauCxUJ3uwPDmUJIQnHVYQBAcdVhCCFjXy8mcVHCl1OmGHupAUDeqdVq3PWBqp/+d955Bxs2bMAHH3yAffv2IRgM4tZbb0XXaFPv7r4bKCmB9Y0KJCePPdTlfLMCufNLkHP3UmRuWIucu5cid34JnG9WjO0DdUBGBoURjJ6oAgwOZ06rrED59hLcu2MlAODeHStRvr0E0yqNe72YxMRM4UuAxpvsbBJvI6K62O3duxff/e53MWPGDMyaNQsvv/wyqqqqcOzYsdF/WE0NUFaG9LcrxpRu73yzApn3l8FSd7nfcUt9DTLvLzOs4AkCVWAIBMyx758Sziz6qAJrXi9Dmqf/9Urz1GDN62UseIxhMFv4UpIodyI/37jLJmLuWLu/dD+yxjKj+eVETtKPy2GzSKNLVJEkpD+6EZAH71EsfPm5aY+VGzakqSSqGH2ROfBlOPMrEtYe2YjBW2wCAuh6Ld9bziFNxhCYKXwJ0FxdaqoxE1MUYrrFTygUQnl5ORYsWIBrr712yPf4/X74+8QoPV+O3kGXC0HlFqKlGSkfH4TnuoURhwNsRw4j1N6C0EgzqW3NsBw5iOD8hZF96BBIUrDfc7wQRbrLOns28rCCLAf7PeuJwurDSJZa0PPl9QoOeAaApGAzJtYeRFXx2K9XLLBYgv2e9QzbGhv62qpsVDx1KlUY0Vvy1VjGAa8XmDyZxp14+gdqjquCLMfuUvzgBz/AW2+9hcOHD6NwGF9+69atePzxxwcd37FjB5KSkmJlGsMwDKNzvF4v1q5dC7fbjbQo9xKKmdg9+OCD+J//+R8cPHgQk0ZYSTmUZ1dUVIS6zExk+3y9xwMVe3ChkDy7SBYz2o4cRvZ3Vl7xfS2v7Inaszt5ch9mzVoGUYz/rqSffgrU10dWukeWgwgG98FmWwZB0NeuuJmfHsacR8PXK+hyYd9vfoNl69fD1qdq9O/X7tGlZzdz5j58/PEyhEL6Oq8DYVtjg2Lr3/62DMnJNsyZQ3ty6pHRjgMNDRRFmj49DsYNoL29Bdddl6+K2KkexpRlGf/8z/+MXbt24cCBAyMKHQA4HA44hti8ztbdDZvPR7OhhYUQv74YSdUienoi29k3NH8xLBnZsNTX9M7R9bNTECDlFyI0fzFEFUoBiKJNE7GbMIEaYyAQ+R6AgmDTndi1T1uMnuRsOJtreufogC/bQXc3ZAjwpBXiUsFiyCF9lm4IhWy6H5QV2NbYIIo2zJhhM0SB5EjGAb8/nBCnRcUUNcdU1adON2zYgFdffRU7duxAamoq6uvrUV9fj+6x7OmizNlt2waLTURa2iiWIIgi3E9sB0DC1hfltefxbcareTOArCza8djwZbdEEacf+PJ6DUhRUV7vXb4NssXY14sxJ8rWW9OnU3q+WVB2VDdyYoqC6mL3wgsvwO12Y8mSJcjPz+99/PGPfxz9hxUWAjt3AqWlAGjVviBEnm7vu70UbS/tRChvQr/jUn4h2l7aCd/tpaO3SWcIAlBURIs9jb5HXMOCUpx4ZCd8Of2vlzu1EK+v2YnKaca/Xoz5CASA9nb6uaBAU1NUpbubxpWiIuMuN+hLTMKYqvDf/w2sWNHP83K5KFTn80UeD/fdXgrfbatgP3IIYmMdpPH5CMxfZHiPri8ZGdTJLl6ksKaRaVhQioavrUJG5UEAHvzhvj14J7AY+YUizHPFGLMgSUBjI3D11fTaDKKg0NJChauNtrvBcMR06UFU3HjjIEGyWCjNvr5+lJO/oojATUtUNU9vFBbSeVEKtRoaUUTbjIVA4E2krliIvKMi6utJ0M00mDDGRpapz+XlAdOmaW2NunR20hhrhgXxCoZb7picHP+1HkYgNZXCDW63/tb1RIPDAcycSfUzm5q0toZhwjQ10c33zJmRJ4cZAVmmsGxhIY0rZsFwYud0Ujizz6oE5ksKCsirM8OOCH1RBhSLxRz1QBnj43ZTe5w507i1IodDqZZipvlHwIBiJwjUuHgPtMEkJZF35/GYo2ZmX/LygBkzaNLcbGLOGIuODmqHM2ZEtr7VSIRCNH4UFel3neBYMZzYAXQRbDbe1HUo8vLoZsAMNTMHUlJCA0xnp/E3r2WMSWcnid2MGdQezYbbTeNHXp7WlqiPIcXO4SDB41DmYJxOqrTe1WW+eU1BoKy3adNIzEe7axTDRENXF7W7GTOoHZotWUqSaL3gxIk0jpgNQ4odQAkL7NkNTW4uLQJV1v6YCYsFuOYaKrLb3h5ezMswscTrpfY2bRoVRDbDTgYDaW+nccNsoVkFw14yZc0dz90NxmYDiovJ8x3LPoB6x2IhsZs6FWhrYw+PiS1dXdTOpk41z5Y9A+npofGiuJjGDzNi2Mtmt1PmodGrhsSKcePo0damtSWxwWKhu+xp02iegefwmFjQ2Unta/p0amtmFDqAyoKNGweMH6+1JbHD0JdOWQNitsxDNbBaKfYeCpl3blPx8KZPp6QBztJk1KSjIzxHZ1aPDqDxQZZpvDBRYalBGPryJSXRg727ocnJoYWhzc2A3CMh89PDAGg7HbNkryhzeNdeS+EmXofHqIHbTe1p5kxqX2YVOlmm8aGwkMYLM2PoS2ixUN02nrcbGkGg9Ojpn1dg8bqS3v3i5jy6Ejd/twS571Zoa6BKWCzAV74CXHcdJS01NpqrigwTP2SZ2k8wCMyaRe3KbFmXfWlrozG0pMTc/ydgcLEDqHyYUhyaGUzm/grc+O9lSGq93O+4s7kGs58sM43gCQJw1VXAvHmUvFRbaxrnlYkTkkTtJimJ2tGkSeYWgECAHIWrrqI+Y3YML3Y2Gy1D4FDmEEgS0h/diME7xKF3g9Spvy43lSrk5gI33EDPtbXUoRnmSgQCQF0dtZt588ybft8XJXyZCP8rYAKxAyhRxWIxZ5p9NNiPHIJYd3mQ0CkIkOFqqkbWp4fialesSU8H5s6lO9bGRl6awIxMVxe1k0mTqN2YrdblcKSkmN977Yt+t/gZBS4XhTO7u81VpTtaxMa6iN7naI3sfUbC6QRmz6Z2cfo0hWvMsNsyoy6treTVXXstLRY3czaighLtmDTJfPUvR8IUnp0g0CRrMMiJCX2RxudH9D5/VmTvMxqiSCnjc+ZQG6mv5/bBEKEQhS0FgdrHlCmJIXQAbcoKJE74UsEUYgfQHQpv/dOfwPxFkPILIQ8Tp5AhoHtcEVpnLIqzZfFDEKiC+w03UHiqpoazdxMdv5/mczMyqF0UFSVOKK+9naIdgHmXUwyHaf5dq5UGM05U6YMowv3EdgAYJHhKysrp729LiFva7Gwa2EpK6M62pYW9vERDlsPXftIkag/Z2VpbFT8CAZqfNONuDZFgGrEDaL7Obmfvri++20vR9tJOhPIm9D8+rhBv/2An6m8q1ciy+JOcDHz1q5SEYLezl5dI+P10ve12uv7XXx/2cBIBWaad1RMp+3IgpkhQUXA4KDTR2GjOLSrGiu/2UvhuWwXLkYMAPGh5ZQ+6rlsM98civM1UEy9RsFio2G1WFiWuVFVRu8nKSpxQViIhy5SE4veTNzdlCmUhJhrNzTQ2fuUrCRHIGRJTeXYArbmz2Xh91SBEEcH5CwEAwfkL4UoRMXkyDQaJWEQ5JYW9PLMzlDeXiEKn9O9rrkmMxePDYSrPDiCPLj2d7ubsdq2t0Tc5ObQJZWUleTdm3dpjONjLMyfszYUJBqnO59SpiTU/ORSm8+wAEjuLhTd3jYTiYmDChMSuJzmUl8cL0Y1JVxd7cwqyDDQ0AAUF1M8THdN5dgC56mlplGabkaG1NfpGFGkxbVcXZamZvfL5cPT18i5cIC9P2bk5kUM/RqG7m7w5p5M8uUmTElfkFJR5ukRZLH8lTCl2AHl37e1UQsxq2v9SHZKSqEOcOkV7eCVyFZqUFNrWpbAQOH+evIT2dgoBcVhcfwQCdJMmiiRwkyYBmZlaW6U9yt6OkycnVpWUkTCtDCQl0aDd1UVeHjMy48ZRx/jsM5q7S/Rs1sxMCm0WF5Po1X1ZUS07O/HmNvVIMBiuBDJhAolcTg7PtQK09KqjgzY1TqRM6ythWrETBBqwOjqoqD+78VemsJDCQefOAXl57BELAg0W2dk0p3nuHJUcs9spvMltKv5IUrieZV4eJViNH5941UCGo6eHwpdXX039mQlj6uGMvbvRYbHQTgHd3VROacIEvlMG6Lzk5ZHw1dWR6NXVkehlZLAXHA8CAQonBwIkblddBeTn8w1HX0IhuhkrKKDzwzcA/TG12FksYe+O5+4iw2aj9Tjd3eTNJGq1haEQRbpbHj+eBpXqarqLBmhX60TNZo0VyvlUQsg5OVTHMi+P50+HoqmJIg7XXMOh9qEw/fCfnEzJKm43Z2ZGSlISrcs5dYpCRrw1Tn/sdprLKyykeaP6ejpeW0u/S0/nG6to6Omh/hoI0DZNV11FApedzd7KcCjriqdM4YSU4TB90xEEIDNNQsrRA7DtfA329w6YamfuWJGZSR2npwfweLS2Rp9YLBTavPZaej17Ng00jY3kjXBR8tHh9dJ5a2wEkp0SVqQcBgAsEg5jXJbEQjcMHg/10ylTOBN1JGLWfJ5//nmUlJTA6XRi/vz5+PDDD2P1VSNTUYGk6SUoXrcU4zauRc7dS5E7vwTONyu0scdA5OWRh+f18iLrSCgpARYtAm68kcJtXV3hUCcXJx8an4/OT3U1tbOiIuCb9go89tsSrHxhJQBgzqMrcfN3S5D7LvfZgXR10XmbOpX6KzM8MRG7P/7xj9i0aRMee+wxHD9+HLNmzcJtt92GxsbGWHzd8FRUAGVlwOXL/Q5b6muQeX8ZC14ETJhAxWPb23nAjgSrlRIn5s4l4Zs9m8KaHR3UDBsaaIBK1Pk9Wab/v6GBzkdHB52f2bPpfK30V2Dp82VwtvTvs87mGsx+sowFrw8+H/XLyZOpnzIjExOxe/bZZ/GP//iP+N73vofp06fjxRdfRFJSEn7zm9/E4uuGRpKAjRuHHFWEL4+lPVbOIc0rIAjksVx9Nc1PcYHtyElPp4FowQJg8WJg3jwSwkCA5vdqa2luyuxNUJLo/1T+50CAzsO8eXReFiyg85SeImHqixsR3m0xjADqs1N/XW7+ExYBymL6q68GJk7krOlIUH0aPRAI4NixY3j44Yd7j1ksFtxyyy14//33B73f7/fD36fcvNvtBgC0trZGZ8j771N8ZKS88NYmdO1/C8G5N475ayQpCK/Xi/b2FoiivlOgorE1Kwtoa6MyWuPGxT4BQ5aDCAa9CAZbIAj6Pq+R2Gq1UjZhTg7N5bndlFTQ1ETzVLJMGXQOBz1idX4tFmoDgUALQqHYnNeeHirC7PfT4m9BoLnMoiJKMklL61+CTZnbzKh8H97OZni/7LNBpxNerxctTidsyk1rRxNsf38L7dPG3mdjQTzba09PeG+6rKzRz6kbacxyu0kHZDVCIbLK1NTUyADk9957r9/xzZs3yzfccMOg9z/22GMyAH7wgx/84Ac/hnycO3cuam3SPEH64YcfxqZNm3pft7e3Y+LEiaiqqkJ6erqGlkWGx+NBUVERqqurkabzletsa2xgW2MD2xobjGSr2+1GcXExslRY/6S62OXk5EAURTQ0NPQ73tDQgLwh0oUcDgccDseg4+np6bq/EH1JS0szjL1sa2xgW2MD2xobjGSrRYV1J6onqNjtdsyZMwdvv/1277FQKIS3334bN96orzg7wzAMkxjEJIy5adMmrFu3DnPnzsUNN9yAbdu2oaurC9/73vdi8XUMwzAMMyIxEbt77rkHTU1NePTRR1FfX4/Zs2dj7969yI2g0KLD4cBjjz02ZGhTjxjJXrY1NrCtsYFtjQ2Jaqsgy4m6vJVhGIZJFLjaHMMwDGN6WOwYhmEY08NixzAMw5geFjuGYRjG9OhO7HSzNdAIPPXUU5g3bx5SU1Mxfvx4rF69Gp9//rnWZkXEz3/+cwiCgPLycq1NGZKamhp8+9vfRnZ2NlwuF2bOnImjR49qbdYgJEnCli1bMGnSJLhcLlx99dX4P//n/6hTwy9KDh48iDvuuAMFBQUQBAG7d+/u93tZlvHoo48iPz8fLpcLt9xyC86ePauNsRjZ3mAwiIceeggzZ85EcnIyCgoK8J3vfAe1tbW6s3UgDzzwAARBwLZt2+JmX18isbWyshJ33nkn0tPTkZycjHnz5qGqqkp3tnZ2duLBBx9EYWEhXC5X7wYDo0FXYqebrYGuwDvvvIMNGzbggw8+wL59+xAMBnHrrbeiS+ebvn300Uf49a9/jeuuu05rU4akra0NCxYsgM1mw1tvvYXPPvsM//Ef/4FMHe5I+fTTT+OFF17Ar371K1RWVuLpp5/GM888g+eee05r09DV1YVZs2bh+eefH/L3zzzzDH75y1/ixRdfxJEjR5CcnIzbbrsNPo32cBrJXq/Xi+PHj2PLli04fvw4Kioq8Pnnn+POO+/UwNIrn1uFXbt24YMPPkBBQUGcLBvMlWw9d+4cFi5ciKlTp+LAgQM4deoUtmzZAudIxfNjxJVs3bRpE/bu3YtXX30VlZWVKC8vx4MPPog33ngj8i+Jurqmitxwww3yhg0bel9LkiQXFBTITz31lIZWXZnGxkYZgPzOO+9obcqwdHR0yJMnT5b37dsn33zzzfLGjRu1NmkQDz30kLxw4UKtzYiIlStXyuvXr+93rLS0VL733ns1smhoAMi7du3qfR0KheS8vDz5F7/4Re+x9vZ22eFwyK+99poGFvZnoL1D8eGHH8oA5EuXLsXHqGEYztbLly/LEyZMkD/55BN54sSJ8n/+53/G3baBDGXrPffcI3/729/WxqARGMrWGTNmyE888US/Y1/96lfln/zkJxF/rm48O2VroFtuuaX32EhbA+kJZVsiNYqVxooNGzZg5cqV/c6v3njjjTcwd+5c3H333Rg/fjyuv/56/Nd//ZfWZg3JTTfdhLfffhtnzpwBAJw8eRKHDx/GihUrNLZsZC5cuID6+vp+7SA9PR3z58/XfT9TcLvdEAQBGRkZWpsyiFAohPvuuw+bN2/GjBkztDZnWEKhEPbs2YNrrrkGt912G8aPH4/58+ePGJbVkptuuglvvPEGampqIMsy9u/fjzNnzuDWW2+N+DN0I3bNzc2QJGlQlZXc3FzU19drZNWVCYVCKC8vx4IFC3Dttddqbc6Q/OEPf8Dx48fx1FNPaW3KiJw/fx4vvPACJk+ejL/+9a/4wQ9+gB/+8If43e9+p7Vpg/i3f/s3fPOb38TUqVNhs9lw/fXXo7y8HPfee6/Wpo2I0peM1s8UfD4fHnroIXzrW9/SZRHjp59+GlarFT/84Q+1NmVEGhsb0dnZiZ///OdYvnw5/vd//xd33XUXSktL8c4772ht3iCee+45TJ8+HYWFhbDb7Vi+fDmef/55LF68OOLP0HyLH6OzYcMGfPLJJzh8+LDWpgxJdXU1Nm7ciH379mkSix8NoVAIc+fOxc9+9jMAwPXXX49PPvkEL774ItatW6exdf15/fXX8fvf/x47duzAjBkzcOLECZSXl6OgoEB3tpqFYDCINWvWQJZlvPDCC1qbM4hjx45h+/btOH78OASdbx0eCoUAAKtWrcKPfvQjAMDs2bPx3nvv4cUXX8TNN9+spXmDeO655/DBBx/gjTfewMSJE3Hw4EFs2LABBQUFEUerdOPZjXZrID3w4IMP4i9/+Qv279+PwsJCrc0ZkmPHjqGxsRFf/epXYbVaYbVa8c477+CXv/wlrFYrJEnS2sRe8vPzMX369H7Hpk2bpkl22JXYvHlzr3c3c+ZM3HffffjRj36ke+9Z6UtG6mdAWOguXbqEffv26dKrO3ToEBobG1FcXNzb1y5duoR/+Zd/QUlJidbm9SMnJwdWq9UQ/a27uxs//vGP8eyzz+KOO+7AddddhwcffBD33HMP/v3f/z3iz9GN2BlpayBZlvHggw9i165d+Nvf/oZJkyZpbdKwfP3rX8fHH3+MEydO9D7mzp2Le++9FydOnIAoilqb2MuCBQsGLeE4c+YMJk6cqJFFw+P1egftsSWKYu8ds16ZNGkS8vLy+vUzj8eDI0eO6K6fKShCd/bsWfy///f/kJ2drbVJQ3Lffffh1KlT/fpaQUEBNm/ejL/+9a9am9cPu92OefPmGaK/BYNBBIPBqPubrsKYRtkaaMOGDdixYwf+53/+B6mpqb1zHenp6XC5XBpb15/U1NRBc4nJycnIzs7W3Rzjj370I9x000342c9+hjVr1uDDDz/ESy+9hJdeeklr0wZxxx134Kc//SmKi4sxY8YM/P3vf8ezzz6L9evXa20aOjs78cUXX/S+vnDhAk6cOIGsrCwUFxejvLwcTz75JCZPnoxJkyZhy5YtKCgowOrVq3Vnb35+PsrKynD8+HH85S9/gSRJvf0tKysLdrtdN7YWFxcPEmKbzYa8vDxMmTIlrnYCV7Z18+bNuOeee7B48WIsXboUe/fuxZ///GccOHBAd7befPPN2Lx5M1wuFyZOnIh33nkHr7zyCp599tnIvyTqPFGVee655+Ti4mLZbrfLN9xwg/zBBx9obdIgAAz5+O1vf6u1aRGh16UHsizLf/7zn+Vrr71Wdjgc8tSpU+WXXnpJa5OGxOPxyBs3bpSLi4tlp9MpX3XVVfJPfvIT2e/3a22avH///iHb57p162RZpuUHW7ZskXNzc2WHwyF//etflz///HNd2nvhwoVh+9v+/ft1ZetQaLn0IBJb/+///b/yV77yFdnpdMqzZs2Sd+/erUtb6+rq5O9+97tyQUGB7HQ65SlTpsj/8R//IYdCoYi/g7f4YRiGYUyPbubsGIZhGCZWsNgxDMMwpofFjmEYhjE9LHYMwzCM6WGxYxiGYUwPix3DMAxjeljsGIZhGNPDYscwDMOYHhY7hmEYxvSw2DEMwzCmh8WOYRiGMT0sdgzDMIzp+f/9CWFrmHINPAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# this is the main function, run this cell!!!\n",
    "\n",
    "# dataset\n",
    "# logger\n",
    "# trainer\n",
    "# trainer.fit\n",
    "# trainer.test\n",
    "# =============================================================================\n",
    "\n",
    "print(\"train kwargs\", train_kwargs)\n",
    "print(\"model kwargs\", model_kwargs)\n",
    "\n",
    "kwargs = {'train_kwargs':train_kwargs, 'model_kwargs':model_kwargs}\n",
    "\n",
    "# \"examples/example_results/lightning_logs\"\n",
    "logger = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name=train_kwargs[\"exp_name\"])\n",
    "trainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                     accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                     devices=[0],\n",
    "                     # inference_mode=False, # do grad manually\n",
    "                     log_every_n_steps=train_kwargs[\"log_every_n_steps\"],\n",
    "                     logger=logger,\n",
    "                     check_val_every_n_epoch=1,\n",
    "                     max_epochs=train_kwargs[\"epochs\"],\n",
    "                     callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_f1\",\n",
    "                                               filename='{epoch}-{val_f1:.2f}-{unpruned:.0f}'),\n",
    "                                DecentModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"unpruned\", save_top_k=-1, save_on_train_epoch_end=True,\n",
    "                                                filename='{epoch}-{unpruned:.0f}-{val_f1:.2f}'),\n",
    "                                LearningRateMonitor(\"epoch\")])\n",
    "\n",
    "trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "# Check whether pretrained model exists. If yes, load it and skip training\n",
    "pretrained_filename = os.path.join(*[train_kwargs[\"result_path\"], \"lightning_logs\", train_kwargs[\"exp_name\"], train_kwargs[\"load_ckpt_file\"]])\n",
    "if os.path.isfile(pretrained_filename):\n",
    "    print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "    model = DecentLightning.load_from_checkpoint(pretrained_filename, model_kwargs=model_kwargs, log_dir=\"example_results/lightning_logs\") # Automatically loads the model with the saved hyperparameters\n",
    "else:\n",
    "    pl.seed_everything(19) # To be reproducable\n",
    "\n",
    "    # Initialize the LightningModule and LightningDataModule\n",
    "    model = DecentLightning(kwargs=kwargs, log_dir=logger.log_dir)\n",
    "\n",
    "\n",
    "    # Train the model using a Trainer\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "    # we don't save the positions here ...\n",
    "    # model = DecentLightning.load_from_checkpoint(trainer.checkpoint_callback.best_model_path, kwargs=kwargs) # Load best checkpoint after training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66262489-1c4a-439e-9673-32a40c5c52f1",
   "metadata": {},
   "source": [
    "## run test routine ****************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73f9f7ff-a6f1-4947-bcf9-07b9141fd3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "C:\\Users\\Christina\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|                                                                     | 0/24 [00:00<?, ?it/s]DECENT NOTE: test_step 0\n",
      "self.feature_maps torch.Size([1, 8, 26, 26])\n",
      "amount of feature maps: 8\n",
      "random_samples range(0, 8)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      4\u001b[0m logger_x \u001b[38;5;241m=\u001b[39m CSVLogger(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightning_logs\u001b[39m\u001b[38;5;124m'\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdumpster\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m explainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(default_root_dir\u001b[38;5;241m=\u001b[39mtrain_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult_path\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m                      accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(train_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m                      devices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m      8\u001b[0m                      logger\u001b[38;5;241m=\u001b[39mlogger_x,\n\u001b[0;32m      9\u001b[0m                      inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxai_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# test_result = trainer.test(model, test_loader, verbose=False)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:755\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:795\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    794\u001b[0m )\n\u001b[1;32m--> 795\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    797\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 990\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    995\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1029\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mzero_grad(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mzero_grad_kwargs)\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[1;32m-> 1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\chrisy\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:416\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtest_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[17], line 273\u001b[0m, in \u001b[0;36mDecentLightning.test_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# this line seems to be useless, always same output no matter what\u001b[39;00m\n\u001b[0;32m    272\u001b[0m layer_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecent1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# 'decent3'  model.model.decent3' # .filter_list[7]weights\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_xai_feature_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecent2\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# this line seems to be useless, always same output no matter what\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 314\u001b[0m, in \u001b[0;36mDecentLightning.run_xai_feature_map\u001b[1;34m(self, batch, batch_idx, layer, layer_str, device)\u001b[0m\n\u001b[0;32m    312\u001b[0m dd \u001b[38;5;241m=\u001b[39m FeatureMap(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, layer\u001b[38;5;241m=\u001b[39mlayer, layer_str\u001b[38;5;241m=\u001b[39mlayer_str, log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    313\u001b[0m dd\u001b[38;5;241m.\u001b[39mrun(tmp_img, batch_idx)\n\u001b[1;32m--> 314\u001b[0m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\datasceyence\\helper\\visualisation\\feature_map.py:148\u001b[0m, in \u001b[0;36mlog\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot possible to show original image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 148\u001b[0m             \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m    150\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_str\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin( \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplt_id\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test best model on test set\n",
    "\n",
    "# we want the grad to work in test\n",
    "logger_x = CSVLogger(os.path.join(train_kwargs[\"result_path\"], 'lightning_logs'), name='dumpster')\n",
    "explainer = pl.Trainer(default_root_dir=train_kwargs[\"result_path\"],\n",
    "                     accelerator=\"gpu\" if str(train_kwargs[\"device\"]).startswith(\"cuda\") else \"cpu\",\n",
    "                     devices=[0],\n",
    "                     logger=logger_x,\n",
    "                     inference_mode=False)\n",
    "\n",
    "test_result = explainer.test(model, xai_dataloader, verbose=False)\n",
    "# test_result = trainer.test(model, test_loader, verbose=False)\n",
    "try:\n",
    "    result = {\"test accuracy on valset\": test_result[0][\"test_acc\"]}\n",
    "except:\n",
    "    result = 0\n",
    "\n",
    "try:     \n",
    "    layer = model.model.decent2 # .filter_list[7]weights\n",
    "    run_explain(model, layer, device='cuda')\n",
    "except Exception as e:\n",
    "    print(\"DecentError: layer not working, run not defined\" )\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696de4a-76ea-4b95-9d11-e1cf8a733a42",
   "metadata": {},
   "source": [
    "# random nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3d0e6-ce41-4a30-b68f-7c70bbf92113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005a8f1-b68b-42f8-a653-cd905542361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing yet - currently part of the main running dev thingi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5d6dd-be92-4db7-bdd8-40d886f10adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43468c27-f130-4118-8d11-275282921c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d301c-cbad-4154-a053-137a3263c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# useless, always the same filters\n",
    "\n",
    "for i_filter in range(100):\n",
    "    try:\n",
    "        layer = model.model.decent2.filter_list[i_filter] # i_filter] # .filter_list[7]weights\n",
    "        run_explain(model, layer, device='cuda')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2568e15-95ba-4c8c-9a51-b8cb5050ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "layer = model.model.decent2 # .filter_list[7]weights\n",
    "run_explain(model, layer, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f4513-419a-433b-97ff-6f54c5a6d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b09e4-644a-4cca-9da3-a47986de1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca471c2c-819f-44aa-a8f9-e318aacf3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfa186-0125-4a4b-adc6-c748cb2587c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['loops'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e68578-ec3f-417e-8144-1e8ff14b20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"example_results/lightning_logs/tmp/version_22/checkpoints/epoch=4-unpruned=10815-val_f1=0.12.ckpt\")['state_dict'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
